name: Microbenchmarks-Perf-Nightly
# Dashboard: https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fao&benchmarkName=micro-benchmark+api

on:
  push:
    tags:
      - ciflow/benchmark/*
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'  # Run daily at 7 AM UTC

jobs:
  benchmark:
    name: microbenchmarks-${{ matrix.gpu }}
    runs-on: ${{ matrix.runner }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - gpu: h100
            runner: linux.aws.h100
            config: benchmarks/dashboard/microbenchmark_quantization_config_h100.yml
            torch-spec: "--pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126"
          - gpu: a100
            runner: linux.aws.a100
            config: benchmarks/dashboard/microbenchmark_quantization_config_a100.yml
            torch-spec: "--pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126"

    steps:
      - uses: actions/checkout@v4

      - name: Setup miniconda
        uses: pytorch/test-infra/.github/actions/setup-miniconda@main
        with:
          python-version: "3.10"

      - name: Run benchmark
        shell: bash
        run: |
          set -eux

          # Upgrade pip
          ${CONDA_RUN} python -m pip install --upgrade pip

          # Install dependencies
          ${CONDA_RUN} pip install "${{ matrix['torch-spec'] }}"
          ${CONDA_RUN} pip install -r dev-requirements.txt
          ${CONDA_RUN} pip install . --no-build-isolation

          # Create benchmark results directory and ensure PYTHONPATH is set
          mkdir -p "$RUNNER_TEMP/benchmark-results"
          export PYTHONPATH="${PYTHONPATH:-$(pwd)}:$(pwd)/benchmarks"

          # Run microbenchmarks for dashboard
          ${CONDA_RUN} bash -c '
            export PYTHONPATH="${PYTHONPATH:-$(pwd)}:$(pwd)/benchmarks"
            echo "PYTHONPATH is: $PYTHONPATH"
            echo "Current directory is: $(pwd)"
            python benchmarks/dashboard/ci_microbenchmark_runner.py \
              --config "${{ matrix.config }}" \
              --output "$RUNNER_TEMP/benchmark-results/microbenchmark-results-${{ matrix.gpu }}.json"'

      - name: Upload the benchmark results to OSS benchmark database for the dashboard
        uses: pytorch/test-infra/.github/actions/upload-benchmark-results@main
        with:
          benchmark-results-dir: ${{ runner.temp }}/benchmark-results
          dry-run: false
          schema-version: v3
          github-token: ${{ secrets.GITHUB_TOKEN }}
