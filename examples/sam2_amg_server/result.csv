furious,batch_size,p99,first,third,likely_failed_stderr,total_img_s,median,load-exported-model,experiment_name,task,bytes_MiB,torch_version,fast,fifth,torchvision_version,export-model,environ,argmax,gpu-preproc,total_time,p999,fourth,allow-recompiles,bytes,num-images,total_ms_per_img,p95,fail_count,meta-folder,run_script_time,mean,percentage,batch-size,baseline,max,points-per-batch,second,miou
,1.0,2148ms,1681ms,972ms,,1.0810058161304865img/s,862ms,,baseline_amg,amg,4350.0,2.7.0.dev20250128+cu124,,733ms,0.22.0.dev20250128+cu124,,None,782,,925.0644030570984s,2359ms,888ms,,4561654784.0,,925.0644030570984ms,1333ms,,,929.3107132911682,919ms,4.0,,None,2413ms,64,987ms,
,1.0,849ms,1162ms,696ms,,1.377023297843264img/s,716ms,,amg_ao,amg,4033.0,2.7.0.dev20250128+cu124,,682ms,0.22.0.dev20250128+cu124,,None,0,,726.2041256427765s,896ms,675ms,,4229362176.0,,726.2041256427765ms,812ms,22.0,,730.4182567596436,721ms,4.0,,,1162ms,64,696ms,0.9996241017231424
,1.0,771ms,963ms,604ms,,1.5855101246777925img/s,617ms,,amg_ao_ppb_1024_basic,amg,33798.0,2.7.0.dev20250128+cu124,,613ms,0.22.0.dev20250128+cu124,,None,109,,630.7118349075317s,964ms,572ms,,35439909376.0,,630.7118349075317ms,726ms,22.0,,634.2797741889954,626ms,34.0,1,,1962ms,1024,607ms,0.9996240807579095
,1.0,604ms,315900ms,413ms,,1.3152970405167455img/s,425ms,,amg_ao_ppb_1024_fast_cold,amg,29373.0,2.7.0.dev20250128+cu124,None,394ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_inductor_cache_dir'},0,,760.2845358848572s,2510ms,381ms,,30800452096.0,,760.2845358848572ms,538ms,,,769.8586721420288,753ms,30.0,1,,315900ms,1024,2197ms,
,1.0,583ms,8354ms,477ms,,2.272002759313654img/s,412ms,,amg_ao_ppb_1024_fast,amg,29373.0,2.7.0.dev20250128+cu124,None,376ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_inductor_cache_dir'},0,,440.14031052589417s,1465ms,379ms,,30800452096.0,,440.14031052589417ms,527ms,184.0,,445.03349256515503,434ms,30.0,1,,8354ms,1024,956ms,0.9938492919957521
,1.0,,,,,0.0img/s,,,amg_ao_ppb_1024_save_export,amg,1326.0,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_inductor_cache_dir'},,,188.1225230693817s,,,,1390578176.0,0,,,,,198.6976227760315,,1.0,1,,,1024,,
,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 540, in _process_batch_fullgraph
    low_res_masks, iou_preds = self.predictor._predict_masks(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 501, in _predict_masks
    low_res_masks, iou_predictions, _, _ = self.model.sam_mask_decoder(
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/mask_decoder.py"", line 137, in forward
    masks, iou_pred, mask_tokens_out, object_score_logits = self.predict_masks(
                                                            ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/mask_decoder.py"", line 218, in predict_masks
    hs, new_src = self.transformer(src, pos_src, tokens)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/transformer.py"", line 123, in forward
    queries, keys = layer(
                    ^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/transformer.py"", line 213, in forward
    keys = self.norm4(keys)
           ^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/normalization.py"", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 94.99 GiB of which 529.06 MiB is free. Including non-PyTorch memory, this process has 94.47 GiB memory in use. Of the allocated memory 78.57 GiB is allocated by PyTorch, and 14.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,amg_ao_ppb_1024_load_export_cold,amg,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_load_export_inductor_cache_dir'},,,,,,,,,,,,,202.41245126724243,,,1,,,1024,,
,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 540, in _process_batch_fullgraph
    low_res_masks, iou_preds = self.predictor._predict_masks(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 501, in _predict_masks
    low_res_masks, iou_predictions, _, _ = self.model.sam_mask_decoder(
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/mask_decoder.py"", line 137, in forward
    masks, iou_pred, mask_tokens_out, object_score_logits = self.predict_masks(
                                                            ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/mask_decoder.py"", line 218, in predict_masks
    hs, new_src = self.transformer(src, pos_src, tokens)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/transformer.py"", line 123, in forward
    queries, keys = layer(
                    ^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/transformer.py"", line 213, in forward
    keys = self.norm4(keys)
           ^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/normalization.py"", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 94.99 GiB of which 529.06 MiB is free. Including non-PyTorch memory, this process has 94.47 GiB memory in use. Of the allocated memory 78.58 GiB is allocated by PyTorch, and 14.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,amg_ao_ppb_1024_load_export,amg,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_load_export_inductor_cache_dir'},,,,,,,,,,,,,197.0110433101654,,,1,,,1024,,
,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 540, in _process_batch_fullgraph
    low_res_masks, iou_preds = self.predictor._predict_masks(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 501, in _predict_masks
    low_res_masks, iou_predictions, _, _ = self.model.sam_mask_decoder(
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/mask_decoder.py"", line 137, in forward
    masks, iou_pred, mask_tokens_out, object_score_logits = self.predict_masks(
                                                            ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/mask_decoder.py"", line 218, in predict_masks
    hs, new_src = self.transformer(src, pos_src, tokens)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/transformer.py"", line 123, in forward
    queries, keys = layer(
                    ^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam/transformer.py"", line 213, in forward
    keys = self.norm4(keys)
           ^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/normalization.py"", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 94.99 GiB of which 334.44 MiB is free. Including non-PyTorch memory, this process has 94.65 GiB memory in use. Of the allocated memory 78.57 GiB is allocated by PyTorch, and 14.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,amg_ao_ppb_1024_load_export_gpu_preproc,amg,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_load_export_inductor_cache_dir'},,None,,,,,,,,,,,195.50690627098083,,,1,,,1024,,
,,,,,"W0128 19:26:11.180000 4052679 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:198: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
E0128 19:26:24.901000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:24.901000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:24.901000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:25.417000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:25.417000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:25.417000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:26.644000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:26.644000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:26.644000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(4194304x256, 256x128)
  mm 8.7688 ms 100.0% 
  triton_mm_146 15.7969 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_143 17.1093 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_141 17.4291 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_139 19.1339 ms 45.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_145 19.2061 ms 45.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_136 28.5122 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_140 28.8875 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_144 28.9252 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_135 29.4575 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9046 seconds and 0.0006 seconds precompiling for 20 choices
E0128 19:26:28.449000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:28.449000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:28.449000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:28.585000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:28.585000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:28.585000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:28.811000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:28.811000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:28.811000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(8192x256, 256x2048)
  mm 0.2836 ms 100.0% 
  triton_mm_184 0.5068 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_181 0.5648 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_179 0.5652 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_177 0.5806 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_183 0.6038 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_174 0.9240 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_178 0.9386 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_182 0.9391 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_173 0.9564 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7634 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:29.800000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:29.800000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:29.800000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:29.879000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:29.879000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:29.879000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:30.028000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:30.028000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:30.028000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(1024x256, 256x256)
  mm 0.0170 ms 100.0% 
  triton_mm_628 0.0352 ms 48.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_625 0.0352 ms 48.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_626 0.0353 ms 48.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_627 0.0356 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_624 0.0391 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_630 0.0422 ms 40.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_629 0.0507 ms 33.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_637 0.0549 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_633 0.0560 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4002 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:30.948000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:30.948000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:30.948000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:31.026000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:31.026000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:31.026000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:31.171000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:31.171000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:31.171000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(1024x256, 256x256)
  mm 0.0167 ms 100.0% 
  triton_mm_644 0.0349 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_645 0.0351 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_647 0.0351 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_646 0.0354 ms 47.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_643 0.0386 ms 43.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_649 0.0420 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_648 0.0501 ms 33.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_656 0.0543 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_652 0.0558 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3970 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:31.682000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:31.682000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:31.682000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:31.760000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:31.760000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:31.760000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:31.905000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:31.905000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:31.905000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(1024x256, 256x256)
  mm 0.0170 ms 100.0% 
  triton_mm_683 0.0353 ms 48.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_681 0.0355 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_680 0.0356 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_682 0.0358 ms 47.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_679 0.0391 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_685 0.0423 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_684 0.0508 ms 33.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_692 0.0550 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_688 0.0559 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7258 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:32.095000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.095000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.095000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:32.179000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.179000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.179000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:32.326000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.326000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.326000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(1024x256, 256x256)
  mm 0.0169 ms 100.0% 
  triton_mm_735 0.0351 ms 48.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_738 0.0351 ms 48.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_736 0.0354 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_737 0.0356 ms 47.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_734 0.0390 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_740 0.0424 ms 39.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_739 0.0508 ms 33.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_747 0.0549 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_743 0.0558 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4048 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:32.524000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.524000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.524000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:32.606000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.606000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.606000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:32.754000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.754000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.754000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(1024x256, 256x256)
  mm 0.0170 ms 100.0% 
  triton_mm_793 0.0351 ms 48.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_790 0.0352 ms 48.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_791 0.0355 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_792 0.0356 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_789 0.0391 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_795 0.0422 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_794 0.0507 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_802 0.0550 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_798 0.0560 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4125 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:32.943000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:32.943000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:32.943000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:33.020000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:33.020000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:33.020000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:33.168000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:33.168000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:33.168000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(1024x256, 256x256)
  mm 0.0169 ms 100.0% 
  triton_mm_845 0.0352 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_848 0.0352 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_846 0.0354 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_847 0.0355 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_844 0.0391 ms 43.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_850 0.0422 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_849 0.0508 ms 33.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_857 0.0549 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_853 0.0560 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3978 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x4)
  mm 0.0146 ms 100.0% 
  triton_mm_884 0.0147 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2
  triton_mm_883 0.0149 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2
  triton_mm_890 0.0217 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_889 0.0218 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_885 0.0219 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_891 0.0219 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_886 0.0228 ms 64.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_892 0.0229 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_882 0.0252 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.2878 seconds and 0.0002 seconds precompiling for 17 choices
AUTOTUNE mm(2048x2, 2x128)
  triton_mm_0 0.0057 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_1 0.0057 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_3 0.0058 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_2 0.0059 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_4 0.0060 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_6 0.0063 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_5 0.0064 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_7 0.0064 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  mm 0.0068 ms 83.2% 
  triton_mm_10 0.0073 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2482 seconds and 0.0002 seconds precompiling for 17 choices
E0128 19:26:35.627000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:35.627000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:35.627000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:35.711000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:35.711000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:35.711000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:35.874000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:35.874000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:35.874000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE addmm(8192x256, 8192x256, 256x256)
  bias_addmm 0.0475 ms 100.0% 
  addmm 0.0658 ms 72.1% 
  triton_mm_27 0.0770 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_29 0.0811 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_25 0.0834 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_31 0.0839 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_32 0.1034 ms 45.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_22 0.1223 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_26 0.1240 ms 38.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_30 0.1242 ms 38.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5941 seconds and 0.0002 seconds precompiling for 21 choices
E0128 19:26:36.134000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:36.134000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:36.134000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:36.212000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:36.212000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:36.212000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:36.355000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:36.355000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:36.355000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE addmm(8192x128, 8192x256, 256x128)
  bias_addmm 0.0295 ms 100.0% 
  addmm 0.0376 ms 78.4% 
  triton_mm_105 0.0560 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_101 0.0583 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_103 0.0601 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_98 0.0692 ms 42.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_106 0.0699 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_102 0.0708 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_97 0.0772 ms 38.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_107 0.0846 ms 34.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.4787 seconds and 0.0002 seconds precompiling for 21 choices
E0128 19:26:36.820000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:36.820000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:36.820000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(8192x128, 128x256)
  mm 0.0314 ms 100.0% 
  triton_mm_160 0.0436 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_162 0.0452 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_158 0.0463 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_164 0.0466 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_165 0.0558 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_161 0.0564 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_155 0.0645 ms 48.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_159 0.0656 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_163 0.0657 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.4637 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:37.225000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:37.225000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:37.225000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:37.356000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:37.356000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:37.356000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:37.584000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:37.584000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:37.584000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(8192x2048, 2048x256)
  mm 0.2634 ms 100.0% 
  triton_mm_198 0.5430 ms 48.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_200 0.5776 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_196 0.5973 ms 44.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_202 0.6026 ms 43.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_203 0.7502 ms 35.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_193 0.9202 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_197 0.9215 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_201 0.9224 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_192 0.9716 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7627 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:42.676000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:42.676000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:42.676000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(4194304x128, 128x256)
  mm 9.4668 ms 100.0% 
  triton_mm_279 16.1090 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_276 17.6468 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_272 18.2244 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_274 18.3748 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_278 19.6870 ms 48.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_275 23.3224 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_269 29.0988 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_277 29.2890 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_273 29.2912 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0895 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:43.019000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:43.019000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:43.019000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:43.109000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:43.109000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:43.109000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:43.259000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:43.259000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:43.259000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE mm(8192x256, 256x256)
  mm 0.0534 ms 100.0% 
  triton_mm_350 0.0765 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_352 0.0807 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_348 0.0831 ms 64.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_354 0.0837 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_355 0.1024 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_345 0.1217 ms 43.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_349 0.1235 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_353 0.1236 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_344 0.1284 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5804 seconds and 0.0002 seconds precompiling for 20 choices
E0128 19:26:46.393000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:46.393000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:46.393000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:46.905000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:46.905000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 294912, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:46.905000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
E0128 19:26:48.092000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Runtime error during autotuning: 
E0128 19:26:48.092000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 262144, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E0128 19:26:48.092000 4052679 site-packages/torch/_inductor/select_algorithm.py:2010] [0/0] Ignoring this choice.
AUTOTUNE addmm(4194304x128, 4194304x256, 256x128)
  bias_addmm 8.5771 ms 100.0% 
  addmm 11.2080 ms 76.5% 
  triton_mm_393 15.9581 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_390 17.1992 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_388 17.4989 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_386 19.1388 ms 44.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_392 19.2948 ms 44.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_383 28.6175 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_387 29.0270 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_391 29.0468 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8320 seconds and 0.0002 seconds precompiling for 21 choices
AUTOTUNE addmm(1024x32, 1024x256, 256x32)
  bias_addmm 0.0165 ms 100.0% 
  addmm 0.0200 ms 82.6% 
  triton_mm_664 0.0215 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_663 0.0217 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_662 0.0317 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_672 0.0346 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_669 0.0348 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_666 0.0353 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_671 0.0353 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_665 0.0354 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.3450 seconds and 0.0002 seconds precompiling for 19 choices
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.46 GiB. GPU 0 has a total capacity of 94.99 GiB of which 12.46 GiB is free. Including non-PyTorch memory, this process has 82.52 GiB memory in use. Of the allocated memory 55.29 GiB is allocated by PyTorch, with 30.14 GiB allocated in private pools (e.g., CUDA Graphs), and 25.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,amg_ao_ppb_1024_fast_export_cold,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_inductor_cache_dir'},,,,,,,,,,,,,127.05878853797913,,,1,,,1024,,
,,,,,"W0128 19:28:18.237000 4063414 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 601, in _process_batch_fullgraph
    data.filter(keep_index)
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 88, in filter
    self._stats[k] = v[keep]
                     ~^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.40 GiB. GPU 0 has a total capacity of 94.99 GiB of which 7.02 GiB is free. Including non-PyTorch memory, this process has 87.96 GiB memory in use. Of the allocated memory 72.24 GiB is allocated by PyTorch, with 22.10 GiB allocated in private pools (e.g., CUDA Graphs), and 14.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,amg_ao_ppb_1024_fast_export,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_inductor_cache_dir'},,,,,,,,,,,,,108.24332284927368,,,1,,,1024,,
,,,,,"W0128 19:30:07.318000 4072344 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 594, in _process_batch_fullgraph
    data[""stability_score""] = self.calculate_stability_score(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 341, in calculate_stability_score
    .sum(-1, dtype=torch.int16)
     ^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.41 GiB. GPU 0 has a total capacity of 94.99 GiB of which 684.44 MiB is free. Including non-PyTorch memory, this process has 94.31 GiB memory in use. Of the allocated memory 78.79 GiB is allocated by PyTorch, with 22.10 GiB allocated in private pools (e.g., CUDA Graphs), and 13.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast,amg_ao_ppb_1024_fast_export_gpu_preproc,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_inductor_cache_dir'},,None,,,,,,,,,,,120.99174785614014,,,1,,,1024,,
None,1.0,280ms,760397ms,157ms,,1.0896865729183136img/s,133ms,,amg_ao_ppb_1024_fast_furious_cold,amg,28359.0,2.7.0.dev20250128+cu124,None,129ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_furious_inductor_cache_dir'},0,,917.6950738430023s,3820ms,124ms,,29737028096.0,,917.6950738430023ms,228ms,315.0,,926.9298198223114,910ms,29.0,1,,760397ms,1024,3062ms,0.97661403656876
None,1.0,277ms,8473ms,152ms,,6.230738558701548img/s,131ms,,amg_ao_ppb_1024_fast_furious,amg,28359.0,2.7.0.dev20250128+cu124,None,125ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_furious_inductor_cache_dir'},0,,160.49461722373962s,998ms,124ms,,29737028096.0,,160.49461722373962ms,226ms,315.0,,164.90438961982727,154ms,29.0,1,,8473ms,1024,990ms,0.97661403656876
None,1.0,,,,,0.0img/s,,,amg_ao_ppb_1024_save_export_furious,amg,861.0,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_furious_inductor_cache_dir'},,,313.32362604141235s,,,,903450624.0,0,,,,,321.06078267097473,,0.0,1,,,1024,,
None,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.71 GiB. GPU 0 has a total capacity of 94.99 GiB of which 16.75 GiB is free. Including non-PyTorch memory, this process has 78.23 GiB memory in use. Of the allocated memory 58.92 GiB is allocated by PyTorch, and 18.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_load_export_furious_cold,amg,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_load_export_furious_inductor_cache_dir'},,,,,,,,,,,,,200.65129685401917,,,1,,,1024,,
None,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.71 GiB. GPU 0 has a total capacity of 94.99 GiB of which 16.75 GiB is free. Including non-PyTorch memory, this process has 78.23 GiB memory in use. Of the allocated memory 58.92 GiB is allocated by PyTorch, and 18.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_load_export_furious,amg,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_load_export_furious_inductor_cache_dir'},,,,,,,,,,,,,202.24141597747803,,,1,,,1024,,
None,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.78 GiB. GPU 0 has a total capacity of 94.99 GiB of which 16.87 GiB is free. Including non-PyTorch memory, this process has 78.11 GiB memory in use. Of the allocated memory 58.97 GiB is allocated by PyTorch, and 17.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_load_export_furious_gpu_preproc,amg,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_load_export_furious_inductor_cache_dir'},,None,,,,,,,,,,,201.55169415473938,,,1,,,1024,,
None,,,,,"W0128 20:07:17.906000 90226 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
AUTOTUNE mm(4194304x256, 256x128)
  triton_mm_147 1.4458 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_146 1.4558 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  mm 1.4782 ms 97.8% 
  triton_mm_141 1.4801 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_139 1.5122 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_143 1.5546 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_140 1.5597 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_144 1.6106 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_148 1.6548 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_137 1.6915 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.1696 seconds and 0.0004 seconds precompiling for 20 choices
AUTOTUNE mm(8192x256, 256x2048)
  mm 0.0339 ms 100.0% 
  triton_mm_184 0.0352 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_185 0.0368 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_177 0.0411 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_179 0.0412 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_181 0.0427 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_183 0.0447 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_178 0.0459 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_174 0.0467 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_186 0.0470 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.7868 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x256)
  triton_mm_628 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_632 0.0075 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_626 0.0078 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_631 0.0078 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_627 0.0079 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_625 0.0080 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_635 0.0080 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_636 0.0083 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_638 0.0089 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_634 0.0090 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.3058 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x256)
  triton_mm_647 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_651 0.0075 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_650 0.0076 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_645 0.0077 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_646 0.0077 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_654 0.0078 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_644 0.0079 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_655 0.0083 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_653 0.0089 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_657 0.0089 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.3076 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x256)
  triton_mm_683 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_687 0.0075 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_681 0.0078 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_686 0.0078 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_682 0.0079 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_690 0.0079 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_680 0.0081 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_691 0.0083 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_693 0.0087 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_689 0.0088 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.3157 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x256)
  triton_mm_738 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_742 0.0074 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_741 0.0076 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_736 0.0079 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_737 0.0079 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_735 0.0079 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_745 0.0082 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_746 0.0083 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_748 0.0088 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_744 0.0089 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.3046 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x256)
  triton_mm_793 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_797 0.0075 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_796 0.0077 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_791 0.0079 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_792 0.0079 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_790 0.0079 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_800 0.0081 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_801 0.0084 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_803 0.0089 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_799 0.0090 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2998 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x256)
  triton_mm_848 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_852 0.0074 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_851 0.0076 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_847 0.0078 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_846 0.0079 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_855 0.0081 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_845 0.0081 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_856 0.0084 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_858 0.0089 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_854 0.0090 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2981 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(1024x256, 256x4)
  triton_mm_886 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_892 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_884 0.0070 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2
  triton_mm_889 0.0071 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_883 0.0073 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2
  triton_mm_885 0.0074 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_896 0.0074 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_897 0.0075 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_891 0.0078 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_894 0.0082 ms 82.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2481 seconds and 0.0002 seconds precompiling for 17 choices
AUTOTUNE mm(2048x2, 2x128)
  triton_mm_1 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_3 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_0 0.0061 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_2 0.0062 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_6 0.0064 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_5 0.0065 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_7 0.0065 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_4 0.0065 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_11 0.0073 ms 83.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12 0.0074 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2601 seconds and 0.0002 seconds precompiling for 17 choices
AUTOTUNE addmm(8192x256, 8192x256, 256x256)
  triton_mm_27 0.0122 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_26 0.0123 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_30 0.0124 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_29 0.0126 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_25 0.0128 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_34 0.0130 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_23 0.0131 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_33 0.0132 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  bias_addmm 0.0134 ms 90.5% 
  triton_mm_32 0.0141 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3495 seconds and 0.0003 seconds precompiling for 21 choices
AUTOTUNE mm(8192x256, 256x256)
  triton_mm_84 0.0111 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_91 0.0116 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_83 0.0117 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_87 0.0117 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_82 0.0117 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_86 0.0119 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_90 0.0120 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_80 0.0121 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_89 0.0128 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  mm 0.0135 ms 82.5% 
SingleProcess AUTOTUNE benchmarking takes 0.3382 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE addmm(8192x128, 8192x256, 256x128)
  triton_mm_99 0.0108 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_103 0.0112 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_104 0.0113 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_106 0.0113 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_102 0.0113 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_105 0.0117 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_101 0.0121 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  bias_addmm 0.0123 ms 87.8% 
  triton_mm_110 0.0126 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_109 0.0129 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3322 seconds and 0.0003 seconds precompiling for 21 choices
AUTOTUNE mm(8192x128, 128x256)
  triton_mm_158 0.0093 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_160 0.0093 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_162 0.0093 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_159 0.0094 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_163 0.0094 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_165 0.0095 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_161 0.0099 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_167 0.0100 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_166 0.0100 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_157 0.0101 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3346 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(8192x2048, 2048x256)
  triton_mm_205 0.0302 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_198 0.0372 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_204 0.0379 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_199 0.0394 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_194 0.0410 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  mm 0.0451 ms 67.0% 
  triton_mm_195 0.0465 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_197 0.0467 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_201 0.0481 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_196 0.0516 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4253 seconds and 80.4725 seconds precompiling for 20 choices
AUTOTUNE mm(4194304x128, 128x256)
  mm 1.5954 ms 100.0% 
  triton_mm_280 1.6229 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_279 1.6422 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_274 1.7163 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_272 1.7584 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_275 1.7716 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_276 1.7787 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_273 1.8945 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_277 1.9451 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_269 2.0581 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.0614 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE addmm(4194304x128, 4194304x256, 256x128)
  bias_addmm 1.4366 ms 100.0% 
  triton_mm_394 1.4816 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_388 1.5449 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_386 1.5909 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_387 1.5965 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_393 1.6160 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_390 1.6305 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_391 1.6313 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_384 1.6860 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_383 1.7253 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.0830 seconds and 0.0002 seconds precompiling for 21 choices
AUTOTUNE addmm(1024x32, 1024x256, 256x32)
  triton_mm_666 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_673 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_664 0.0073 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_672 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_663 0.0076 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  bias_addmm 0.0078 ms 93.4% 
  triton_mm_669 0.0078 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_665 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_671 0.0082 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_677 0.0082 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2996 seconds and 0.0002 seconds precompiling for 19 choices
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.69 GiB. GPU 0 has a total capacity of 94.99 GiB of which 6.91 GiB is free. Including non-PyTorch memory, this process has 88.06 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, with 11.12 GiB allocated in private pools (e.g., CUDA Graphs), and 16.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_fast_export_furious_cold,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_furious_inductor_cache_dir'},,,,,,,,,,,,,345.87962532043457,,,1,,,1024,,
None,,,,,"W0128 20:13:04.034000 118311 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.69 GiB. GPU 0 has a total capacity of 94.99 GiB of which 6.00 GiB is free. Including non-PyTorch memory, this process has 88.98 GiB memory in use. Of the allocated memory 70.99 GiB is allocated by PyTorch, with 12.04 GiB allocated in private pools (e.g., CUDA Graphs), and 16.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_fast_export_furious,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_furious_inductor_cache_dir'},,,,,,,,,,,,,102.90269041061401,,,1,,,1024,,
None,,,,,"W0128 20:14:46.469000 125868 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
V0128 20:15:42.271000 125868 site-packages/torch/_dynamo/guards.py:2940] [1/1] [__recompiles] Recompiling function calculate_stability_score in /home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py:329
V0128 20:15:42.271000 125868 site-packages/torch/_dynamo/guards.py:2940] [1/1] [__recompiles]     triggered by the following guard failure(s):
V0128 20:15:42.271000 125868 site-packages/torch/_dynamo/guards.py:2940] [1/1] [__recompiles]     - 1/0: L['masks'].size()[0]*L['masks'].size()[1]*L['masks'].size()[2] < 2147483648  # (_inductor/codegen/simd.py:1343 in can_use_32bit_indexing)
V0128 20:15:52.940000 125868 site-packages/torch/_dynamo/guards.py:2940] [2/1] [__recompiles] Recompiling function batched_mask_to_box in /home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py:480
V0128 20:15:52.940000 125868 site-packages/torch/_dynamo/guards.py:2940] [2/1] [__recompiles]     triggered by the following guard failure(s):
V0128 20:15:52.940000 125868 site-packages/torch/_dynamo/guards.py:2940] [2/1] [__recompiles]     - 2/0: L['masks'].size()[0]*L['masks'].size()[1]*L['masks'].size()[2] < 2147483648  # (_inductor/codegen/simd.py:1343 in can_use_32bit_indexing)
V0128 20:16:11.749000 125868 site-packages/torch/_dynamo/guards.py:2940] [3/1] [__recompiles] Recompiling function _mask_to_rle_pytorch_2_0_0 in /home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py:225
V0128 20:16:11.749000 125868 site-packages/torch/_dynamo/guards.py:2940] [3/1] [__recompiles]     triggered by the following guard failure(s):
V0128 20:16:11.749000 125868 site-packages/torch/_dynamo/guards.py:2940] [3/1] [__recompiles]     - 3/0: L['tensor'].size()[0] + L['tensor'].size()[0]*L['tensor'].size()[1]*L['tensor'].size()[2] < 2147483648  # (_inductor/codegen/simd.py:1343 in can_use_32bit_indexing)
V0128 20:16:25.550000 125868 site-packages/torch/_dynamo/guards.py:2940] [4/1] [__recompiles] Recompiling function _mask_to_rle_pytorch_2_0_1 in /home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py:247
V0128 20:16:25.550000 125868 site-packages/torch/_dynamo/guards.py:2940] [4/1] [__recompiles]     triggered by the following guard failure(s):
V0128 20:16:25.550000 125868 site-packages/torch/_dynamo/guards.py:2940] [4/1] [__recompiles]     - 4/0: L['tensor'].size()[0]*L['tensor'].size()[1]*L['tensor'].size()[2] < 2147483648  # (_inductor/codegen/simd.py:1345 in can_use_32bit_indexing)
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 601, in _process_batch_fullgraph
    data.filter(keep_index)
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 88, in filter
    self._stats[k] = v[keep]
                     ~^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.52 GiB. GPU 0 has a total capacity of 94.99 GiB of which 3.10 GiB is free. Including non-PyTorch memory, this process has 91.89 GiB memory in use. Of the allocated memory 85.24 GiB is allocated by PyTorch, with 12.04 GiB allocated in private pools (e.g., CUDA Graphs), and 5.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_fast_export_furious_recompiles,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_furious_inductor_cache_dir'},,,,,,None,,,,,,,209.9063582420349,,,1,,,1024,,
None,,,,,"W0128 20:18:18.193000 149922 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 685, in _process_batch
    data[""rles""] = _mask_to_rle_pytorch_2_0(data[""masks""])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 291, in _mask_to_rle_pytorch_2_0
    alt_lens_nt, counts_init = _mask_to_rle_pytorch_2_0_1(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 252, in _mask_to_rle_pytorch_2_0_1
    alt_lens = diff.sum(dim=1)
               ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.77 GiB. GPU 0 has a total capacity of 94.99 GiB of which 6.21 GiB is free. Including non-PyTorch memory, this process has 88.77 GiB memory in use. Of the allocated memory 71.02 GiB is allocated by PyTorch, with 12.04 GiB allocated in private pools (e.g., CUDA Graphs), and 16.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_fast_export_furious_gpu_preproc,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_furious_inductor_cache_dir'},,None,,,,,,,,,,,101.78867602348328,,,1,,,1024,,
None,,,,,"W0128 20:19:59.316000 159723 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
V0128 20:20:04.733000 159723 site-packages/torch/_dynamo/guards.py:2940] [1/1] [__recompiles] Recompiling function calculate_stability_score in /home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py:329
V0128 20:20:04.733000 159723 site-packages/torch/_dynamo/guards.py:2940] [1/1] [__recompiles]     triggered by the following guard failure(s):
V0128 20:20:04.733000 159723 site-packages/torch/_dynamo/guards.py:2940] [1/1] [__recompiles]     - 1/0: L['masks'].size()[0]*L['masks'].size()[1]*L['masks'].size()[2] < 2147483648  # (_inductor/codecache.py:1082 in _lookup_graph)
V0128 20:20:05.001000 159723 site-packages/torch/_dynamo/guards.py:2940] [2/1] [__recompiles] Recompiling function batched_mask_to_box in /home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py:480
V0128 20:20:05.001000 159723 site-packages/torch/_dynamo/guards.py:2940] [2/1] [__recompiles]     triggered by the following guard failure(s):
V0128 20:20:05.001000 159723 site-packages/torch/_dynamo/guards.py:2940] [2/1] [__recompiles]     - 2/0: L['masks'].size()[0]*L['masks'].size()[1]*L['masks'].size()[2] < 2147483648  # (_inductor/codecache.py:1082 in _lookup_graph)
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 215, in gen_masks_ao
    masks = mask_generator.generate(image_tensor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 208, in generate
    mask_data = self._generate_masks(image)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 254, in _generate_masks
    crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 313, in _process_crop
    return self._process_crop_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 331, in _process_crop_points
    batch_data = self._process_batch(
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 672, in _process_batch
    data = self._process_batch_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/automatic_mask_generator.py"", line 601, in _process_batch_fullgraph
    data.filter(keep_index)
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/amg.py"", line 88, in filter
    self._stats[k] = v[keep]
                     ~^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.50 GiB. GPU 0 has a total capacity of 94.99 GiB of which 824.44 MiB is free. Including non-PyTorch memory, this process has 94.18 GiB memory in use. Of the allocated memory 85.21 GiB is allocated by PyTorch, with 12.04 GiB allocated in private pools (e.g., CUDA Graphs), and 7.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/amg_ao_fast_furious,amg_ao_ppb_1024_fast_export_furious_gpu_preproc_recompiles,amg,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_fast_export_furious_inductor_cache_dir'},,None,,,,None,,,,,,,109.39516997337341,,,1,,,1024,,
,1.0,291ms,469ms,102ms,,7.425957473597097img/s,111ms,,baseline_sps,sps,1337.0,2.7.0.dev20250128+cu124,,129ms,0.22.0.dev20250128+cu124,,None,0,,134.66276955604553s,324ms,132ms,,1402492416.0,,134.66276955604553ms,223ms,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,139.30715155601501,130ms,1.0,,None,469ms,1,113ms,
,1.0,179ms,569ms,107ms,,8.246249693049018img/s,107ms,,sps_ao,sps,1363.0,2.7.0.dev20250128+cu124,,103ms,0.22.0.dev20250128+cu124,,None,0,,121.2672472000122s,189ms,99ms,,1430046208.0,,121.2672472000122ms,169ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,125.39695453643799,116ms,1.0,,,569ms,1,176ms,0.999993383705616
,1.0,180ms,574ms,111ms,,7.675066633560993img/s,111ms,,sps_ao_ppb_1_basic,sps,1363.0,2.7.0.dev20250128+cu124,,105ms,0.22.0.dev20250128+cu124,,None,0,,130.29202842712402s,191ms,101ms,,1430046208.0,,130.29202842712402ms,175ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,134.87022042274475,125ms,1.0,1,,574ms,1,109ms,0.999993383705616
,1.0,174ms,257779ms,162ms,,2.644460109545762img/s,101ms,,sps_ao_ppb_1_fast_cold,sps,1348.0,2.7.0.dev20250128+cu124,None,160ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir'},0,,378.1490204334259s,3233ms,146ms,,1413669376.0,,378.1490204334259ms,165ms,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,388.4281177520752,371ms,1.0,1,,257779ms,1,2979ms,
,1.0,179ms,8861ms,103ms,,7.644001934291677img/s,105ms,,sps_ao_ppb_1_fast,sps,1313.0,2.7.0.dev20250128+cu124,None,98ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir'},0,,130.8215262889862s,989ms,94ms,,1377259520.0,,130.8215262889862ms,167ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,136.9651584625244,124ms,1.0,1,,8861ms,1,981ms,0.9998684964776039
,1.0,,,,,0.0img/s,,,sps_ao_ppb_1_save_export,sps,1326.0,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir'},,,260.2240617275238s,,,,1390578176.0,0,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,272.2452085018158,,1.0,1,,,1,,
,,,,,"[E128 21:01:25.428098036 shim_common.cpp:400] Exception in aoti_torch: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 17.06 MiB is free. Including non-PyTorch memory, this process has 94.97 GiB memory in use. Of the allocated memory 92.35 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1338 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fc7fed78398 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x3c73c (0x7fc7ff68073c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x3ca35 (0x7fc7ff680a35 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x3d0ff (0x7fc7ff6810ff in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1617cba (0x7fc772417cba in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: at::detail::empty_strided_generic(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) + 0xc7 (0x7fc772414b67 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>) + 0x8d (0x7fc73c30af2d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x6f (0x7fc73c30b0bf in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x3a (0x7fc73c52ca6a in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x352ad78 (0x7fc73e92ad78 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x352ae60 (0x7fc73e92ae60 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0xf8 (0x7fc773342c58 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x28eedb8 (0x7fc7736eedb8 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::_ops::empty_strided::call(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x161 (0x7fc7733989b1 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #14: aoti_torch_empty_strided + 0x206 (0x7fc77604ca36 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #15: torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0x256cd (0x7fc30b24440d in /tmp/HUCXEh/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #16: torch::aot_inductor::AOTInductorModelContainer::run(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0xe4 (0x7fc30b28a0f4 in /tmp/HUCXEh/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #17: AOTInductorModelContainerRun + 0x6d (0x7fc30b26499d in /tmp/HUCXEh/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #18: torch::inductor::AOTIModelContainerRunner::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0xf6 (0x7fc77603acb6 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::inductor::AOTIModelContainerRunnerCuda::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0x1e (0x7fc73ece2ffe in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #20: torch::inductor::AOTIModelContainerRunner::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0x78 (0x7fc77603ab28 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::inductor::AOTIModelPackageLoader::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0xe (0x7fc77602683e in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x7e63f2 (0x7fc7859e63f2 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #23: <unknown function> + 0x7e6f6c (0x7fc7859e6f6c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #24: <unknown function> + 0x37b41d (0x7fc78557b41d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #25: python() [0x54c584]
<omitting python frames>
frame #30: python() [0x62fb66]
frame #33: python() [0x578017]
frame #35: python() [0x578017]
frame #39: python() [0x62fb66]
frame #43: python() [0x60aae7]
frame #44: python() [0x605cc7]
frame #45: python() [0x61e022]
frame #50: <unknown function> + 0x295d0 (0x7fc802a295d0 in /lib64/libc.so.6)
frame #51: __libc_start_main + 0x80 (0x7fc802a29680 in /lib64/libc.so.6)
frame #52: python() [0x5cf8b9]

Error: aoti_torch_empty_strided(4, int_array_140, int_array_141, cached_torch_dtype_float32, cached_torch_device_type_cuda, this->device_idx_, &buf932_handle) API call failed at /home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir/caeynnbi6ri2cruayot6mqcloycgl2wxb5cmes7zrqsrksprxqeb/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.cpp, line 12816
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 217, in gen_masks_ao
    mask_generator.predictor.set_image(image_tensor)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 124, in set_image
    backbone_out = self.model.forward_image(input_image)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam2_base.py"", line 472, in forward_image
    backbone_out = self.image_encoder(img_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/compile_export_utils.py"", line 261, in forward
    return self.aoti_compiled_model(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/package/package.py"", line 250, in __call__
    flat_outputs = self.loader.boxed_run(flat_inputs)  # type: ignore[attr-defined]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: run_func_( container_handle_, input_handles.data(), input_handles.size(), output_handles.data(), output_handles.size(), reinterpret_cast<AOTInductorStreamHandle>(stream_handle), proxy_executor_handle_) API call failed at /pytorch/torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 104
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,sps_ao_ppb_1_load_export_cold,sps,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_load_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,78.05514073371887,,,1,,,1,,
,,,,,"[E128 21:02:47.135509911 shim_common.cpp:400] Exception in aoti_torch: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 17.06 MiB is free. Including non-PyTorch memory, this process has 94.97 GiB memory in use. Of the allocated memory 92.35 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1338 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f65d094b398 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x3c73c (0x7f664158c73c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x3ca35 (0x7f664158ca35 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x3d0ff (0x7f664158d0ff in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1617cba (0x7f65b4c17cba in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: at::detail::empty_strided_generic(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) + 0xc7 (0x7f65b4c14b67 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>) + 0x8d (0x7f657eb0af2d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x6f (0x7f657eb0b0bf in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x3a (0x7f657ed2ca6a in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x352ad78 (0x7f658112ad78 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x352ae60 (0x7f658112ae60 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0xf8 (0x7f65b5b42c58 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x28eedb8 (0x7f65b5eeedb8 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::_ops::empty_strided::call(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x161 (0x7f65b5b989b1 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #14: aoti_torch_empty_strided + 0x206 (0x7f65b884ca36 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #15: torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0x256cd (0x7f614b24440d in /tmp/oyqjWa/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #16: torch::aot_inductor::AOTInductorModelContainer::run(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0xe4 (0x7f614b28a0f4 in /tmp/oyqjWa/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #17: AOTInductorModelContainerRun + 0x6d (0x7f614b26499d in /tmp/oyqjWa/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #18: torch::inductor::AOTIModelContainerRunner::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0xf6 (0x7f65b883acb6 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::inductor::AOTIModelContainerRunnerCuda::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0x1e (0x7f65814e2ffe in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #20: torch::inductor::AOTIModelContainerRunner::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0x78 (0x7f65b883ab28 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::inductor::AOTIModelPackageLoader::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0xe (0x7f65b882683e in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x7e63f2 (0x7f65c81e63f2 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #23: <unknown function> + 0x7e6f6c (0x7f65c81e6f6c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #24: <unknown function> + 0x37b41d (0x7f65c7d7b41d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #25: python() [0x54c584]
<omitting python frames>
frame #30: python() [0x62fb66]
frame #33: python() [0x578017]
frame #35: python() [0x578017]
frame #39: python() [0x62fb66]
frame #43: python() [0x60aae7]
frame #44: python() [0x605cc7]
frame #45: python() [0x61e022]
frame #50: <unknown function> + 0x295d0 (0x7f66450295d0 in /lib64/libc.so.6)
frame #51: __libc_start_main + 0x80 (0x7f6645029680 in /lib64/libc.so.6)
frame #52: python() [0x5cf8b9]

Error: aoti_torch_empty_strided(4, int_array_140, int_array_141, cached_torch_dtype_float32, cached_torch_device_type_cuda, this->device_idx_, &buf932_handle) API call failed at /home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir/caeynnbi6ri2cruayot6mqcloycgl2wxb5cmes7zrqsrksprxqeb/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.cpp, line 12816
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 217, in gen_masks_ao
    mask_generator.predictor.set_image(image_tensor)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 124, in set_image
    backbone_out = self.model.forward_image(input_image)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam2_base.py"", line 472, in forward_image
    backbone_out = self.image_encoder(img_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/compile_export_utils.py"", line 261, in forward
    return self.aoti_compiled_model(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/package/package.py"", line 250, in __call__
    flat_outputs = self.loader.boxed_run(flat_inputs)  # type: ignore[attr-defined]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: run_func_( container_handle_, input_handles.data(), input_handles.size(), output_handles.data(), output_handles.size(), reinterpret_cast<AOTInductorStreamHandle>(stream_handle), proxy_executor_handle_) API call failed at /pytorch/torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 104
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,sps_ao_ppb_1_load_export,sps,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_load_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,82.01684617996216,,,1,,,1,,
,,,,,"[E128 21:04:09.125905770 shim_common.cpp:400] Exception in aoti_torch: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 16.44 MiB is free. Including non-PyTorch memory, this process has 94.96 GiB memory in use. Of the allocated memory 92.26 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1338 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7ff0aa14b398 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x3c73c (0x7ff11b68c73c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x3ca35 (0x7ff11b68ca35 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x3d0ff (0x7ff11b68d0ff in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1617cba (0x7ff08e417cba in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: at::detail::empty_strided_generic(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) + 0xc7 (0x7ff08e414b67 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>) + 0x8d (0x7ff05830af2d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x6f (0x7ff05830b0bf in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x3a (0x7ff05852ca6a in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x352ad78 (0x7ff05a92ad78 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x352ae60 (0x7ff05a92ae60 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0xf8 (0x7ff08f342c58 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x28eedb8 (0x7ff08f6eedb8 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::_ops::empty_strided::call(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x161 (0x7ff08f3989b1 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #14: aoti_torch_empty_strided + 0x206 (0x7ff09204ca36 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #15: torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0x259c0 (0x7fec33244700 in /tmp/WYKBkq/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #16: torch::aot_inductor::AOTInductorModelContainer::run(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0xe4 (0x7fec3328a0f4 in /tmp/WYKBkq/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #17: AOTInductorModelContainerRun + 0x6d (0x7fec3326499d in /tmp/WYKBkq/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #18: torch::inductor::AOTIModelContainerRunner::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0xf6 (0x7ff09203acb6 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::inductor::AOTIModelContainerRunnerCuda::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0x1e (0x7ff05ace2ffe in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #20: torch::inductor::AOTIModelContainerRunner::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0x78 (0x7ff09203ab28 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::inductor::AOTIModelPackageLoader::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0xe (0x7ff09202683e in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x7e63f2 (0x7ff0a19e63f2 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #23: <unknown function> + 0x7e6f6c (0x7ff0a19e6f6c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #24: <unknown function> + 0x37b41d (0x7ff0a157b41d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #25: python() [0x54c584]
<omitting python frames>
frame #30: python() [0x62fb66]
frame #33: python() [0x578017]
frame #35: python() [0x578017]
frame #39: python() [0x62fb66]
frame #43: python() [0x60aae7]
frame #44: python() [0x605cc7]
frame #45: python() [0x61e022]
frame #50: <unknown function> + 0x295d0 (0x7ff11e8295d0 in /lib64/libc.so.6)
frame #51: __libc_start_main + 0x80 (0x7ff11e829680 in /lib64/libc.so.6)
frame #52: python() [0x5cf8b9]

Error: aoti_torch_empty_strided(4, int_array_75, int_array_76, cached_torch_dtype_float32, cached_torch_device_type_cuda, this->device_idx_, &buf935_handle) API call failed at /home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir/caeynnbi6ri2cruayot6mqcloycgl2wxb5cmes7zrqsrksprxqeb/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.cpp, line 12862
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 217, in gen_masks_ao
    mask_generator.predictor.set_image(image_tensor)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 124, in set_image
    backbone_out = self.model.forward_image(input_image)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam2_base.py"", line 472, in forward_image
    backbone_out = self.image_encoder(img_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/compile_export_utils.py"", line 261, in forward
    return self.aoti_compiled_model(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/package/package.py"", line 250, in __call__
    flat_outputs = self.loader.boxed_run(flat_inputs)  # type: ignore[attr-defined]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: run_func_( container_handle_, input_handles.data(), input_handles.size(), output_handles.data(), output_handles.size(), reinterpret_cast<AOTInductorStreamHandle>(stream_handle), proxy_executor_handle_) API call failed at /pytorch/torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 104
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,sps_ao_ppb_1_load_export_gpu_preproc,sps,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_load_export_inductor_cache_dir'},,None,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,82.43726706504822,,,1,,,1,,
,,,,,"[E128 21:05:20.671229329 shim_common.cpp:400] Exception in aoti_torch: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 17.06 MiB is free. Including non-PyTorch memory, this process has 94.97 GiB memory in use. Of the allocated memory 92.35 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1338 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7ff339b78398 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x3c73c (0x7ff33a48c73c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x3ca35 (0x7ff33a48ca35 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x3d0ff (0x7ff33a48d0ff in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1617cba (0x7ff2ad217cba in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: at::detail::empty_strided_generic(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) + 0xc7 (0x7ff2ad214b67 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>) + 0x8d (0x7ff27710af2d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x6f (0x7ff27710b0bf in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x3a (0x7ff27732ca6a in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x352ad78 (0x7ff27972ad78 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x352ae60 (0x7ff27972ae60 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0xf8 (0x7ff2ae142c58 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x28eedb8 (0x7ff2ae4eedb8 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::_ops::empty_strided::call(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x161 (0x7ff2ae1989b1 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #14: aoti_torch_empty_strided + 0x206 (0x7ff2b0e4ca36 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #15: torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0x256cd (0x7fee5324440d in /tmp/UjpxA8/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #16: torch::aot_inductor::AOTInductorModelContainer::run(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0xe4 (0x7fee5328a0f4 in /tmp/UjpxA8/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #17: AOTInductorModelContainerRun + 0x6d (0x7fee5326499d in /tmp/UjpxA8/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #18: torch::inductor::AOTIModelContainerRunner::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0xf6 (0x7ff2b0e3acb6 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::inductor::AOTIModelContainerRunnerCuda::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0x1e (0x7ff279ae2ffe in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #20: torch::inductor::AOTIModelContainerRunner::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0x78 (0x7ff2b0e3ab28 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::inductor::AOTIModelPackageLoader::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0xe (0x7ff2b0e2683e in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x7e63f2 (0x7ff2c07e63f2 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #23: <unknown function> + 0x7e6f6c (0x7ff2c07e6f6c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #24: <unknown function> + 0x37b41d (0x7ff2c037b41d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #25: python() [0x54c584]
<omitting python frames>
frame #30: python() [0x62fb66]
frame #33: python() [0x578017]
frame #35: python() [0x578017]
frame #39: python() [0x62fb66]
frame #43: python() [0x60aae7]
frame #44: python() [0x605cc7]
frame #45: python() [0x61e022]
frame #50: <unknown function> + 0x295d0 (0x7ff33d8295d0 in /lib64/libc.so.6)
frame #51: __libc_start_main + 0x80 (0x7ff33d829680 in /lib64/libc.so.6)
frame #52: python() [0x5cf8b9]

Error: aoti_torch_empty_strided(4, int_array_140, int_array_141, cached_torch_dtype_float32, cached_torch_device_type_cuda, this->device_idx_, &buf932_handle) API call failed at /home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir/caeynnbi6ri2cruayot6mqcloycgl2wxb5cmes7zrqsrksprxqeb/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.cpp, line 12816
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 217, in gen_masks_ao
    mask_generator.predictor.set_image(image_tensor)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 124, in set_image
    backbone_out = self.model.forward_image(input_image)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam2_base.py"", line 472, in forward_image
    backbone_out = self.image_encoder(img_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/compile_export_utils.py"", line 261, in forward
    return self.aoti_compiled_model(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/package/package.py"", line 250, in __call__
    flat_outputs = self.loader.boxed_run(flat_inputs)  # type: ignore[attr-defined]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: run_func_( container_handle_, input_handles.data(), input_handles.size(), output_handles.data(), output_handles.size(), reinterpret_cast<AOTInductorStreamHandle>(stream_handle), proxy_executor_handle_) API call failed at /pytorch/torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 104
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,sps_ao_ppb_1_fast_export_cold,sps,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,70.69939756393433,,,1,,,1,,
,,,,,"[E128 21:06:28.961537328 shim_common.cpp:400] Exception in aoti_torch: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 17.06 MiB is free. Including non-PyTorch memory, this process has 94.97 GiB memory in use. Of the allocated memory 92.35 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1338 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f300014b398 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x3c73c (0x7f307165873c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x3ca35 (0x7f3071658a35 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x3d0ff (0x7f30716590ff in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1617cba (0x7f2fe4417cba in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: at::detail::empty_strided_generic(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) + 0xc7 (0x7f2fe4414b67 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>) + 0x8d (0x7f2fae30af2d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x6f (0x7f2fae30b0bf in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x3a (0x7f2fae52ca6a in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x352ad78 (0x7f2fb092ad78 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x352ae60 (0x7f2fb092ae60 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0xf8 (0x7f2fe5342c58 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x28eedb8 (0x7f2fe56eedb8 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::_ops::empty_strided::call(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x161 (0x7f2fe53989b1 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #14: aoti_torch_empty_strided + 0x206 (0x7f2fe804ca36 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #15: torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0x256cd (0x7f2b7724440d in /tmp/VeqogA/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #16: torch::aot_inductor::AOTInductorModelContainer::run(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0xe4 (0x7f2b7728a0f4 in /tmp/VeqogA/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #17: AOTInductorModelContainerRun + 0x6d (0x7f2b7726499d in /tmp/VeqogA/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #18: torch::inductor::AOTIModelContainerRunner::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0xf6 (0x7f2fe803acb6 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::inductor::AOTIModelContainerRunnerCuda::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0x1e (0x7f2fb0ce2ffe in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #20: torch::inductor::AOTIModelContainerRunner::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0x78 (0x7f2fe803ab28 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::inductor::AOTIModelPackageLoader::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0xe (0x7f2fe802683e in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x7e63f2 (0x7f2ff79e63f2 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #23: <unknown function> + 0x7e6f6c (0x7f2ff79e6f6c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #24: <unknown function> + 0x37b41d (0x7f2ff757b41d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #25: python() [0x54c584]
<omitting python frames>
frame #30: python() [0x62fb66]
frame #33: python() [0x578017]
frame #35: python() [0x578017]
frame #39: python() [0x62fb66]
frame #43: python() [0x60aae7]
frame #44: python() [0x605cc7]
frame #45: python() [0x61e022]
frame #50: <unknown function> + 0x295d0 (0x7f30748295d0 in /lib64/libc.so.6)
frame #51: __libc_start_main + 0x80 (0x7f3074829680 in /lib64/libc.so.6)
frame #52: python() [0x5cf8b9]

Error: aoti_torch_empty_strided(4, int_array_140, int_array_141, cached_torch_dtype_float32, cached_torch_device_type_cuda, this->device_idx_, &buf932_handle) API call failed at /home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir/caeynnbi6ri2cruayot6mqcloycgl2wxb5cmes7zrqsrksprxqeb/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.cpp, line 12816
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 217, in gen_masks_ao
    mask_generator.predictor.set_image(image_tensor)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 124, in set_image
    backbone_out = self.model.forward_image(input_image)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam2_base.py"", line 472, in forward_image
    backbone_out = self.image_encoder(img_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/compile_export_utils.py"", line 261, in forward
    return self.aoti_compiled_model(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/package/package.py"", line 250, in __call__
    flat_outputs = self.loader.boxed_run(flat_inputs)  # type: ignore[attr-defined]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: run_func_( container_handle_, input_handles.data(), input_handles.size(), output_handles.data(), output_handles.size(), reinterpret_cast<AOTInductorStreamHandle>(stream_handle), proxy_executor_handle_) API call failed at /pytorch/torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 104
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,sps_ao_ppb_1_fast_export,sps,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,68.33181285858154,,,1,,,1,,
,,,,,"[E128 21:07:36.787099806 shim_common.cpp:400] Exception in aoti_torch: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 8.44 MiB is free. Including non-PyTorch memory, this process has 94.97 GiB memory in use. Of the allocated memory 92.27 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1338 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fea9f34b398 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x3c73c (0x7feb1088073c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x3ca35 (0x7feb10880a35 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x3d0ff (0x7feb108810ff in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1617cba (0x7fea83617cba in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: at::detail::empty_strided_generic(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) + 0xc7 (0x7fea83614b67 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ScalarType, std::optional<c10::Device>) + 0x8d (0x7fea4d50af2d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::detail::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x6f (0x7fea4d50b0bf in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::empty_strided_cuda(c10::ArrayRef<long>, c10::ArrayRef<long>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x3a (0x7fea4d72ca6a in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x352ad78 (0x7fea4fb2ad78 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x352ae60 (0x7fea4fb2ae60 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0xf8 (0x7fea84542c58 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x28eedb8 (0x7fea848eedb8 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::_ops::empty_strided::call(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>) + 0x161 (0x7fea845989b1 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #14: aoti_torch_empty_strided + 0x206 (0x7fea8724ca36 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #15: torch::aot_inductor::AOTInductorModel::run_impl(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0x259c0 (0x7fe615244700 in /tmp/RWe4EB/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #16: torch::aot_inductor::AOTInductorModelContainer::run(AtenTensorOpaque**, AtenTensorOpaque**, CUstream_st*, AOTIProxyExecutorOpaque*) + 0xe4 (0x7fe61528a0f4 in /tmp/RWe4EB/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #17: AOTInductorModelContainerRun + 0x6d (0x7fe61526499d in /tmp/RWe4EB/data/aotinductor/model/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.so)
frame #18: torch::inductor::AOTIModelContainerRunner::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0xf6 (0x7fea8723acb6 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::inductor::AOTIModelContainerRunnerCuda::run_impl(std::vector<AtenTensorOpaque*, std::allocator<AtenTensorOpaque*> >&, void*) + 0x1e (0x7fea4fee2ffe in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #20: torch::inductor::AOTIModelContainerRunner::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0x78 (0x7fea8723ab28 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::inductor::AOTIModelPackageLoader::boxed_run(std::vector<at::Tensor, std::allocator<at::Tensor> >&&, void*) + 0xe (0x7fea8722683e in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x7e63f2 (0x7fea96be63f2 in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #23: <unknown function> + 0x7e6f6c (0x7fea96be6f6c in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #24: <unknown function> + 0x37b41d (0x7fea9677b41d in /home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #25: python() [0x54c584]
<omitting python frames>
frame #30: python() [0x62fb66]
frame #33: python() [0x578017]
frame #35: python() [0x578017]
frame #39: python() [0x62fb66]
frame #43: python() [0x60aae7]
frame #44: python() [0x605cc7]
frame #45: python() [0x61e022]
frame #50: <unknown function> + 0x295d0 (0x7feb13a295d0 in /lib64/libc.so.6)
frame #51: __libc_start_main + 0x80 (0x7feb13a29680 in /lib64/libc.so.6)
frame #52: python() [0x5cf8b9]

Error: aoti_torch_empty_strided(4, int_array_75, int_array_76, cached_torch_dtype_float32, cached_torch_device_type_cuda, this->device_idx_, &buf935_handle) API call failed at /home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_inductor_cache_dir/caeynnbi6ri2cruayot6mqcloycgl2wxb5cmes7zrqsrksprxqeb/ceq2sd6l322nzur4wd7wxhajclcdi2jd4dybmmspc7jdmurg2fbz.cpp, line 12862
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 217, in gen_masks_ao
    mask_generator.predictor.set_image(image_tensor)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 124, in set_image
    backbone_out = self.model.forward_image(input_image)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/modeling/sam2_base.py"", line 472, in forward_image
    backbone_out = self.image_encoder(img_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1749, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/modules/module.py"", line 1760, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/compile_export_utils.py"", line 261, in forward
    return self.aoti_compiled_model(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/package/package.py"", line 250, in __call__
    flat_outputs = self.loader.boxed_run(flat_inputs)  # type: ignore[attr-defined]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: run_func_( container_handle_, input_handles.data(), input_handles.size(), output_handles.data(), output_handles.size(), reinterpret_cast<AOTInductorStreamHandle>(stream_handle), proxy_executor_handle_) API call failed at /pytorch/torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 104
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast,sps_ao_ppb_1_fast_export_gpu_preproc,sps,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_inductor_cache_dir'},,None,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,68.0843300819397,,,1,,,1,,
None,1.0,28ms,532112ms,65ms,,1.78662257106496img/s,19ms,,sps_ao_ppb_1_fast_furious_cold,sps,861.0,2.7.0.dev20250128+cu124,None,21ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_furious_inductor_cache_dir'},0,,559.715306520462s,2431ms,18ms,,903450624.0,,559.715306520462ms,27ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,570.9369330406189,554ms,0.0,1,,532112ms,1,1901ms,0.9996669061779976
None,1.0,30ms,8582ms,24ms,,27.469295447070298img/s,20ms,,sps_ao_ppb_1_fast_furious,sps,861.0,2.7.0.dev20250128+cu124,None,22ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_furious_inductor_cache_dir'},0,,36.40428280830383s,887ms,18ms,,903450624.0,,36.40428280830383ms,29ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,42.018455505371094,31ms,0.0,1,,8582ms,1,879ms,0.9996669061779976
None,1.0,,,,,0.0img/s,,,sps_ao_ppb_1_save_export_furious,sps,861.0,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_furious_inductor_cache_dir'},,,390.7763693332672s,,,,903450624.0,0,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,403.25808334350586,,0.0,1,,,1,,
None,1.0,29ms,735ms,21ms,,36.26170633717303img/s,20ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_load_export_furious_cold,sps,91384.0,2.7.0.dev20250128+cu124,,22ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_load_export_furious_inductor_cache_dir'},0,,27.577301263809204s,32ms,17ms,,95823785472.0,,27.577301263809204ms,27ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,32.450793504714966,22ms,93.0,1,,735ms,1,22ms,0.011158332573977532
None,1.0,28ms,728ms,21ms,,36.3957295579206img/s,19ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_load_export_furious,sps,91384.0,2.7.0.dev20250128+cu124,,22ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_load_export_furious_inductor_cache_dir'},0,,27.47575092315674s,31ms,17ms,,95823785472.0,,27.47575092315674ms,27ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,31.926239728927612,21ms,93.0,1,,728ms,1,22ms,0.011158332573977532
None,1.0,18ms,1347ms,16ms,,43.87198271623557img/s,16ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_load_export_furious_gpu_preproc,sps,91384.0,2.7.0.dev20250128+cu124,,16ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_load_export_furious_inductor_cache_dir'},0,None,22.793590307235718s,60ms,15ms,,95823785472.0,,22.793590307235718ms,17ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,27.03327250480652,17ms,93.0,1,,1347ms,1,17ms,0.011125353369905496
None,1.0,29ms,758ms,21ms,,36.79988575841129img/s,19ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_fast_export_furious_cold,sps,91384.0,2.7.0.dev20250128+cu124,None,22ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_furious_inductor_cache_dir'},0,,27.173997402191162s,31ms,17ms,,95823785472.0,,27.173997402191162ms,27ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,31.737152099609375,21ms,93.0,1,,758ms,1,22ms,0.011158332573977532
None,1.0,30ms,720ms,21ms,,36.12095408970227img/s,20ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_fast_export_furious,sps,91384.0,2.7.0.dev20250128+cu124,None,22ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_furious_inductor_cache_dir'},0,,27.68476152420044s,34ms,17ms,,95823785472.0,,27.68476152420044ms,28ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,32.54826760292053,22ms,93.0,1,,720ms,1,21ms,0.011158332573977532
None,1.0,28ms,7528ms,20ms,,30.83781717546627img/s,19ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_fast_export_furious_recompiles,sps,91384.0,2.7.0.dev20250128+cu124,None,21ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_furious_inductor_cache_dir'},0,,32.42771673202515s,98ms,17ms,None,95823785472.0,,32.42771673202515ms,26ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,38.46320629119873,28ms,93.0,1,,7528ms,1,20ms,0.23777991676407556
None,1.0,18ms,1469ms,17ms,,42.37203310686851img/s,16ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_fast_export_furious_gpu_preproc,sps,91384.0,2.7.0.dev20250128+cu124,None,16ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_furious_inductor_cache_dir'},0,None,23.600472450256348s,20ms,16ms,,95823785472.0,,23.600472450256348ms,17ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,28.026813745498657,18ms,93.0,1,,1469ms,1,17ms,0.011125353369905496
None,1.0,17ms,2758ms,17ms,,43.1101653874493img/s,16ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/sps_ao_fast_furious,sps_ao_ppb_1_fast_export_furious_gpu_preproc_recompiles,sps,91384.0,2.7.0.dev20250128+cu124,None,17ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/sps_fast_export_furious_inductor_cache_dir'},0,None,23.196385145187378s,75ms,16ms,None,95823785472.0,,23.196385145187378ms,17ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,28.922001123428345,19ms,93.0,1,,2758ms,1,17ms,0.24209651788129144
,1.0,1534ms,565ms,311ms,,2.6248870147522076img/s,290ms,,baseline_mps,mps,1337.0,2.7.0.dev20250128+cu124,,472ms,0.22.0.dev20250128+cu124,,None,678,,380.9687786102295s,1982ms,152ms,,1402492416.0,,380.9687786102295ms,901ms,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,384.86253023147583,374ms,1.0,,None,2544ms,,288ms,
,1.0,220ms,521ms,118ms,,6.722275268003411img/s,131ms,,mps_ao,mps,8045.0,2.7.0.dev20250128+cu124,,128ms,0.22.0.dev20250128+cu124,,None,0,,148.75915670394897s,325ms,126ms,,8436582912.0,,148.75915670394897ms,192ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,153.27010130882263,142ms,8.0,,,521ms,,192ms,0.9998915305137634
,1.0,195ms,528ms,173ms,,7.420922641124568img/s,118ms,,mps_ao_ppb_None_basic,mps,8045.0,2.7.0.dev20250128+cu124,,128ms,0.22.0.dev20250128+cu124,,None,0,,134.75413346290588s,229ms,126ms,,8436341248.0,,134.75413346290588ms,180ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,139.31654810905457,128ms,8.0,1,,528ms,,124ms,0.9998915305137634
,1.0,214ms,272137ms,179ms,,2.093482078812495img/s,145ms,,mps_ao_ppb_None_fast_cold,mps,8045.0,2.7.0.dev20250128+cu124,None,123ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_inductor_cache_dir'},0,,477.67306447029114s,33236ms,160ms,,8436059648.0,,477.67306447029114ms,187ms,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,489.3060779571533,470ms,8.0,1,,272137ms,,1562ms,
,1.0,191ms,9007ms,114ms,,7.2369407085396285img/s,109ms,,mps_ao_ppb_None_fast,mps,8045.0,2.7.0.dev20250128+cu124,None,114ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_inductor_cache_dir'},0,,138.17993545532227s,1388ms,96ms,,8436059648.0,,138.17993545532227ms,177ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,144.31452822685242,131ms,8.0,1,,9007ms,,647ms,0.9984545631408691
,1.0,,,,,0.0img/s,,,mps_ao_ppb_None_save_export,mps,1326.0,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_inductor_cache_dir'},,,203.987309217453s,,,,1390578176.0,0,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,211.97910571098328,,1.0,1,,,,,
,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 240, in gen_masks_ao
    masks, scores, _ = mask_generator.predictor.predict(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 315, in predict
    masks, iou_predictions, low_res_masks = self._predict(
                                            ^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 445, in _predict
    masks, low_res_masks = self._predict_masks_postprocess(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 529, in _predict_masks_postprocess
    masks = self._transforms.postprocess_masks(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/transforms.py"", line 121, in postprocess_masks
    masks = F.interpolate(masks, orig_hw, mode=""bilinear"", align_corners=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4531, in interpolate
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/overrides.py"", line 1743, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 750, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4693, in interpolate
    return torch._C._nn.upsample_bilinear2d(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 740, in __torch_dispatch__
    res = MAP_TENSOR_ATEN_OP_TABLE[func](func, types, args, kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 21, in wrapper
    return func(f, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 304, in upsample_bilinear2d_impl
    resa = func(*((a,) + unwrapped_args[1:])).contiguous()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 94.36 GiB memory in use. Of the allocated memory 85.02 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,mps_ao_ppb_None_load_export_cold,mps,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_load_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,72.99805307388306,,,1,,,,,
,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 240, in gen_masks_ao
    masks, scores, _ = mask_generator.predictor.predict(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 315, in predict
    masks, iou_predictions, low_res_masks = self._predict(
                                            ^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 445, in _predict
    masks, low_res_masks = self._predict_masks_postprocess(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 529, in _predict_masks_postprocess
    masks = self._transforms.postprocess_masks(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/transforms.py"", line 121, in postprocess_masks
    masks = F.interpolate(masks, orig_hw, mode=""bilinear"", align_corners=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4531, in interpolate
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/overrides.py"", line 1743, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 750, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4693, in interpolate
    return torch._C._nn.upsample_bilinear2d(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 740, in __torch_dispatch__
    res = MAP_TENSOR_ATEN_OP_TABLE[func](func, types, args, kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 21, in wrapper
    return func(f, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 304, in upsample_bilinear2d_impl
    resa = func(*((a,) + unwrapped_args[1:])).contiguous()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 445.06 MiB is free. Including non-PyTorch memory, this process has 94.55 GiB memory in use. Of the allocated memory 86.23 GiB is allocated by PyTorch, and 6.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,mps_ao_ppb_None_load_export,mps,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_load_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,71.11832118034363,,,1,,,,,
,,,,,"Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 240, in gen_masks_ao
    masks, scores, _ = mask_generator.predictor.predict(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 315, in predict
    masks, iou_predictions, low_res_masks = self._predict(
                                            ^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 445, in _predict
    masks, low_res_masks = self._predict_masks_postprocess(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 529, in _predict_masks_postprocess
    masks = self._transforms.postprocess_masks(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/transforms.py"", line 121, in postprocess_masks
    masks = F.interpolate(masks, orig_hw, mode=""bilinear"", align_corners=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4531, in interpolate
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/overrides.py"", line 1743, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 750, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4693, in interpolate
    return torch._C._nn.upsample_bilinear2d(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 740, in __torch_dispatch__
    res = MAP_TENSOR_ATEN_OP_TABLE[func](func, types, args, kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 21, in wrapper
    return func(f, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 304, in upsample_bilinear2d_impl
    resa = func(*((a,) + unwrapped_args[1:])).contiguous()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 536.44 MiB is free. Including non-PyTorch memory, this process has 94.46 GiB memory in use. Of the allocated memory 85.02 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,mps_ao_ppb_None_load_export_gpu_preproc,mps,,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_load_export_inductor_cache_dir'},,None,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,71.54371809959412,,,1,,,,,
,,,,,"W0128 22:14:18.627000 788880 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:198: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
V0128 22:14:53.632000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles] Recompiling function _predict_masks in /home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py:450
V0128 22:14:53.632000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles]     triggered by the following guard failure(s):
V0128 22:14:53.632000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles]     - 0/0: (L['self']._modules['model']._modules['sam_mask_decoder']._modules['transformer']._modules['final_attn_token_to_image'].num_heads*(128 // L['self']._modules['model']._modules['sam_mask_decoder']._modules['transformer']._modules['final_attn_token_to_image'].num_heads)) != (8*L['point_coords'].elems.size()[0])  # (_inductor/pattern_matcher.py:1343 in <genexpr>)
V0128 22:15:20.253000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/2] [__recompiles] Recompiling function _predict_masks in /home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py:450
V0128 22:15:20.253000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/2] [__recompiles]     triggered by the following guard failure(s):
V0128 22:15:20.253000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/2] [__recompiles]     - 0/0: 2 <= L['point_coords'].elems.size()[0] <= 1023  # if point_coords is not None:  # dev/ao/torchao/_models/sam2/sam2_image_predictor.py:464 in _predict_masks (user code shown is first use of this value--the guard itself is not due user code but due to 0/1 specialization in the framework; to avoid specialization try torch._dynamo.mark_unbacked(tensor, dim)) and (_inductor/codegen/simd.py:1343 in can_use_32bit_indexing)
V0128 22:15:20.253000 788880 site-packages/torch/_dynamo/guards.py:2940] [0/2] [__recompiles]     - 0/1: (L['self']._modules['model']._modules['sam_mask_decoder']._modules['transformer']._modules['final_attn_token_to_image'].num_heads*(128 // L['self']._modules['model']._modules['sam_mask_decoder']._modules['transformer']._modules['final_attn_token_to_image'].num_heads)) == (8*L['point_coords'].elems.size()[0])  # (_inductor/pattern_matcher.py:1343 in <genexpr>)
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 240, in gen_masks_ao
    masks, scores, _ = mask_generator.predictor.predict(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 315, in predict
    masks, iou_predictions, low_res_masks = self._predict(
                                            ^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 445, in _predict
    masks, low_res_masks = self._predict_masks_postprocess(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 529, in _predict_masks_postprocess
    masks = self._transforms.postprocess_masks(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/transforms.py"", line 121, in postprocess_masks
    masks = F.interpolate(masks, orig_hw, mode=""bilinear"", align_corners=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4531, in interpolate
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/overrides.py"", line 1743, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 750, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4693, in interpolate
    return torch._C._nn.upsample_bilinear2d(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 740, in __torch_dispatch__
    res = MAP_TENSOR_ATEN_OP_TABLE[func](func, types, args, kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 21, in wrapper
    return func(f, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 304, in upsample_bilinear2d_impl
    resa = func(*((a,) + unwrapped_args[1:])).contiguous()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 437.06 MiB is free. Including non-PyTorch memory, this process has 94.56 GiB memory in use. Of the allocated memory 85.03 GiB is allocated by PyTorch, and 8.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,mps_ao_ppb_None_fast_export_cold,mps,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,157.22293376922607,,,1,,,,,
,,,,,"W0128 22:16:57.108000 806498 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
V0128 22:17:16.228000 806498 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles] Recompiling function _predict_masks in /home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py:450
V0128 22:17:16.228000 806498 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles]     triggered by the following guard failure(s):
V0128 22:17:16.228000 806498 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles]     - 0/0: 2 <= L['point_coords'].elems.size()[0]  # if point_coords is not None:  # dev/ao/torchao/_models/sam2/sam2_image_predictor.py:464 in _predict_masks (user code shown is first use of this value--the guard itself is not due user code but due to 0/1 specialization in the framework; to avoid specialization try torch._dynamo.mark_unbacked(tensor, dim))
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 240, in gen_masks_ao
    masks, scores, _ = mask_generator.predictor.predict(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 315, in predict
    masks, iou_predictions, low_res_masks = self._predict(
                                            ^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 445, in _predict
    masks, low_res_masks = self._predict_masks_postprocess(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 529, in _predict_masks_postprocess
    masks = self._transforms.postprocess_masks(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/transforms.py"", line 121, in postprocess_masks
    masks = F.interpolate(masks, orig_hw, mode=""bilinear"", align_corners=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4531, in interpolate
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/overrides.py"", line 1743, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 750, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4693, in interpolate
    return torch._C._nn.upsample_bilinear2d(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 740, in __torch_dispatch__
    res = MAP_TENSOR_ATEN_OP_TABLE[func](func, types, args, kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 21, in wrapper
    return func(f, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 304, in upsample_bilinear2d_impl
    resa = func(*((a,) + unwrapped_args[1:])).contiguous()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 599.06 MiB is free. Including non-PyTorch memory, this process has 94.40 GiB memory in use. Of the allocated memory 85.04 GiB is allocated by PyTorch, and 7.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,mps_ao_ppb_None_fast_export,mps,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_inductor_cache_dir'},,,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,86.91639137268066,,,1,,,,,
,,,,,"W0128 22:18:26.610000 814899 site-packages/torch/_logging/_internal.py:1093] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
V0128 22:18:42.968000 814899 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles] Recompiling function _predict_masks in /home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py:450
V0128 22:18:42.968000 814899 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles]     triggered by the following guard failure(s):
V0128 22:18:42.968000 814899 site-packages/torch/_dynamo/guards.py:2940] [0/1] [__recompiles]     - 0/0: 2 <= L['point_coords'].elems.size()[0]  # if point_coords is not None:  # dev/ao/torchao/_models/sam2/sam2_image_predictor.py:464 in _predict_masks (user code shown is first use of this value--the guard itself is not due user code but due to 0/1 specialization in the framework; to avoid specialization try torch._dynamo.mark_unbacked(tensor, dim))
Traceback (most recent call last):
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 648, in <module>
    fire.Fire(main)
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 606, in main
    masks_batch = gen_masks(
                  ^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 302, in gen_masks
    masks = gen_masks_ao(
            ^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/contextlib.py"", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/examples/sam2_amg_server/generate_data.py"", line 240, in gen_masks_ao
    masks, scores, _ = mask_generator.predictor.predict(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 315, in predict
    masks, iou_predictions, low_res_masks = self._predict(
                                            ^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/utils/_contextlib.py"", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 445, in _predict
    masks, low_res_masks = self._predict_masks_postprocess(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/sam2_image_predictor.py"", line 529, in _predict_masks_postprocess
    masks = self._transforms.postprocess_masks(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/utils/transforms.py"", line 121, in postprocess_masks
    masks = F.interpolate(masks, orig_hw, mode=""bilinear"", align_corners=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4531, in interpolate
    return handle_torch_function(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/overrides.py"", line 1743, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 750, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/.conda/envs/nightly20250128py312/lib/python3.12/site-packages/torch/nn/functional.py"", line 4693, in interpolate
    return torch._C._nn.upsample_bilinear2d(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 740, in __torch_dispatch__
    res = MAP_TENSOR_ATEN_OP_TABLE[func](func, types, args, kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 21, in wrapper
    return func(f, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/cpuhrsch/dev/ao/torchao/_models/sam2/map_tensor.py"", line 304, in upsample_bilinear2d_impl
    resa = func(*((a,) + unwrapped_args[1:])).contiguous()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 334.44 MiB is free. Including non-PyTorch memory, this process has 94.65 GiB memory in use. Of the allocated memory 85.04 GiB is allocated by PyTorch, and 8.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
",,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast,mps_ao_ppb_None_fast_export_gpu_preproc,mps,,2.7.0.dev20250128+cu124,None,,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_inductor_cache_dir'},,None,,,,,,,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,79.01440286636353,,,1,,,,,
None,1.0,47ms,540247ms,31ms,,1.5902771119585888img/s,24ms,,mps_ao_ppb_None_fast_furious_cold,mps,4246.0,2.7.0.dev20250128+cu124,None,32ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_furious_inductor_cache_dir'},0,,628.8212239742279s,32740ms,22ms,,4453008384.0,,628.8212239742279ms,38ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,640.8134334087372,621ms,4.0,1,,540247ms,,1612ms,0.9970166206955909
None,1.0,44ms,8029ms,26ms,,25.355611001905373img/s,21ms,,mps_ao_ppb_None_fast_furious,mps,4246.0,2.7.0.dev20250128+cu124,None,28ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_furious_inductor_cache_dir'},0,,39.43900227546692s,1282ms,18ms,,4453008384.0,,39.43900227546692ms,35ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,44.51449799537659,33ms,4.0,1,,8029ms,,325ms,0.9970166568756104
None,1.0,,,,,0.0img/s,,,mps_ao_ppb_None_save_export_furious,mps,861.0,2.7.0.dev20250128+cu124,,,0.22.0.dev20250128+cu124,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_furious_inductor_cache_dir'},,,253.08157634735107s,,,,903450624.0,0,,,,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,264.0102846622467,,0.0,1,,,,,
None,1.0,55ms,713ms,35ms,,23.396681625976953img/s,33ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_load_export_furious_cold,mps,90887.0,2.7.0.dev20250128+cu124,,38ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_load_export_furious_inductor_cache_dir'},0,,42.74110388755798s,288ms,31ms,,95302864384.0,,42.74110388755798ms,46ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,47.52962946891785,36ms,93.0,1,,713ms,,38ms,0.9963298424184323
None,1.0,55ms,657ms,35ms,,23.782855967457795img/s,33ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_load_export_furious,mps,90886.0,2.7.0.dev20250128+cu124,,40ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_load_export_furious_inductor_cache_dir'},0,,42.04709482192993s,297ms,31ms,,95301212672.0,,42.04709482192993ms,45ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,46.57646465301514,35ms,93.0,1,,657ms,,38ms,0.9963298424184323
None,1.0,47ms,1355ms,32ms,,26.409352187261856img/s,29ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_load_export_furious_gpu_preproc,mps,90887.0,2.7.0.dev20250128+cu124,,34ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_load_export_furious_inductor_cache_dir'},0,None,37.86537408828735s,303ms,29ms,,95302864384.0,,37.86537408828735ms,37ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,42.85316228866577,32ms,93.0,1,,1355ms,,35ms,0.9235443816127954
None,1.0,46ms,37469ms,26ms,,8.765228773736574img/s,23ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_fast_export_furious_cold,mps,90826.0,2.7.0.dev20250128+cu124,None,30ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_furious_inductor_cache_dir'},0,,114.08715343475342s,26387ms,19ms,,95238118912.0,,114.08715343475342ms,37ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,121.66991114616394,107ms,93.0,1,,37469ms,,24ms,0.9957003740370274
None,1.0,45ms,3519ms,26ms,,27.78232190346751img/s,22ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_fast_export_furious,mps,90826.0,2.7.0.dev20250128+cu124,None,29ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_furious_inductor_cache_dir'},0,,35.99411177635193s,1716ms,19ms,,95238118912.0,,35.99411177635193ms,36ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,41.722914934158325,30ms,93.0,1,,3519ms,,25ms,0.9957003907859325
None,1.0,42ms,11600ms,26ms,,19.777818584844226img/s,23ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_fast_export_furious_recompiles,mps,90693.0,2.7.0.dev20250128+cu124,None,30ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_furious_inductor_cache_dir'},0,,50.5616934299469s,7380ms,19ms,None,95099412480.0,,50.5616934299469ms,36ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,56.7949116230011,43ms,93.0,1,,11600ms,,27ms,0.9951186454892158
None,1.0,36ms,4283ms,22ms,,31.504678158195443img/s,19ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_fast_export_furious_gpu_preproc,mps,90826.0,2.7.0.dev20250128+cu124,None,24ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_furious_inductor_cache_dir'},0,None,31.741317749023438s,1547ms,17ms,,95238118912.0,,31.741317749023438ms,28ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,36.976972341537476,26ms,93.0,1,,4283ms,,21ms,0.9234285098117543
None,1.0,33ms,4603ms,22ms,,31.84332023316853img/s,19ms,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/exported_models/mps_ao_fast_furious,mps_ao_ppb_None_fast_export_furious_gpu_preproc_recompiles,mps,90693.0,2.7.0.dev20250128+cu124,None,24ms,0.22.0.dev20250128+cu124,,{'TORCHINDUCTOR_CACHE_DIR': '/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/mps_fast_export_furious_inductor_cache_dir'},0,None,31.40376043319702s,1514ms,17ms,None,95099412480.0,,31.40376043319702ms,27ms,0.0,/home/cpuhrsch/blogs/tmp/sam2_amg_example_run_15/amg_baseline_annotations,36.41001343727112,26ms,93.0,1,,4603ms,,21ms,0.9224292650026037
