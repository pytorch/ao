


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.quantization.quant_api &mdash; torchao 0.14 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>0.14 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization_overview.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributor_guide.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_api_guide.html">Benchmarking API Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_user_guide.html">Benchmarking User Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_qat.html">torchao.quantization.qat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_float8.html">torchao.float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_utils.html">torchao.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_utils.html#torchao-quantization-quantize-common">torchao.quantization.quantize_.common</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Eager Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../pretraining.html">(Part 1) Pre-training with float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../finetuning.html">(Part 2) Fine-tuning with QAT, QLoRA, and float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serving.html">(Part 3) Serving on vLLM, SGLang, ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchao_vllm_integration.html">Integration with VLLM: Architecture and Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchao_hf_integration.html">Hugging Face Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../static_quantization.html">Static Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PT2E Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_ptq.html">PyTorch 2 Export Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_qat.html">PyTorch 2 Export Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">PyTorch 2 Export Quantization with X86 Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">PyTorch 2 Export Quantization with Intel GPU Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">PyTorch 2 Export Quantization for OpenVINO torch.compile Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quantizer.html">How to Write a <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code> for PyTorch 2 Export Quantization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.quantization.quant_api</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.quantization.quant_api</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># Copyright 2024-2025 Arm Limited and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Quantization APIs</span>

<span class="sd">Generally these APIs can be applied directly to any model</span>
<span class="sd">with Linear modules to obtain quantized linear ops. The intended</span>
<span class="sd">usage involves applying torch.compile to the model afterwards</span>
<span class="sd">both because primitives were designed based on the fusions that</span>
<span class="sd">come along with it and because that is how we access the intended quantized</span>
<span class="sd">and mixed GEMM kernels</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span> <span class="k">as</span> <span class="n">OrderedDictType</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.parametrize</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">parametrize</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torchao</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.core.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">AOBaseConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">CutlassInt4PackedLayout</span><span class="p">,</span>
    <span class="n">CutlassSemiSparseLayout</span><span class="p">,</span>
    <span class="n">Float8Layout</span><span class="p">,</span>
    <span class="n">Int4CPULayout</span><span class="p">,</span>
    <span class="n">Int4XPULayout</span><span class="p">,</span>
    <span class="n">Int8DynamicActInt4WeightCPULayout</span><span class="p">,</span>
    <span class="n">MarlinQQQLayout</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">,</span>
    <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">,</span>
    <span class="n">PlainLayout</span><span class="p">,</span>
    <span class="n">QDQLayout</span><span class="p">,</span>
    <span class="n">SemiSparseLayout</span><span class="p">,</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">,</span>
    <span class="n">UintxLayout</span><span class="p">,</span>
    <span class="n">to_affine_quantized_floatx</span><span class="p">,</span>
    <span class="n">to_affine_quantized_floatx_static</span><span class="p">,</span>
    <span class="n">to_affine_quantized_intx</span><span class="p">,</span>
    <span class="n">to_marlinqqq_quantized_intx</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.uintx.packed_linear_int8_dynamic_activation_intx_weight_layout</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Target</span><span class="p">,</span>
    <span class="n">make_packed_linear_int8_dynamic_activation_intx_weight_tensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Layout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">e4m3_dtype</span><span class="p">,</span> <span class="n">e5m2_dtype</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.float8_linear</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float8Linear</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.inference</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Float8MMConfig</span><span class="p">,</span>
    <span class="n">FP8Granularity</span><span class="p">,</span>
    <span class="n">_check_hardware_support</span><span class="p">,</span>
    <span class="n">_normalize_granularity</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.linear_activation_weight_observed_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationWeightObservedTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.observer</span><span class="w"> </span><span class="kn">import</span> <span class="n">AffineQuantizedObserverBase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.common</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">KernelPreference</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.workflows</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Float8Tensor</span><span class="p">,</span>
    <span class="n">Int4ChooseQParamsAlgorithm</span><span class="p">,</span>
    <span class="n">Int4MarlinSparseTensor</span><span class="p">,</span>
    <span class="n">Int4OpaqueTensor</span><span class="p">,</span>
    <span class="n">Int4PackingFormat</span><span class="p">,</span>
    <span class="n">Int4PlainInt32Tensor</span><span class="p">,</span>
    <span class="n">Int4PreshuffledTensor</span><span class="p">,</span>
    <span class="n">Int4Tensor</span><span class="p">,</span>
    <span class="n">Int4TilePackedTo4dTensor</span><span class="p">,</span>
    <span class="n">IntxChooseQParamsAlgorithm</span><span class="p">,</span>
    <span class="n">IntxOpaqueTensor</span><span class="p">,</span>
    <span class="n">IntxPackingFormat</span><span class="p">,</span>
    <span class="n">IntxUnpackedToInt8Tensor</span><span class="p">,</span>
    <span class="n">QuantizeTensorToFloat8Kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.transform_module</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">,</span>
    <span class="n">register_quantize_module_handler</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_block_size</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.weight_tensor_linear_activation_quantization</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">to_weight_tensor_with_linear_activation_quantization_metadata</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_ConfigDeprecationWrapper</span><span class="p">,</span>
    <span class="n">is_MI300</span><span class="p">,</span>
    <span class="n">is_sm_at_least_89</span><span class="p">,</span>
    <span class="n">is_sm_at_least_90</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.autoquant</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">,</span> <span class="n">autoquant</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.GPTQ</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyGPTQQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Granularity</span><span class="p">,</span>
    <span class="n">PerAxis</span><span class="p">,</span>
    <span class="n">PerGroup</span><span class="p">,</span>
    <span class="n">PerRow</span><span class="p">,</span>
    <span class="n">PerTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.linear_activation_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationQuantizedTensor</span><span class="p">,</span>
    <span class="n">to_linear_activation_quantized</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.linear_quant_modules</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyQuantizer</span><span class="p">,</span>
    <span class="n">Int8DynActInt4WeightQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.qat</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">intx_quantization_aware_training</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">,</span>
    <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">ZeroPointDomain</span><span class="p">,</span>
    <span class="n">quantize_affine</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.subclass</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">QuantizedLinearWeightBase</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.unified</span><span class="w"> </span><span class="kn">import</span> <span class="n">Quantizer</span><span class="p">,</span> <span class="n">TwoStepQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_per_token_block_size</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># TODO: revisit this list?</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;swap_conv2d_1x1_to_linear&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Quantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TwoStepQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyGPTQQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;autoquant&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_get_subclass_inserter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_dynamic_activation_int4_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_dynamic_activation_int8_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_dynamic_activation_int8_semi_sparse_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int4_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;intx_quantization_aware_training&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float8_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;uintx_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fpx_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gemlite_uintx_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float8_dynamic_activation_float8_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float8_static_activation_float8_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int8DynActInt4WeightQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Float8DynamicActivationFloat8SemiSparseWeightConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ModuleFqnToConfig&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">],</span>
    <span class="n">MarlinSparseLayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">],</span>
    <span class="n">Int4CPULayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">],</span>
    <span class="n">Int4XPULayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">LAYOUT_TO_PRESERVE_ZEROS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">Int4CPULayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">Int4XPULayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">replacement_fn</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">,</span>
    <span class="n">cur_fqn</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively replaces each child module in `model` with the result of `replacement_fn(child)`</span>
<span class="sd">    if `filter_fn(child)` returns `True`.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model containing modules to be replaced.</span>
<span class="sd">        replacement_fn (Callable[[torch.nn.Module], torch.nn.Module]): The function to replace matching modules.</span>
<span class="sd">        filter_fn (Callable[[torch.nn.Module], bool]): The filter function to determine which modules to replace.</span>
<span class="sd">        cur_fqn (str, optional): The current fully qualified name of the module being processed. Defaults to &quot;&quot;.</span>
<span class="sd">        device (device, optional): Device to move the model to before applying `filter_fn`. Defaults to None.</span>
<span class="sd">        extra_args (Tuple[Any, ...], optional): optional extra args to pass to `replacement_fn`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">new_module</span>
    <span class="k">if</span> <span class="n">filter_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move to device before quantization</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">replacement_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">named_children_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">named_children_list</span><span class="p">:</span>
            <span class="n">new_child</span> <span class="o">=</span> <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span>
                <span class="n">replacement_fn</span><span class="p">,</span>
                <span class="n">filter_fn</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cur_fqn</span><span class="si">}{</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">device</span><span class="p">,</span>
                <span class="n">extra_args</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">child</span> <span class="ow">and</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_child</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move parent module to device</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_replace_with_custom_fn_if_matches_filter_with_name</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">replacement_fn</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">,</span>
    <span class="n">cur_fqn</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A variant of _replace_with_custom_fn_if_matches_filter where replacement_fn takes module name as well</span>
<span class="sd">        ...</span>
<span class="sd">        replacement_fn (Callable[[torch.nn.Module, str], torch.nn.Module]): The function to replace matching modules.</span>
<span class="sd">        ...</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">new_module</span>
    <span class="k">if</span> <span class="n">filter_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move to device before quantization</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">replacement_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">named_children_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">named_children_list</span><span class="p">:</span>
            <span class="n">new_child</span> <span class="o">=</span> <span class="n">_replace_with_custom_fn_if_matches_filter_with_name</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span>
                <span class="n">replacement_fn</span><span class="p">,</span>
                <span class="n">filter_fn</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cur_fqn</span><span class="si">}{</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">device</span><span class="p">,</span>
                <span class="n">extra_args</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">child</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_child</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move parent module to device</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_linear</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># avoid circular dependencies</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat.affine_fake_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">_AffineFakeQuantizedTensor</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># adding weight tensor subclass isinstance check to make sure the weight is only quantized once</span>
    <span class="c1"># when it is shared by multiple linear modules</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">QuantizedLinearWeightBase</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">_AffineFakeQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">NonDynamicallyQuantizableLinear</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_subclass_inserter</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">enable_parametrization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function which inserts the given subclass into all linear modules</span>
<span class="sd">    in the model. The inserted module will have its weight set to the result of</span>
<span class="sd">    `cls(mod.weight, **kwargs)`. If parametrization is enabled then this will be done using</span>
<span class="sd">    torch.nn.utils.parametrize instead of directly setting the attribute on the module.</span>

<span class="sd">    Args:</span>
<span class="sd">        cls (torch.Tensor): The class to insert as a child module.</span>
<span class="sd">        kwargs (Any): Any additional arguments for the constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">constructor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;constructor&quot;</span><span class="p">,</span> <span class="s2">&quot;subclass_constructor&quot;</span><span class="p">)</span>
    <span class="n">from_float</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;method&quot;</span><span class="p">,</span> <span class="s2">&quot;from_float&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">insert_subclass</span><span class="p">(</span><span class="n">lin</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">enable_parametrization</span><span class="p">:</span>
            <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
            <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
                <span class="n">lin</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">constructor</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="c1"># cls.from_float(...)</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">from_float</span><span class="p">)(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">lin</span>

    <span class="k">return</span> <span class="n">insert_subclass</span>


<span class="k">def</span><span class="w"> </span><span class="nf">swap_conv2d_1x1_to_linear</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Changes all conv2d 1x1 modules to equivalent linear modules so that they can then be quantized.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">PermuteSandwich</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="o">-</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replace_conv2d_1x1</span><span class="p">(</span><span class="n">conv</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">lin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">conv</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">conv</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">PermuteSandwich</span><span class="p">(</span><span class="n">lin</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="n">mod</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">replace_conv2d_1x1</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="n">filter_fn</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">insert_observers_</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">input_observer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AffineQuantizedObserverBase</span><span class="p">],</span>
    <span class="n">weight_observer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AffineQuantizedObserverBase</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts the weight of a linear module to a LinearActivationWeightObservedTensor.</span>

<span class="sd">    This function wraps the weight of the given linear module with a LinearActivationWeightObservedTensor,</span>
<span class="sd">    which enables observation of both input and weight tensors during forward passes.</span>
<span class="sd">    The wrapped weight is then re-wrapped as a nn.Parameter to maintain compatibility</span>
<span class="sd">    with PyTorch&#39;s module system.</span>

<span class="sd">    Example::</span>

<span class="sd">    ```</span>
<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>
<span class="sd">        from torchao.quantization.linear_observer_tensor import insert_observers_</span>
<span class="sd">        from torchao.quantization.observer import (</span>
<span class="sd">            AffineQuantizedMinMaxObserver,</span>
<span class="sd">            PerTensor,</span>
<span class="sd">            MappingType</span>
<span class="sd">        )</span>

<span class="sd">        # Create observers</span>
<span class="sd">        input_observer = AffineQuantizedMinMaxObserver(</span>
<span class="sd">            MappingType.SYMMETRIC,</span>
<span class="sd">            torch.float8_e4m3fn,</span>
<span class="sd">            granularity_type=PerTensor(),</span>
<span class="sd">            eps=torch.finfo(torch.float32).eps,</span>
<span class="sd">            scale_dtype=torch.float,</span>
<span class="sd">            zero_point_dtype=torch.int,</span>
<span class="sd">            zero_point_domain=ZeroPointDomain.NONE,</span>
<span class="sd">        )</span>

<span class="sd">        # Create a linear module</span>
<span class="sd">        linear_module = nn.Linear(10, 20)</span>

<span class="sd">        # Convert the linear module&#39;s weight to an observed tensor</span>
<span class="sd">        insert_observers_(linear_module, input_observer, weight_observer=None)</span>

<span class="sd">        # The linear_module can now be used as usual, with observers calculating statistics</span>
<span class="sd">        output = linear_module(torch.randn(10, 10))</span>

<span class="sd">        # Get the scale and zero point of the input observer</span>
<span class="sd">        scale, zero_point = linear_module.weight.input_observer.calculate_qparams()</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        model (nn.Module): The nn.Module to convert.</span>
<span class="sd">        input_observer (Optional[AffineQuantizedObserverBase]): Observer for input tensor.</span>
<span class="sd">        weight_observer (Optional[AffineQuantizedObserverBase]): Observer for weight tensor.</span>
<span class="sd">        filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): Filter function to select which modules to convert.</span>
<span class="sd">            If not provided, all linear modules will be converted. This function should take a module and its fully qualified name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Linear: The modified linear module with its weight wrapped in a LinearActivationWeightObservedTensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">convert_to_linear_observer</span><span class="p">(</span><span class="n">linear_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="c1"># Wrap the weight with LinearActivationWeightObservedTensor and then with nn.Parameter</span>
        <span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">LinearActivationWeightObservedTensor</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
                <span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">input_observer</span><span class="o">=</span><span class="n">input_observer</span><span class="p">,</span>
                <span class="n">weight_observer</span><span class="o">=</span><span class="n">weight_observer</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">linear_module</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">convert_to_linear_observer</span><span class="p">,</span>
        <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantization_type</span><span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">_quantization_type</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(activation=</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">input_quant_func</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">original_weight_tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="s2">&quot;_quantization_type&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">_quantization_type</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;not recognized: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_linear_extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_embedding_extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;num_embeddings=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, embedding_dim=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_linear_subclass_inserter</span><span class="p">(</span>
    <span class="n">constructor</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">allow_requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">propagate_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to apply the constructor that quantizes the weight Tensor (with additional kwargs)</span>
<span class="sd">    to the weight of linear module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">insert_subclass</span><span class="p">(</span><span class="n">lin</span><span class="p">):</span>
        <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">allow_requires_grad</span> <span class="ow">and</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="k">if</span> <span class="n">propagate_bias</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">constructor</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span>
        <span class="p">)</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">lin</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lin</span>

    <span class="k">return</span> <span class="n">insert_subclass</span>


<div class="viewcode-block" id="quantize_"><a class="viewcode-back" href="../../../generated/torchao.quantization.quantize_.html#torchao.quantization.quantize_">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">quantize_</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">AOBaseConfig</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the weight of linear modules in the model with `config`, model is modified inplace</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): input model</span>
<span class="sd">        config (AOBaseConfig): a workflow configuration object.</span>
<span class="sd">        filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): function that takes a nn.Module instance and fully qualified name of the module, returns True if we want to run `config` on</span>
<span class="sd">        the weight of the module</span>
<span class="sd">        device (device, optional): Device to move module to before applying `filter_fn`. This can be set to `&quot;cuda&quot;` to speed up quantization. The final model will be on the specified `device`.</span>
<span class="sd">            Defaults to None (do not change device).</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>
<span class="sd">        from torchao import quantize_</span>

<span class="sd">        # quantize with some predefined `config` method that corresponds to</span>
<span class="sd">        # optimized execution paths or kernels (e.g. int4 tinygemm kernel)</span>
<span class="sd">        # also customizable with arguments</span>
<span class="sd">        # currently options are</span>
<span class="sd">        # Int8DynamicActivationInt4WeightConfig (for executorch)</span>
<span class="sd">        # Int8DynamicActivationInt8WeightConfig (optimized with int8 mm op and torch.compile)</span>
<span class="sd">        # Int4WeightOnlyConfig (optimized with int4 tinygemm kernel and torch.compile)</span>
<span class="sd">        # Int8WeightOnlyConfig (optimized with int8 mm op and torch.compile</span>
<span class="sd">        from torchao.quantization.quant_api import int4_weight_only</span>

<span class="sd">        m = nn.Sequential(nn.Linear(32, 1024), nn.Linear(1024, 32))</span>
<span class="sd">        quantize_(m, Int4WeightOnlyConfig(group_size=32, version=1))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.quantize_&quot;</span><span class="p">)</span>

    <span class="n">filter_fn</span> <span class="o">=</span> <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ModuleFqnToConfig</span><span class="p">):</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter_with_name</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">_module_fqn_to_config_handler</span><span class="p">,</span>
            <span class="n">filter_fn</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">extra_args</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">,),</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">AOBaseConfig</span><span class="p">):</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)]</span>
        <span class="c1"># for each linear in the model, apply the transform if filtering passes</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">handler</span><span class="p">,</span>
            <span class="n">filter_fn</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">extra_args</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">,),</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Passing a generic Callable to `quantize_` is no longer recommended and will be deprecated at a later release. Please see https://github.com/pytorch/ao/issues/1690 for instructions on how to pass in workflow configuration instead.&quot;&quot;&quot;</span>
        <span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_asymm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is defined here instead of local function to support serialization&quot;&quot;&quot;</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_uint8_asymm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">255</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>

    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="Int8DynamicActivationInt4WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.html#torchao.quantization.Int8DynamicActivationInt4WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration for applying int8 dynamic per token asymmetric activation quantization and int4 per group weight symmetric quantization to linear</span>
<span class="sd">    This is used to produce a model for executorch backend, but currently executorch did not</span>
<span class="sd">    support lowering for the quantized model from this flow yet</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `MarlinQQQLayout()` and `CutlassInt4PackedLayout()` for now</span>
<span class="sd">        `mapping_type`: quantization type for weight, controls the weight quantization is symmetric or asymmetric</span>
<span class="sd">        `act_mapping_type`: quantization type for activation, controls the activation quantization is symmetric or asymmetric</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int8DynamicActivationInt4WeightConfig&quot;</span>
        <span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">int8_dynamic_activation_int4_weight</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;int8_dynamic_activation_int4_weight&quot;</span><span class="p">,</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span>
<span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationInt4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int4_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">module</span>

    <span class="c1"># weight settings</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">7</span>

    <span class="c1"># input settings</span>
    <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int8DynamicActInt4WeightCPULayout</span><span class="p">):</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_uint8_asymm_per_token_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>
    <span class="k">elif</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinQQQLayout</span><span class="p">):</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">):</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_cutlass_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported activation mapping type: </span><span class="si">{</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinQQQLayout</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_marlinqqq_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int8DynamicActInt4WeightCPULayout</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
            <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
            <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
            <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
            <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationIntxWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for dynamically quantizing activations to torch.int8 and weights to torch.intx, with 1 &lt;= x &lt;= 8.</span>
<span class="sd">    More specifically, activations are dynamically quantized to 8-bits at a per-token granularity with scales/zeros.</span>
<span class="sd">    Weights are quantized with scales/zeros in a groupwise or channelwise manner using the number of bits specified by weight_dtype.</span>

<span class="sd">    This layout is identical to Int8DynamicActivationInt4WeightConfig when weight_dtype is torch.int4 and other args</span>
<span class="sd">    are the same.  However, this layout is more general and supports other weight dtypes.</span>

<span class="sd">    args:</span>
<span class="sd">        `weight_dtype`: The dtype to use for weight quantization.  Must be torch.intx, where 1 &lt;= x &lt;= 8.</span>
<span class="sd">       ` weight_granularity`: The granularity to use for weight quantization.  Must be PerGroup or PerAxis(axis=0).</span>
<span class="sd">        `weight_mapping_type`: The type of mapping to use for the weight quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.  MappingType.SYMMETRIC requires ZeroPointDomain.NONE</span>
<span class="sd">        `weight_scale_dtype`: The dtype to use for the weight scale.</span>
<span class="sd">        `act_mapping_type`: The type of mapping to use for the activation quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.</span>
<span class="sd">        `layout`: The layout to use for the packed weight tensor:</span>
<span class="sd">            - PackedLinearInt8DynamicActivationIntxWeightLayout: this layout is optimized for CPU performance.</span>
<span class="sd">            - QDQLayout: this layout represents the quantization with Q/DQ quant primitives, and is intended for</span>
<span class="sd">                export applications like ExecuTorch.</span>
<span class="sd">        `intx_packing_format`: The format to use for the packed weight tensor (version 2 only).</span>
<span class="sd">            - unpacked_to_int8: this format is the default and is intended for export applications like ExecuTorch.</span>
<span class="sd">            - opaque_torchao_auto: this format is optimized for CPU performance.</span>
<span class="sd">        `intx_choose_qparams_algorithm`: The algorithm to use for choosing the quantization parameters.</span>
<span class="sd">        `version`: version of the config to use, only subset of above args are valid based on version, see note for more details.</span>

<span class="sd">        Note:</span>

<span class="sd">        Current state for Int8DynamicActivationIntxWeightConfig is that it supports both v1 (legacy) and v2.</span>

<span class="sd">        * `intx_packing_format` is used for version 2.</span>
<span class="sd">        * `layout` is only used for version 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">weight_granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerGroup</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">weight_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="c1"># TODO: add weight_scale_dtype to Int8DynamicActivationInt4WeightConfig</span>
    <span class="n">weight_scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">QDQLayout</span><span class="p">()</span>
    <span class="n">intx_packing_format</span><span class="p">:</span> <span class="n">IntxPackingFormat</span> <span class="o">=</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span>
    <span class="n">intx_choose_qparams_algorithm</span><span class="p">:</span> <span class="n">IntxChooseQParamsAlgorithm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span>
    <span class="p">)</span>

    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int8DynamicActivationIntxWeightConfig&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;int</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_dtype must be torch.intx, where 1 &lt;= x &lt;= 8, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="p">(</span><span class="n">PerAxis</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">)),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_granularity must be PerAxis or PerGroup, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;axis must be 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC or MappingType.SYMMETRIC_NO_CLIPPING_ERR, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;act_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="p">(</span><span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">,</span> <span class="n">QDQLayout</span><span class="p">)</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;layout must be PackedLinearInt8DynamicActivationIntxWeightLayout or QDQLayout, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Target</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Target</span><span class="o">.</span><span class="n">KLEIDIAI</span><span class="p">,</span> <span class="n">Target</span><span class="o">.</span><span class="n">ATEN</span><span class="p">]:</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="p">):</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;When using layout PackedLinearInt8DynamicActivationIntxWeightLayout with target </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;the weight scale may be cast to bfloat16 by the kernel, but weight_scale_dtype is set to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="s2">&quot;Explicitly set weight_scale_dtype to torch.bfloat16 to suppress this warning. &quot;</span>
                        <span class="s2">&quot;If you need weight_scale_dtype = torch.float32, use target=Target.UNIVERSAL instead.&quot;</span>
                    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_intx_weight_quantize_tensor</span><span class="p">(</span>
    <span class="n">weight</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">,</span>
    <span class="n">config</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_granularity</span>
    <span class="n">weight_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_mapping_type</span>
    <span class="n">weight_scale_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_scale_dtype</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">intx_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_packing_format</span>
    <span class="n">intx_choose_qparams_algorithm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_choose_qparams_algorithm</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Int8DynamicActivationIntxWeightConfig only works for 2-d Tensor, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">):</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight_granularity</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_granularity must be PerGroup or PerAxis, got </span><span class="si">{</span><span class="n">weight_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
        <span class="n">opaque_formats</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_ATEN_KLEIDIAI</span><span class="p">,</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_TORCHAO_AUTO</span><span class="p">,</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_TORCHAO_KLEIDIAI</span><span class="p">,</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_TORCHAO_LOWBIT</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">intx_packing_format</span> <span class="o">==</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span>
            <span class="ow">or</span> <span class="n">intx_packing_format</span> <span class="ow">in</span> <span class="n">opaque_formats</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Unsupported packing format: </span><span class="si">{</span><span class="n">intx_packing_format</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">custom_zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">custom_zero_point</span> <span class="o">=</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="o">=</span><span class="n">weight_mapping_type</span><span class="p">,</span>
            <span class="n">activation_quantization</span><span class="o">=</span><span class="s2">&quot;int8_asym_per_token&quot;</span><span class="p">,</span>
            <span class="n">intx_choose_qparams_algorithm</span><span class="o">=</span><span class="n">intx_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
            <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_scale_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight_scale_dtype</span> <span class="o">!=</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="n">_adjust_scale_dtype_in_intx_unpacked_tensor</span><span class="p">(</span>
                <span class="n">new_weight</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">weight_scale_dtype</span>
            <span class="p">)</span>

        <span class="n">new_bias</span> <span class="o">=</span> <span class="n">bias</span>

        <span class="c1"># Create packed tensor</span>
        <span class="k">if</span> <span class="n">intx_packing_format</span> <span class="ow">in</span> <span class="n">opaque_formats</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">IntxOpaqueTensor</span><span class="o">.</span><span class="n">from_intx_unpacked_to_int8_tensor</span><span class="p">(</span>
                <span class="n">new_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">new_bias</span><span class="p">,</span> <span class="n">intx_packing_format</span><span class="o">=</span><span class="n">intx_packing_format</span>
            <span class="p">)</span>
            <span class="n">new_bias</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># bias is packed with weights</span>

        <span class="k">return</span> <span class="n">new_weight</span><span class="p">,</span> <span class="n">new_bias</span>

    <span class="c1"># Version 1</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">intx_choose_qparams_algorithm</span> <span class="o">==</span> <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;IntxChooseQParamsAlgorithm.AFFINE is the only supported algorithm for version 1&quot;</span>
    <span class="p">)</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Config Deprecation: version 1 of Int8DynamicActivationIntxWeightConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2967 for more details&quot;</span>
    <span class="p">)</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">weight_dtype</span><span class="p">]</span>

    <span class="c1"># We quantize with QDQLayout, and then construct the packed weight tensor later</span>
    <span class="c1"># set preserve_zero based on weight mapping type</span>
    <span class="n">preserve_zero</span> <span class="o">=</span> <span class="n">weight_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">weight_mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">weight_scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">QDQLayout</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">QDQLayout</span><span class="p">):</span>
        <span class="c1"># TODO: _int8_asymm_per_token_quant uses scale_dtype=torch.float64, zero_point_dtype=torch.int64,</span>
        <span class="c1"># which is not great for export with QDQLayout.  It is also not consistent with _int8_symm_per_token_quant,</span>
        <span class="c1"># which uses scale_dtype=torch.float32, zero_point_dtype=torch.int32.</span>
        <span class="c1"># Maybe introduce new fp32/int32 versions of _int8_asymm_per_token_quant?</span>
        <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">:</span>
            <span class="n">activation_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>
        <span class="k">elif</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">activation_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported activation mapping type: </span><span class="si">{</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">activation_quant_func</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">):</span>
        <span class="c1"># PackedLinearInt8DynamicActivationIntxWeightLayout has dynamic activation quantization</span>
        <span class="c1"># fused with the kernel and it should not be applied separately</span>
        <span class="k">assert</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PackedLinearInt8DynamicActivationIntxWeightLayout requires act_mapping_type=MappingType.ASYMMETRIC&quot;</span>
        <span class="p">)</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
        <span class="n">groups_per_row</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">group_size</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups_per_row</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups_per_row</span><span class="p">)</span>
        <span class="n">has_weight_zeros</span> <span class="o">=</span> <span class="p">(</span><span class="n">zero_point</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">make_packed_linear_int8_dynamic_activation_intx_weight_tensor</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">scale</span><span class="p">,</span>
            <span class="n">zero_point</span> <span class="k">if</span> <span class="n">has_weight_zeros</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">layout</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
            <span class="n">validate_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># bias is packed with weights if present</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_intx_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">new_weight</span><span class="p">,</span> <span class="n">new_bias</span> <span class="o">=</span> <span class="n">_int8_dynamic_activation_intx_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
        <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">new_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies int4 dynamic per token symmetric activation quantization and int4 per row weight symmetric quantization to linear</span>

<span class="sd">    Args:</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `MarlinQQQLayout()` and `CutlassInt4PackedLayout()` for now</span>
<span class="sd">        `mapping_type`: quantization type for weight, controls the weight quantization is symmetric or asymmetric</span>
<span class="sd">        `act_mapping_type`: quantization type for activation, controls the activation quantization is symmetric or asymmetric</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">()</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int4DynamicActivationInt4WeightConfig&quot;</span>
        <span class="p">)</span>


<span class="c1"># for bc</span>
<span class="n">int4_dynamic_activation_int4_weight</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;int4_dynamic_activation_int4_weight&quot;</span><span class="p">,</span> <span class="n">Int4DynamicActivationInt4WeightConfig</span>
<span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int4DynamicActivationInt4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int4_dynamic_activation_int4_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int4DynamicActivationInt4WeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Only CutlassInt4PackedLayout layout is supported. Received </span><span class="si">{</span><span class="n">layout</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">mapping_type</span> <span class="o">!=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only mapping_type=SYMMETRIC is supported.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">!=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only act_mapping_type=SYMMETRIC is supported.&quot;</span><span class="p">)</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">_int4_symm_cutlass_quant</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="GemliteUIntXWeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.html#torchao.quantization.GemliteUIntXWeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GemliteUIntXWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    applies weight only 4 or 8 bit integer quantization and utilizes the gemlite triton kernel and its associated weight packing format.</span>
<span class="sd">    This only works for fp16 models. 8 bit quantization is symmetric, 4 bit quantization is asymmetric.</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained</span>
<span class="sd">        `bit_width`: bit width of the quantized weight.</span>
<span class="sd">        `packing_bitwidth`: bit width of the packed weight, should be 8 or 32. Can have performance impacts depending on hardware.</span>
<span class="sd">        `mode`: if set to &quot;dynamic&quot;, activations are quantized at runtime; default is &quot;weight_only&quot; (weight-only quantization).</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">bit_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">packing_bitwidth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;weight_only&quot;</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.GemliteUIntXWeightOnlyConfig&quot;</span>
        <span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">gemlite_uintx_weight_only</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;gemlite_uintx_weight_only&quot;</span><span class="p">,</span> <span class="n">GemliteUIntXWeightOnlyConfig</span>
<span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">GemliteUIntXWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_gemlite_uintx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">GemliteUIntXWeightOnlyConfig</span>
<span class="p">):</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">bit_width</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">bit_width</span>
    <span class="n">packing_bitwidth</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">packing_bitwidth</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mode</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.uintx.gemlite_layout</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_gemlite_aqt_kwargs</span>

    <span class="n">use_hqq</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">bit_width</span> <span class="o">==</span> <span class="mi">4</span> <span class="k">else</span> <span class="kc">False</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="o">**</span><span class="n">get_gemlite_aqt_kwargs</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span>
            <span class="n">bit_width</span><span class="o">=</span><span class="n">bit_width</span><span class="p">,</span>
            <span class="n">packing_bitwidth</span><span class="o">=</span><span class="n">packing_bitwidth</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Int4WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int4WeightOnlyConfig.html#torchao.quantization.Int4WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for int4 weight only quantization, only groupwise quantization is supported</span>
<span class="sd">    right now, and we support version 1 and version 2, that are implemented differently although with</span>
<span class="sd">    same support. In version 2, different target are mainly distinguished by `packing_format` arg, and in version 1, mainly by `layout`.</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained, choices are [256, 128, 64, 32], used in both version 1 and 2</span>
<span class="sd">        `int4_packing_format`: the packing format for int4 tensor, used in version 2 only</span>
<span class="sd">         `int4_choose_qparams_algorithm`: variants of choose qparams algorithm to use for int4,</span>
<span class="sd">         currently support TINYGEMM (&quot;tinygemm&quot;) and HQQ (&quot;hqq&quot;), used in version 2 only</span>
<span class="sd">        `layout`: layout type for quantized tensor, default is `TensorCoreTiledLayout(inner_k_tiles=8)`, used in version 1 only</span>
<span class="sd">        `use_hqq`: whether to use hqq or default quantization mode, default is False, used in version 1 only</span>
<span class="sd">        `zero_point_domain`: data type of zeros points, choices are [ZeroPointDomain.FLOAT, ZeroPointDomain.INT, ZeroPointDomain.NONE], used in version 1 only</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values. used in both version 1 and 2</span>
<span class="sd">        `preserve_zero`: whether to preserve zero, default is None. Will be set to True if zero_point_domain is ZeroPointDomain.INT, used in version 1 only</span>
<span class="sd">        `version`: version of the config to use, only subset of above args are valid for version 1, and subset of above args are valid for version 2, default is 2, see note for more details</span>

<span class="sd">    Note:</span>
<span class="sd">        Current state for Int4WeightOnlyConfig is that it supports both v1 (legacy) and v2</span>

<span class="sd">        For v2 (version = 2), only `group_size`, `int4_packing_format`, `int4_choose_qparams_algorithm` and `set_inductor_config` are valid, all other args will be ignored</span>
<span class="sd">        For v1 (version = 1), only `group_size`, `layout`, `use_hqq`, `zero_point_domain`, `preserve_zero` and `set_inductor_config` are valid, we plan to deprecate v1 in torchao 0.15 to make this config</span>
<span class="sd">        less confusing</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorCoreTiledLayout</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorCoreTiledLayout</span><span class="p">(</span><span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">use_hqq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ZeroPointDomain</span><span class="p">]</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">preserve_zero</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># only used in version &gt;= 2</span>
    <span class="n">int4_packing_format</span><span class="p">:</span> <span class="n">Int4PackingFormat</span> <span class="o">=</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PLAIN</span>
    <span class="n">int4_choose_qparams_algorithm</span><span class="p">:</span> <span class="n">Int4ChooseQParamsAlgorithm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">Int4ChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">TINYGEMM</span>
    <span class="p">)</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.Int4WeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="c1"># TODO maybe change other callsites</span>
<span class="n">int4_weight_only</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span><span class="s2">&quot;int4_weight_only&quot;</span><span class="p">,</span> <span class="n">Int4WeightOnlyConfig</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int4_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="c1"># TODO(future PR): perhaps move this logic to a different file, to keep the API</span>
    <span class="c1"># file clean of implementation details</span>

    <span class="c1"># for now, make these local variables to allow the rest of the function</span>
    <span class="c1"># to be a direct copy-paste</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">use_hqq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_hqq</span>
    <span class="n">int4_choose_qparams_algorithm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">int4_choose_qparams_algorithm</span>
    <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">zero_point_domain</span>
    <span class="n">int4_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">int4_packing_format</span>

    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping quantizing weight with int4 weight only quantization because the shape of weight </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not compatible with group_size </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">int4_choose_qparams_algorithm</span> <span class="o">==</span> <span class="n">Int4ChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">HQQ</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">int4_packing_format</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">TILE_PACKED_TO_4D</span><span class="p">,</span>
                <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">OPAQUE</span><span class="p">,</span>
            <span class="p">],</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Int4ChooseQParamsAlgorithm.HQQ is not supported by packing format </span><span class="si">{</span><span class="n">int4_packing_format</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;it&#39;s only supported by Int4PackingFormat.TILE_PACKED_TO_4D and Int4PackingFormat.OPAQUE currently&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PRESHUFFLED</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4PreshuffledTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PLAIN</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PLAIN_INT32</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4PlainInt32Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">MARLIN_SPARSE</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4MarlinSparseTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">OPAQUE</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4OpaqueTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">int4_choose_qparams_algorithm</span><span class="o">=</span><span class="n">int4_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">TILE_PACKED_TO_4D</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4TilePackedTo4dTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">int4_choose_qparams_algorithm</span><span class="o">=</span><span class="n">int4_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported int4 packing format: </span><span class="si">{</span><span class="n">int4_packing_format</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span>

    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Config Deprecation: version 1 of Int4WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2948 for more details&quot;</span>
    <span class="p">)</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int4CPULayout</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="p">)</span>

    <span class="c1"># nonlocal zero_point_domain</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)</span> <span class="ow">in</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Only support layout: </span><span class="si">{</span><span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
        <span class="c1"># the first value is the default one</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="ow">in</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Layout only support </span><span class="si">{</span><span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="n">layout</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int4XPULayout</span><span class="p">):</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>

    <span class="n">preserve_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">config</span><span class="o">.</span><span class="n">preserve_zero</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">preserve_zero</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">LAYOUT_TO_PRESERVE_ZEROS</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="c1"># Sparse Marlin only supports symmetric quantization.</span>
    <span class="c1"># NOTE: If we start having lots of layouts that require different configurations,</span>
    <span class="c1"># we should consider moving this logic somewhere else.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">):</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="k">assert</span> <span class="n">group_size</span> <span class="o">==</span> <span class="mi">128</span> <span class="ow">or</span> <span class="n">group_size</span> <span class="o">==</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;MarlinSparseLayout only supports 128 group size or per channel quantization, got </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int4WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int4_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int4WeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int4_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Float8DynamicActivationInt4WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8DynamicActivationInt4WeightConfig.html#torchao.quantization.Float8DynamicActivationInt4WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration for apply float8 dynamic per row quantization and int4</span>
<span class="sd">    per group weight quantization to linear</span>
<span class="sd">    (only group_size 128 is supported right now since underlying kernel used only supports 128</span>
<span class="sd">    and above and no benefits of making it bigger)</span>

<span class="sd">    Args:</span>
<span class="sd">        `int4_packing_format`: how the weight is packed, only preshuffled is supported</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">int4_packing_format</span><span class="p">:</span> <span class="n">Int4PackingFormat</span> <span class="o">=</span> <span class="s2">&quot;preshuffled&quot;</span></div>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationInt4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_int4_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationInt4WeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">int4_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">int4_packing_format</span>

    <span class="k">assert</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="s2">&quot;preshuffled&quot;</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;only preshuffled int4_packing_format supported right now, got: </span><span class="si">{</span><span class="n">int4_packing_format</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4PreshuffledTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Int8WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8WeightOnlyConfig.html#torchao.quantization.Int8WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 weight-only symmetric per-channel quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        group_size: Optional[int] = None - Controls the granularity of quantization. If None, applies per-channel quantization.</span>
<span class="sd">            Otherwise, applies per-group quantization with the specified group size.</span>
<span class="sd">        set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values</span>
<span class="sd">            for better performance with this quantization scheme.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.Int8WeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">int8_weight_only</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span><span class="s2">&quot;int8_weight_only&quot;</span><span class="p">,</span> <span class="n">Int8WeightOnlyConfig</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_weight_only_transform</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int8WeightOnlyConfig</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int8_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_reduced_range_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_reduced_range_quant_noop_decode</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_cutlass_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=-</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">CutlassInt4PackedLayout</span><span class="p">(),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_cutlass_quant</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_cutlass_quant_sparse</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">CutlassSemiSparseLayout</span><span class="p">(),</span>
    <span class="p">)</span>


<div class="viewcode-block" id="Int8DynamicActivationInt8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.html#torchao.quantization.Int8DynamicActivationInt8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationInt8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 dynamic symmetric per-token activation and int8 per-channel weight</span>
<span class="sd">    quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        layout: Optional[Layout] = PlainLayout() - Tensor layout for the quantized weights. Controls how the</span>
<span class="sd">            quantized data is stored and accessed.</span>
<span class="sd">        act_mapping_type: Optional[MappingType] = MappingType.SYMMETRIC - Mapping type for activation quantization.</span>
<span class="sd">            SYMMETRIC uses symmetric quantization around zero.</span>
<span class="sd">        weight_only_decode: bool = False - If True, only quantizes weights during forward pass and keeps activations</span>
<span class="sd">            in original precision during decode operations.</span>
<span class="sd">        set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values</span>
<span class="sd">            for better performance with this quantization scheme.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Layout</span><span class="p">]</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MappingType</span><span class="p">]</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">weight_only_decode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int8DynamicActivationInt8WeightConfig&quot;</span>
        <span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">int8_dynamic_activation_int8_weight</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;int8_dynamic_activation_int8_weight&quot;</span><span class="p">,</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int8_weight_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="n">weight_only_decode</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_only_decode</span>

    <span class="n">in_features</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># int8 dynamic quantization only has benefit when in_feature &gt; 16</span>
    <span class="k">if</span> <span class="n">in_features</span> <span class="o">&lt;=</span> <span class="mi">16</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping applying Int8DynamicActivationInt8WeightConfig to weight of shape </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; because `in_feature` is &lt;= 16: </span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="c1"># weight settings</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">weight_zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>

    <span class="k">if</span> <span class="n">weight_only_decode</span><span class="p">:</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant_noop_decode</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># input settings</span>
        <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_weight_block_size</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">weight_zero_point_domain</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationInt8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 dynamic activation int8 weight quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot;but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int8_dynamic_activation_int8_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">int8_dynamic_activation_int8_semi_sparse_weight</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies int8 dnynamic symmetric per-token activation and int8 per-channel weight</span>
<span class="sd">    quantization + 2:4 sparsity to linear layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;int8_dyanmic_activation_int8_semi_sparse_weight() will be deprecated at a later release. Please use the layout kwarg in Int8DynamicActivationInt8WeightConfig instead.</span>

<span class="sd">    from torchao.dtypes import SemiSparseLayout</span>
<span class="sd">    Int8DynamicActivationInt8WeightConfig(layout=SemiSparseLayout()&quot;&quot;&quot;</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">SemiSparseLayout</span><span class="p">())</span>


<div class="viewcode-block" id="Float8WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8WeightOnlyConfig.html#torchao.quantization.Float8WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 weight-only symmetric per-channel quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">        version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Float8Tensor (default)</span>

<span class="sd">    Note:</span>
<span class="sd">        The actual matmul will be computed in original precision of the weight tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.Float8WeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">float8_weight_only</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;float8_weight_only&quot;</span><span class="p">,</span> <span class="n">Float8WeightOnlyConfig</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_weight_only_quant_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Config Deprecation: version 1 of Float8WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2649 for more details&quot;</span>
        <span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_floatx</span>

        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unexpected version: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Float8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">float8_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span> <span class="n">granularity</span><span class="o">=</span><span class="n">PerRow</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8WeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_float8_weight_only_quant_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_input_activation_quant_func_fp8</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">activation_granularity</span><span class="p">:</span> <span class="n">FP8Granularity</span><span class="p">,</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function is used to quantize the input activation tensor for an aqt_float variant. If scale</span>
<span class="sd">    is not provided it will be dynamically calculate the scales otherwise it will use the provided scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Zero point is not supported for dynamic FP8 quantization&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerRow</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PerRow quantization only works for bfloat16 precision input activation&quot;</span>
        <span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">activation_granularity</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Config is stored on weight</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerTensor</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Static quantization only supports PerTensor granularity&quot;</span>
        <span class="p">)</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx_static</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Config is stored on weight</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">activation</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check if a weight tensor meets float8 quantization requirements.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (torch.Tensor): The weight tensor to check</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if the tensor can be quantized to float8, False otherwise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="mi">2</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;float8 quantization only works for 2/3-D tensors, got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">D tensor&quot;</span>

    <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
    <span class="n">is_compatible</span> <span class="o">=</span> <span class="p">(</span><span class="n">in_dim</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">out_dim</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_compatible</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping float8 quantization: weight shape </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not compatible with _scaled_mm. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Both input dimension (</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s2">) and output dimension (</span><span class="si">{</span><span class="n">out_dim</span><span class="si">}</span><span class="s2">) must be multiples of 16. &quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">is_compatible</span>


<div class="viewcode-block" id="Float8DynamicActivationFloat8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.html#torchao.quantization.Float8DynamicActivationFloat8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationFloat8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 dynamic symmetric quantization to both activations and weights of linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_dtype (torch.dtype): The target data type for activation quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        granularity (Optional[Union[FP8Granularity, List[FP8Granularity]]]):</span>
<span class="sd">            The granularity for quantization. Can be either a single granularity (applied to both</span>
<span class="sd">            activations and weights) or a tuple of two granularities (one for activations, one for weights).</span>
<span class="sd">            If None, defaults to PerTensor for both. Currently both quantizations need to be the same type. And</span>
<span class="sd">            only PerTensor and PerRow are supported.</span>
<span class="sd">        mm_config (Float8MMConfig): Configuration for the matrix multiplication. Default uses fast accumulation.</span>
<span class="sd">        activation_value_lb (Optional[float]): the lower bound for activation value for calculating scale</span>
<span class="sd">        activation_value_ub (Optional[float]): the upper bound for activation value for calculating scale</span>
<span class="sd">        kernel_preference (KernelPreference): kernel preference for ops like matmul, grouped matmul etc. by defalut (KernelPreference.AUTO) it will be chosen for user based on hardware or other information, this only needs to be set in weight</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">        version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Float8Tensor (default)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Float8MMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">activation_value_lb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">activation_value_ub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">kernel_preference</span><span class="p">:</span> <span class="n">KernelPreference</span> <span class="o">=</span> <span class="n">KernelPreference</span><span class="o">.</span><span class="n">AUTO</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Float8DynamicActivationFloat8WeightConfig&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="o">=</span> <span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">_normalize_granularity</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span> <span class="o">=</span> <span class="p">[</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span><span class="p">]</span></div>


<span class="c1"># for bc</span>
<span class="n">float8_dynamic_activation_float8_weight</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;float8_dynamic_activation_float8_weight&quot;</span><span class="p">,</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_weight_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mm_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mm_config</span>
    <span class="n">activation_value_lb</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_value_lb</span>
    <span class="n">activation_value_ub</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_value_ub</span>
    <span class="n">kernel_preference</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">kernel_preference</span>

    <span class="c1"># Ensure works on device</span>
    <span class="n">_check_hardware_support</span><span class="p">(</span><span class="n">granularity</span><span class="p">)</span>
    <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">granularity</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="c1"># TODO(future PR): this should really throw an exception instead of silently</span>
        <span class="c1"># not doing what the user asked</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerRow</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PerRow quantization only works for bfloat16 precision input weight&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Config Deprecation: version 1 of Float8DynamicActivationFloat8WeightConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2649 for more details&quot;</span>
        <span class="p">)</span>

        <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">weight_granularity</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">block_size</span><span class="p">))</span>
        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
        <span class="n">input_quant_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="n">activation_granularity</span><span class="p">,</span>
            <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
            <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">,</span> <span class="n">quant_kwargs</span><span class="o">=</span><span class="n">input_quant_kwargs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unexpected version: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">act_quant_kwargs</span> <span class="o">=</span> <span class="n">QuantizeTensorToFloat8Kwargs</span><span class="p">(</span>
            <span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">activation_granularity</span><span class="p">,</span>
            <span class="n">hp_value_lb</span><span class="o">=</span><span class="n">activation_value_lb</span><span class="p">,</span>
            <span class="n">hp_value_ub</span><span class="o">=</span><span class="n">activation_value_ub</span><span class="p">,</span>
            <span class="n">kernel_preference</span><span class="o">=</span><span class="n">kernel_preference</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">Float8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">float8_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">granularity</span><span class="o">=</span><span class="n">weight_granularity</span><span class="p">,</span>
            <span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">,</span>
            <span class="n">kernel_preference</span><span class="o">=</span><span class="n">kernel_preference</span><span class="p">,</span>
            <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">act_quant_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">quantized_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_89</span><span class="p">()</span> <span class="ow">or</span> <span class="n">is_MI300</span><span class="p">(),</span> <span class="p">(</span>
        <span class="s2">&quot;Float8 dynamic activation quantization is only supported on CUDA&gt;=8.9 and MI300+&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying float8 dynamic activation quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;but </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">_float8_dynamic_activation_float8_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationFloat8SemiSparseWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies float8 dynamic quantization to activations and float8 quantization followed by compression to sparse semi-structured tensor to weights of linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `CutlassSemiSparseLayout` at the moment.</span>
<span class="sd">        `activation_dtype`: data type for quantized activation tensor.</span>
<span class="sd">        `weight_dtype`: data type for quantized weight tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">CutlassSemiSparseLayout</span><span class="p">()</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e5m2_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Float8DynamicActivationFloat8SemiSparseWeightConfig&quot;</span>
        <span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationFloat8SemiSparseWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_semi_sparse_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationFloat8SemiSparseWeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_90</span><span class="p">(),</span> <span class="s2">&quot;Float8 quantization is only supported on CUDA&gt;=9.0&quot;</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassSemiSparseLayout</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Only CutlassSemiSparseLayout layout is supported. Received </span><span class="si">{</span><span class="n">layout</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">_float8_cutlass_quant_sparse</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant</span><span class="p">,</span>
        <span class="n">quant_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;target_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Float8StaticActivationFloat8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.html#torchao.quantization.Float8StaticActivationFloat8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8StaticActivationFloat8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 static symmetric quantization to</span>

<span class="sd">    Args:</span>
<span class="sd">        scale (torch.Tensor): The scale tensor for activation quantization.</span>
<span class="sd">        activation_dtype (torch.dtype): The target data type for activation quantization. Default is torch.float8_e4m</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m</span>
<span class="sd">        mm_config (Float8MMConfig): Configuration for the matrix multiplication. Default uses fast accumulation.</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">FP8Granularity</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Float8MMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Float8StaticActivationFloat8WeightConfig&quot;</span>
        <span class="p">)</span></div>


<span class="c1"># for bc</span>
<span class="n">float8_static_activation_float8_weight</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;float8_static_activation_float8_weight&quot;</span><span class="p">,</span> <span class="n">Float8StaticActivationFloat8WeightConfig</span>
<span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8StaticActivationFloat8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_static_activation_float8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8StaticActivationFloat8WeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_89</span><span class="p">()</span> <span class="ow">or</span> <span class="n">is_MI300</span><span class="p">(),</span> <span class="p">(</span>
        <span class="s2">&quot;Float8 static activation quantization is only supported on CUDA 8.9 and above&quot;</span>
    <span class="p">)</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mm_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mm_config</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">_normalize_granularity</span><span class="p">(</span><span class="n">granularity</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerTensor</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;Static quantization only supports PerTensor granularity&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="c1"># TODO(future PR): this should really throw an exception instead of silently</span>
        <span class="c1"># not doing what the user asked</span>
        <span class="k">return</span> <span class="n">module</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">weight_granularity</span><span class="p">)</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
    <span class="n">input_quant_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="n">activation_granularity</span><span class="p">,</span>
        <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_weight_tensor_with_linear_activation_quantization_metadata</span><span class="p">(</span>
        <span class="n">quantized_weight</span><span class="p">,</span>
        <span class="n">input_quant_func</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_kwargs</span><span class="o">=</span><span class="n">input_quant_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="UIntXWeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.UIntXWeightOnlyConfig.html#torchao.quantization.UIntXWeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">UIntXWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying uintx weight-only asymmetric per-group quantization to linear layers, using uintx quantization where</span>
<span class="sd">    x is the number of bits specified by `dtype`</span>

<span class="sd">    Args:</span>
<span class="sd">        `dtype`: torch.uint1 to torch.uint7 sub byte dtypes</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained, defaults to 64</span>
<span class="sd">        `pack_dim`: the dimension we use for packing, defaults to -1</span>
<span class="sd">        `use_hqq`: whether to use hqq algorithm or the default algorithm to quantize the weight</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">pack_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">use_hqq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.UIntXWeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">uintx_weight_only</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span>
    <span class="s2">&quot;uintx_weight_only&quot;</span><span class="p">,</span> <span class="n">UIntXWeightOnlyConfig</span>
<span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">UIntXWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_uintx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">UIntXWeightOnlyConfig</span>
<span class="p">):</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">pack_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pack_dim</span>
    <span class="n">use_hqq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_hqq</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span>

    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint1</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint3</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint5</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint6</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint7</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">assert</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">SUPPORTED_DTYPES</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported dtype for hqq: </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_hqq</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Recommended to use `Int4WeightOnlyConfig(group_size, use_hqq=True, version=1)` for the best performance&quot;</span>
            <span class="p">)</span>
        <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">dtype</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span>
        <span class="n">preserve_zero</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span>
        <span class="n">preserve_zero</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="n">UintxLayout</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">pack_dim</span><span class="o">=</span><span class="n">pack_dim</span><span class="p">)</span>

    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
        <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_adjust_scale_dtype_in_intx_unpacked_tensor</span><span class="p">(</span>
    <span class="n">intx_unpacked_tensor</span><span class="p">:</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="p">,</span>
    <span class="n">hp_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adjusts the scale_dtype on IntxUnpackedToInt8Tensor.</span>
<span class="sd">    Updating the scale dtype requires updating the qdata because qdata is calculated after the scale.</span>
<span class="sd">    This is used in IntxWeightOnlyConfig and Int8DynamicActivationIntxWeightConfig to make</span>
<span class="sd">    version=2 and version=1 numerically equivalent when the scale_dtype differs from the input dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">intx_unpacked_tensor</span><span class="p">,</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="p">)</span>
    <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">scale_dtype</span><span class="p">)</span>
    <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">]</span>
    <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">qdata</span> <span class="o">=</span> <span class="n">quantize_affine</span><span class="p">(</span>
        <span class="n">hp_tensor</span><span class="p">,</span>
        <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">zero_point</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">qmin</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">qmax</span><span class="p">,</span>
    <span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">IntxWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for quantizing weights to torch.intx, with 1 &lt;= x &lt;= 8.</span>
<span class="sd">    Weights are quantized with scales/zeros in a groupwise or channelwise</span>
<span class="sd">    manner using the number of bits specified by weight_dtype.</span>
<span class="sd">    args:</span>
<span class="sd">        `weight_dtype`: The dtype to use for weight quantization.  Must be torch.intx, where 1 &lt;= x &lt;= 8.</span>
<span class="sd">        `granularity`: The granularity to use for weight quantization.  Must be PerGroup or PerAxis(0).</span>
<span class="sd">        `mapping_type`: The type of mapping to use for the weight quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.</span>
<span class="sd">        `scale_dtype`: The dtype to use for the weight scale.</span>
<span class="sd">        `layout`: The layout to use for the packed weight tensor:</span>
<span class="sd">            - QDQLayout: this layout is designed for export to ExecuTorch.this layout represents the quantization with Q/DQ quant primitives,</span>
<span class="sd">                and is intended for export applications like ExecuTorch.</span>
<span class="sd">        `intx_packing_format`: The format to use for the packed weight tensor (version 2 only).</span>
<span class="sd">        `intx_choose_qparams_algorithm`: The algorithm to use for choosing the quantization parameters.</span>
<span class="sd">        `version`: version of the config to use, only subset of above args are valid based on version, see note for more details.</span>

<span class="sd">        Note:</span>

<span class="sd">        Current state for IntxWeightOnlyConfig is that it supports both v1 (legacy) and v2.</span>

<span class="sd">        * `intx_packing_format` is used for version 2.</span>
<span class="sd">        * `layout` is only used for version 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerAxis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">QDQLayout</span><span class="p">()</span>
    <span class="n">intx_packing_format</span><span class="p">:</span> <span class="n">IntxPackingFormat</span> <span class="o">=</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span>
    <span class="n">intx_choose_qparams_algorithm</span><span class="p">:</span> <span class="n">IntxChooseQParamsAlgorithm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span>
    <span class="p">)</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.IntxWeightOnlyConfig&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;int</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_dtype must be torch.intx, where 1 &lt;= x &lt;= 8, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="p">(</span><span class="n">PerAxis</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">)),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;granularity must be PerAxis or PerGroup, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;mapping_type must be MappingType.ASYMMETRIC, MappingType.SYMMETRIC, or MappingType.SYMMETRIC_NO_CLIPPING_ERR, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_intx_weight_only_quantize_tensor</span><span class="p">(</span>
    <span class="n">weight</span><span class="p">,</span>
    <span class="n">config</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_dtype</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">intx_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_packing_format</span>
    <span class="n">intx_choose_qparams_algorithm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_choose_qparams_algorithm</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;IntxWeightOnlyConfig only works for 2-d Tensor, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">):</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">granularity</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;granularity must be PerGroup or PerAxis, got </span><span class="si">{</span><span class="n">granularity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_packing_format</span> <span class="o">==</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">custom_zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
                <span class="n">custom_zero_point</span> <span class="o">=</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">weight_dtype</span><span class="p">,</span>
                <span class="n">mapping_type</span><span class="o">=</span><span class="n">mapping_type</span><span class="p">,</span>
                <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
                <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
                <span class="n">intx_choose_qparams_algorithm</span><span class="o">=</span><span class="n">intx_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_dtype</span> <span class="o">!=</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="n">_adjust_scale_dtype_in_intx_unpacked_tensor</span><span class="p">(</span>
                    <span class="n">new_weight</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">scale_dtype</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported packing format: </span><span class="si">{</span><span class="n">intx_packing_format</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Version 1</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_choose_qparams_algorithm</span> <span class="o">==</span> <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;version 1 only supports affine algorithm&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Config Deprecation: version 1 of IntxWeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2967 for more details&quot;</span>
    <span class="p">)</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">weight_dtype</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="p">(</span><span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">),</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">IntxWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_intx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">IntxWeightOnlyConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying intx weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_intx_weight_only_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
        <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_embedding_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="FPXWeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.FPXWeightOnlyConfig.html#torchao.quantization.FPXWeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FPXWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sub-byte floating point dtypes defined by `ebits`: exponent bits and `mbits`: mantissa bits</span>
<span class="sd">    e.g. fp6_e3_m2, fp6_e2_m3, ...</span>
<span class="sd">    The packing format and kernels are from the fp6-llm paper: https://arxiv.org/abs/2401.14112</span>
<span class="sd">    github repo: https://github.com/usyd-fsalab/fp6_llm, now renamed to quant-llm</span>
<span class="sd">    For more details for packing please see: :class:`~torchao.dtypes.fpx.FpxTensorCoreAQTTensorImpl`</span>

<span class="sd">    This is experimental, will be merged with `to_affine_quantized_floatx`</span>
<span class="sd">    in the future</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.FPXWeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="c1"># for BC</span>
<span class="n">fpx_weight_only</span> <span class="o">=</span> <span class="n">_ConfigDeprecationWrapper</span><span class="p">(</span><span class="s2">&quot;fpx_weight_only&quot;</span><span class="p">,</span> <span class="n">FPXWeightOnlyConfig</span><span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">FPXWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_fpx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FPXWeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">ebits</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ebits</span>
    <span class="n">mbits</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mbits</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_fpx</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.floatx</span><span class="w"> </span><span class="kn">import</span> <span class="n">FloatxTensorCoreLayout</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;floatx only works for 2-d Tensor, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">in_dim</span> <span class="o">%</span> <span class="mi">64</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">out_dim</span> <span class="o">%</span> <span class="mi">256</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping floatx quantization float</span><span class="si">{</span><span class="n">ebits</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">mbits</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">ebits</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">mbits</span><span class="si">}</span><span class="s2"> because &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;the shape is not compatible with the kernel: in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s2">, out_dim=</span><span class="si">{</span><span class="n">out_dim</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;expected in_dim % 64 == 0 and out_dim % 256 == 0&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span>

    <span class="n">_layout</span> <span class="o">=</span> <span class="n">FloatxTensorCoreLayout</span><span class="p">(</span><span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_fpx</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ModuleFqnToConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Per module configurations for torchao quantize_ API</span>

<span class="sd">    Args:</span>
<span class="sd">        `module_fqn_to_config`: typing.OrderedDict[str, Optional[AOBaseConfig]]: an</span>
<span class="sd">         ordered dictionary from</span>
<span class="sd">             (1). fully qualified name (fqn) of module or</span>
<span class="sd">             (2). regex of fully qualified name (in python `re` module regex format), should</span>
<span class="sd">                  start with prefix &quot;re:&quot; or</span>
<span class="sd">             (3). &quot;_default&quot;</span>
<span class="sd">         to the config that we want to apply to the module or None</span>

<span class="sd">         Config key ordered by precedence:</span>
<span class="sd">           * fully qualified module name, e.g. `language.layers.0.q_proj`</span>
<span class="sd">           * regex for module names, must start with `re:`, e.g. `re:language\.layers\..+\.q_proj`,</span>
<span class="sd">             whiever regex fully matches the module fqn first will be applied</span>
<span class="sd">             (order of keys for dictionary are kept consistent since we are using OrderedDict)</span>
<span class="sd">           * &quot;_default&quot;, fallback for **all modules** if no match for all previous keys</span>
<span class="sd">             (Note, when using `_default`, the config is applied to all modules, to apply</span>
<span class="sd">              it to only a subset of modules, e.g. with some types, it&#39;s better to filter</span>
<span class="sd">              the modules that we don&#39;t want to quantize before hand and configure them to</span>
<span class="sd">              None, e.g. `{&quot;re:.+norm.+&quot;: None, &quot;_default&quot;: linear_config}`)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">module_fqn_to_config</span><span class="p">:</span> <span class="n">OrderedDictType</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AOBaseConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">OrderedDict</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.ModuleFqnToConfig&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_module_fqn_to_config_handler</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">module_fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ModuleFqnToConfig</span>
<span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">module_fqn</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">:</span>
        <span class="c1"># Maybe: we can add module type specific config in the future, in needed</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">[</span><span class="n">module_fqn</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">maybe_module_fqn_pattern</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">maybe_module_fqn_pattern</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;re:&quot;</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="n">re</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">maybe_module_fqn_pattern</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">module_fqn</span><span class="p">):</span>
                <span class="c1"># we&#39;ll apply the config for first fully matched pattern</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">[</span><span class="n">maybe_module_fqn_pattern</span><span class="p">]</span>
                <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># fallback to use default if no module specific config is provided</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_default&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">module</span>


<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">_int8_asymm_per_token_quant</span><span class="p">,</span>
        <span class="n">_int8_symm_per_token_reduced_range_quant</span><span class="p">,</span>
        <span class="n">_input_activation_quant_func_fp8</span><span class="p">,</span>
        <span class="n">_int4_symm_cutlass_quant</span><span class="p">,</span>
        <span class="n">_int8_symm_cutlass_quant</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant_sparse</span><span class="p">,</span>
        <span class="n">Target</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
         <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
         <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
         <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>