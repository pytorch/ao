# Configuration for training benchmarks with float8 and other low precision dtypes
benchmark_mode: "training"
output_dir: "benchmarks/microbenchmarks/results/training"

# Float8 training specific configurations
quantization_config_recipe_names:
  - "float8dq-tensor"
  # - "float8dq-row"
  # - "baseline"  # Always include baseline for comparison

# Training specific configurations
scaling_type_input: "dynamic"
scaling_type_weight: "dynamic"
scaling_type_grad_output: "dynamic"
scaling_granularity: "tensorwise"
use_fast_accum: true
repeat_n: 100  # Number of iterations for benchmarking

model_params:
  - name: "float8_linear_training"
    matrix_shapes:
      - name: "custom"
        shapes: [
          [1024, 1024, 1024],  # [m, k, n]
          [2048, 4096, 1024],
          [4096, 4096, 1024],
          [16384, 16384, 16384],
        ]
      # - name: "sweep"
    high_precision_dtype: "torch.bfloat16"
    use_torch_compile: true
    torch_compile_mode: "max-autotune"
    device: "cuda"
    model_type: "linear"
    enable_profiler: true
    enable_memory_profiler: true

  # - name: "float8_ln_linear_training"
  #   matrix_shapes:
  #     - name: "custom"
  #       shapes: [
  #         [2048, 4096, 1024],
  #       ]
  #   high_precision_dtype: "torch.bfloat16"
  #   use_torch_compile: true
  #   torch_compile_mode: "max-autotune"
  #   device: "cuda"
  #   model_type: "ln_linear_relu"
  #   enable_profiler: true
  #   enable_memory_profiler: true

  # - name: "float8_transformer_block_training"
  #   matrix_shapes:
  #     - name: "custom"
  #       shapes: [
  #         [32, 4096, 4096],  # For transformer_block, k is the hidden dimension
  #       ]
  #   high_precision_dtype: "torch.bfloat16"
  #   use_torch_compile: true
  #   torch_compile_mode: "max-autotune"
  #   device: "cuda"
  #   model_type: "transformer_block"
  #   enable_profiler: true
  #   enable_memory_profiler: true

  # - name: "float8_llama_shapes_training"
  #   matrix_shapes:
  #     - name: "llama"  # Using LLaMa shapes
  #   high_precision_dtype: "torch.bfloat16"
  #   use_torch_compile: false  # Disable torch.compile for LLaMA shapes to avoid CUDA graph issues
  #   device: "cuda"
  #   model_type: "linear"
  #   enable_profiler: false  # Disable profiling for large models to avoid memory issues
  #   enable_memory_profiler: false
