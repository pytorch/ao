# Benchmark configuration for microbenchmarks
benchmark_mode: "inference"
quantization_config_recipe_names: # Will run a baseline inference for model by default, without quantization for comparison
  # - "int8wo" TODO: Re-enable once we debug the delay in the benchmark
  - "int8dq"
  - "float8dq-tensor"
  - "float8dq-row"
  - "float8wo"
output_dir: "benchmarks/microbenchmarks/results"
model_params:
  - name: "small_bf16_linear"
    matrix_shapes:
      - name: "llama4"
      - name: "deepseek_v3_236b"
      - name: "deepseek_v3_671b"
      - name: "qwen3_32b"
      - name: "gemma3_27b"
    high_precision_dtype: "torch.bfloat16"
    torch_compile_mode: "max-autotune"
    device: "cuda"
    model_type: "linear"
    enable_memory_profiler: true
