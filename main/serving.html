


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>(Part 3) Serving on vLLM, SGLang, ExecuTorch &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Integration with VLLM: Architecture and Usage Guide" href="torchao_vllm_integration.html" />
    <link rel="prev" title="(Part 2) Fine-tuning with QAT, QLoRA, and float8" href="finetuning.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributor_guide.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking_api_guide.html">Benchmarking API Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking_user_guide.html">Benchmarking User Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_ref_float8.html">torchao.float8</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Eager Quantization Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pretraining.html">(Part 1) Pre-training with float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">(Part 2) Fine-tuning with QAT, QLoRA, and float8</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">(Part 3) Serving on vLLM, SGLang, ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchao_vllm_integration.html">Integration with VLLM: Architecture and Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="static_quantization.html">Static Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PT2E Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials_source/pt2e_quant_ptq.html">PyTorch 2 Export Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials_source/pt2e_quant_qat.html">PyTorch 2 Export Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials_source/pt2e_quant_x86_inductor.html">PyTorch 2 Export Quantization with X86 Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials_source/pt2e_quant_xpu_inductor.html">PyTorch 2 Export Quantization with Intel GPU Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials_source/pt2e_quant_openvino_inductor.html">PyTorch 2 Export Quantization for OpenVINO torch.compile Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials_source/pt2e_quantizer.html">How to Write a <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code> for PyTorch 2 Export Quantization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>(Part 3) Serving on vLLM, SGLang, ExecuTorch</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/serving.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="part-3-serving-on-vllm-sglang-executorch">
<h1>(Part 3) Serving on vLLM, SGLang, ExecuTorch<a class="headerlink" href="#part-3-serving-on-vllm-sglang-executorch" title="Permalink to this heading">¶</a></h1>
<p>TorchAO provides an end-to-end pre-training, fine-tuning, and serving model optimization flow by leveraging our quantization and sparsity techniques integrated into our partner frameworks. This is part 3 of 3 such tutorials showcasing this end-to-end flow, focusing on the serving step.</p>
<img alt="_images/e2e_flow_part3.png" src="_images/e2e_flow_part3.png" />
<p>This tutorial demonstrates how to perform post-training quantization and deploy models for inference using torchao as the underlying optimization engine, seamlessly integrated through HuggingFace Transformers, vLLM, and ExecuTorch.</p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#post-training-quantization-with-huggingface" id="id1">Post-training Quantization with HuggingFace</a></p></li>
<li><p><a class="reference internal" href="#serving-and-inference" id="id2">Serving and Inference</a></p>
<ul>
<li><p><a class="reference internal" href="#serving-and-inference-with-vllm" id="id3">Serving and Inference with vLLM</a></p></li>
<li><p><a class="reference internal" href="#serving-and-inference-with-sglang" id="id4">Serving and Inference with SGLang</a></p></li>
<li><p><a class="reference internal" href="#inference-with-transformers" id="id5">Inference with Transformers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#mobile-deployment-with-executorch" id="id6">Mobile Deployment with ExecuTorch</a></p>
<ul>
<li><p><a class="reference internal" href="#optional-untie-embedding-weights" id="id7">[Optional] Untie Embedding Weights</a></p></li>
<li><p><a class="reference internal" href="#step-1-create-mobile-optimized-quantization" id="id8">Step 1: Create Mobile-Optimized Quantization</a></p></li>
<li><p><a class="reference internal" href="#step-2-export-to-executorch" id="id9">Step 2: Export to ExecuTorch</a></p></li>
<li><p><a class="reference internal" href="#mobile-performance-characteristics" id="id10">Mobile Performance Characteristics</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#evaluation" id="id11">Evaluation</a></p>
<ul>
<li><p><a class="reference internal" href="#model-quality-assessment" id="id12">Model Quality Assessment</a></p></li>
<li><p><a class="reference internal" href="#memory-benchmarking" id="id13">Memory Benchmarking</a></p></li>
<li><p><a class="reference internal" href="#performance-benchmarking" id="id14">Performance Benchmarking</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id15">Conclusion</a></p></li>
</ul>
</nav>
<section id="post-training-quantization-with-huggingface">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Post-training Quantization with HuggingFace</a><a class="headerlink" href="#post-training-quantization-with-huggingface" title="Permalink to this heading">¶</a></h2>
<p>HuggingFace Transformers provides seamless integration with torchao quantization. The <code class="docutils literal notranslate"><span class="pre">TorchAoConfig</span></code> automatically applies torchao’s optimized quantization algorithms during model loading.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/huggingface/transformers@main
pip<span class="w"> </span>install<span class="w"> </span>--pre<span class="w"> </span>torchao<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/nightly/cu126
pip<span class="w"> </span>install<span class="w"> </span>torch
pip<span class="w"> </span>install<span class="w"> </span>accelerate
</pre></div>
</div>
<p>For this example, we’ll use <code class="docutils literal notranslate"><span class="pre">Float8DynamicActivationFloat8WeightConfig</span></code> on the Phi-4 mini-instruct model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TorchAoConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">,</span> <span class="n">PerRow</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-4-mini-instruct&quot;</span>

<span class="n">quant_config</span> <span class="o">=</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">(</span><span class="n">granularity</span><span class="o">=</span><span class="n">PerRow</span><span class="p">())</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">TorchAoConfig</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># Push the model to hub</span>
<span class="n">USER_ID</span> <span class="o">=</span> <span class="s2">&quot;YOUR_USER_ID&quot;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">save_to</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-float8dq&quot;</span>
<span class="n">quantized_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on supported quantization and sparsity configurations, see <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/quantization/torchao">HF-Torchao Docs</a>.</p>
</div>
</section>
<section id="serving-and-inference">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Serving and Inference</a><a class="headerlink" href="#serving-and-inference" title="Permalink to this heading">¶</a></h2>
<section id="serving-and-inference-with-vllm">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Serving and Inference with vLLM</a><a class="headerlink" href="#serving-and-inference-with-vllm" title="Permalink to this heading">¶</a></h3>
<p>vLLM automatically leverages torchao’s optimized kernels when serving quantized models, providing significant throughput improvements.</p>
<p>First, install vLLM with torchao support:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>vllm<span class="w"> </span>--pre<span class="w"> </span>--extra-index-url<span class="w"> </span>https://wheels.vllm.ai/nightly
pip<span class="w"> </span>install<span class="w"> </span>--pre<span class="w"> </span>torchao<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/nightly/cu126
</pre></div>
</div>
<p>To serve in vLLM, we’re using the model we quantized and pushed to Hugging Face hub in the previous step <a class="reference internal" href="#post-training-quantization-with-huggingface"><span class="std std-ref">Post-training Quantization with HuggingFace</span></a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Server</span>
vllm<span class="w"> </span>serve<span class="w"> </span>pytorch/Phi-4-mini-instruct-float8dq<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>-O3

<span class="c1"># Client</span>
curl<span class="w"> </span>http://localhost:8000/v1/chat/completions<span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;model&quot;: &quot;pytorch/Phi-4-mini-instruct-float8dq&quot;,</span>
<span class="s1">    &quot;messages&quot;: [</span>
<span class="s1">        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;}</span>
<span class="s1">    ],</span>
<span class="s1">    &quot;temperature&quot;: 0.6,</span>
<span class="s1">    &quot;top_p&quot;: 0.95,</span>
<span class="s1">    &quot;top_k&quot;: 20,</span>
<span class="s1">    &quot;max_tokens&quot;: 32768</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p>Serving a float8 dynamic quantized model with vLLM shows 36% VRAM reduction, 1.15x-1.2x inference speedup and little to no accuracy impact on H100. <a class="reference internal" href="#memory-benchmarking"><span class="std std-ref">Memory Benchmarking</span></a> and <a class="reference internal" href="#performance-benchmarking"><span class="std std-ref">Performance Benchmarking</span></a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on vLLM Integration, please refer to the detailed guide <a class="reference internal" href="torchao_vllm_integration.html#torchao-vllm-integration"><span class="std std-ref">Integration with VLLM: Architecture and Usage Guide</span></a>.</p>
</div>
</section>
<section id="serving-and-inference-with-sglang">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Serving and Inference with SGLang</a><a class="headerlink" href="#serving-and-inference-with-sglang" title="Permalink to this heading">¶</a></h3>
<p>(Coming soon!)</p>
</section>
<section id="inference-with-transformers">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Inference with Transformers</a><a class="headerlink" href="#inference-with-transformers" title="Permalink to this heading">¶</a></h3>
<p>Install the required packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/huggingface/transformers@main
pip<span class="w"> </span>install<span class="w"> </span>torchao
pip<span class="w"> </span>install<span class="w"> </span>torch
pip<span class="w"> </span>install<span class="w"> </span>accelerate
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;pytorch/Phi-4-mini-instruct-float8dq&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful AI assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Can you provide ways to eat combinations of bananas and dragonfruits?&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What about solving an 2x + 3 = 7 equation?&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s2">&quot;return_full_text&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="mobile-deployment-with-executorch">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Mobile Deployment with ExecuTorch</a><a class="headerlink" href="#mobile-deployment-with-executorch" title="Permalink to this heading">¶</a></h2>
<p>ExecuTorch enables on-device inference using torchao’s mobile-optimized quantization schemes. The 8da4w (8-bit dynamic activation, 4-bit weight) configuration is specifically designed for mobile deployment. Optionally, before lowering to ExecuTorch, we can finetune a model using QAT <a class="reference internal" href="finetuning.html"><span class="doc">(Part 2) Fine-tuning with QAT, QLoRA, and float8</span></a>, which has demonstrated some improvements in the quality of quantized models.</p>
<section id="optional-untie-embedding-weights">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">[Optional] Untie Embedding Weights</a><a class="headerlink" href="#optional-untie-embedding-weights" title="Permalink to this heading">¶</a></h3>
<p>Optionally, we can quantize the embedding and lm_head differently, since those layers are tied, we first need to untie the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoProcessor</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">find_tied_parameters</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-4-mini-instruct&quot;</span>
<span class="n">untied_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">untied_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tied weights:&quot;</span><span class="p">,</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">untied_model</span><span class="p">))</span>
<span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">untied_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_text_config</span><span class="p">(</span><span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">):</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">untied_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_text_config</span><span class="p">(</span><span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">untied_model</span><span class="o">.</span><span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">untied_model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">untied_model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tied weights:&quot;</span><span class="p">,</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">untied_model</span><span class="p">))</span>

<span class="n">USER_ID</span> <span class="o">=</span> <span class="s2">&quot;YOUR_USER_ID&quot;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">save_to</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>

<span class="n">untied_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>

<span class="c1"># or save locally</span>
<span class="n">save_to_local_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>
<span class="n">untied_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_to_local_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-1-create-mobile-optimized-quantization">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Step 1: Create Mobile-Optimized Quantization</a><a class="headerlink" href="#step-1-create-mobile-optimized-quantization" title="Permalink to this heading">¶</a></h3>
<p>Quantizing the model for mobile deployment using TorchAO’s <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationIntxWeightConfig</span></code> configuration. If we’ve untied the embedding and lm_head following the previous step, we can quantize embedding using <code class="docutils literal notranslate"><span class="pre">IntxWeightOnlyConfig</span></code> configuration, and lm_head using <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationIntxWeightConfig</span></code> configuration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoProcessor</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">TorchAoConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">IntxWeightOnlyConfig</span><span class="p">,</span>
    <span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">,</span>
    <span class="n">ModuleFqnToConfig</span><span class="p">,</span>
    <span class="n">quantize_</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="n">PerGroup</span><span class="p">,</span> <span class="n">PerAxis</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># we start from the model with untied weights</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-4-mini-instruct&quot;</span>
<span class="n">USER_ID</span> <span class="o">=</span> <span class="s2">&quot;YOUR_USER_ID&quot;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">untied_model_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>
<span class="n">untied_model_local_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>

<span class="c1"># embedding_config is required only if we untied the embedding and lm_head in the previous step, else we can use only linear config for quantization</span>
<span class="n">embedding_config</span> <span class="o">=</span> <span class="n">IntxWeightOnlyConfig</span><span class="p">(</span>
    <span class="n">weight_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
    <span class="n">granularity</span><span class="o">=</span><span class="n">PerAxis</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">linear_config</span> <span class="o">=</span> <span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">(</span>
    <span class="n">weight_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">,</span>
    <span class="n">weight_granularity</span><span class="o">=</span><span class="n">PerGroup</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">weight_scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">ModuleFqnToConfig</span><span class="p">({</span><span class="s2">&quot;_default&quot;</span><span class="p">:</span> <span class="n">linear_config</span><span class="p">,</span> <span class="s2">&quot;model.embed_tokens&quot;</span><span class="p">:</span> <span class="n">embedding_config</span><span class="p">})</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">TorchAoConfig</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">include_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">untie_embedding_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">modules_to_not_convert</span><span class="o">=</span><span class="p">[])</span>

<span class="c1"># either use `untied_model_id` or `untied_model_local_path`</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">untied_model_id</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># Push to hub</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">save_to</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-8da4w&quot;</span>
<span class="n">quantized_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-2-export-to-executorch">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Step 2: Export to ExecuTorch</a><a class="headerlink" href="#step-2-export-to-executorch" title="Permalink to this heading">¶</a></h3>
<p>Convert the quantized model to .pte file, which can be run on mobile device.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install ExecuTorch</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/pytorch/executorch.git
<span class="nb">cd</span><span class="w"> </span>executorch
./install_requirements.sh

<span class="c1"># Convert checkpoint format for ExecuTorch</span>
python<span class="w"> </span>-m<span class="w"> </span>executorch.examples.models.phi_4_mini.convert_weights<span class="w"> </span>pytorch_model.bin<span class="w"> </span>pytorch_model_converted.bin

<span class="c1"># Export to PTE format with torchao optimizations preserved</span>
<span class="nv">PARAMS</span><span class="o">=</span><span class="s2">&quot;executorch/examples/models/phi_4_mini/config.json&quot;</span>
python<span class="w"> </span>-m<span class="w"> </span>executorch.examples.models.llama.export_llama<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="s2">&quot;phi_4_mini&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpoint<span class="w"> </span><span class="s2">&quot;pytorch_model_converted.bin&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--params<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$PARAMS</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-kv<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_sdpa_with_kv_cache<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-X<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--metadata<span class="w"> </span><span class="s1">&#39;{&quot;get_bos_id&quot;:199999, &quot;get_eos_ids&quot;:[200020,199999]}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_context_length<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_name<span class="o">=</span><span class="s2">&quot;phi4-mini-8da4w.pte&quot;</span>
</pre></div>
</div>
<p>The .pte file can be run with ExecuTorch on a mobile phone. Follow the <a class="reference external" href="https://docs.pytorch.org/executorch/main/llm/llama-demo-ios.html">instructions</a> for doing this on an iOS device.</p>
</section>
<section id="mobile-performance-characteristics">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Mobile Performance Characteristics</a><a class="headerlink" href="#mobile-performance-characteristics" title="Permalink to this heading">¶</a></h3>
<p>The torchao-optimized 8da4w model provides:</p>
<ul class="simple">
<li><p><strong>Memory</strong>: ~3.2GB on iPhone 15 Pro</p></li>
<li><p><strong>Speed</strong>: ~17 tokens/sec on iPhone 15 Pro</p></li>
<li><p><strong>Accuracy</strong>: Maintained within 5-10% of original model on most benchmarks</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For detailed instructions on testing the ExecuTorch model and reproducing benchmarks please refer to the <a class="reference external" href="https://huggingface.co/pytorch/Phi-4-mini-instruct-8da4w">HF Phi-4-mini-instruct-8da4w model</a>.</p>
</div>
</section>
</section>
<section id="evaluation">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Evaluation</a><a class="headerlink" href="#evaluation" title="Permalink to this heading">¶</a></h2>
<section id="model-quality-assessment">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Model Quality Assessment</a><a class="headerlink" href="#model-quality-assessment" title="Permalink to this heading">¶</a></h3>
<p>Evaluate quantized models using lm-evaluation-harness:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install evaluation framework</span>
<span class="c1"># Need to install lm-eval from source: https://github.com/EleutherAI/lm-evaluation-harness#install</span>

<span class="c1"># Evaluate baseline model</span>
lm_eval<span class="w"> </span>--model<span class="w"> </span>hf<span class="w"> </span>--model_args<span class="w"> </span><span class="nv">pretrained</span><span class="o">=</span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--tasks<span class="w"> </span>hellaswag<span class="w"> </span>--device<span class="w"> </span>cuda:0<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">8</span>

<span class="c1"># Evaluate torchao-quantized model (float8dq)</span>
lm_eval<span class="w"> </span>--model<span class="w"> </span>hf<span class="w"> </span>--model_args<span class="w"> </span><span class="nv">pretrained</span><span class="o">=</span>pytorch/Phi-4-mini-instruct-float8dq<span class="w"> </span>--tasks<span class="w"> </span>hellaswag<span class="w"> </span>--device<span class="w"> </span>cuda:0<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
</section>
<section id="memory-benchmarking">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Memory Benchmarking</a><a class="headerlink" href="#memory-benchmarking" title="Permalink to this heading">¶</a></h3>
<p>For Phi-4-mini-instruct, when quantized with float8 dynamic quant, we can reduce the peak memory usage by 36% compared to the baseline model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># use &quot;microsoft/Phi-4-mini-instruct&quot; or &quot;pytorch/Phi-4-mini-instruct-float8dq&quot;</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;pytorch/Phi-4-mini-instruct-float8dq&quot;</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">templated_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prompt:&quot;</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Templated prompt:&quot;</span><span class="p">,</span> <span class="n">templated_prompt</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">templated_prompt</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">quantized_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span>
    <span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Response:&quot;</span><span class="p">,</span> <span class="n">output_text</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):])</span>

<span class="n">mem</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak Memory Usage: </span><span class="si">{</span><span class="n">mem</span><span class="si">:</span><span class="s2">.02f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Prompt: Hey, are you conscious? Can you talk to me?</span>
<span class="go">Templated prompt: &lt;|system|&gt;&lt;|end|&gt;&lt;|user|&gt;Hey, are you conscious? Can you talk to me?&lt;|end|&gt;&lt;|assistant|&gt;</span>
<span class="go">Response: Hello! Yes, I am a digital assistant, and I am fully operational and ready to assist you. How can I help you today?</span>
<span class="go">Peak Memory Usage: 5.70 GB</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Benchmark</p></th>
<th class="head"><p>Phi-4 mini-instruct</p></th>
<th class="head"><p>Phi-4-mini-instruct-float8dq</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Peak Memory (GB)</p></td>
<td><p>8.91</p></td>
<td><p>5.70 (36% reduction)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="performance-benchmarking">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">Performance Benchmarking</a><a class="headerlink" href="#performance-benchmarking" title="Permalink to this heading">¶</a></h3>
<section id="latency-benchmarking">
<h4>Latency Benchmarking<a class="headerlink" href="#latency-benchmarking" title="Permalink to this heading">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># baseline</span>
python<span class="w"> </span>benchmarks/benchmark_latency.py<span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--model<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span>

<span class="c1"># float8dq</span>
<span class="nv">VLLM_DISABLE_COMPILE_CACHE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>python<span class="w"> </span>benchmarks/benchmark_latency.py<span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--model<span class="w"> </span>pytorch/Phi-4-mini-instruct-float8dq<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="serving-benchmarking">
<h4>Serving Benchmarking<a class="headerlink" href="#serving-benchmarking" title="Permalink to this heading">¶</a></h4>
<p>We benchmarked the throughput in a serving environment.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup: Get vllm source code</span>
git<span class="w"> </span>clone<span class="w"> </span>git@github.com:vllm-project/vllm.git

<span class="c1"># Install vllm</span>
<span class="nv">VLLM_USE_PRECOMPILED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--editable<span class="w"> </span>.

<span class="c1"># Run the benchmarks under vllm root folder:</span>

<span class="c1"># Download sharegpt dataset:</span>
wget<span class="w"> </span>https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json

<span class="c1"># Other datasets can be found in: https://github.com/vllm-project/vllm/tree/main/benchmarks</span>
<span class="c1"># Note: you can change the number of prompts to be benchmarked with --num-prompts argument for benchmark_serving script.</span>

<span class="c1"># For baseline</span>
<span class="c1"># Server:</span>
vllm<span class="w"> </span>serve<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>-O3
<span class="c1"># Client:</span>
python<span class="w"> </span>benchmarks/benchmark_serving.py<span class="w"> </span>--backend<span class="w"> </span>vllm<span class="w"> </span>--dataset-name<span class="w"> </span>sharegpt<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--dataset-path<span class="w"> </span>./ShareGPT_V3_unfiltered_cleaned_split.json<span class="w"> </span>--model<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">1</span>

<span class="c1"># For float8dq</span>
<span class="c1"># Server:</span>
<span class="nv">VLLM_DISABLE_COMPILE_CACHE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>vllm<span class="w"> </span>serve<span class="w"> </span>pytorch/Phi-4-mini-instruct-float8dq<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>-O3
<span class="c1"># Client:</span>
python<span class="w"> </span>benchmarks/benchmark_serving.py<span class="w"> </span>--backend<span class="w"> </span>vllm<span class="w"> </span>--dataset-name<span class="w"> </span>sharegpt<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--dataset-path<span class="w"> </span>./ShareGPT_V3_unfiltered_cleaned_split.json<span class="w"> </span>--model<span class="w"> </span>pytorch/Phi-4-mini-instruct-float8dq<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="results-h100-machine">
<h4>Results (H100 machine)<a class="headerlink" href="#results-h100-machine" title="Permalink to this heading">¶</a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Benchmark</p></th>
<th class="head"><p>Phi-4-mini-instruct</p></th>
<th class="head"><p>Phi-4-mini-instruct-float8dq</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>latency (batch_size=1)</p></td>
<td><p>1.64s</p></td>
<td><p>1.41s (1.16x speedup)</p></td>
</tr>
<tr class="row-odd"><td><p>latency (batch_size=128)</p></td>
<td><p>3.1s</p></td>
<td><p>2.72s (1.14x speedup)</p></td>
</tr>
<tr class="row-even"><td><p>serving (num_prompts=1)</p></td>
<td><p>1.35 req/s</p></td>
<td><p>1.57 req/s (1.16x speedup)</p></td>
</tr>
<tr class="row-odd"><td><p>serving (num_prompts=1000)</p></td>
<td><p>66.68 req/s</p></td>
<td><p>80.53 req/s (1.21x speedup)</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id15" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>This tutorial demonstrated how torchao’s quantization and sparsity techniques integrate seamlessly across the entire ML deployment stack:</p>
<ul class="simple">
<li><p><strong>HuggingFace Transformers</strong> provides easy model loading with torchao quantization</p></li>
<li><p><strong>vLLM</strong> leverages torchao’s optimized kernels for high-throughput serving</p></li>
<li><p><strong>ExecuTorch</strong> enables mobile deployment with torchao’s mobile-optimized schemes</p></li>
<li><p><strong>lm-evaluation-harness</strong> provides model quality assessment</p></li>
</ul>
<p>All these frameworks use torchao as the underlying optimization engine, ensuring consistent performance gains and ease of integration. The quantization techniques shown provide significant memory reduction (3-4x) and performance improvements (1.5-2x) while maintaining model quality within acceptable bounds for most applications.</p>
<p>For production deployments, always benchmark on your specific use case and hardware to validate the performance and accuracy trade-offs.</p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torchao_vllm_integration.html" class="btn btn-neutral float-right" title="Integration with VLLM: Architecture and Usage Guide" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="finetuning.html" class="btn btn-neutral" title="(Part 2) Fine-tuning with QAT, QLoRA, and float8" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">(Part 3) Serving on vLLM, SGLang, ExecuTorch</a><ul>
<li><a class="reference internal" href="#post-training-quantization-with-huggingface">Post-training Quantization with HuggingFace</a></li>
<li><a class="reference internal" href="#serving-and-inference">Serving and Inference</a><ul>
<li><a class="reference internal" href="#serving-and-inference-with-vllm">Serving and Inference with vLLM</a></li>
<li><a class="reference internal" href="#serving-and-inference-with-sglang">Serving and Inference with SGLang</a></li>
<li><a class="reference internal" href="#inference-with-transformers">Inference with Transformers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mobile-deployment-with-executorch">Mobile Deployment with ExecuTorch</a><ul>
<li><a class="reference internal" href="#optional-untie-embedding-weights">[Optional] Untie Embedding Weights</a></li>
<li><a class="reference internal" href="#step-1-create-mobile-optimized-quantization">Step 1: Create Mobile-Optimized Quantization</a></li>
<li><a class="reference internal" href="#step-2-export-to-executorch">Step 2: Export to ExecuTorch</a></li>
<li><a class="reference internal" href="#mobile-performance-characteristics">Mobile Performance Characteristics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#evaluation">Evaluation</a><ul>
<li><a class="reference internal" href="#model-quality-assessment">Model Quality Assessment</a></li>
<li><a class="reference internal" href="#memory-benchmarking">Memory Benchmarking</a></li>
<li><a class="reference internal" href="#performance-benchmarking">Performance Benchmarking</a><ul>
<li><a class="reference internal" href="#latency-benchmarking">Latency Benchmarking</a></li>
<li><a class="reference internal" href="#serving-benchmarking">Serving Benchmarking</a></li>
<li><a class="reference internal" href="#results-h100-machine">Results (H100 machine)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
         <script src="_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>