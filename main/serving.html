

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="(Part 3) Serving on vLLM, SGLang, ExecuTorch" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pytorch.org/serving.html" />
<meta property="og:site_name" content="torchao" />
<meta property="og:description" content="TorchAO provides an end-to-end pre-training, fine-tuning, and serving model optimization flow by leveraging our quantization and sparsity techniques integrated into our partner frameworks. This is ..." />
<meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
<meta property="og:image:alt" content="torchao" />
<meta name="description" content="TorchAO provides an end-to-end pre-training, fine-tuning, and serving model optimization flow by leveraging our quantization and sparsity techniques integrated into our partner frameworks. This is ..." />

    <title>(Part 3) Serving on vLLM, SGLang, ExecuTorch &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/design-tabs.js"></script>
    <link rel="canonical" href="https://pytorch.org/ao/serving.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Integration with VLLM: Architecture and Usage Guide" href="torchao_vllm_integration.html" />
    <link rel="prev" title="(Part 2) Fine-tuning with QAT, QLoRA, and float8" href="finetuning.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="quick_start.html">
    Quick Start Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quantization_overview.html">
    Quantization Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="contributor_guide.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="sparsity.html">
    Sparsity Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="benchmarking_api_guide.html">
    Benchmarking API Guide
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="benchmarking_user_guide.html">
    Benchmarking User Guide
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="api_ref_dtypes.html">
    torchao.dtypes
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="api_ref_quantization.html">
    torchao.quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="api_ref_qat.html">
    torchao.quantization.qat
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="api_ref_sparsity.html">
    torchao.sparsity
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="api_ref_float8.html">
    torchao.float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="api_ref_utils.html">
    torchao.utils
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="pretraining.html">
    (Part 1) Pre-training with float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="finetuning.html">
    (Part 2) Fine-tuning with QAT, QLoRA, and float8
  </a>
</li>


<li class=" current active">
  <a class="nav-link dropdown-item nav-internal" href="#">
    (Part 3) Serving on vLLM, SGLang, ExecuTorch
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="torchao_vllm_integration.html">
    Integration with VLLM: Architecture and Usage Guide
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="torchao_hf_integration.html">
    Hugging Face Integration
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="serialization.html">
    Serialization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="static_quantization.html">
    Static Quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="subclass_basic.html">
    Writing Your Own Quantized Tensor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="subclass_advanced.html">
    Writing Your Own Quantized Tensor (advanced)
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tutorials_source/pt2e_quant_ptq.html">
    PyTorch 2 Export Post Training Quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tutorials_source/pt2e_quant_qat.html">
    PyTorch 2 Export Quantization-Aware Training (QAT)
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tutorials_source/pt2e_quant_x86_inductor.html">
    PyTorch 2 Export Quantization with X86 Backend through Inductor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tutorials_source/pt2e_quant_xpu_inductor.html">
    PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tutorials_source/pt2e_quant_openvino_inductor.html">
    PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tutorials_source/pt2e_quantizer.html">
    How to Write a Quantizer for PyTorch 2 Export Quantization
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="quick_start.html">
    Quick Start Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quantization_overview.html">
    Quantization Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="contributor_guide.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="sparsity.html">
    Sparsity Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="benchmarking_api_guide.html">
    Benchmarking API Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="benchmarking_user_guide.html">
    Benchmarking User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api_ref_dtypes.html">
    torchao.dtypes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api_ref_quantization.html">
    torchao.quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api_ref_qat.html">
    torchao.quantization.qat
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api_ref_sparsity.html">
    torchao.sparsity
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api_ref_float8.html">
    torchao.float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api_ref_utils.html">
    torchao.utils
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="pretraining.html">
    (Part 1) Pre-training with float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="finetuning.html">
    (Part 2) Fine-tuning with QAT, QLoRA, and float8
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    (Part 3) Serving on vLLM, SGLang, ExecuTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="torchao_vllm_integration.html">
    Integration with VLLM: Architecture and Usage Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="torchao_hf_integration.html">
    Hugging Face Integration
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="serialization.html">
    Serialization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="static_quantization.html">
    Static Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="subclass_basic.html">
    Writing Your Own Quantized Tensor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="subclass_advanced.html">
    Writing Your Own Quantized Tensor (advanced)
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorials_source/pt2e_quant_ptq.html">
    PyTorch 2 Export Post Training Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorials_source/pt2e_quant_qat.html">
    PyTorch 2 Export Quantization-Aware Training (QAT)
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorials_source/pt2e_quant_x86_inductor.html">
    PyTorch 2 Export Quantization with X86 Backend through Inductor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorials_source/pt2e_quant_xpu_inductor.html">
    PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorials_source/pt2e_quant_openvino_inductor.html">
    PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorials_source/pt2e_quantizer.html">
    How to Write a Quantizer for PyTorch 2 Export Quantization
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">(Part 3)...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="(Part 3) Serving on vLLM, SGLang, ExecuTorch">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="part-3-serving-on-vllm-sglang-executorch">
<h1>(Part 3) Serving on vLLM, SGLang, ExecuTorch<a class="headerlink" href="#part-3-serving-on-vllm-sglang-executorch" title="Permalink to this heading">#</a></h1>
<p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Dec 14, 2025 | Last Updated On: Dec 14, 2025</p>
<p>TorchAO provides an end-to-end pre-training, fine-tuning, and serving model optimization flow by leveraging our quantization and sparsity techniques integrated into our partner frameworks. This is part 3 of 3 such tutorials showcasing this end-to-end flow, focusing on the serving step.</p>
<img alt="_images/e2e_flow_part3.png" src="_images/e2e_flow_part3.png" />
<p>This tutorial demonstrates how to perform post-training quantization and deploy models for inference using torchao as the underlying optimization engine, seamlessly integrated through HuggingFace Transformers, vLLM, and ExecuTorch.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#post-training-quantization-with-huggingface" id="id1">Post-training Quantization with HuggingFace</a></p></li>
<li><p><a class="reference internal" href="#serving-and-inference" id="id2">Serving and Inference</a></p>
<ul>
<li><p><a class="reference internal" href="#serving-and-inference-with-vllm" id="id3">Serving and Inference with vLLM</a></p></li>
<li><p><a class="reference internal" href="#serving-and-inference-with-sglang" id="id4">Serving and Inference with SGLang</a></p></li>
<li><p><a class="reference internal" href="#inference-with-transformers" id="id5">Inference with Transformers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#mobile-deployment-with-executorch" id="id6">Mobile Deployment with ExecuTorch</a></p>
<ul>
<li><p><a class="reference internal" href="#optional-untie-embedding-weights" id="id7">[Optional] Untie Embedding Weights</a></p></li>
<li><p><a class="reference internal" href="#step-1-create-mobile-optimized-quantization" id="id8">Step 1: Create Mobile-Optimized Quantization</a></p></li>
<li><p><a class="reference internal" href="#step-2-export-to-executorch" id="id9">Step 2: Export to ExecuTorch</a></p></li>
<li><p><a class="reference internal" href="#mobile-performance-characteristics" id="id10">Mobile Performance Characteristics</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#evaluation" id="id11">Evaluation</a></p>
<ul>
<li><p><a class="reference internal" href="#model-quality-assessment" id="id12">Model Quality Assessment</a></p></li>
<li><p><a class="reference internal" href="#memory-benchmarking" id="id13">Memory Benchmarking</a></p></li>
<li><p><a class="reference internal" href="#performance-benchmarking" id="id14">Performance Benchmarking</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id15">Conclusion</a></p></li>
</ul>
</div>
<section id="post-training-quantization-with-huggingface">
<h2><a class="toc-backref" href="#id1">Post-training Quantization with HuggingFace</a><a class="headerlink" href="#post-training-quantization-with-huggingface" title="Permalink to this heading">#</a></h2>
<p>HuggingFace Transformers provides seamless integration with torchao quantization. The <code class="docutils literal notranslate"><span class="pre">TorchAoConfig</span></code> automatically applies torchao’s optimized quantization algorithms during model loading.
Please check out our <a class="reference external" href="torchao_hf_integration.html">HF Integration Docs</a> for examples on how to use quantization and sparsity in Transformers and Diffusers and <a class="reference external" href="api_ref_quantization.html#inference-apis-for-quantize">TorchAOConfig Reference</a> for all available torchao configs to use.</p>
</section>
<section id="serving-and-inference">
<h2><a class="toc-backref" href="#id2">Serving and Inference</a><a class="headerlink" href="#serving-and-inference" title="Permalink to this heading">#</a></h2>
<section id="serving-and-inference-with-vllm">
<h3><a class="toc-backref" href="#id3">Serving and Inference with vLLM</a><a class="headerlink" href="#serving-and-inference-with-vllm" title="Permalink to this heading">#</a></h3>
<p>vLLM automatically leverages torchao’s optimized kernels when serving quantized models, providing significant throughput improvements.</p>
<p>First, install vLLM with torchao support:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>vllm<span class="w"> </span>--pre<span class="w"> </span>--extra-index-url<span class="w"> </span>https://download.pytorch.org/whl/nightly/vllm/
pip<span class="w"> </span>install<span class="w"> </span>--pre<span class="w"> </span>torchao<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/nightly/cu128
</pre></div>
</div>
<p>To serve in vLLM, we’re using the model we quantized and pushed to Hugging Face hub in the previous step <a class="reference internal" href="#post-training-quantization-with-huggingface"><span class="std std-ref">Post-training Quantization with HuggingFace</span></a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Server</span>
vllm<span class="w"> </span>serve<span class="w"> </span>pytorch/Phi-4-mini-instruct-FP8<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>-O3

<span class="c1"># Client</span>
curl<span class="w"> </span>http://localhost:8000/v1/chat/completions<span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;model&quot;: &quot;pytorch/Phi-4-mini-instruct-FP8&quot;,</span>
<span class="s1">    &quot;messages&quot;: [</span>
<span class="s1">        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;}</span>
<span class="s1">    ],</span>
<span class="s1">    &quot;temperature&quot;: 0.6,</span>
<span class="s1">    &quot;top_p&quot;: 0.95,</span>
<span class="s1">    &quot;top_k&quot;: 20,</span>
<span class="s1">    &quot;max_tokens&quot;: 32768</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p>Serving a float8 dynamic quantized model with vLLM shows 36% VRAM reduction, 1.15x-1.2x inference speedup and little to no accuracy impact on H100. <a class="reference internal" href="#memory-benchmarking"><span class="std std-ref">Memory Benchmarking</span></a> and <a class="reference internal" href="#performance-benchmarking"><span class="std std-ref">Performance Benchmarking</span></a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on vLLM Integration, please refer to the detailed guide <a class="reference internal" href="torchao_vllm_integration.html#torchao-vllm-integration"><span class="std std-ref">Integration with VLLM: Architecture and Usage Guide</span></a>.</p>
</div>
</section>
<section id="serving-and-inference-with-sglang">
<h3><a class="toc-backref" href="#id4">Serving and Inference with SGLang</a><a class="headerlink" href="#serving-and-inference-with-sglang" title="Permalink to this heading">#</a></h3>
<p>(Coming soon!)</p>
</section>
<section id="inference-with-transformers">
<h3><a class="toc-backref" href="#id5">Inference with Transformers</a><a class="headerlink" href="#inference-with-transformers" title="Permalink to this heading">#</a></h3>
<p>Install the required packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/huggingface/transformers@main
pip<span class="w"> </span>install<span class="w"> </span>torchao
pip<span class="w"> </span>install<span class="w"> </span>torch
pip<span class="w"> </span>install<span class="w"> </span>accelerate
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;pytorch/Phi-4-mini-instruct-float8dq&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful AI assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Can you provide ways to eat combinations of bananas and dragonfruits?&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What about solving an 2x + 3 = 7 equation?&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s2">&quot;return_full_text&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="mobile-deployment-with-executorch">
<h2><a class="toc-backref" href="#id6">Mobile Deployment with ExecuTorch</a><a class="headerlink" href="#mobile-deployment-with-executorch" title="Permalink to this heading">#</a></h2>
<p>ExecuTorch enables on-device inference using torchao’s mobile-optimized quantization schemes. The 8da4w (8-bit dynamic activation, 4-bit weight) configuration is specifically designed for mobile deployment. Optionally, before lowering to ExecuTorch, we can finetune a model using QAT <a class="reference internal" href="finetuning.html"><span class="doc">(Part 2) Fine-tuning with QAT, QLoRA, and float8</span></a>, which has demonstrated some improvements in the quality of quantized models.</p>
<section id="optional-untie-embedding-weights">
<h3><a class="toc-backref" href="#id7">[Optional] Untie Embedding Weights</a><a class="headerlink" href="#optional-untie-embedding-weights" title="Permalink to this heading">#</a></h3>
<p>Optionally, we can quantize the embedding and lm_head differently, since those layers are tied, we first need to untie the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoProcessor</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.modeling_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">find_tied_parameters</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-4-mini-instruct&quot;</span>
<span class="n">untied_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">untied_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tied weights:&quot;</span><span class="p">,</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">untied_model</span><span class="p">))</span>
<span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">untied_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_text_config</span><span class="p">(</span><span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">):</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">untied_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_text_config</span><span class="p">(</span><span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">untied_model</span><span class="o">.</span><span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">untied_model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">untied_model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tied weights:&quot;</span><span class="p">,</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">untied_model</span><span class="p">))</span>

<span class="n">USER_ID</span> <span class="o">=</span> <span class="s2">&quot;YOUR_USER_ID&quot;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">save_to</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>

<span class="n">untied_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>

<span class="c1"># or save locally</span>
<span class="n">save_to_local_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>
<span class="n">untied_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_to_local_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-1-create-mobile-optimized-quantization">
<h3><a class="toc-backref" href="#id8">Step 1: Create Mobile-Optimized Quantization</a><a class="headerlink" href="#step-1-create-mobile-optimized-quantization" title="Permalink to this heading">#</a></h3>
<p>Quantizing the model for mobile deployment using TorchAO’s <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationIntxWeightConfig</span></code> configuration. If we’ve untied the embedding and lm_head following the previous step, we can quantize embedding using <code class="docutils literal notranslate"><span class="pre">IntxWeightOnlyConfig</span></code> configuration, and lm_head using <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationIntxWeightConfig</span></code> configuration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoProcessor</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">TorchAoConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">IntxWeightOnlyConfig</span><span class="p">,</span>
    <span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">,</span>
    <span class="n">FqnToConfig</span><span class="p">,</span>
    <span class="n">quantize_</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="n">PerGroup</span><span class="p">,</span> <span class="n">PerAxis</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># we start from the model with untied weights</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-4-mini-instruct&quot;</span>
<span class="n">USER_ID</span> <span class="o">=</span> <span class="s2">&quot;YOUR_USER_ID&quot;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">untied_model_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>
<span class="n">untied_model_local_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-untied-weights&quot;</span>

<span class="c1"># embedding_config is required only if we untied the embedding and lm_head in the previous step, else we can use only linear config for quantization</span>
<span class="n">embedding_config</span> <span class="o">=</span> <span class="n">IntxWeightOnlyConfig</span><span class="p">(</span>
    <span class="n">weight_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
    <span class="n">granularity</span><span class="o">=</span><span class="n">PerAxis</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">linear_config</span> <span class="o">=</span> <span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">(</span>
    <span class="n">weight_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">,</span>
    <span class="n">weight_granularity</span><span class="o">=</span><span class="n">PerGroup</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">weight_scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">FqnToConfig</span><span class="p">({</span><span class="s2">&quot;_default&quot;</span><span class="p">:</span> <span class="n">linear_config</span><span class="p">,</span> <span class="s2">&quot;model.embed_tokens&quot;</span><span class="p">:</span> <span class="n">embedding_config</span><span class="p">})</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">TorchAoConfig</span><span class="p">(</span><span class="n">quant_type</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">include_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">untie_embedding_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">modules_to_not_convert</span><span class="o">=</span><span class="p">[])</span>

<span class="c1"># either use `untied_model_id` or `untied_model_local_path`</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">untied_model_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># Push to hub</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">model_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">save_to</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">USER_ID</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">-8da4w&quot;</span>
<span class="n">quantized_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">save_to</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-2-export-to-executorch">
<h3><a class="toc-backref" href="#id9">Step 2: Export to ExecuTorch</a><a class="headerlink" href="#step-2-export-to-executorch" title="Permalink to this heading">#</a></h3>
<p>Convert the quantized model to .pte file, which can be run on mobile device.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install ExecuTorch</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/pytorch/executorch.git
<span class="nb">cd</span><span class="w"> </span>executorch
./install_requirements.sh

<span class="c1"># Convert checkpoint format for ExecuTorch</span>
python<span class="w"> </span>-m<span class="w"> </span>executorch.examples.models.phi_4_mini.convert_weights<span class="w"> </span>pytorch_model.bin<span class="w"> </span>pytorch_model_converted.bin

<span class="c1"># Export to PTE format with torchao optimizations preserved</span>
<span class="nv">PARAMS</span><span class="o">=</span><span class="s2">&quot;executorch/examples/models/phi_4_mini/config.json&quot;</span>
python<span class="w"> </span>-m<span class="w"> </span>executorch.examples.models.llama.export_llama<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="s2">&quot;phi_4_mini&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpoint<span class="w"> </span><span class="s2">&quot;pytorch_model_converted.bin&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--params<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$PARAMS</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-kv<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_sdpa_with_kv_cache<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-X<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--metadata<span class="w"> </span><span class="s1">&#39;{&quot;get_bos_id&quot;:199999, &quot;get_eos_ids&quot;:[200020,199999]}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_context_length<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_name<span class="o">=</span><span class="s2">&quot;phi4-mini-8da4w.pte&quot;</span>
</pre></div>
</div>
<p>The .pte file can be run with ExecuTorch on a mobile phone. Follow the <a class="reference external" href="https://docs.pytorch.org/executorch/main/llm/llama-demo-ios.html">instructions</a> for doing this on an iOS device.</p>
</section>
<section id="mobile-performance-characteristics">
<h3><a class="toc-backref" href="#id10">Mobile Performance Characteristics</a><a class="headerlink" href="#mobile-performance-characteristics" title="Permalink to this heading">#</a></h3>
<p>The torchao-optimized 8da4w model provides:</p>
<ul class="simple">
<li><p><strong>Memory</strong>: ~3.2GB on iPhone 15 Pro</p></li>
<li><p><strong>Speed</strong>: ~17 tokens/sec on iPhone 15 Pro</p></li>
<li><p><strong>Accuracy</strong>: Maintained within 5-10% of original model on most benchmarks</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For detailed instructions on testing the ExecuTorch model and reproducing benchmarks please refer to the <a class="reference external" href="https://huggingface.co/pytorch/Phi-4-mini-instruct-8da4w">HF Phi-4-mini-instruct-8da4w model</a>.</p>
</div>
</section>
</section>
<section id="evaluation">
<h2><a class="toc-backref" href="#id11">Evaluation</a><a class="headerlink" href="#evaluation" title="Permalink to this heading">#</a></h2>
<section id="model-quality-assessment">
<h3><a class="toc-backref" href="#id12">Model Quality Assessment</a><a class="headerlink" href="#model-quality-assessment" title="Permalink to this heading">#</a></h3>
<p>Evaluate quantized models using lm-evaluation-harness:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install evaluation framework</span>
<span class="c1"># Need to install lm-eval from source: https://github.com/EleutherAI/lm-evaluation-harness#install</span>

<span class="c1"># Evaluate baseline model</span>
lm_eval<span class="w"> </span>--model<span class="w"> </span>hf<span class="w"> </span>--model_args<span class="w"> </span><span class="nv">pretrained</span><span class="o">=</span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--tasks<span class="w"> </span>hellaswag<span class="w"> </span>--device<span class="w"> </span>cuda:0<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">8</span>

<span class="c1"># Evaluate torchao-quantized model (FP8)</span>
lm_eval<span class="w"> </span>--model<span class="w"> </span>hf<span class="w"> </span>--model_args<span class="w"> </span><span class="nv">pretrained</span><span class="o">=</span>pytorch/Phi-4-mini-instruct-FP8<span class="w"> </span>--tasks<span class="w"> </span>hellaswag<span class="w"> </span>--device<span class="w"> </span>cuda:0<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
</section>
<section id="memory-benchmarking">
<h3><a class="toc-backref" href="#id13">Memory Benchmarking</a><a class="headerlink" href="#memory-benchmarking" title="Permalink to this heading">#</a></h3>
<p>For Phi-4-mini-instruct, when quantized with float8 dynamic quant, we can reduce the peak memory usage by 36% compared to the baseline model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># use &quot;microsoft/Phi-4-mini-instruct&quot; or &quot;pytorch/Phi-4-mini-instruct-FP8&quot;</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;pytorch/Phi-4-mini-instruct-FP8&quot;</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">templated_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prompt:&quot;</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Templated prompt:&quot;</span><span class="p">,</span> <span class="n">templated_prompt</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">templated_prompt</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">quantized_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span>
    <span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Response:&quot;</span><span class="p">,</span> <span class="n">output_text</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):])</span>

<span class="n">mem</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak Memory Usage: </span><span class="si">{</span><span class="n">mem</span><span class="si">:</span><span class="s2">.02f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Prompt: Hey, are you conscious? Can you talk to me?</span>
<span class="go">Templated prompt: &lt;|system|&gt;&lt;|end|&gt;&lt;|user|&gt;Hey, are you conscious? Can you talk to me?&lt;|end|&gt;&lt;|assistant|&gt;</span>
<span class="go">Response: Hello! Yes, I am a digital assistant, and I am fully operational and ready to assist you. How can I help you today?</span>
<span class="go">Peak Memory Usage: 5.70 GB</span>
</pre></div>
</div>
</section>
<section id="performance-benchmarking">
<h3><a class="toc-backref" href="#id14">Performance Benchmarking</a><a class="headerlink" href="#performance-benchmarking" title="Permalink to this heading">#</a></h3>
<section id="latency-benchmarking">
<h4>Latency Benchmarking<a class="headerlink" href="#latency-benchmarking" title="Permalink to this heading">#</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># baseline</span>
vllm<span class="w"> </span>bench<span class="w"> </span>latency<span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--model<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span>

<span class="c1"># FP8</span>
<span class="nv">VLLM_DISABLE_COMPILE_CACHE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>vllm<span class="w"> </span>bench<span class="w"> </span>latency<span class="w"> </span>--input-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--model<span class="w"> </span>pytorch/Phi-4-mini-instruct-FP8<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="serving-benchmarking">
<h4>Serving Benchmarking<a class="headerlink" href="#serving-benchmarking" title="Permalink to this heading">#</a></h4>
<p>We benchmarked the throughput in a serving environment.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup: Get vllm source code</span>
git<span class="w"> </span>clone<span class="w"> </span>git@github.com:vllm-project/vllm.git

<span class="c1"># Install vllm</span>
<span class="nv">VLLM_USE_PRECOMPILED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--editable<span class="w"> </span>.

<span class="c1"># Run the benchmarks under vllm root folder:</span>

<span class="c1"># Download sharegpt dataset:</span>
wget<span class="w"> </span>https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json

<span class="c1"># Other datasets can be found in: https://github.com/vllm-project/vllm/tree/main/benchmarks</span>
<span class="c1"># Note: you can change the number of prompts to be benchmarked with --num-prompts argument for benchmark_serving script.</span>

<span class="c1"># For baseline</span>
<span class="c1"># Server:</span>
vllm<span class="w"> </span>serve<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>-O3
<span class="c1"># Client:</span>
vllm<span class="w"> </span>bench<span class="w"> </span>serve<span class="w"> </span>--backend<span class="w"> </span>vllm<span class="w"> </span>--dataset-name<span class="w"> </span>sharegpt<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--dataset-path<span class="w"> </span>./ShareGPT_V3_unfiltered_cleaned_split.json<span class="w"> </span>--model<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">1</span>

<span class="c1"># For FP8</span>
<span class="c1"># Server:</span>
<span class="nv">VLLM_DISABLE_COMPILE_CACHE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>vllm<span class="w"> </span>serve<span class="w"> </span>pytorch/Phi-4-mini-instruct-FP8<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>-O3
<span class="c1"># Client:</span>
vllm<span class="w"> </span>bench<span class="w"> </span>serve<span class="w"> </span>--backend<span class="w"> </span>vllm<span class="w"> </span>--dataset-name<span class="w"> </span>sharegpt<span class="w"> </span>--tokenizer<span class="w"> </span>microsoft/Phi-4-mini-instruct<span class="w"> </span>--dataset-path<span class="w"> </span>./ShareGPT_V3_unfiltered_cleaned_split.json<span class="w"> </span>--model<span class="w"> </span>pytorch/Phi-4-mini-instruct-FP8<span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="results-h100-machine">
<h4>Results (H100 machine)<a class="headerlink" href="#results-h100-machine" title="Permalink to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 35%" />
<col style="width: 27%" />
<col style="width: 38%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Benchmark</p></th>
<th class="head"><p>Phi-4-mini-instruct</p></th>
<th class="head"><p>Phi-4-mini-instruct-float8dq</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>latency (batch_size=1)</p></td>
<td><p>1.64s</p></td>
<td><p>1.41s (1.16x speedup)</p></td>
</tr>
<tr class="row-odd"><td><p>latency (batch_size=128)</p></td>
<td><p>3.1s</p></td>
<td><p>2.72s (1.14x speedup)</p></td>
</tr>
<tr class="row-even"><td><p>serving (num_prompts=1)</p></td>
<td><p>1.35 req/s</p></td>
<td><p>1.57 req/s (1.16x speedup)</p></td>
</tr>
<tr class="row-odd"><td><p>serving (num_prompts=1000)</p></td>
<td><p>66.68 req/s</p></td>
<td><p>80.53 req/s (1.21x speedup)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id15">Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>This tutorial demonstrated how torchao’s quantization and sparsity techniques integrate seamlessly across the entire ML deployment stack:</p>
<ul class="simple">
<li><p><strong>HuggingFace Transformers</strong> provides easy model loading with torchao quantization</p></li>
<li><p><strong>vLLM</strong> leverages torchao’s optimized kernels for high-throughput serving</p></li>
<li><p><strong>ExecuTorch</strong> enables mobile deployment with torchao’s mobile-optimized schemes</p></li>
<li><p><strong>lm-evaluation-harness</strong> provides model quality assessment</p></li>
</ul>
<p>All these frameworks use torchao as the underlying optimization engine, ensuring consistent performance gains and ease of integration. The quantization techniques shown provide significant memory reduction (3-4x) and performance improvements (1.5-2x) while maintaining model quality within acceptable bounds for most applications.</p>
<p>For production deployments, always benchmark on your specific use case and hardware to validate the performance and accuracy trade-offs.</p>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="finetuning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">(Part 2) Fine-tuning with QAT, QLoRA, and float8</p>
      </div>
    </a>
    <a class="right-next"
       href="torchao_vllm_integration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Integration with VLLM: Architecture and Usage Guide</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="finetuning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">(Part 2) Fine-tuning with QAT, QLoRA, and float8</p>
      </div>
    </a>
    <a class="right-next"
       href="torchao_vllm_integration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Integration with VLLM: Architecture and Usage Guide</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#post-training-quantization-with-huggingface">Post-training Quantization with HuggingFace</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-and-inference">Serving and Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-and-inference-with-vllm">Serving and Inference with vLLM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-and-inference-with-sglang">Serving and Inference with SGLang</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-with-transformers">Inference with Transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mobile-deployment-with-executorch">Mobile Deployment with ExecuTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-untie-embedding-weights">[Optional] Untie Embedding Weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-create-mobile-optimized-quantization">Step 1: Create Mobile-Optimized Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-export-to-executorch">Step 2: Export to ExecuTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mobile-performance-characteristics">Mobile Performance Characteristics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-quality-assessment">Model Quality Assessment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-benchmarking">Memory Benchmarking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-benchmarking">Performance Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-benchmarking">Latency Benchmarking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#serving-benchmarking">Serving Benchmarking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#results-h100-machine">Results (H100 machine)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/ao/edit/main/docs/source/serving.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/serving.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "(Part 3) Serving on vLLM, SGLang, ExecuTorch",
       "headline": "(Part 3) Serving on vLLM, SGLang, ExecuTorch",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/serving.html",
       "articleBody": "(Part 3) Serving on vLLM, SGLang, ExecuTorch# Created On: Dec 14, 2025 | Last Updated On: Dec 14, 2025 TorchAO provides an end-to-end pre-training, fine-tuning, and serving model optimization flow by leveraging our quantization and sparsity techniques integrated into our partner frameworks. This is part 3 of 3 such tutorials showcasing this end-to-end flow, focusing on the serving step. This tutorial demonstrates how to perform post-training quantization and deploy models for inference using torchao as the underlying optimization engine, seamlessly integrated through HuggingFace Transformers, vLLM, and ExecuTorch. Post-training Quantization with HuggingFace Serving and Inference Serving and Inference with vLLM Serving and Inference with SGLang Inference with Transformers Mobile Deployment with ExecuTorch [Optional] Untie Embedding Weights Step 1: Create Mobile-Optimized Quantization Step 2: Export to ExecuTorch Mobile Performance Characteristics Evaluation Model Quality Assessment Memory Benchmarking Performance Benchmarking Conclusion Post-training Quantization with HuggingFace# HuggingFace Transformers provides seamless integration with torchao quantization. The TorchAoConfig automatically applies torchao\u2019s optimized quantization algorithms during model loading. Please check out our HF Integration Docs for examples on how to use quantization and sparsity in Transformers and Diffusers and TorchAOConfig Reference for all available torchao configs to use. Serving and Inference# Serving and Inference with vLLM# vLLM automatically leverages torchao\u2019s optimized kernels when serving quantized models, providing significant throughput improvements. First, install vLLM with torchao support: pip install vllm --pre --extra-index-url https://download.pytorch.org/whl/nightly/vllm/ pip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu128 To serve in vLLM, we\u2019re using the model we quantized and pushed to Hugging Face hub in the previous step Post-training Quantization with HuggingFace. # Server vllm serve pytorch/Phi-4-mini-instruct-FP8 --tokenizer microsoft/Phi-4-mini-instruct -O3 # Client curl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d \u0027{ \"model\": \"pytorch/Phi-4-mini-instruct-FP8\", \"messages\": [ {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"} ], \"temperature\": 0.6, \"top_p\": 0.95, \"top_k\": 20, \"max_tokens\": 32768 }\u0027 Serving a float8 dynamic quantized model with vLLM shows 36% VRAM reduction, 1.15x-1.2x inference speedup and little to no accuracy impact on H100. Memory Benchmarking and Performance Benchmarking for more details. Note For more information on vLLM Integration, please refer to the detailed guide Integration with VLLM: Architecture and Usage Guide. Serving and Inference with SGLang# (Coming soon!) Inference with Transformers# Install the required packages: pip install git+https://github.com/huggingface/transformers@main pip install torchao pip install torch pip install accelerate import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline torch.random.manual_seed(0) model_path = \"pytorch/Phi-4-mini-instruct-float8dq\" model = AutoModelForCausalLM.from_pretrained( model_path, device_map=\"auto\", dtype=\"auto\", trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(model_path) messages = [ {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, ] pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, ) generation_args = { \"max_new_tokens\": 500, \"return_full_text\": False, \"temperature\": 0.0, \"do_sample\": False, } output = pipe(messages, **generation_args) print(output[0][\u0027generated_text\u0027]) Mobile Deployment with ExecuTorch# ExecuTorch enables on-device inference using torchao\u2019s mobile-optimized quantization schemes. The 8da4w (8-bit dynamic activation, 4-bit weight) configuration is specifically designed for mobile deployment. Optionally, before lowering to ExecuTorch, we can finetune a model using QAT (Part 2) Fine-tuning with QAT, QLoRA, and float8, which has demonstrated some improvements in the quality of quantized models. [Optional] Untie Embedding Weights# Optionally, we can quantize the embedding and lm_head differently, since those layers are tied, we first need to untie the model: from transformers import ( AutoModelForCausalLM, AutoProcessor, AutoTokenizer, ) import torch from transformers.modeling_utils import find_tied_parameters model_id = \"microsoft/Phi-4-mini-instruct\" untied_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\") tokenizer = AutoTokenizer.from_pretrained(model_id) print(untied_model) print(\"tied weights:\", find_tied_parameters(untied_model)) if getattr(untied_model.config.get_text_config(decoder=True), \"tie_word_embeddings\"): setattr(untied_model.config.get_text_config(decoder=True), \"tie_word_embeddings\", False) untied_model._tied_weights_keys = [] untied_model.lm_head.weight = torch.nn.Parameter(untied_model.lm_head.weight.clone()) print(\"tied weights:\", find_tied_parameters(untied_model)) USER_ID = \"YOUR_USER_ID\" MODEL_NAME = model_id.split(\"/\")[-1] save_to = f\"{USER_ID}/{MODEL_NAME}-untied-weights\" untied_model.push_to_hub(save_to) tokenizer.push_to_hub(save_to) # or save locally save_to_local_path = f\"{MODEL_NAME}-untied-weights\" untied_model.save_pretrained(save_to_local_path) tokenizer.save_pretrained(save_to) Step 1: Create Mobile-Optimized Quantization# Quantizing the model for mobile deployment using TorchAO\u2019s Int8DynamicActivationIntxWeightConfig configuration. If we\u2019ve untied the embedding and lm_head following the previous step, we can quantize embedding using IntxWeightOnlyConfig configuration, and lm_head using Int8DynamicActivationIntxWeightConfig configuration. from transformers import ( AutoModelForCausalLM, AutoProcessor, AutoTokenizer, TorchAoConfig, ) from torchao.quantization.quant_api import ( IntxWeightOnlyConfig, Int8DynamicActivationIntxWeightConfig, FqnToConfig, quantize_, ) from torchao.quantization.granularity import PerGroup, PerAxis import torch # we start from the model with untied weights model_id = \"microsoft/Phi-4-mini-instruct\" USER_ID = \"YOUR_USER_ID\" MODEL_NAME = model_id.split(\"/\")[-1] untied_model_id = f\"{USER_ID}/{MODEL_NAME}-untied-weights\" untied_model_local_path = f\"{MODEL_NAME}-untied-weights\" # embedding_config is required only if we untied the embedding and lm_head in the previous step, else we can use only linear config for quantization embedding_config = IntxWeightOnlyConfig( weight_dtype=torch.int8, granularity=PerAxis(0), ) linear_config = Int8DynamicActivationIntxWeightConfig( weight_dtype=torch.int4, weight_granularity=PerGroup(32), weight_scale_dtype=torch.bfloat16, ) quant_config = FqnToConfig({\"_default\": linear_config, \"model.embed_tokens\": embedding_config}) quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True, untie_embedding_weights=True, modules_to_not_convert=[]) # either use `untied_model_id` or `untied_model_local_path` quantized_model = AutoModelForCausalLM.from_pretrained(untied_model_id, dtype=torch.float32, device_map=\"auto\", quantization_config=quantization_config) tokenizer = AutoTokenizer.from_pretrained(model_id) # Push to hub MODEL_NAME = model_id.split(\"/\")[-1] save_to = f\"{USER_ID}/{MODEL_NAME}-8da4w\" quantized_model.push_to_hub(save_to, safe_serialization=False) tokenizer.push_to_hub(save_to) Step 2: Export to ExecuTorch# Convert the quantized model to .pte file, which can be run on mobile device. # Install ExecuTorch git clone https://github.com/pytorch/executorch.git cd executorch ./install_requirements.sh # Convert checkpoint format for ExecuTorch python -m executorch.examples.models.phi_4_mini.convert_weights pytorch_model.bin pytorch_model_converted.bin # Export to PTE format with torchao optimizations preserved PARAMS=\"executorch/examples/models/phi_4_mini/config.json\" python -m executorch.examples.models.llama.export_llama \\ --model \"phi_4_mini\" \\ --checkpoint \"pytorch_model_converted.bin\" \\ --params \"$PARAMS\" \\ -kv \\ --use_sdpa_with_kv_cache \\ -X \\ --metadata \u0027{\"get_bos_id\":199999, \"get_eos_ids\":[200020,199999]}\u0027 \\ --max_seq_length 128 \\ --max_context_length 128 \\ --output_name=\"phi4-mini-8da4w.pte\" The .pte file can be run with ExecuTorch on a mobile phone. Follow the instructions for doing this on an iOS device. Mobile Performance Characteristics# The torchao-optimized 8da4w model provides: Memory: ~3.2GB on iPhone 15 Pro Speed: ~17 tokens/sec on iPhone 15 Pro Accuracy: Maintained within 5-10% of original model on most benchmarks Note For detailed instructions on testing the ExecuTorch model and reproducing benchmarks please refer to the HF Phi-4-mini-instruct-8da4w model. Evaluation# Model Quality Assessment# Evaluate quantized models using lm-evaluation-harness: # Install evaluation framework # Need to install lm-eval from source: https://github.com/EleutherAI/lm-evaluation-harness#install # Evaluate baseline model lm_eval --model hf --model_args pretrained=microsoft/Phi-4-mini-instruct --tasks hellaswag --device cuda:0 --batch_size 8 # Evaluate torchao-quantized model (FP8) lm_eval --model hf --model_args pretrained=pytorch/Phi-4-mini-instruct-FP8 --tasks hellaswag --device cuda:0 --batch_size 8 Memory Benchmarking# For Phi-4-mini-instruct, when quantized with float8 dynamic quant, we can reduce the peak memory usage by 36% compared to the baseline model. import torch from transformers import AutoModelForCausalLM, AutoTokenizer # use \"microsoft/Phi-4-mini-instruct\" or \"pytorch/Phi-4-mini-instruct-FP8\" model_id = \"pytorch/Phi-4-mini-instruct-FP8\" quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", dtype=torch.bfloat16) tokenizer = AutoTokenizer.from_pretrained(model_id) torch.cuda.reset_peak_memory_stats() prompt = \"Hey, are you conscious? Can you talk to me?\" messages = [ { \"role\": \"system\", \"content\": \"\", }, {\"role\": \"user\", \"content\": prompt}, ] templated_prompt = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, ) print(\"Prompt:\", prompt) print(\"Templated prompt:\", templated_prompt) inputs = tokenizer( templated_prompt, return_tensors=\"pt\", ).to(\"cuda\") generated_ids = quantized_model.generate(**inputs, max_new_tokens=128) output_text = tokenizer.batch_decode( generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False ) print(\"Response:\", output_text[0][len(prompt):]) mem = torch.cuda.max_memory_reserved() / 1e9 print(f\"Peak Memory Usage: {mem:.02f} GB\") Output: Prompt: Hey, are you conscious? Can you talk to me? Templated prompt: \u003c|system|\u003e\u003c|end|\u003e\u003c|user|\u003eHey, are you conscious? Can you talk to me?\u003c|end|\u003e\u003c|assistant|\u003e Response: Hello! Yes, I am a digital assistant, and I am fully operational and ready to assist you. How can I help you today? Peak Memory Usage: 5.70 GB Performance Benchmarking# Latency Benchmarking# # baseline vllm bench latency --input-len 256 --output-len 256 --model microsoft/Phi-4-mini-instruct --batch-size 1 # FP8 VLLM_DISABLE_COMPILE_CACHE=1 vllm bench latency --input-len 256 --output-len 256 --model pytorch/Phi-4-mini-instruct-FP8 --batch-size 1 Serving Benchmarking# We benchmarked the throughput in a serving environment. # Setup: Get vllm source code git clone git@github.com:vllm-project/vllm.git # Install vllm VLLM_USE_PRECOMPILED=1 pip install --editable . # Run the benchmarks under vllm root folder: # Download sharegpt dataset: wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json # Other datasets can be found in: https://github.com/vllm-project/vllm/tree/main/benchmarks # Note: you can change the number of prompts to be benchmarked with --num-prompts argument for benchmark_serving script. # For baseline # Server: vllm serve microsoft/Phi-4-mini-instruct --tokenizer microsoft/Phi-4-mini-instruct -O3 # Client: vllm bench serve --backend vllm --dataset-name sharegpt --tokenizer microsoft/Phi-4-mini-instruct --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json --model microsoft/Phi-4-mini-instruct --num-prompts 1 # For FP8 # Server: VLLM_DISABLE_COMPILE_CACHE=1 vllm serve pytorch/Phi-4-mini-instruct-FP8 --tokenizer microsoft/Phi-4-mini-instruct -O3 # Client: vllm bench serve --backend vllm --dataset-name sharegpt --tokenizer microsoft/Phi-4-mini-instruct --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json --model pytorch/Phi-4-mini-instruct-FP8 --num-prompts 1 Results (H100 machine)# Benchmark Phi-4-mini-instruct Phi-4-mini-instruct-float8dq latency (batch_size=1) 1.64s 1.41s (1.16x speedup) latency (batch_size=128) 3.1s 2.72s (1.14x speedup) serving (num_prompts=1) 1.35 req/s 1.57 req/s (1.16x speedup) serving (num_prompts=1000) 66.68 req/s 80.53 req/s (1.21x speedup) Conclusion# This tutorial demonstrated how torchao\u2019s quantization and sparsity techniques integrate seamlessly across the entire ML deployment stack: HuggingFace Transformers provides easy model loading with torchao quantization vLLM leverages torchao\u2019s optimized kernels for high-throughput serving ExecuTorch enables mobile deployment with torchao\u2019s mobile-optimized schemes lm-evaluation-harness provides model quality assessment All these frameworks use torchao as the underlying optimization engine, ensuring consistent performance gains and ease of integration. The quantization techniques shown provide significant memory reduction (3-4x) and performance improvements (1.5-2x) while maintaining model quality within acceptable bounds for most applications. For production deployments, always benchmark on your specific use case and hardware to validate the performance and accuracy trade-offs.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/serving.html"
       },
       "datePublished": "Dec 14, 2025T00:00:00Z",
       "dateModified": "Dec 14, 2025T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>