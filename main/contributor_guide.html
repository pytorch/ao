


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao Contributor Guide &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Serialization" href="serialization.html" />
    <link rel="prev" title="apply_fake_sparsity" href="generated/torchao.sparsity.apply_fake_sparsity.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='tba'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_ref_sparsity.html">torchao.sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contributor Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">torchao Contributor Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Serialization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torchao Contributor Guide</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/contributor_guide.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="torchao-contributor-guide">
<h1>torchao Contributor Guide<a class="headerlink" href="#torchao-contributor-guide" title="Permalink to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<section id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this heading">¶</a></h2>
<p>In this doc we’ll talk about
(1). How different optimization techniques are structured in torchao
(2). How to contribute to torchao</p>
<p>Note: the doc is heavily focused on inference right now, but we plan to expand to cover training techniques in the future as well.</p>
</section>
<section id="torchao-stack-overview">
<h2>torchao Stack Overview<a class="headerlink" href="#torchao-stack-overview" title="Permalink to this heading">¶</a></h2>
<p>First we want to lay out the torchao stack:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Quantization</span> <span class="n">Algorithms</span><span class="o">/</span><span class="n">Flows</span><span class="p">:</span> <span class="n">weight</span> <span class="n">only</span><span class="o">/</span><span class="n">dynamic</span><span class="o">/</span><span class="n">static</span> <span class="n">quantization</span><span class="p">,</span> <span class="n">hqq</span><span class="p">,</span> <span class="n">awq</span><span class="p">,</span> <span class="n">gptq</span> <span class="n">etc</span><span class="o">.</span>
<span class="o">---------------------------------------------------------------------------------------------</span>
        <span class="n">Quantized</span> <span class="n">Tensors</span> <span class="p">(</span><span class="n">derived</span> <span class="n">dtypes</span><span class="p">):</span> <span class="n">AffineQuantizedTensor</span><span class="p">,</span> <span class="n">CodebookQuantizedTensor</span>
<span class="o">---------------------------------------------------------------------------------------------</span>
  <span class="n">Quantization</span> <span class="n">Primitive</span> <span class="n">Ops</span><span class="o">/</span><span class="n">Efficient</span> <span class="n">Kernels</span><span class="p">:</span> <span class="n">matmul</span><span class="p">,</span> <span class="n">quantize</span><span class="p">,</span> <span class="n">dequantize</span>
<span class="o">---------------------------------------------------------------------------------------------</span>
            <span class="n">Basic</span> <span class="n">dtypes</span><span class="p">:</span> <span class="n">uint1</span><span class="o">-</span><span class="n">uint7</span><span class="p">,</span> <span class="n">int1</span><span class="o">-</span><span class="n">int8</span><span class="p">,</span> <span class="n">float3</span><span class="o">-</span><span class="n">float8</span>
</pre></div>
</div>
<p>Any quantization algorithm will be using some components from the above stack, for example int4_weight_only quantization uses:
(1) weight only quantization flow
(2) <a class="reference external" href="https://github.com/pytorch/pytorch/blob/136e28f616140fdc9fb78bb0390aeba16791f1e3/aten/src/ATen/native/native_functions.yaml#L4148">tinygemm bf16 activation + int4 weight kernel</a> and <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_primitives.py">quant primitive ops</a>
(3) <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/dtypes/affine_quantized_tensor.py">AffineQuantizedTensor</a> tensor subclass with <a class="reference external" href="https://github.com/pytorch/ao/blob/e41ca4ee41f5f1fe16c59e00cffb4dd33d25e56d/torchao/dtypes/affine_quantized_tensor.py#L573">TensorCoreTiledLayout</a>
(4) torch.uint4 dtype (simulated with quant_min/quant_max right now)</p>
<p>Note: we’ll also talk about how to compose sparsity with quantization in the Quantized Tensors section</p>
<section id="basic-dtypes">
<h3>Basic DTypes<a class="headerlink" href="#basic-dtypes" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Data_type">dtype</a> is a bit of overloaded term, by basic dtype, we mean the dtypes that makes sense without any extra metadata (e.g. makes sense when people call <code class="docutils literal notranslate"><span class="pre">torch.empty(..,</span> <span class="pre">dtype)</span></code>), for more details please check out: dev-discuss.pytorch.org/t/supporting-new-dtypes-in-pytorch/1833</p>
<p>No matter what quantization we are doing, in the end we will be using some low precision dtypes to represent the quantized data, the dtypes we aim to support in torchao are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.uint1</span></code> to <code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code> available in pytorch 2.3 and later</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int1</span></code> to <code class="docutils literal notranslate"><span class="pre">torch.int8</span></code> available in pytorch 2.6 and later</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.float3_e2_m0</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float4_e2_m1</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float4_e3_m0</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float5_e2_m2</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float5_e3_m1</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float6_e2_m3</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float6_e3_m2</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e4m3fn</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e5m2</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e4m3fnuz</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e5m2fnuz</span></code> (float8 is added to torch, we also plan to add float4 and float6 to torch if they become popular)</p></li>
</ul>
<p>Note some of the above are prototype only for now. We’ll consider adding then to pytorch core when they become popular and have hardware support.</p>
<section id="current-support">
<h4>Current Support<a class="headerlink" href="#current-support" title="Permalink to this heading">¶</a></h4>
<p>In terms of actual implementation, there are two parts:
1). In PyTorch, we need to add the dtype to torch.dtype, e.g. torch.uint2, example: pytorch/pytorch#117208, but these are just placeholders so that we can use torch.uint2.
2). Outside of PyTorch (e.g. in torchao), we implement the tensor operations for these dtypes with tensor subclasses, also a standard packing format is needed.</p>
<section id="adding-placeholder-dtype-in-pytorch">
<h5>Adding placeholder dtype in PyTorch<a class="headerlink" href="#adding-placeholder-dtype-in-pytorch" title="Permalink to this heading">¶</a></h5>
<p>As mentioned in dev-discuss.pytorch.org/t/supporting-new-dtypes-in-pytorch/1833, the criteria for adding dtype in PyTorch is that it shows wide adoption. For the above mentioned fundamental dtypes, the ones that are supported in PyTorch are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.uint1</span></code> to <code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.int1</span></code> to <code class="docutils literal notranslate"><span class="pre">torch.int8</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e4m3fn</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e5m2</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e4m3fnuz</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float8_e5m2fnuz</span></code></p></li>
</ul>
<p>For the other types we plan to wait until there is more evidence of wide adoption and hardware support.</p>
</section>
<section id="implementing-tensor-operations-for-these-dtypes-with-tensor-subclasses">
<h5>Implementing tensor operations for these dtypes with Tensor subclasses<a class="headerlink" href="#implementing-tensor-operations-for-these-dtypes-with-tensor-subclasses" title="Permalink to this heading">¶</a></h5>
<p>For this, the requirement is we decide on a “standard” packing format, and hopefully one that is amenable to efficient implementation, but for both uintx and floatx we haven’t integrate enough kernels to decide on this. So current <a class="reference external" href="https://github.com/pytorch/ao/blob/d2bce6a56eae5701cb72eb0cf6359626e7bd0190/torchao/dtypes/uintx/uintx.py#L36">packing implementations</a> are ont final. We can revisit after there are more uintx, intx and floatx kernels being integrated into torchao.</p>
</section>
<section id="integrate-tensor-subclass-to-pytorch-native-factory-functions">
<h5>Integrate Tensor subclass to pytorch native factory functions<a class="headerlink" href="#integrate-tensor-subclass-to-pytorch-native-factory-functions" title="Permalink to this heading">¶</a></h5>
<p>After that we can connect the factory function with the tensor subclass, for example: <code class="docutils literal notranslate"><span class="pre">torch.empty(...,</span> <span class="pre">dtype=torch.int4,</span> <span class="pre">...)</span></code> can create a <code class="docutils literal notranslate"><span class="pre">Int4Tensor</span></code> tensor subclass with the packing format decided in the previous step.</p>
</section>
</section>
</section>
<section id="quantization-primitive-ops">
<h3>Quantization Primitive Ops<a class="headerlink" href="#quantization-primitive-ops" title="Permalink to this heading">¶</a></h3>
<p>Quantization primitive ops means the operators used to convert between low preicison quantized tensors and high precision tensors. We will mainly have the following quantization primitive operators:
choose_qparams ops: that chooses quantization parameter based on the original Tensor, typically used in dynamic quantization, e.g. scale and zero_point for affine quantization
quantize op: quantizes the original high precision tensor to the low precision tensor with the dtypes mentioned in previous section based on the quantization parameters
dequantize op: dequantizes the low precision tensor into the high precision tensor based on quantization parameters</p>
<p>There could be variations of the above to accommodate specific use cases, for example for static quantization we may have <code class="docutils literal notranslate"><span class="pre">choose_qparams_affine_with_min_max</span></code> that will choose quantization parameters based on min/max values derived from the observation process.</p>
</section>
<section id="efficient-kernels">
<h3>Efficient kernels<a class="headerlink" href="#efficient-kernels" title="Permalink to this heading">¶</a></h3>
<p>We’ll also have efficient kernels that works with the low precision tensors, for example</p>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/136e28f616140fdc9fb78bb0390aeba16791f1e3/aten/src/ATen/native/native_functions.yaml#L4148">_weight_int4pack_mm</a> the tinygemm int4 kernel (bf16 activation + int4 weight)
<a class="reference external" href="https://github.com/pytorch/ao/blob/3e9746cf636e39e3c1ec0de6e0ef2e31f75c4c02/torchao/kernel/intmm.py#L90">int_matmul</a> that takes two int8 tensors and outputs an int32 tensor
<a class="reference external" href="https://github.com/pytorch/ao/blob/3e9746cf636e39e3c1ec0de6e0ef2e31f75c4c02/torchao/kernel/intmm.py#L107">int_scaled_matmul</a> that does matmul and also applies a scale to the result.</p>
<p>Note: We can also rely on torch.compile to generate kernels (through triton), for example the current int8 weight only quantization <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/dtypes/affine_quantized_tensor.py#L1292-L1309">kernel</a> just relies on torch.compile to get speedup. In this case there is no specific “efficient kernel” that’s corresponding to the type of quantization.</p>
</section>
<section id="quantized-tensors-derived-dtypes">
<h3>Quantized Tensors (derived dtypes)<a class="headerlink" href="#quantized-tensors-derived-dtypes" title="Permalink to this heading">¶</a></h3>
<p>On top of the basic dtypes, quantization primitive operators and efficient kernels, we can glue everything together and build out a Quantized (low precision) Tensor by subclassing torch.Tensor that can be constructed from a high precision Tensor and some parameters that can configure the specific quantization user wants, we can also call this derived dtypes since it can be represented with Tensors of basic dtypes and some extra metadata like scale.</p>
<p>Existing example in torchao is <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code>, meaning the low precision Tensor is quantized from the high precision Tensor by an affine mapping, that is: <code class="docutils literal notranslate"><span class="pre">low_precision_val</span> <span class="pre">=</span> <span class="pre">high_precision_val</span> <span class="pre">/</span> <span class="pre">scale</span> <span class="pre">+</span> <span class="pre">zero_point</span></code>, where <code class="docutils literal notranslate"><span class="pre">scale</span></code>/<code class="docutils literal notranslate"><span class="pre">zero_point</span></code> are the quantization parameters that can be calculated by quantization primitive ops or through some optimization procedure. Affine quantization is a very common type of quantization, since it’s straightforward that when we try to map from higher precision values to lower precision values, we do an affine transformation (<code class="docutils literal notranslate"><span class="pre">high_preicsion_val</span> <span class="pre">/</span> <span class="pre">scale</span> <span class="pre">+</span> <span class="pre">zero_point</span></code>). Another common type of quantization, especially for lower bitwidths (e.g. lower than 4 bit) is codebook / look up table based quantization.</p>
<section id="layout-and-tensorimpl">
<h4>Layout and TensorImpl<a class="headerlink" href="#layout-and-tensorimpl" title="Permalink to this heading">¶</a></h4>
<p>Native tensors have a hardcoded list of selections of <a class="reference external" href="mailto:pytorch/pytorch&#37;&#52;&#48;6478150/c10/core/Layout&#46;h#L10">layout</a>, most common one is strided layout, it provides a strided, multi-dimensional view of storage, we also have some sparse and mkldnn layout.</p>
<p>Take <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-tensors">sparse COO tensor</a> as an example, it has <cite>torch.sparse_coo</cite> layout, and <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/SparseTensorImpl.h">SparseTensorImpl</a> which changes how the tensor is stored.</p>
<p>The idea of packing the tensor into different formats fits nicely with the layout concept, that’s why we want to reuse this for packing. We can use <cite>Layout</cite> for different type of packing format and <cite>TensorImpl</cite> for different storage format implementations. And new TensorImpl that stores the Tensor in a packed format can be added at python level tensor subclasses without modifying C++ pytorch core code.</p>
<p>For example, for <code class="docutils literal notranslate"><span class="pre">_weight_int4pack_mm</span></code> we need to pack the weight to an format that is friendly for Tensor Core, we call it <a class="reference external" href="https://github.com/pytorch/ao/blob/e41ca4ee41f5f1fe16c59e00cffb4dd33d25e56d/torchao/dtypes/affine_quantized_tensor.py#L573">TensorCoreTiledLayout</a>. We add a <code class="docutils literal notranslate"><span class="pre">tensor_impl</span></code> for the quantized tensor to store the packed (or unpacked) weight, and we use <code class="docutils literal notranslate"><span class="pre">layout</span></code> to store different parameters that’s relevant for packing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AffineQuantizedTensor</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
  <span class="c1"># tensor_impl is also implemented with tensor subclass</span>
  <span class="n">tensor_impl</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

  <span class="c1"># to not conflict with existing layout property, we use `_layout`</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">_layout</span>
</pre></div>
</div>
<p>Note that layout is an abstraction not only for custom data representation, it is also used for how the
<cite>TensorImpl</cite> interacts with different operators, e.g. the same data representation can have different
implementations when running the same operator, e.g. transpose, quantized_linear, but the operator semantics should stay the same.</p>
<p>Quantize + Sparse Tensor can also be supported through the Layout abstraction, for example, <a class="reference external" href="https://github.com/pytorch/ao/pull/621">int4 weight only quantization + sparse</a>. We also provide some common utils that helps people to add different layouts to a quantized tensor, please check out the developer guide below for code examples.</p>
</section>
</section>
<section id="quantization-algorithms-flows">
<h3>Quantization Algorithms/Flows<a class="headerlink" href="#quantization-algorithms-flows" title="Permalink to this heading">¶</a></h3>
<p>On the top of the stack will be the final quantization algorithms and quantization flows. Traditionally we have weight only quantization, dynamic quantization and static quantization, but now we are also seeing more types of quantization coming up.</p>
<p>For demonstration purposes, let’s say after previous step we have <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> and <code class="docutils literal notranslate"><span class="pre">to_affine_quantized</span></code> factory function defined. For simplicity, let’s say <code class="docutils literal notranslate"><span class="pre">to_affine_quantized</span></code> takes a high precision floating point Tensor and a target_dtype (e.g. torch.int8) and converts it to an <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> with corresponding dtype.</p>
<p>Note: below are all for explaining the concepts, more detailed introduction for utils and examples we provide can be found in <code class="docutils literal notranslate"><span class="pre">Tensor</span> <span class="pre">Subclass</span> <span class="pre">Developer</span> <span class="pre">Guide</span></code> section.</p>
<section id="weight-only-quantization">
<h4>Weight Only Quantization<a class="headerlink" href="#weight-only-quantization" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>This is the simplest form of quantization and it’s easy to apply weight only quantization to the model, especially since we have Quantized Tensor. all we need to do is::</dt><dd><p>linear_module.weight = torch.nn.Parameter(to_affine_quantized_intx(linear_module.weight, …), requires_grad=False))</p>
</dd>
</dl>
<p>apply the above to all linear modules in the model and we’ll get a weight only quantized model.</p>
</section>
<section id="dynamic-activation-and-weight-quantization">
<h4>Dynamic Activation and Weight Quantization<a class="headerlink" href="#dynamic-activation-and-weight-quantization" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>This is called “dynamic quantization” before but it means we quantize activation dynamically at runtime, and also quantize the weights as well. Compared to the weight only quantization, the main question is how do we apply the quantization to activation. In torchao, the common pattern we use is by applying <code class="docutils literal notranslate"><span class="pre">to_linear_activation_quantized</span></code> on top of quantized weight::</dt><dd><p>quantized_weight = to_affine_quantized(linear_module.weight)
activation_and_weight_quantized = to_linear_activation_quantized(quantized_weight)
linear_module.weight = torch.nn.Parameter(activation_and_weight_quantized, requires_grad=False))</p>
</dd>
</dl>
<p><code class="docutils literal notranslate"><span class="pre">to_linear_activation_quantized</span></code> is used to apply quantization to activation, it takes a <code class="docutils literal notranslate"><span class="pre">input_quant_func</span></code> that will quantize the activation and the original weight, and during runtime when it encounters a <code class="docutils literal notranslate"><span class="pre">F.linear</span></code> op, it will apply the stored input_qunat_func to activation and redispatch to <code class="docutils literal notranslate"><span class="pre">F.linear</span></code> with quantized activation and weight.</p>
<p>If the above does not work, user can also do module swaps, or use <code class="docutils literal notranslate"><span class="pre">torch.fx.symbolic_trace()</span></code> to get a traced module that you can <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#direct-graph-manipulation">modify</a>.</p>
<p>But using tensor subclass is preferred because it is easier for serialization/deserialization, if we use tensor subclasses to support dynamic quantization, then we can load the quantized weights directly without further preparation for the model. Otherwise, we’d need to do module swap or other modifications to the model first before loading the quantized weights.</p>
</section>
<section id="static-activation-quantization-and-weight-quantization">
<h4>Static Activation Quantization and Weight Quantization<a class="headerlink" href="#static-activation-quantization-and-weight-quantization" title="Permalink to this heading">¶</a></h4>
<p>Static quantization means activation is statically quantized instead of dynamically quantized at runtime. In terms of flow, static quantization requires calibration with sample data in order that we can figure out the appropriate quantization parameters.</p>
<p>At the high level there are three steps for static quantization: (1) insert observers (2) calibration (3) quantize the model</p>
<section id="insert-observers">
<h5>Insert Observers<a class="headerlink" href="#insert-observers" title="Permalink to this heading">¶</a></h5>
<p>In insert observers step, we need to add observer modules to input (and output) activation and weight of the operator to collect statistics of the Tensor. So there are two things we need to address, how to define observer module? how to add observer module to the model.</p>
<section id="how-to-define-observer-module">
<h6>How to define observer module<a class="headerlink" href="#how-to-define-observer-module" title="Permalink to this heading">¶</a></h6>
<p>Observers are specific to: (1) type of quantization (e.g. affine quantization, look up table based quantization) (2) type of stats we want to track, e.g. min max observer, moving average observer.</p>
<p>Generally an observer module should define <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/quantization/observer.py#L165">forward</a> and <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/quantization/observer.py#L172">calculate_qparams</a></p>
<p>For affine quantization, we defined <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/quantization/observer.py#L179">AffineQuantizedMinMaxObserver</a> that records min_val/max_val based on the granularity of affine quantization, and also defines how to calculate_qparams based on the recorded stats.</p>
</section>
<section id="how-to-add-observer-module-to-the-model">
<h6>How to add observer module to the model<a class="headerlink" href="#how-to-add-observer-module-to-the-model" title="Permalink to this heading">¶</a></h6>
<ol class="arabic simple">
<li><p>Use Tensor Subclasses
If the only operator you are interested in quantizing is linear, you can use <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/quantization/linear_activation_weight_observer.py">linear activation weight observer</a>, we also have a corresponding <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/quantization/quant_api.py#L291">insert_observer_</a> API that handles modifying the weight of linear.</p></li>
<li><p>Module Swap
Alternatively, you could also define and <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/tutorials/calibration_flow/static_quant.py#L29">ObservedLinear</a> module (or other module types) and swap the non observed with the observed module</p></li>
</ol>
</section>
<section id="calibration">
<h6>Calibration<a class="headerlink" href="#calibration" title="Permalink to this heading">¶</a></h6>
<p>Calibration step is typically straightforward, typically we just need to run the model through the calibration dataset. For more complicated calibration (e.g. where we record all inputs and do optimizations based on all inputs), we’ll cover some of them in next section.</p>
</section>
<section id="quantize">
<h6>Quantize<a class="headerlink" href="#quantize" title="Permalink to this heading">¶</a></h6>
<p>We can reuse the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> API but provide a different <code class="docutils literal notranslate"><span class="pre">apply_tensor_subclass</span></code> function that converts the observed linear module to a linear module with quantized weight and statically quantized input activation, this can be done in the same manner as the dynamic quantization (with <code class="docutils literal notranslate"><span class="pre">to_linear_activation_quantized</span></code>), see <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/tutorials/calibration_flow/static_quant.py#L59">example</a>.</p>
<p>Alternatively, user can do <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/tutorials/calibration_flow/static_quant.py#L130">module swap</a> as well.</p>
</section>
</section>
</section>
<section id="other-quantization-flows">
<h4>Other Quantization Flows<a class="headerlink" href="#other-quantization-flows" title="Permalink to this heading">¶</a></h4>
<p>For other quantization flow/algorithms that does not fit into any of the above, we also intend to provide examples for common patterns. For example, <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/tutorials/calibration_flow/gptq_like.py">GPTQ like quantization flow</a> that is adopted by <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/prototype/autoround/README.md">Autoround</a>, it uses <a class="reference external" href="https://gist.github.com/HDCharles/a1b575bbf8875f994af8a01b225e1227">MultiTensor</a> and module hooks to optimize the module.</p>
<p>If you are working on a new quantization algorithm/flow and not sure how to implement it in a PyTorch native way, please feel free to open an issue to describe how your algorithm works and we can help advise on the implementation details.</p>
</section>
<section id="training">
<h4>Training<a class="headerlink" href="#training" title="Permalink to this heading">¶</a></h4>
<p>The above flow are mainly focused on inference, but low bit dtype Tensors can be used in training as well.</p>
<section id="quantization-aware-training">
<h5>Quantization Aware Training<a class="headerlink" href="#quantization-aware-training" title="Permalink to this heading">¶</a></h5>
<p>TODO</p>
</section>
<section id="low-bit-optimizers">
<h5>Low Bit Optimizers<a class="headerlink" href="#low-bit-optimizers" title="Permalink to this heading">¶</a></h5>
<p>Today we have some prototype low bit optimizers: <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/prototype/low_bit_optim">main/torchao/prototype/low_bit_optim</a> that implements a specific type of 4 bit, 8 bit and float8, and is also composable with FSDP (with look up table quantization).</p>
</section>
<section id="quantized-training">
<h5>Quantized Training<a class="headerlink" href="#quantized-training" title="Permalink to this heading">¶</a></h5>
<p>Similar to low bit optimizers, we have quantized training prototype in <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/prototype/quantized_training">main/torchao/prototype/quantized_training</a>, and we could extend AffineQuantizedTensor to support training as well, initial enablement is in progress, but there will be a lot of follow up work needed including making it work for different kernels etc.</p>
<p>You can also checkout the tutorial for <a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/my_trainable_tensor_subclass.py">Quantized Training</a> that talks about how to make a dtype tensor subclass trainable.</p>
</section>
</section>
</section>
<section id="case-study-how-int4-weight-only-quantization-works-in-torchao">
<h3>Case Study: How int4 weight only quantization works in torchao?<a class="headerlink" href="#case-study-how-int4-weight-only-quantization-works-in-torchao" title="Permalink to this heading">¶</a></h3>
<p>To connect everything together, here is a more detailed walk through for how int4 weight only quantization is implemented in torchao.</p>
<section id="high-level-summary">
<h4>High Level Summary<a class="headerlink" href="#high-level-summary" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>::</dt><dd><dl class="simple">
<dt>Quantization Flow: quantize_(model, int4_weight_only())</dt><dd><ul class="simple">
<li><p>What happens: linear.weight = torch.nn.Parameter(to_affine_quantized_intx(linear.weight), requires_grad=False)</p></li>
<li><p>quantization primitive ops: choose_qparams and quantize_affine are called to quantize the Tensor</p></li>
<li><p>quantized Tensor will be <cite>AffineQuantizedTensor</cite>, a quantized tensor with derived dtype (e.g. int4 with scale and zero_point)</p></li>
<li><p>packing op <cite>_convert_weight_to_int4pack</cite> to pack the quantized weight for efficient execution</p></li>
</ul>
</dd>
<dt>During Model Execution: model(input)</dt><dd><ul class="simple">
<li><p><cite>torch.ops.aten._weight_int4pack_mm</cite> is called on input and the packed weight</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</section>
<section id="during-quantization">
<h4>During Quantization<a class="headerlink" href="#during-quantization" title="Permalink to this heading">¶</a></h4>
<p>First we start with the API call: <code class="docutils literal notranslate"><span class="pre">quantize_(model,</span> <span class="pre">int4_weight_only())</span></code> what this does is it converts the weights of nn.Linear modules in the model to int4 quantized tensor (<code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> that is int4 dtype, asymmetric, per group quantized), using the layout for tinygemm kernel: <code class="docutils literal notranslate"><span class="pre">tensor_core_tiled</span></code> layout.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/4865ee61340cc63a1469f437388067b853c9289e/torchao/quantization/quant_api.py#L403">quantize_</a>: the model level API that quantizes the weight of linear by applying the conversion function from user (second argument)</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/242f181fe59e233b458740b06464ad42da8df6af/torchao/quantization/quant_api.py#L522">int4_weight_only</a>: the function that returns a function that converts weight of linear to int4 weight only quantized weight
* Calls quantization primitives ops like choose_qparams_affine and quantize_affine to quantize the Tensor</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/242f181fe59e233b458740b06464ad42da8df6af/torchao/dtypes/affine_quantized_tensor.py#L573">TensorCoreTiledLayout</a>: the tensor core tiled layout type, storing parameters for the packing format</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/242f181fe59e233b458740b06464ad42da8df6af/torchao/dtypes/affine_quantized_tensor.py#L1376">TensorCoreTiledAQTTensorImpl</a>: the tensor core tiled TensorImpl, stores the packed weight for efficient int4 weight only kernel (tinygemm kernel)</p></li>
</ul>
</section>
<section id="during-model-execution">
<h4>During Model Execution<a class="headerlink" href="#during-model-execution" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>When we run the quantized model <code class="docutils literal notranslate"><span class="pre">model(inputs)</span></code>, we’ll run through the functional linear operator in nn.Linear::</dt><dd><p>return F.linear(input, weight, bias)</p>
</dd>
<dt>where input is a <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> Tensor, weight is an int4 <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code>, it calls into a <code class="docutils literal notranslate"><span class="pre">__torch_function__</span></code> of the <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> subclass, which will end up in an implementation for <code class="docutils literal notranslate"><span class="pre">F.linear</span></code> when one of the input is <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code>, so it calls::</dt><dd><p>return weight_tensor._quantized_linear_op(input_tensor, weight_tensor, bias)</p>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">_quantized_linear_op</span></code> goes through the <code class="docutils literal notranslate"><span class="pre">_AQT_QLINEAR_DISPATCH_TABLE</span></code> and checks each dispatch conditions, if the dispatch condition passes, it will call the implementation with <code class="docutils literal notranslate"><span class="pre">input</span></code>/<code class="docutils literal notranslate"><span class="pre">weight</span></code>/<code class="docutils literal notranslate"><span class="pre">bias</span></code>. Please check out <a class="reference external" href="https://github.com/pytorch/ao/blob/4865ee61340cc63a1469f437388067b853c9289e/torchao/dtypes/affine_quantized_tensor.py#L97">this doc</a> for the explanation of <code class="docutils literal notranslate"><span class="pre">dispatch_condition</span></code> and <code class="docutils literal notranslate"><span class="pre">impl</span></code>.</p>
<p>int4 weight only <a class="reference external" href="https://github.com/pytorch/ao/blob/242f181fe59e233b458740b06464ad42da8df6af/torchao/dtypes/affine_quantized_tensor.py#L1784">dispatch_condition</a> checks if the input is <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> Tensor and weight is a uint4 <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code>
wint4 weight only quantization <a class="reference external" href="https://github.com/pytorch/ao/blob/242f181fe59e233b458740b06464ad42da8df6af/torchao/dtypes/affine_quantized_tensor.py#L1800">kernel implementation</a> takes an bfloat16 input Tensor and an int4 AffineQuantizedTensor, and call <code class="docutils literal notranslate"><span class="pre">torch.ops.aten._weight_int4pack_mm</span></code> with the input Tensor and the packed weight that’s stored in <code class="docutils literal notranslate"><span class="pre">weight_tensor.tensor_impl</span></code>.</p>
</section>
<section id="during-save-load">
<h4>During Save/Load<a class="headerlink" href="#during-save-load" title="Permalink to this heading">¶</a></h4>
<p>Since <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> weight is still a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, save/load works the same way as the original high precision floating point model. See the <a class="reference external" href="https://pytorch.org/ao/stable/serialization.html">serialization doc</a> for more details.</p>
</section>
</section>
</section>
<section id="tensor-subclass-developer-guide">
<h2>Tensor Subclass Developer Guide<a class="headerlink" href="#tensor-subclass-developer-guide" title="Permalink to this heading">¶</a></h2>
<p>We have covered high level overview and how everything is connected together in the previous section, this section will focus on Tensor Subclasses, which is the main extension point we rely on to provide flexibility of supporting inference, training and fine tuning with low precision Tensors and composability with torch.compile, autograd, distributed primitives in these scenarios.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h3>
<p>Some externally available resources for tensor subclasses:</p>
<ul class="simple">
<li><p><a class="reference external" href="pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor">tensor subclass doc</a></p></li>
<li><p><a class="reference external" href="https://podcasts.apple.com/us/podcast/tensor-subclasses-and-pt2/id1566080008?i=1000646728968">Edward’s podcast about tensor subclasses</a></p></li>
<li><p><a class="reference external" href="https://github.com/albanD/subclass_zoo">Tensor subclass zoo</a></p></li>
</ul>
</section>
<section id="why-tensor-subclass">
<h3>Why Tensor Subclass?<a class="headerlink" href="#why-tensor-subclass" title="Permalink to this heading">¶</a></h3>
<p>There are multiple ways people can implement quantization techniques or new dtypes, main motivation for us to recommend the tensor subclass based approach are three things:
(1). It’s natural for quantization to be modeled as a dtype conversion, so implementing it with tensor subclass means we are not introducing new concepts but reusing existing concepts like dtype, layout that already exists in pytorch core
(2). Since tensor subclass intercepts computation at torch function or aten ops level, as long as the same function/operator is used, we will be able to quantize the model. This allows the model that’s using variants of native modules (e.g. a slightly modified version of nn.Linear) to still be compatible with quantization
(3). Tensor subclass is also the approach adopted by other techniques like sparsity and distributed, so implementing quantization or dtype conversion with tensor subclass would make it easier for it to be composable with these techniques</p>
</section>
<section id="example-code-for-a-new-dtype">
<h3>Example Code for a new DType<a class="headerlink" href="#example-code-for-a-new-dtype" title="Permalink to this heading">¶</a></h3>
<p>Please feel free to start with <a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/my_dtype_tensor_subclass.py">tutorial</a> for a end to end working example that combines everything we talked about together and come back to the doc for clarifications and documentations.</p>
</section>
<section id="basic-structure">
<h3>Basic Structure<a class="headerlink" href="#basic-structure" title="Permalink to this heading">¶</a></h3>
<p>A tensor subclass needs to define a few basic methods: <code class="docutils literal notranslate"><span class="pre">__new__</span></code>, <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">__tensor_flatten__</span></code>, <code class="docutils literal notranslate"><span class="pre">__tensor_unflatten__</span></code>
and also dispatch functions for torch functions <code class="docutils literal notranslate"><span class="pre">__torch_function__</span></code> and aten ops <code class="docutils literal notranslate"><span class="pre">__torch_dispatch__</span></code>.</p>
<dl>
<dt>Here is an example of basic structure::</dt><dd><p># check out docs in <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/utils.py#L437">https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/utils.py#L437</a>
from torchao.utils import TorchAOBaseTensor</p>
<dl>
<dt>class MyDTypeLayout(TorchAOBaseTensor):</dt><dd><p># see tutorial code for details
pass</p>
</dd>
<dt>class MyDtypeTensor(TorchAOBaseTensor):</dt><dd><blockquote>
<div><p>“””We need to define <cite>__new__</cite> for constructing a new tensor subclass instance and <cite>__init__</cite> for initialize
the instance. There is no requirement on what the argument list should look like here, only requirement is
that <cite>__new__</cite> must return a Tensor instance with <cite>torch.Tensor._make_wrapper_subclass(cls, shape, …)</cite> call
“””
&#64;staticmethod
def __new__(</p>
<blockquote>
<div><p>cls,
tensor_impl: MyDTypeLayout,
shape: torch.Size,
dtype: Optional[torch.dtype] = None,</p>
</div></blockquote>
<dl class="simple">
<dt>):</dt><dd><p>…
return torch.Tensor._make_wrapper_subclass(cls, shape, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs)  # type: ignore[attr-defined]</p>
</dd>
<dt>def __init__(</dt><dd><p>self,
tensor_impl: MyDTypeLayout,
shape: torch.Size, …</p>
</dd>
<dt>):</dt><dd><p>self.tensor_impl = tensor_impl</p>
</dd>
</dl>
<p>“””<cite>__tensor_flatten__</cite> and <cite>__tensor_unflatten__</cite> are used to desugar the tensor into native Tensors/attributes and
reconstruct the tensor subclass instance from the desugared tensor and attributes, these are required to define
a Tensor subclass for torch.compile support
“””
def __tensor_flatten__(self):</p>
<blockquote>
<div><p>return [“tensor_impl”], [self.shape]</p>
</div></blockquote>
<p>“””see <a class="reference external" href="https://github.com/pytorch/pytorch/blob/3bc2004f9123a32f381ef64202252d59109507f3/torch/utils/_python_dispatch.py#L289">https://github.com/pytorch/pytorch/blob/3bc2004f9123a32f381ef64202252d59109507f3/torch/utils/_python_dispatch.py#L289</a> for documentations for outer_size and outer_stride
“””
&#64;classmethod
def __tensor_unflatten__(</p>
<blockquote>
<div><p>cls, tensor_data_dict, tensor_attributes, outer_size, outer_stride</p>
</div></blockquote>
<dl>
<dt>):</dt><dd><dl>
<dt>tensor_impl = tensor_data_dict[“tensor_impl”]</dt><dd><p>shape, = tensor_attributes
return cls(</p>
<blockquote>
<div><p>tensor_impl,
shape if outer_size is None else outer_size,</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
</dd>
</dl>
<p>“””classmethod that converts from a floating point Tensor (fp32/fp16/bf16) to the current dtype
“””</p>
</div></blockquote>
<dl>
<dt>&#64;classmethod</dt><dd><dl class="simple">
<dt>def from_float(</dt><dd><p>cls,
input_float: torch.Tensor,</p>
</dd>
<dt>):</dt><dd><p>mapping_type = MappingType.SYMMETRIC
block_size = input_float.shape
dtype = torch.int16
scale, _ = choose_qparams_affine(input_float, mapping_type, block_size, dtype)
int_data = (input_float / scale).to(torch.int8)
tensor_impl = MyDTypeLayout.from_plain(int_data, scale)
return cls(tensor_impl, input_float.shape)</p>
</dd>
</dl>
<p>“””[Optional] see docs for <cite>Layout/Packing</cite> under <cite>Quantized Tensors</cite> section to understand what layout_type is
“””
&#64;property
def _layout(self) -&gt; LayoutType:</p>
<blockquote>
<div><p>return self.tensor_impl._layout</p>
</div></blockquote>
<p>“””There are two entry points that we can modify the behavior of a pytorch op: torch_function and torch_dispatch:</p>
<p>__torch_function__: will be called whenever a torch level function is called on the Tensor object, for example: torch.nn.functional.linear,
tensor.detach, tensor.reshape, tensor.t etc.</p>
<p>__torch_dispatch__: will be called in the C++ dispatcher, when an aten operator is called on the Tensor object, for example:
aten.mm, aten.addmm, aten.detach.default, aten.t.default etc.
you can checkout <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/utils.py#L361-L389">https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/utils.py#L361-L389</a> to understand what <cite>__torch_function__</cite> and <cite>__torch_dispatch__</cite> are doing, but with <cite>TorchAoBaseTensor</cite> user can use
some helper functions directly (see next section)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</section>
<section id="operator-support">
<h3>Operator Support<a class="headerlink" href="#operator-support" title="Permalink to this heading">¶</a></h3>
<p>There are two types of operator support, torch function and aten ops. For torch functions (e.g. <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear</span></code>), we’ll need to overwrite <code class="docutils literal notranslate"><span class="pre">__torch_function__</span></code> callback in the Tensor subclass, for aten ops (e.g. <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.mm</span></code>), we’ll need to overwrite <code class="docutils literal notranslate"><span class="pre">__torch_dispatch__</span></code> callback function.</p>
<dl>
<dt>For a new dtype, we’d like people to define the following decorator::</dt><dd><p>if your dtype class is inherited from <cite>torchao.utils.TorchAoBaseTensor</cite>, you can do:</p>
<p>implements = my_dtype_tensor_cls.implements</p>
</dd>
<dt>And we can implement the operator dispatch with the following::</dt><dd><p># Example for torch_function dispatch for torch.nn.functional.linear
def _quantized_linear_op(input_tensor, weight_tensor, bias):</p>
<blockquote>
<div><dl class="simple">
<dt>if isinstance(input_tensor, MyDtypeTensor):</dt><dd><p>input_tensor = input_tensor.dequantize()</p>
</dd>
<dt>if isinstance(weight_tensor, MyDtypeTensor):</dt><dd><p>weight_tensor = weight_tensor.dequantize()</p>
</dd>
</dl>
<p>return torch.nn.functional.linear(input_tensor, weight_tensor, bias)</p>
</div></blockquote>
<p>&#64;implements(torch.nn.functional.linear)
def _(<a href="#id3"><span class="problematic" id="id4">*</span></a>args, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs):</p>
<blockquote>
<div><dl class="simple">
<dt>input_tensor, weight_tensor, bias = (</dt><dd><p>args[0],
args[1],
args[2] if len(args) &gt; 2 else None,</p>
</dd>
</dl>
<p>)
# using try/except here so that we can have a general fallback when input_tensor/weight_tensor
# is not picked up by any of the dispatch paths in <cite>_quantized_linear_op</cite>, this allows us to
# make the branches easier to understand in <cite>_quantized_linear_op</cite>
try:</p>
<blockquote>
<div><p>return _quantized_linear_op(input_tensor, weight_tensor, bias)</p>
</div></blockquote>
<dl>
<dt>except NotImplementedError:</dt><dd><dl class="simple">
<dt>if isinstance(input_tensor, MyDtypeTensor):</dt><dd><p>input_tensor = input_tensor.dequantize()</p>
</dd>
<dt>if isinstance(weight_tensor, MyDtypeTensor):</dt><dd><p>weight_tensor = weight_tensor.dequantize()</p>
</dd>
</dl>
<p>return torch.nn.functional.linear(input_tensor, weight_tensor, bias)</p>
</dd>
</dl>
</div></blockquote>
<p># Example for aten op dispatch for aten.detach.default
&#64;implements(aten.detach.default)
def _(func, <a href="#id7"><span class="problematic" id="id8">*</span></a>args, <a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs):</p>
<blockquote>
<div><p># <cite>return_and_correct_aliasing</cite> should be used by wrapper tensor <code class="docutils literal notranslate"><span class="pre">__torch_dispatch__</span></code> subclasses that would like to
# work with torch.compile. It ensures that the subclass properly implements the aliasing behavior of every op,
# which is needed for correctness in AOTAutograd.</p>
<p># <cite>_apply_fn_to_data</cite> just applies the function to the tensor data in <cite>args[0]</cite>, <cite>args[0]</cite> is a tensor subclass
# of <cite>my_dtype</cite>
return return_and_correct_aliasing(</p>
<blockquote>
<div><p>func, args, kwargs, args[0]._apply_fn_to_data(torch.detach)</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
</dd>
</dl>
<p>What ops do we need to overwrite? This depends on the model we are trying to quantize, commonly overwritten ops are:
<code class="docutils literal notranslate"><span class="pre">__torch_function__</span></code>: <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear</span></code>
<code class="docutils literal notranslate"><span class="pre">__torch_dispatch__</span></code>: <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.addmm.default</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.mm.default</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.detach.default</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.t.default</span></code></p>
<dl>
<dt>You can also find the ops that can be overwritten in <code class="docutils literal notranslate"><span class="pre">__torch_function__</span></code> or <code class="docutils literal notranslate"><span class="pre">__torch_dispatch__</span></code> with the following code, and you can start with a model that you want to optimize, start with just overwriting the important ops like linear, and gradually expand the coverage until the test runs and you get the expected optimized generated code (see Optimized Operators section for more details)::</dt><dd><dl class="simple">
<dt>class M(torch.nn.Module):</dt><dd><dl class="simple">
<dt>def __init__(self) -&gt; None:</dt><dd><p>super().__init__()
self.linear = torch.nn.Linear(10, 10)</p>
</dd>
<dt>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</dt><dd><p>return self.linear(x) + x</p>
</dd>
</dl>
</dd>
</dl>
<p>from torch.overrides import TorchFunctionMode
class TorchFunctionLoggingMode(TorchFunctionMode):</p>
<blockquote>
<div><dl>
<dt>def __torch_function__(cls, func, types, args=(), kwargs=None):</dt><dd><dl class="simple">
<dt>if kwargs is None:</dt><dd><p>kwargs = {}</p>
</dd>
</dl>
<p>print(f”TORCH_FUNC={str(func)}”)
return func(<a href="#id11"><span class="problematic" id="id12">*</span></a>args, <a href="#id13"><span class="problematic" id="id14">**</span></a>kwargs)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>with TorchFunctionLoggingMode():</dt><dd><p>m(<a href="#id15"><span class="problematic" id="id16">*</span></a>example_inputs)</p>
</dd>
</dl>
<p>## Example output
# TORCH_FUNC=&lt;built-in function linear&gt;
# TORCH_FUNC=&lt;method ‘add’ of ‘torch._C.TensorBase’ objects&gt;</p>
<p>from torch.utils._python_dispatch import TorchDispatchMode
class TorchDispatchLoggingMode(TorchDispatchMode):</p>
<blockquote>
<div><dl>
<dt>def __torch_dispatch__(cls, func, types, args=(), kwargs=None):</dt><dd><dl class="simple">
<dt>if kwargs is None:</dt><dd><p>kwargs = {}</p>
</dd>
</dl>
<p>print(f”ATEN_FUNC={str(func)}”)
return func(<a href="#id17"><span class="problematic" id="id18">*</span></a>args, <a href="#id19"><span class="problematic" id="id20">**</span></a>kwargs)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>with TorchDispatchLoggingMode():</dt><dd><p>m(<a href="#id21"><span class="problematic" id="id22">*</span></a>example_inputs)</p>
</dd>
</dl>
<p>## Example output
# ATEN_FUNC=aten.t.default
# ATEN_FUNC=aten.addmm.default
# ATEN_FUNC=aten.add.Tensor</p>
<p># or a more polished logging for torch_dispatch (aten) ops: <a class="reference external" href="https://github.com/albanD/subclass_zoo/blob/main/logging_mode.py">https://github.com/albanD/subclass_zoo/blob/main/logging_mode.py</a></p>
</dd>
</dl>
<p>Alternatively, you can run a test example (e.g. use your quantized model with tensor parallelism, FSDP etc.) and discover the missing ops and add them until the test passes.</p>
<p>We are still working on a table that talks about for each feature what are the operators that need to be supported.</p>
</section>
<section id="adding-efficient-kernels">
<h3>Adding Efficient Kernels<a class="headerlink" href="#adding-efficient-kernels" title="Permalink to this heading">¶</a></h3>
<section id="custom-triton-kernels">
<h4>Custom triton kernels<a class="headerlink" href="#custom-triton-kernels" title="Permalink to this heading">¶</a></h4>
<p>Custom triton kernels can be implemented and registered in <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/kernel">torchao/kernel</a></p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/0bdde92114b470823aa24725bf3b0811e980c8ce/torchao/kernel/intmm_triton.py#L270-L302">Implementation Example</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/0bdde92114b470823aa24725bf3b0811e980c8ce/torchao/kernel/intmm_triton.py#L337-L364">Register as a custom op</a></p></li>
</ul>
<p>You may need to define you own <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/kernel/autotuner.py">autotuner</a> as well.</p>
</section>
<section id="custom-hand-written-kernels">
<h4>Custom hand written kernels<a class="headerlink" href="#custom-hand-written-kernels" title="Permalink to this heading">¶</a></h4>
<p>Custom kernels (implementations) for cpu/cuda/mps can be implemented through <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/csrc">torchao/csrc</a> e.g. int4 cuda, and accessible through torch.ops.my_custom_op</p>
</section>
</section>
<section id="dispatches">
<h3>Dispatches<a class="headerlink" href="#dispatches" title="Permalink to this heading">¶</a></h3>
<p>For dispatching to optimized kernels for cpu/cuda/mps devices, we can have checks for the dispatch conditions in <code class="docutils literal notranslate"><span class="pre">__torch_function__</span></code> or <code class="docutils literal notranslate"><span class="pre">__torch_dispatch__</span></code> and dispatch to target operators, for example, condition for bfloat16 activation and uint4 weight kernel can be found <a class="reference external" href="https://github.com/pytorch/ao/blob/242f181fe59e233b458740b06464ad42da8df6af/torchao/dtypes/affine_quantized_tensor.py#L1784-L1797">here</a>.</p>
<p>Specifically for <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code>, we also allow people to extend the quantized linear to use a new efficient kernel or implement by defining two functions:
<code class="docutils literal notranslate"><span class="pre">dispatch_condition</span></code> (defines the condition to dispatch to the kernel) and impl (actual implementation that takes activation, (quantized) weight, bias Tensor and runs the efficient kernel), both taking <code class="docutils literal notranslate"><span class="pre">input_tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">bias</span></code> as argument, and can be registered into dispatch of quantized linear in <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> with <code class="docutils literal notranslate"><span class="pre">register_aqt_quantized_linear_dispatch</span></code>. <a class="reference external" href="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/test/dtypes/test_affine_quantized.py#L92-L113">Here</a> is an example showing how it works.</p>
</section>
<section id="layout-tensorimpl">
<h3>Layout/TensorImpl<a class="headerlink" href="#layout-tensorimpl" title="Permalink to this heading">¶</a></h3>
<p>Sometimes the quantized weights has to be packed in order to yield optimal performance. And this can be abstracted with <code class="docutils literal notranslate"><span class="pre">layout</span></code>. See <a class="reference external" href="https://github.com/pytorch/ao/blob/17a0a96d24ebfc154a23342b84e788d9ed6776f4/tutorials/developer_api_guide/my_dtype_tensor_subclass.py#L215-L317">here</a> for full example.</p>
</section>
<section id="flow">
<h3>Flow<a class="headerlink" href="#flow" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>After the tensor subclass is implemented, we can also wrap that into factory functions, e.g.::</dt><dd><p># convert from floating point tensor to my dtype tensor subclass
to_my_dtype = MyDTypeTensor.from_float</p>
</dd>
</dl>
<p>For model level API, people can reuse <code class="docutils literal notranslate"><span class="pre">torchao.quantize_</span></code> that allows people to apply a tensor subclass conversion to weight of linear, and allows <a class="reference external" href="https://github.com/pytorch/ao/blob/17a0a96d24ebfc154a23342b84e788d9ed6776f4/torchao/quantization/quant_api.py#L421">filtering function</a> to choose which module the tensor subclass conversion should be applied to.</p>
<p>See Quantization Algorithms/Flows section for examples of weight only/dynamic quant/static quant and other types of model level APIs based on the factory function.</p>
</section>
<section id="using-torch-compile-for-performance">
<h3>Using torch.compile for Performance<a class="headerlink" href="#using-torch-compile-for-performance" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>Note: for pytorch 2.4 and below, we need to use the following::</dt><dd><p>from torchao.utils import unwrap_tensor_subclass
m_unwrapped = unwrap_tensor_subclass(m)</p>
</dd>
<dt>In order to be compatible with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, to aim for performance optimization, we should run through <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> with <code class="docutils literal notranslate"><span class="pre">fullgraph=True</span></code> first, and remove any unnecessary graph breaks. You can add <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=&quot;output_code&quot;</span></code> when you run the script in order to see the inductor generated code. e.g. <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=&quot;output_code&quot;</span> <span class="pre">python</span> <span class="pre">example.py</span></code>::</dt><dd><p>model = torch.compile(model, mode=”max-autotune”, fullgraph=True)</p>
</dd>
</dl>
</section>
<section id="serialization">
<h3>Serialization<a class="headerlink" href="#serialization" title="Permalink to this heading">¶</a></h3>
<p>Please checkout the <a class="reference external" href="https://pytorch.org/ao/stable/serialization.html">serialization doc</a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We are integrated with huggingface transformer and supports serialization/deserialization through the huggingface save_pretrained/push_to_hub/from_pretrained APIs: <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/quantization/torchao">https://huggingface.co/docs/transformers/main/en/quantization/torchao</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Another example can be found in integration with diffuser: <a class="reference external" href="https://github.com/sayakpaul/diffusers-torchao/blob/main/inference/serialization_and_loading.md">https://github.com/sayakpaul/diffusers-torchao/blob/main/inference/serialization_and_loading.md</a></p>
</div>
</section>
<section id="other-feature-support">
<h3>Other Feature Support<a class="headerlink" href="#other-feature-support" title="Permalink to this heading">¶</a></h3>
<p>The above just talks about basic feature support, we also provide examples on how to add supports for training, tensor parallel, FSDP by extending the <a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/my_dtype_tensor_subclass.py">MyDTypeTensor</a>, we’ll put more examples in <a class="reference external" href="https://github.com/pytorch/ao/tree/main/tutorials/developer_api_guide">developer_api_guide</a> folder covering the following use cases.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/my_trainable_tensor_subclass.py">Quantized Training</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/tensor_parallel.py">Tensor Parallel Support for Quantized Tensor</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/export_to_executorch.py">Compatibility with executorch / torchchat</a></p></li>
<li><p>[TODO] FSDP</p></li>
<li><p>[TODO] QAT</p></li>
</ul>
</section>
<section id="general-guide-on-extending-torchao">
<h3>General Guide on Extending torchao<a class="headerlink" href="#general-guide-on-extending-torchao" title="Permalink to this heading">¶</a></h3>
<p>For a new use case, for example, a training dtype (like fp4 training), it’s fine to start with adding a new tensor subclass in prototype folder <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/prototype">torchao/prototype</a>, but you could also take a look at <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> if what you want to do is mostly supported there, e.g. adding int3 kernel for the exact same affine quantization. Please feel free to open an issue and if you have questions on what to do for a specific new use case.</p>
<p>To contribute to existing code base:</p>
<ul class="simple">
<li><p>Adding features to AffineQuantizedTensor, e.g. making it trainable, add tensor parallelism support etc.: <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/dtypes/affine_quantized_tensor.py">torchao/dtypes/affine_quantized_tensor.py</a></p></li>
<li><p>Adding new quantization APIs: <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py">torchao/quantization/quant_api.py</a></p></li>
<li><p>Adding new quantization primitive ops, e.g. slight variations of existing quantization primitive ops: <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_primitives.py">torchao/quantization/quant_primitives.py</a></p></li>
<li><p>Adding new autotuned triton kernels: <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/kernel">torchao/kernel</a></p></li>
<li><p>Adding new custom cpu/cuda/mps kernels: <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/csrc">torchao/csrc</a></p></li>
<li><p>Integrating custom kernel with AffineQuantizedTensor (maybe a new layout as well): Add sparse marlin AQT layout <a class="reference external" href="https://github.com/pytorch/ao/pull/621">#621</a> as an example. We are still not decided if we want to split <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> to more tensor subclasses or not.</p></li>
</ul>
</section>
<section id="tensor-subclass-functionality-composability-testing">
<h3>Tensor Subclass Functionality/Composability Testing<a class="headerlink" href="#tensor-subclass-functionality-composability-testing" title="Permalink to this heading">¶</a></h3>
<p>We are also working on test suites to test out the functionalities of tensor subclass and the composability with different systems like torch.compile, DTensor etc. (we recommend to copy paste the tests and adapt to test your own tensor subclass for now):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/17a0a96d24ebfc154a23342b84e788d9ed6776f4/torchao/testing/utils.py#L74">Basic Test</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/17a0a96d24ebfc154a23342b84e788d9ed6776f4/torchao/testing/utils.py#L147">Compile Test</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/blob/17a0a96d24ebfc154a23342b84e788d9ed6776f4/torchao/testing/utils.py#L227">Tensor Parallel Test</a></p></li>
</ul>
</section>
<section id="kernel-microbenchmarks">
<h3>Kernel Microbenchmarks<a class="headerlink" href="#kernel-microbenchmarks" title="Permalink to this heading">¶</a></h3>
<p>Before we test performance on models, we can also do some microbenchmarks on single linear operator (or other compute intensive/memory intensive) operators with different input dimensions to get a sense of speedup. For a specific kernel that you’d like to benchmark, you can create a benchmark file like <a class="reference external" href="https://github.com/pytorch/ao/blob/main/benchmarks/benchmark_aq.py">benchmarks/benchmark_aq.py</a> and run benchmark with different shapes that’s important for target model. A quick way to get the relevant shape for linear op and other ops is by running the example with <a class="reference external" href="https://github.com/pytorch/ao/blob/main/tutorials/developer_api_guide/print_op_and_shapes.py">this</a>.</p>
<dl>
<dt>Change the model with the model you are interested in optimizing, and run the following::</dt><dd><p>python tutorials/developer_api_guide/print_op_and_shapes.py</p>
</dd>
<dt>Example output::</dt><dd><p>TORCH_FUNC=&lt;built-in function linear&gt; (M, K, N): 10 10 10
TORCH_FUNC=&lt;method ‘add’ of ‘torch._C.TensorBase’ objects&gt; args[0] shape: torch.Size([10, 10])</p>
<p>all linear shapes (M, K, N): [(10, 10, 10)]</p>
</dd>
</dl>
<p>The output of all linear shapes can be copy pasted to microbenchmarking script code under <code class="docutils literal notranslate"><span class="pre">benchmarks/benchmark_your_kernel.py</span></code> for benchmarking.</p>
<p>For benchmark helper functions, right now we have <a class="reference external" href="https://github.com/pytorch/ao/blob/0bdde92114b470823aa24725bf3b0811e980c8ce/torchao/utils.py#L55">1</a> and <a class="reference external" href="https://github.com/pytorch/ao/blob/0bdde92114b470823aa24725bf3b0811e980c8ce/torchao/utils.py#L139">2</a>, feel free to use either one for now, but we’ll probably keep one in the future.</p>
</section>
<section id="model-benchmarks-and-eval">
<h3>Model Benchmarks and Eval<a class="headerlink" href="#model-benchmarks-and-eval" title="Permalink to this heading">¶</a></h3>
<p>After you have the quantization flow implemented, you can run benchmark and eval on llama (llama2/llama3) or sam models that are already modified to be friendly to torch.compile, and compare with existing techniques in torchao.</p>
<p>Note: llama model (llama2/llama3) is our representative model for memory bound models and sam is our representative model for compute bound models.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/_models/llama">llama</a>
* <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/_models/llama/generate.py">benchmark</a>
* <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/_models/llama/eval.py">eval</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/_models/sam">sam</a>
* <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/_models/sam/eval_combo.py">benchmark and eval</a></p></li>
</ul>
<p>Please checkout the <code class="docutils literal notranslate"><span class="pre">--help</span></code> option for each of the script to understand the supported options, e.g. you can use <code class="docutils literal notranslate"><span class="pre">--profile=profile_path</span></code> to get the chrome trace of the run to understand detailed <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-tracing-functionality">chrome trace</a>.</p>
<p>Please let us know if there are any new important models that makes sense to be added to torchao model benchmark/eval folder.</p>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="serialization.html" class="btn btn-neutral float-right" title="Serialization" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="generated/torchao.sparsity.apply_fake_sparsity.html" class="btn btn-neutral" title="apply_fake_sparsity" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torchao Contributor Guide</a><ul>
<li><a class="reference internal" href="#objective">Objective</a></li>
<li><a class="reference internal" href="#torchao-stack-overview">torchao Stack Overview</a><ul>
<li><a class="reference internal" href="#basic-dtypes">Basic DTypes</a><ul>
<li><a class="reference internal" href="#current-support">Current Support</a><ul>
<li><a class="reference internal" href="#adding-placeholder-dtype-in-pytorch">Adding placeholder dtype in PyTorch</a></li>
<li><a class="reference internal" href="#implementing-tensor-operations-for-these-dtypes-with-tensor-subclasses">Implementing tensor operations for these dtypes with Tensor subclasses</a></li>
<li><a class="reference internal" href="#integrate-tensor-subclass-to-pytorch-native-factory-functions">Integrate Tensor subclass to pytorch native factory functions</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#quantization-primitive-ops">Quantization Primitive Ops</a></li>
<li><a class="reference internal" href="#efficient-kernels">Efficient kernels</a></li>
<li><a class="reference internal" href="#quantized-tensors-derived-dtypes">Quantized Tensors (derived dtypes)</a><ul>
<li><a class="reference internal" href="#layout-and-tensorimpl">Layout and TensorImpl</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantization-algorithms-flows">Quantization Algorithms/Flows</a><ul>
<li><a class="reference internal" href="#weight-only-quantization">Weight Only Quantization</a></li>
<li><a class="reference internal" href="#dynamic-activation-and-weight-quantization">Dynamic Activation and Weight Quantization</a></li>
<li><a class="reference internal" href="#static-activation-quantization-and-weight-quantization">Static Activation Quantization and Weight Quantization</a><ul>
<li><a class="reference internal" href="#insert-observers">Insert Observers</a><ul>
<li><a class="reference internal" href="#how-to-define-observer-module">How to define observer module</a></li>
<li><a class="reference internal" href="#how-to-add-observer-module-to-the-model">How to add observer module to the model</a></li>
<li><a class="reference internal" href="#calibration">Calibration</a></li>
<li><a class="reference internal" href="#quantize">Quantize</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#other-quantization-flows">Other Quantization Flows</a></li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#quantization-aware-training">Quantization Aware Training</a></li>
<li><a class="reference internal" href="#low-bit-optimizers">Low Bit Optimizers</a></li>
<li><a class="reference internal" href="#quantized-training">Quantized Training</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#case-study-how-int4-weight-only-quantization-works-in-torchao">Case Study: How int4 weight only quantization works in torchao?</a><ul>
<li><a class="reference internal" href="#high-level-summary">High Level Summary</a></li>
<li><a class="reference internal" href="#during-quantization">During Quantization</a></li>
<li><a class="reference internal" href="#during-model-execution">During Model Execution</a></li>
<li><a class="reference internal" href="#during-save-load">During Save/Load</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tensor-subclass-developer-guide">Tensor Subclass Developer Guide</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li><a class="reference internal" href="#why-tensor-subclass">Why Tensor Subclass?</a></li>
<li><a class="reference internal" href="#example-code-for-a-new-dtype">Example Code for a new DType</a></li>
<li><a class="reference internal" href="#basic-structure">Basic Structure</a></li>
<li><a class="reference internal" href="#operator-support">Operator Support</a></li>
<li><a class="reference internal" href="#adding-efficient-kernels">Adding Efficient Kernels</a><ul>
<li><a class="reference internal" href="#custom-triton-kernels">Custom triton kernels</a></li>
<li><a class="reference internal" href="#custom-hand-written-kernels">Custom hand written kernels</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dispatches">Dispatches</a></li>
<li><a class="reference internal" href="#layout-tensorimpl">Layout/TensorImpl</a></li>
<li><a class="reference internal" href="#flow">Flow</a></li>
<li><a class="reference internal" href="#using-torch-compile-for-performance">Using torch.compile for Performance</a></li>
<li><a class="reference internal" href="#serialization">Serialization</a></li>
<li><a class="reference internal" href="#other-feature-support">Other Feature Support</a></li>
<li><a class="reference internal" href="#general-guide-on-extending-torchao">General Guide on Extending torchao</a></li>
<li><a class="reference internal" href="#tensor-subclass-functionality-composability-testing">Tensor Subclass Functionality/Composability Testing</a></li>
<li><a class="reference internal" href="#kernel-microbenchmarks">Kernel Microbenchmarks</a></li>
<li><a class="reference internal" href="#model-benchmarks-and-eval">Model Benchmarks and Eval</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
         <script src="_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch-labs/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>