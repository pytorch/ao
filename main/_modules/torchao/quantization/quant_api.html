


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.quantization.quant_api &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributor_guide.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_api_guide.html">Benchmarking API Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_user_guide.html">Benchmarking User Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_float8.html">torchao.float8</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Eager Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../pretraining.html">(Part 1) Pre-training with float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../finetuning.html">(Part 2) Fine-tuning with QAT, QLoRA, and float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serving.html">(Part 3) Serving on vLLM, SGLang, ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchao_vllm_integration.html">Integration with VLLM: Architecture and Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../static_quantization.html">Static Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PT2E Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_ptq.html">PyTorch 2 Export Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_qat.html">PyTorch 2 Export Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">PyTorch 2 Export Quantization with X86 Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">PyTorch 2 Export Quantization with Intel GPU Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">PyTorch 2 Export Quantization for OpenVINO torch.compile Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quantizer.html">How to Write a <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code> for PyTorch 2 Export Quantization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.quantization.quant_api</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.quantization.quant_api</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>

<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Quantization APIs</span>

<span class="sd">Generally these APIs can be applied directly to any model</span>
<span class="sd">with Linear modules to obtain quantized linear ops. The intended</span>
<span class="sd">usage involves applying torch.compile to the model afterwards</span>
<span class="sd">both because primitives were designed based on the fusions that</span>
<span class="sd">come along with it and because that is how we access the intended quantized</span>
<span class="sd">and mixed GEMM kernels</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.parametrize</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">parametrize</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torchao</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.core.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">AOBaseConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">CutlassInt4PackedLayout</span><span class="p">,</span>
    <span class="n">CutlassSemiSparseLayout</span><span class="p">,</span>
    <span class="n">Float8Layout</span><span class="p">,</span>
    <span class="n">Int4CPULayout</span><span class="p">,</span>
    <span class="n">Int4XPULayout</span><span class="p">,</span>
    <span class="n">Int8DynamicActInt4WeightCPULayout</span><span class="p">,</span>
    <span class="n">MarlinQQQLayout</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">,</span>
    <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">,</span>
    <span class="n">PlainLayout</span><span class="p">,</span>
    <span class="n">QDQLayout</span><span class="p">,</span>
    <span class="n">SemiSparseLayout</span><span class="p">,</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">,</span>
    <span class="n">UintxLayout</span><span class="p">,</span>
    <span class="n">to_affine_quantized_floatx</span><span class="p">,</span>
    <span class="n">to_affine_quantized_floatx_static</span><span class="p">,</span>
    <span class="n">to_affine_quantized_intx</span><span class="p">,</span>
    <span class="n">to_fbgemm_fp8</span><span class="p">,</span>
    <span class="n">to_fbgemm_int4</span><span class="p">,</span>
    <span class="n">to_marlinqqq_quantized_intx</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.uintx.packed_linear_int8_dynamic_activation_intx_weight_layout</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Target</span><span class="p">,</span>
    <span class="n">make_packed_linear_int8_dynamic_activation_intx_weight_tensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Layout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">e4m3_dtype</span><span class="p">,</span> <span class="n">e5m2_dtype</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.float8_linear</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float8Linear</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.inference</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Float8MMConfig</span><span class="p">,</span>
    <span class="n">FP8Granularity</span><span class="p">,</span>
    <span class="n">_check_hardware_support</span><span class="p">,</span>
    <span class="n">_normalize_granularity</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.linear_activation_weight_observed_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationWeightObservedTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.observer</span><span class="w"> </span><span class="kn">import</span> <span class="n">AffineQuantizedObserverBase</span><span class="p">,</span> <span class="n">get_block_size</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.workflows</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4PreshuffledTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.transform_module</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">,</span>
    <span class="n">register_quantize_module_handler</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.weight_tensor_linear_activation_quantization</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">to_weight_tensor_with_linear_activation_quantization_metadata</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_4</span><span class="p">,</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">,</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">,</span>
    <span class="n">_is_fbgemm_genai_gpu_available</span><span class="p">,</span>
    <span class="n">is_MI300</span><span class="p">,</span>
    <span class="n">is_sm_at_least_89</span><span class="p">,</span>
    <span class="n">is_sm_at_least_90</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.autoquant</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">,</span> <span class="n">autoquant</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.GPTQ</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyGPTQQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Granularity</span><span class="p">,</span>
    <span class="n">PerAxis</span><span class="p">,</span>
    <span class="n">PerGroup</span><span class="p">,</span>
    <span class="n">PerRow</span><span class="p">,</span>
    <span class="n">PerTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.linear_activation_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationQuantizedTensor</span><span class="p">,</span>
    <span class="n">to_linear_activation_quantized</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.linear_quant_modules</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyQuantizer</span><span class="p">,</span>
    <span class="n">Int8DynActInt4WeightQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.qat</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">intx_quantization_aware_training</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">,</span>
    <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">ZeroPointDomain</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.subclass</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">Int8DynamicallyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">Int8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">QuantizedLinearWeightBase</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.unified</span><span class="w"> </span><span class="kn">import</span> <span class="n">Quantizer</span><span class="p">,</span> <span class="n">TwoStepQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_per_token_block_size</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;swap_conv2d_1x1_to_linear&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Quantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TwoStepQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyGPTQQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;autoquant&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_get_subclass_inserter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_dynamic_activation_int4_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_dynamic_activation_int8_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_dynamic_activation_int8_semi_sparse_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int4_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;int8_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;intx_quantization_aware_training&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float8_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;uintx_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fpx_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gemlite_uintx_weight_only&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float8_dynamic_activation_float8_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float8_static_activation_float8_weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int8DynActInt4WeightQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Float8DynamicActivationFloat8SemiSparseWeightConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ModuleFqnToConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FbgemmConfig&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">],</span>
    <span class="n">MarlinSparseLayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">],</span>
    <span class="n">Int4CPULayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">],</span>
    <span class="n">Int4XPULayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">LAYOUT_TO_PRESERVE_ZEROS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">Int4CPULayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">Int4XPULayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>


<span class="c1">######</span>
<span class="c1"># TO BE DEPRECATED START</span>
<span class="c1">######</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_in_features_greater_than_16</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;in_features&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">mod</span><span class="o">.</span><span class="n">in_features</span> <span class="o">&gt;</span> <span class="mi">16</span>


<span class="k">def</span><span class="w"> </span><span class="nf">change_linear_weights_to_int8_dqtensors</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts all linear weight tensors to the `Int8DynamicallyQuantizedLinearWeight`</span>
<span class="sd">    Tensor subclass, effectively applying the same form of quantization</span>
<span class="sd">    as apply_dynamic_quant while not modifying the linear modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;This API is deprecated for pytorch 2.4+, please checkout quantization/README.md for most up to date APIs&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_is_linear</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="n">_in_features_greater_than_16</span><span class="p">(</span>
            <span class="o">*</span><span class="n">args</span>
        <span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">_get_subclass_inserter</span><span class="p">(</span>
            <span class="n">Int8DynamicallyQuantizedLinearWeight</span><span class="p">,</span> <span class="n">enable_parametrization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">),</span>
        <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">change_linear_weights_to_int8_woqtensors</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts all linear weight tensors to the</span>
<span class="sd">    `Int8WeightOnlyQuantizedLinearWeight` tensor subclass,</span>
<span class="sd">    effectively applying the same form of quantization</span>
<span class="sd">    as apply_weight_only_int8_quant while not modifying the linear modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;This API is deprecated for pytorch 2.4+, please checkout quantization/README.md for most up to date APIs&quot;</span>
        <span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">_get_subclass_inserter</span><span class="p">(</span>
            <span class="n">Int8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="n">enable_parametrization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">),</span>
        <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">change_linear_weights_to_int4_woqtensors</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">groupsize</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span>
    <span class="n">preserve_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts all linear weight tensors to the</span>
<span class="sd">    `Int4WeightOnlyQuantizedLinearWeight` tensor subclass,</span>
<span class="sd">    effectively applying the same form of quantization</span>
<span class="sd">    as apply_dynamic_quant while not modifying the linear modules.</span>
<span class="sd">    Args:</span>
<span class="sd">        `groupsize`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained, choices are [256, 128, 64, 32]</span>
<span class="sd">        `inner_k_tiles`: parameter for int4 mm kernel, choices are [8, 4, 2]</span>
<span class="sd">        `filter_fn`: function that takes a nn.Module instance and fully qualified name of the module, \</span>
<span class="sd">            returns True if we want to run `config` on</span>
<span class="sd">        `zero_point_domain`: data type of zeros points, choices are [ZeroPointDomain.FLOAT, \</span>
<span class="sd">            ZeroPointDomain.INT, ZeroPointDomain.NONE]</span>
<span class="sd">        `preserve_zero`: whether to preserve zero, default is False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;This API is deprecated for pytorch 2.4+, please checkout quantization/README.md for most up to date APIs&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filter_fn</span> <span class="o">=</span> <span class="n">_is_linear</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">_get_subclass_inserter</span><span class="p">(</span>
            <span class="n">Int4WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
            <span class="n">enable_parametrization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groupsize</span><span class="o">=</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="n">inner_k_tiles</span><span class="o">=</span><span class="n">inner_k_tiles</span><span class="p">,</span>
            <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1">########</span>
<span class="c1"># TO BE DEPRECATED END</span>
<span class="c1">########</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">replacement_fn</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">,</span>
    <span class="n">cur_fqn</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively replaces each child module in `model` with the result of `replacement_fn(child)`</span>
<span class="sd">    if `filter_fn(child)` returns `True`.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model containing modules to be replaced.</span>
<span class="sd">        replacement_fn (Callable[[torch.nn.Module], torch.nn.Module]): The function to replace matching modules.</span>
<span class="sd">        filter_fn (Callable[[torch.nn.Module], bool]): The filter function to determine which modules to replace.</span>
<span class="sd">        cur_fqn (str, optional): The current fully qualified name of the module being processed. Defaults to &quot;&quot;.</span>
<span class="sd">        device (device, optional): Device to move the model to before applying `filter_fn`. Defaults to None.</span>
<span class="sd">        extra_args (Tuple[Any, ...], optional): optional extra args to pass to `replacement_fn`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">new_module</span>
    <span class="k">if</span> <span class="n">filter_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move to device before quantization</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">replacement_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">named_children_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">named_children_list</span><span class="p">:</span>
            <span class="n">new_child</span> <span class="o">=</span> <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span>
                <span class="n">replacement_fn</span><span class="p">,</span>
                <span class="n">filter_fn</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cur_fqn</span><span class="si">}{</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">device</span><span class="p">,</span>
                <span class="n">extra_args</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">child</span> <span class="ow">and</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_child</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move parent module to device</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_replace_with_custom_fn_if_matches_filter_with_name</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">replacement_fn</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">,</span>
    <span class="n">cur_fqn</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A variant of _replace_with_custom_fn_if_matches_filter where replacement_fn takes module name as well</span>
<span class="sd">        ...</span>
<span class="sd">        replacement_fn (Callable[[torch.nn.Module, str], torch.nn.Module]): The function to replace matching modules.</span>
<span class="sd">        ...</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">new_module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">new_module</span>
    <span class="k">if</span> <span class="n">filter_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move to device before quantization</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">replacement_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">named_children_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">named_children_list</span><span class="p">:</span>
            <span class="n">new_child</span> <span class="o">=</span> <span class="n">_replace_with_custom_fn_if_matches_filter_with_name</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span>
                <span class="n">replacement_fn</span><span class="p">,</span>
                <span class="n">filter_fn</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cur_fqn</span><span class="si">}{</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">device</span><span class="p">,</span>
                <span class="n">extra_args</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">child</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_child</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move parent module to device</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_linear</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># avoid circular dependencies</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat.affine_fake_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">AffineFakeQuantizedTensor</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># adding weight tensor subclass isinstance check to make sure the weight is only quantized once</span>
    <span class="c1"># when it is shared by multiple linear modules</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">QuantizedLinearWeightBase</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AffineFakeQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">NonDynamicallyQuantizableLinear</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_subclass_inserter</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">enable_parametrization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function which inserts the given subclass into all linear modules</span>
<span class="sd">    in the model. The inserted module will have its weight set to the result of</span>
<span class="sd">    `cls(mod.weight, **kwargs)`. If parametrization is enabled then this will be done using</span>
<span class="sd">    torch.nn.utils.parametrize instead of directly setting the attribute on the module.</span>

<span class="sd">    Args:</span>
<span class="sd">        cls (torch.Tensor): The class to insert as a child module.</span>
<span class="sd">        kwargs (Any): Any additional arguments for the constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">constructor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;constructor&quot;</span><span class="p">,</span> <span class="s2">&quot;subclass_constructor&quot;</span><span class="p">)</span>
    <span class="n">from_float</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;method&quot;</span><span class="p">,</span> <span class="s2">&quot;from_float&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">insert_subclass</span><span class="p">(</span><span class="n">lin</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">enable_parametrization</span><span class="p">:</span>
            <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
            <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
                <span class="n">lin</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">constructor</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="c1"># cls.from_float(...)</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">from_float</span><span class="p">)(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">lin</span>

    <span class="k">return</span> <span class="n">insert_subclass</span>


<span class="k">def</span><span class="w"> </span><span class="nf">swap_conv2d_1x1_to_linear</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Changes all conv2d 1x1 modules to equivalent linear modules so that they can then be quantized.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">PermuteSandwich</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="o">-</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replace_conv2d_1x1</span><span class="p">(</span><span class="n">conv</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">lin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">conv</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">conv</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">PermuteSandwich</span><span class="p">(</span><span class="n">lin</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="n">mod</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">replace_conv2d_1x1</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="n">filter_fn</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">insert_observers_</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">input_observer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AffineQuantizedObserverBase</span><span class="p">],</span>
    <span class="n">weight_observer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AffineQuantizedObserverBase</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts the weight of a linear module to a LinearActivationWeightObservedTensor.</span>

<span class="sd">    This function wraps the weight of the given linear module with a LinearActivationWeightObservedTensor,</span>
<span class="sd">    which enables observation of both input and weight tensors during forward passes.</span>
<span class="sd">    The wrapped weight is then re-wrapped as a nn.Parameter to maintain compatibility</span>
<span class="sd">    with PyTorch&#39;s module system.</span>

<span class="sd">    Example::</span>

<span class="sd">    ```</span>
<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>
<span class="sd">        from torchao.quantization.linear_observer_tensor import insert_observers_</span>
<span class="sd">        from torchao.quantization.observer import (</span>
<span class="sd">            AffineQuantizedMinMaxObserver,</span>
<span class="sd">            PerTensor,</span>
<span class="sd">            MappingType</span>
<span class="sd">        )</span>

<span class="sd">        # Create observers</span>
<span class="sd">        input_observer = AffineQuantizedMinMaxObserver(</span>
<span class="sd">            MappingType.SYMMETRIC,</span>
<span class="sd">            torch.float8_e4m3fn,</span>
<span class="sd">            granularity_type=PerTensor(),</span>
<span class="sd">            eps=torch.finfo(torch.float32).eps,</span>
<span class="sd">            scale_dtype=torch.float,</span>
<span class="sd">            zero_point_dtype=torch.int,</span>
<span class="sd">            zero_point_domain=ZeroPointDomain.NONE,</span>
<span class="sd">        )</span>

<span class="sd">        # Create a linear module</span>
<span class="sd">        linear_module = nn.Linear(10, 20)</span>

<span class="sd">        # Convert the linear module&#39;s weight to an observed tensor</span>
<span class="sd">        insert_observers_(linear_module, input_observer, weight_observer=None)</span>

<span class="sd">        # The linear_module can now be used as usual, with observers calculating statistics</span>
<span class="sd">        output = linear_module(torch.randn(10, 10))</span>

<span class="sd">        # Get the scale and zero point of the input observer</span>
<span class="sd">        scale, zero_point = linear_module.weight.input_observer.calculate_qparams()</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        model (nn.Module): The nn.Module to convert.</span>
<span class="sd">        input_observer (Optional[AffineQuantizedObserverBase]): Observer for input tensor.</span>
<span class="sd">        weight_observer (Optional[AffineQuantizedObserverBase]): Observer for weight tensor.</span>
<span class="sd">        filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): Filter function to select which modules to convert.</span>
<span class="sd">            If not provided, all linear modules will be converted. This function should take a module and its fully qualified name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Linear: The modified linear module with its weight wrapped in a LinearActivationWeightObservedTensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">convert_to_linear_observer</span><span class="p">(</span><span class="n">linear_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="c1"># Wrap the weight with LinearActivationWeightObservedTensor and then with nn.Parameter</span>
        <span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">LinearActivationWeightObservedTensor</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
                <span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">input_observer</span><span class="o">=</span><span class="n">input_observer</span><span class="p">,</span>
                <span class="n">weight_observer</span><span class="o">=</span><span class="n">weight_observer</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">linear_module</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">convert_to_linear_observer</span><span class="p">,</span>
        <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantization_type</span><span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">_quantization_type</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(activation=</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">input_quant_func</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">original_weight_tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="s2">&quot;_quantization_type&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">_quantization_type</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;not quantized&quot;</span>

    <span class="k">return</span> <span class="s2">&quot;not recognized&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_linear_extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_linear_subclass_inserter</span><span class="p">(</span>
    <span class="n">constructor</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">allow_requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">propagate_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to apply the constructor that quantizes the weight Tensor (with additional kwargs)</span>
<span class="sd">    to the weight of linear module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">insert_subclass</span><span class="p">(</span><span class="n">lin</span><span class="p">):</span>
        <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">allow_requires_grad</span> <span class="ow">and</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="k">if</span> <span class="n">propagate_bias</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">constructor</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span>
        <span class="p">)</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">lin</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lin</span>

    <span class="k">return</span> <span class="n">insert_subclass</span>


<div class="viewcode-block" id="quantize_"><a class="viewcode-back" href="../../../generated/torchao.quantization.quantize_.html#torchao.quantization.quantize_">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">quantize_</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">AOBaseConfig</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the weight of linear modules in the model with `config`, model is modified inplace</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): input model</span>
<span class="sd">        config (AOBaseConfig): a workflow configuration object.</span>
<span class="sd">        filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): function that takes a nn.Module instance and fully qualified name of the module, returns True if we want to run `config` on</span>
<span class="sd">        the weight of the module</span>
<span class="sd">        device (device, optional): Device to move module to before applying `filter_fn`. This can be set to `&quot;cuda&quot;` to speed up quantization. The final model will be on the specified `device`.</span>
<span class="sd">            Defaults to None (do not change device).</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>
<span class="sd">        from torchao import quantize_</span>

<span class="sd">        # quantize with some predefined `config` method that corresponds to</span>
<span class="sd">        # optimized execution paths or kernels (e.g. int4 tinygemm kernel)</span>
<span class="sd">        # also customizable with arguments</span>
<span class="sd">        # currently options are</span>
<span class="sd">        # int8_dynamic_activation_int4_weight (for executorch)</span>
<span class="sd">        # int8_dynamic_activation_int8_weight (optimized with int8 mm op and torch.compile)</span>
<span class="sd">        # int4_weight_only (optimized with int4 tinygemm kernel and torch.compile)</span>
<span class="sd">        # int8_weight_only (optimized with int8 mm op and torch.compile</span>
<span class="sd">        from torchao.quantization.quant_api import int4_weight_only</span>

<span class="sd">        m = nn.Sequential(nn.Linear(32, 1024), nn.Linear(1024, 32))</span>
<span class="sd">        quantize_(m, int4_weight_only(group_size=32))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">filter_fn</span> <span class="o">=</span> <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ModuleFqnToConfig</span><span class="p">):</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter_with_name</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">_module_fqn_to_config_handler</span><span class="p">,</span>
            <span class="n">filter_fn</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">extra_args</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">,),</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">AOBaseConfig</span><span class="p">):</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)]</span>
        <span class="c1"># for each linear in the model, apply the transform if filtering passes</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">handler</span><span class="p">,</span>
            <span class="n">filter_fn</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">extra_args</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">,),</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Passing a generic Callable to `quantize_` is no longer recommended and will be deprecated at a later release. Please see https://github.com/pytorch/ao/issues/1690 for instructions on how to pass in workflow configuration instead.&quot;&quot;&quot;</span>
        <span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_asymm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is defined here instead of local function to support serialization&quot;&quot;&quot;</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">mapping_type</span><span class="p">,</span> <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">target_dtype</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_uint8_asymm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">255</span>
    <span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>

    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="Int8DynamicActivationInt4WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.html#torchao.quantization.Int8DynamicActivationInt4WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration for applying int8 dynamic per token asymmetric activation quantization and int4 per group weight symmetric quantization to linear</span>
<span class="sd">    This is used to produce a model for executorch backend, but currently executorch did not</span>
<span class="sd">    support lowering for the quantized model from this flow yet</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `MarlinQQQLayout()` and `CutlassInt4PackedLayout()` for now</span>
<span class="sd">        `mapping_type`: quantization type for weight, controls the weight quantization is symmetric or asymmetric</span>
<span class="sd">        `act_mapping_type`: quantization type for activation, controls the activation quantization is symmetric or asymmetric</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">int8_dynamic_activation_int4_weight</span> <span class="o">=</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationInt4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int4_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span>
<span class="p">):</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">module</span>

    <span class="c1"># weight settings</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">7</span>

    <span class="c1"># input settings</span>
    <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int8DynamicActInt4WeightCPULayout</span><span class="p">):</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_uint8_asymm_per_token_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>
    <span class="k">elif</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinQQQLayout</span><span class="p">):</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">):</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_cutlass_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported activation mapping type: </span><span class="si">{</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinQQQLayout</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_marlinqqq_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int8DynamicActInt4WeightCPULayout</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationIntxWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for dynamically quantizing activations to torch.int8 and weights to torch.intx, with 1 &lt;= x &lt;= 8.</span>
<span class="sd">    More specifically, activations are dynamically quantized to 8-bits at a per-token granularity with scales/zeros.</span>
<span class="sd">    Weights are quantized with scales/zeros in a groupwise or channelwise manner using the number of bits specified by weight_dtype.</span>

<span class="sd">    This layout is identical to Int8DynamicActivationInt4WeightConfig when weight_dtype is torch.int4 and other args</span>
<span class="sd">    are the same.  However, this layout is more general and supports other weight dtypes.</span>

<span class="sd">    args:</span>
<span class="sd">        weight_dtype: The dtype to use for weight quantization.  Must be torch.intx, where 1 &lt;= x &lt;= 8.</span>
<span class="sd">            torch.intx with x &lt; 8 requires TORCH_VERSION_AT_LEAST_2_6</span>
<span class="sd">        weight_granularity: The granularity to use for weight quantization.  Must be PerGroup or PerAxis(axis=0).</span>
<span class="sd">        weight_mapping_type: The type of mapping to use for the weight quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.  MappingType.SYMMETRIC requires ZeroPointDomain.NONE</span>
<span class="sd">        weight_scale_dtype: The dtype to use for the weight scale.</span>
<span class="sd">        act_mapping_type: The type of mapping to use for the activation quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.</span>
<span class="sd">        layout: The layout to use for the packed weight tensor:</span>
<span class="sd">            - PackedLinearInt8DynamicActivationIntxWeightLayout: this layout is optimized for CPU performance.</span>
<span class="sd">            - QDQLayout: this layout represents the quantization with Q/DQ quant primitives, and is intended for</span>
<span class="sd">                export applications like ExecuTorch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">weight_granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerGroup</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">weight_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="c1"># TODO: add weight_scale_dtype to Int8DynamicActivationInt4WeightConfig</span>
    <span class="n">weight_scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">QDQLayout</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Int8DynamicActivationIntxWeightConfig requires torch 2.6+&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;int</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_dtype must be torch.intx, where 1 &lt;= x &lt;= 8, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="p">(</span><span class="n">PerAxis</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">)),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_granularity must be PerAxis or PerGroup, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;axis must be 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;act_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="p">(</span><span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">,</span> <span class="n">QDQLayout</span><span class="p">)</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;layout must be PackedLinearInt8DynamicActivationIntxWeightLayout or QDQLayout, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Target</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Target</span><span class="o">.</span><span class="n">KLEIDIAI</span><span class="p">,</span> <span class="n">Target</span><span class="o">.</span><span class="n">ATEN</span><span class="p">]:</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="p">):</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;When using layout PackedLinearInt8DynamicActivationIntxWeightLayout with target </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;the weight scale may be cast to bfloat16 by the kernel, but weight_scale_dtype is set to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="s2">&quot;Explicitly set weight_scale_dtype to torch.bfloat16 to suppress this warning. &quot;</span>
                        <span class="s2">&quot;If you need weight_scale_dtype = torch.float32, use target=Target.UNIVERSAL instead.&quot;</span>
                    <span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_intx_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationIntxWeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_granularity</span>
    <span class="n">weight_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_mapping_type</span>
    <span class="n">weight_scale_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_scale_dtype</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;weight must be 2D, but got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">D&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">):</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight_granularity</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;axis must be 0&quot;</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_granularity must be PerGroup or PerAxis, got </span><span class="si">{</span><span class="n">weight_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">weight_dtype</span><span class="p">]</span>

    <span class="c1"># We quantize with QDQLayout, and then construct the packed weight tensor later</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">weight_mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">weight_scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="p">(</span><span class="n">weight_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">),</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">QDQLayout</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">QDQLayout</span><span class="p">):</span>
        <span class="c1"># TODO: _int8_asymm_per_token_quant uses scale_dtype=torch.float64, zero_point_dtype=torch.int64,</span>
        <span class="c1"># which is not great for export with QDQLayout.  It is also not consistent with _int8_symm_per_token_quant,</span>
        <span class="c1"># which uses scale_dtype=torch.float32, zero_point_dtype=torch.int32.</span>
        <span class="c1"># Maybe introduce new fp32/int32 versions of _int8_asymm_per_token_quant?</span>
        <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">:</span>
            <span class="n">activation_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>
        <span class="k">elif</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">activation_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported activation mapping type: </span><span class="si">{</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">activation_quant_func</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">):</span>
        <span class="c1"># PackedLinearInt8DynamicActivationIntxWeightLayout has dynamic activation quantization</span>
        <span class="c1"># fused with the kernel and it should not be applied separately</span>
        <span class="k">assert</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PackedLinearInt8DynamicActivationIntxWeightLayout requires act_mapping_type=MappingType.ASYMMETRIC&quot;</span>
        <span class="p">)</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
        <span class="n">groups_per_row</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">group_size</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups_per_row</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups_per_row</span><span class="p">)</span>
        <span class="n">has_weight_zeros</span> <span class="o">=</span> <span class="p">(</span><span class="n">zero_point</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">make_packed_linear_int8_dynamic_activation_intx_weight_tensor</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">scale</span><span class="p">,</span>
            <span class="n">zero_point</span> <span class="k">if</span> <span class="n">has_weight_zeros</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">layout</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
            <span class="n">validate_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># bias is packed with weights if present</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies int4 dynamic per token symmetric activation quantization and int4 per row weight symmetric quantization to linear</span>

<span class="sd">    Args:</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `MarlinQQQLayout()` and `CutlassInt4PackedLayout()` for now</span>
<span class="sd">        `mapping_type`: quantization type for weight, controls the weight quantization is symmetric or asymmetric</span>
<span class="sd">        `act_mapping_type`: quantization type for activation, controls the activation quantization is symmetric or asymmetric</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">()</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># for bc</span>
<span class="n">int4_dynamic_activation_int4_weight</span> <span class="o">=</span> <span class="n">Int4DynamicActivationInt4WeightConfig</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int4DynamicActivationInt4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int4_dynamic_activation_int4_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int4DynamicActivationInt4WeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassInt4PackedLayout</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Only CutlassInt4PackedLayout layout is supported. Received </span><span class="si">{</span><span class="n">layout</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">mapping_type</span> <span class="o">!=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only mapping_type=SYMMETRIC is supported.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">!=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only act_mapping_type=SYMMETRIC is supported.&quot;</span><span class="p">)</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">_int4_symm_cutlass_quant</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="GemliteUIntXWeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.html#torchao.quantization.GemliteUIntXWeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GemliteUIntXWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    applies weight only 4 or 8 bit integer quantization and utilizes the gemlite triton kernel and its associated weight packing format.</span>
<span class="sd">    This only works for fp16 models. 8 bit quantization is symmetric, 4 bit quantization is asymmetric.</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained</span>
<span class="sd">        `bit_width`: bit width of the quantized weight.</span>
<span class="sd">        `packing_bitwidth`: bit width of the packed weight, should be 8 or 32. Can have performance impacts depending on hardware.</span>
<span class="sd">        `mode`: if set to &quot;dynamic&quot;, activations are quantized at runtime; default is &quot;weight_only&quot; (weight-only quantization).</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">bit_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">packing_bitwidth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;weight_only&quot;</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">gemlite_uintx_weight_only</span> <span class="o">=</span> <span class="n">GemliteUIntXWeightOnlyConfig</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">GemliteUIntXWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_gemlite_uintx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">GemliteUIntXWeightOnlyConfig</span>
<span class="p">):</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">bit_width</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">bit_width</span>
    <span class="n">packing_bitwidth</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">packing_bitwidth</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mode</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.uintx.gemlite_layout</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_gemlite_aqt_kwargs</span>

    <span class="n">use_hqq</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">bit_width</span> <span class="o">==</span> <span class="mi">4</span> <span class="k">else</span> <span class="kc">False</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="o">**</span><span class="n">get_gemlite_aqt_kwargs</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span>
            <span class="n">bit_width</span><span class="o">=</span><span class="n">bit_width</span><span class="p">,</span>
            <span class="n">packing_bitwidth</span><span class="o">=</span><span class="n">packing_bitwidth</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Int4WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int4WeightOnlyConfig.html#torchao.quantization.Int4WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying uint4 weight-only asymmetric per-group quantization to linear layers, using</span>
<span class="sd">    &quot;tensor_core_tiled&quot; layout for speedup with tinygemm kernel</span>

<span class="sd">    Note:</span>
<span class="sd">        This is targeting `tinygemm` int4mm kernel (`torch.ops.aten._weight_int4pack_mm`</span>
<span class="sd">        and `torch.ops.aten._weight_int4pack_mm_for_cpu`), the main difference</span>
<span class="sd">        of quantization algorithm compared to the more traditional type of integer quantization is the following:</span>
<span class="sd">        1). zero_point is in floating point domain instead of integer domain (`zero_point_domain`=`ZeroPointDomain.FLOAT`)</span>
<span class="sd">        2). floating point zero does not have to be exactly representable (`preserve_zero`=False in `choose_qparams_affine`)</span>
<span class="sd">        please follow the relevant code in `choose_qparams_affine`, `quantize_affine` and `dequantize_affine`</span>
<span class="sd">        to learn about how the quantization parameters are chosen and how the Tensor is quantized/dequantized for tinygemm</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained, choices are [256, 128, 64, 32]</span>
<span class="sd">        `layout`: layout type for quantized tensor, default is `TensorCoreTiledLayout(inner_k_tiles=8)`</span>
<span class="sd">        `use_hqq`: whether to use hqq or default quantization mode, default is False</span>
<span class="sd">        `zero_point_domain`: data type of zeros points, choices are [ZeroPointDomain.FLOAT, ZeroPointDomain.INT, ZeroPointDomain.NONE]</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">        `preserve_zero`: whether to preserve zero, default is None. Will be set to True if zero_point_domain is ZeroPointDomain.INT</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorCoreTiledLayout</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorCoreTiledLayout</span><span class="p">(</span><span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">use_hqq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ZeroPointDomain</span><span class="p">]</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">preserve_zero</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span></div>


<span class="c1"># for BC</span>
<span class="c1"># TODO maybe change other callsites</span>
<span class="n">int4_weight_only</span> <span class="o">=</span> <span class="n">Int4WeightOnlyConfig</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int4_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="c1"># TODO(future PR): perhaps move this logic to a different file, to keep the API</span>
    <span class="c1"># file clean of implementation details</span>

    <span class="c1"># for now, make these local variables to allow the rest of the function</span>
    <span class="c1"># to be a direct copy-paste</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">use_hqq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_hqq</span>
    <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">zero_point_domain</span>

    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping quantizing weight with int4 weight only quantization because the shape of weight </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not compatible with group_size </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int4CPULayout</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="p">)</span>

    <span class="c1"># nonlocal zero_point_domain</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)</span> <span class="ow">in</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Only support layout: </span><span class="si">{</span><span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
        <span class="c1"># the first value is the default one</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="ow">in</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Layout only support </span><span class="si">{</span><span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="n">layout</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int4XPULayout</span><span class="p">):</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>

    <span class="n">preserve_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">config</span><span class="o">.</span><span class="n">preserve_zero</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">preserve_zero</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">LAYOUT_TO_PRESERVE_ZEROS</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="c1"># Sparse Marlin only supports symmetric quantization.</span>
    <span class="c1"># NOTE: If we start having lots of layouts that require different configurations,</span>
    <span class="c1"># we should consider moving this logic somewhere else.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">):</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="k">assert</span> <span class="n">group_size</span> <span class="o">==</span> <span class="mi">128</span> <span class="ow">or</span> <span class="n">group_size</span> <span class="o">==</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;MarlinSparseLayout only supports 128 group size or per channel quantization, got </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int4WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int4_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int4WeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int4_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Int8WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8WeightOnlyConfig.html#torchao.quantization.Int8WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 weight-only symmetric per-channel quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        group_size: Optional[int] = None - Controls the granularity of quantization. If None, applies per-channel quantization.</span>
<span class="sd">            Otherwise, applies per-group quantization with the specified group size.</span>
<span class="sd">        set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values</span>
<span class="sd">            for better performance with this quantization scheme.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">int8_weight_only</span> <span class="o">=</span> <span class="n">Int8WeightOnlyConfig</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_weight_only_transform</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int8WeightOnlyConfig</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int8_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_reduced_range_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_reduced_range_quant_noop_decode</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_cutlass_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=-</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">CutlassInt4PackedLayout</span><span class="p">(),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_cutlass_quant</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_cutlass_quant_sparse</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">CutlassSemiSparseLayout</span><span class="p">(),</span>
    <span class="p">)</span>


<div class="viewcode-block" id="Int8DynamicActivationInt8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.html#torchao.quantization.Int8DynamicActivationInt8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationInt8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 dynamic symmetric per-token activation and int8 per-channel weight</span>
<span class="sd">    quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        layout: Optional[Layout] = PlainLayout() - Tensor layout for the quantized weights. Controls how the</span>
<span class="sd">            quantized data is stored and accessed.</span>
<span class="sd">        act_mapping_type: Optional[MappingType] = MappingType.SYMMETRIC - Mapping type for activation quantization.</span>
<span class="sd">            SYMMETRIC uses symmetric quantization around zero.</span>
<span class="sd">        weight_only_decode: bool = False - If True, only quantizes weights during forward pass and keeps activations</span>
<span class="sd">            in original precision during decode operations.</span>
<span class="sd">        set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values</span>
<span class="sd">            for better performance with this quantization scheme.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Layout</span><span class="p">]</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MappingType</span><span class="p">]</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">weight_only_decode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">int8_dynamic_activation_int8_weight</span> <span class="o">=</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int8_weight_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="n">weight_only_decode</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_only_decode</span>

    <span class="n">in_features</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># int8 dynamic quantization only has benefit when in_feature &gt; 16</span>
    <span class="k">if</span> <span class="n">in_features</span> <span class="o">&lt;=</span> <span class="mi">16</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping applying int8_dynamic_activation_int8_weight to weight of shape </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; because `in_feature` is &lt;= 16: </span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="c1"># weight settings</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">weight_zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>

    <span class="k">if</span> <span class="n">weight_only_decode</span><span class="p">:</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant_noop_decode</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># input settings</span>
        <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_weight_block_size</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">weight_zero_point_domain</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationInt8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 dynamic activation int8 weight quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot;but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int8_dynamic_activation_int8_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">int8_dynamic_activation_int8_semi_sparse_weight</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies int8 dnynamic symmetric per-token activation and int8 per-channel weight</span>
<span class="sd">    quantization + 2:4 sparsity to linear layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;int8_dyanmic_activation_int8_semi_sparse_weight() will be deprecated at a later release. Please use the layout kwarg in int8_dynamic_activation_int8_weight instead.</span>

<span class="s2">    from torchao.dtypes import SemiSparseLayout</span>
<span class="s2">    int8_dynamic_activation_int8_weight(layout=SemiSparseLayout()&quot;&quot;&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">int8_dynamic_activation_int8_weight</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">SemiSparseLayout</span><span class="p">())</span>


<div class="viewcode-block" id="Float8WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8WeightOnlyConfig.html#torchao.quantization.Float8WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 weight-only symmetric per-channel quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>

<span class="sd">    Note:</span>
<span class="sd">        The actual matmul will be computed in original precision of the weight tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">float8_weight_only</span> <span class="o">=</span> <span class="n">Float8WeightOnlyConfig</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_weight_only_quant_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_floatx</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8WeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_float8_weight_only_quant_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_input_activation_quant_func_fp8</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">activation_granularity</span><span class="p">:</span> <span class="n">FP8Granularity</span><span class="p">,</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function is used to quantize the input activation tensor for an aqt_float variant. If scale</span>
<span class="sd">    is not provided it will be dynamically calculate the scales otherwise it will use the provided scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Zero point is not supported for dynamic FP8 quantization&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerRow</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PerRow quantization only works for bfloat16 precision input activation&quot;</span>
        <span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">activation_granularity</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Config is stored on weight</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerTensor</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Static quantization only supports PerTensor granularity&quot;</span>
        <span class="p">)</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx_static</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Config is stored on weight</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">activation</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check if a weight tensor meets float8 quantization requirements.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (torch.Tensor): The weight tensor to check</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if the tensor can be quantized to float8, False otherwise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="mi">2</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;float8 quantization only works for 2/3-D tensors, got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">D tensor&quot;</span>

    <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
    <span class="n">is_compatible</span> <span class="o">=</span> <span class="p">(</span><span class="n">in_dim</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">out_dim</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_compatible</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping float8 quantization: weight shape </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not compatible with _scaled_mm. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Both input dimension (</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s2">) and output dimension (</span><span class="si">{</span><span class="n">out_dim</span><span class="si">}</span><span class="s2">) must be multiples of 16. &quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">is_compatible</span>


<div class="viewcode-block" id="Float8DynamicActivationFloat8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.html#torchao.quantization.Float8DynamicActivationFloat8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationFloat8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 dynamic symmetric quantization to both activations and weights of linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_dtype (torch.dtype): The target data type for activation quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        granularity:</span>
<span class="sd">            The granularity for quantization. Can be either a single granularity (applied to both</span>
<span class="sd">            activations and weights) or a tuple of two granularities (one for activations, one for weights).</span>
<span class="sd">            If None, defaults to PerTensor for both. Currently both quantizations need to be the same type. And</span>
<span class="sd">            only PerTensor and PerRow are supported.</span>
<span class="sd">        mm_config (Float8MMConfig): Configuration for the matrix multiplication. Default uses fast accumulation.</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Float8MMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="o">=</span> <span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">_normalize_granularity</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span> <span class="o">=</span> <span class="p">[</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span><span class="p">]</span></div>


<span class="c1"># for bc</span>
<span class="n">float8_dynamic_activation_float8_weight</span> <span class="o">=</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_weight_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mm_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mm_config</span>

    <span class="c1"># Ensure works on device</span>
    <span class="n">_check_hardware_support</span><span class="p">(</span><span class="n">granularity</span><span class="p">)</span>
    <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">granularity</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="c1"># TODO(future PR): this should really throw an exception instead of silently</span>
        <span class="c1"># not doing what the user asked</span>
        <span class="k">return</span> <span class="n">weight</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerRow</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PerRow quantization only works for bfloat16 precision input weight&quot;</span>
        <span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">weight_granularity</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">block_size</span><span class="p">))</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
    <span class="n">input_quant_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="n">activation_granularity</span><span class="p">,</span>
        <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
        <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">,</span> <span class="n">quant_kwargs</span><span class="o">=</span><span class="n">input_quant_kwargs</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_89</span><span class="p">()</span> <span class="ow">or</span> <span class="n">is_MI300</span><span class="p">(),</span> <span class="p">(</span>
        <span class="s2">&quot;Float8 dynamic activation quantization is only supported on CUDA&gt;=8.9 and MI300+&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying float8 dynamic activation quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;but </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">_float8_dynamic_activation_float8_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationFloat8SemiSparseWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies float8 dynamic quantization to activations and float8 quantization followed by compression to sparse semi-structured tensor to weights of linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `CutlassSemiSparseLayout` at the moment.</span>
<span class="sd">        `activation_dtype`: data type for quantized activation tensor.</span>
<span class="sd">        `weight_dtype`: data type for quantized weight tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">CutlassSemiSparseLayout</span><span class="p">()</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e5m2_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationFloat8SemiSparseWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_semi_sparse_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationFloat8SemiSparseWeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_90</span><span class="p">(),</span> <span class="s2">&quot;Float8 quantization is only supported on CUDA&gt;=9.0&quot;</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassSemiSparseLayout</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Only CutlassSemiSparseLayout layout is supported. Received </span><span class="si">{</span><span class="n">layout</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">_float8_cutlass_quant_sparse</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant</span><span class="p">,</span>
        <span class="n">quant_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;target_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Float8StaticActivationFloat8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.html#torchao.quantization.Float8StaticActivationFloat8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8StaticActivationFloat8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 static symmetric quantization to</span>

<span class="sd">    Args:</span>
<span class="sd">        scale (torch.Tensor): The scale tensor for activation quantization.</span>
<span class="sd">        activation_dtype (torch.dtype): The target data type for activation quantization. Default is torch.float8_e4m</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m</span>
<span class="sd">        mm_config (Float8MMConfig): Configuration for the matrix multiplication. Default uses fast accumulation.</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">FP8Granularity</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Float8MMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="o">=</span> <span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<span class="c1"># for bc</span>
<span class="n">float8_static_activation_float8_weight</span> <span class="o">=</span> <span class="n">Float8StaticActivationFloat8WeightConfig</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8StaticActivationFloat8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_static_activation_float8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8StaticActivationFloat8WeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_89</span><span class="p">()</span> <span class="ow">or</span> <span class="n">is_MI300</span><span class="p">(),</span> <span class="p">(</span>
        <span class="s2">&quot;Float8 static activation quantization is only supported on CUDA 8.9 and above&quot;</span>
    <span class="p">)</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mm_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mm_config</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">_normalize_granularity</span><span class="p">(</span><span class="n">granularity</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerTensor</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;Static quantization only supports PerTensor granularity&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="c1"># TODO(future PR): this should really throw an exception instead of silently</span>
        <span class="c1"># not doing what the user asked</span>
        <span class="k">return</span> <span class="n">module</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">weight_granularity</span><span class="p">)</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
    <span class="n">input_quant_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="n">activation_granularity</span><span class="p">,</span>
        <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_weight_tensor_with_linear_activation_quantization_metadata</span><span class="p">(</span>
        <span class="n">quantized_weight</span><span class="p">,</span>
        <span class="n">input_quant_func</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_kwargs</span><span class="o">=</span><span class="n">input_quant_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="UIntXWeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.UIntXWeightOnlyConfig.html#torchao.quantization.UIntXWeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">UIntXWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying uintx weight-only asymmetric per-group quantization to linear layers, using uintx quantization where</span>
<span class="sd">    x is the number of bits specified by `dtype`</span>

<span class="sd">    Args:</span>
<span class="sd">        `dtype`: torch.uint1 to torch.uint7 sub byte dtypes</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained, defaults to 64</span>
<span class="sd">        `pack_dim`: the dimension we use for packing, defaults to -1</span>
<span class="sd">        `use_hqq`: whether to use hqq algorithm or the default algorithm to quantize the weight</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">pack_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">use_hqq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">uintx_weight_only</span> <span class="o">=</span> <span class="n">UIntXWeightOnlyConfig</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">UIntXWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_uintx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">UIntXWeightOnlyConfig</span>
<span class="p">):</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">pack_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pack_dim</span>
    <span class="n">use_hqq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_hqq</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span>

    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint1</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint3</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint5</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint6</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint7</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">assert</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">SUPPORTED_DTYPES</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported dtype for hqq: </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_hqq</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Recommended to use `int4_weight_only(group_size, use_hqq=True)` for the best performance&quot;</span>
            <span class="p">)</span>
        <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">dtype</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span>
        <span class="n">preserve_zero</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span>
        <span class="n">preserve_zero</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="n">UintxLayout</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">pack_dim</span><span class="o">=</span><span class="n">pack_dim</span><span class="p">)</span>

    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
        <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">IntxWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for quantizing weights to torch.intx, with 1 &lt;= x &lt;= 8.</span>
<span class="sd">    Weights are quantized with scales/zeros in a groupwise or channelwise</span>
<span class="sd">    manner using the number of bits specified by weight_dtype.</span>
<span class="sd">    args:</span>
<span class="sd">        weight_dtype: The dtype to use for weight quantization.  Must be torch.intx, where 1 &lt;= x &lt;= 8.</span>
<span class="sd">            torch.intx with x &lt; 8 requires TORCH_VERSION_AT_LEAST_2_6</span>
<span class="sd">        granularity: The granularity to use for weight quantization.  Must be PerGroup or PerAxis(0).</span>
<span class="sd">        mapping_type: The type of mapping to use for the weight quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.</span>
<span class="sd">        scale_dtype: The dtype to use for the weight scale.</span>
<span class="sd">        layout: The layout to use for the packed weight tensor:</span>
<span class="sd">            - QDQLayout: this layout is designed for export to ExecuTorch.this layout represents the quantization with Q/DQ quant primitives,</span>
<span class="sd">                and is intended for export applications like ExecuTorch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerAxis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">QDQLayout</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">,</span> <span class="s2">&quot;IntxWeightOnlyConfig requires torch 2.6+&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;int</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_dtype must be torch.intx, where 1 &lt;= x &lt;= 8, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="p">(</span><span class="n">PerAxis</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">)),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;granularity must be PerAxis or PerGroup, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">IntxWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_intx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">IntxWeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_dtype</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;IntxWeightOnlyConfig only works for 2-d Tensor, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">):</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">granularity</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;granularity must be PerGroup or PerAxis, got </span><span class="si">{</span><span class="n">granularity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">weight_dtype</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="p">(</span><span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">),</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="FPXWeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.FPXWeightOnlyConfig.html#torchao.quantization.FPXWeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FPXWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sub-byte floating point dtypes defined by `ebits`: exponent bits and `mbits`: mantissa bits</span>
<span class="sd">    e.g. fp6_e3_m2, fp6_e2_m3, ...</span>
<span class="sd">    The packing format and kernels are from the fp6-llm paper: https://arxiv.org/abs/2401.14112</span>
<span class="sd">    github repo: https://github.com/usyd-fsalab/fp6_llm, now renamed to quant-llm</span>
<span class="sd">    For more details for packing please see: :class:`~torchao.dtypes.fpx.FpxTensorCoreAQTTensorImpl`</span>

<span class="sd">    This is experimental, will be merged with `to_affine_quantized_floatx`</span>
<span class="sd">    in the future</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span></div>


<span class="c1"># for BC</span>
<span class="n">fpx_weight_only</span> <span class="o">=</span> <span class="n">FPXWeightOnlyConfig</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">FPXWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_fpx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FPXWeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">ebits</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ebits</span>
    <span class="n">mbits</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mbits</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_fpx</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.floatx</span><span class="w"> </span><span class="kn">import</span> <span class="n">FloatxTensorCoreLayout</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;floatx only works for 2-d Tensor, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">in_dim</span> <span class="o">%</span> <span class="mi">64</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">out_dim</span> <span class="o">%</span> <span class="mi">256</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping floatx quantization float</span><span class="si">{</span><span class="n">ebits</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">mbits</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">ebits</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">mbits</span><span class="si">}</span><span class="s2"> because &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;the shape is not compatible with the kernel: in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s2">, out_dim=</span><span class="si">{</span><span class="n">out_dim</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;expected in_dim % 64 == 0 and out_dim % 256 == 0&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span>

    <span class="n">_layout</span> <span class="o">=</span> <span class="n">FloatxTensorCoreLayout</span><span class="p">(</span><span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_fpx</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FbgemmConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantization Config for fbgemm-genai kernels</span>
<span class="sd">    Args:</span>
<span class="sd">       input_dtype (torch.dtype): input dtype of the kernel</span>
<span class="sd">       weight_dtype (torch.dtype): weight dtype of the kernel</span>
<span class="sd">       output_dtype (torch.dtype): output dtype of the kernel</span>
<span class="sd">       group_size (int): The group size for weight</span>
<span class="sd">       preshuffle (bool): whether preshuffle the weights or not</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">activation_scale_ub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">preshuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">FbgemmConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FbgemmConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_fbgemm_genai_gpu_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Requires fbgemm-gpu-genai &gt;= 1.2.0&quot;</span><span class="p">)</span>

    <span class="n">_SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">),</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">preshuffle</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">Int4PreshuffledTensor</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
                <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">to_fbgemm_int4</span><span class="p">(</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">e4m3_dtype</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">preshuffle</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">Int4PreshuffledTensor</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
                <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
                <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">module</span>
    <span class="k">elif</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">e4m3_dtype</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="o">==</span> <span class="n">e4m3_dtype</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_fbgemm_fp8</span><span class="p">(</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">activation_scale_ub</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">config</span><span class="si">}</span><span class="s2"> is not supported. supported input, weight, output kernel dtypes are: </span><span class="si">{</span><span class="n">_SUPPORTED_DTYPES</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ModuleFqnToConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Per module configurations for torchao quantize_ API</span>

<span class="sd">    Args:</span>
<span class="sd">        `module_fqn_to_config`: Dict[str, Optional[AOBaseConfig]]: a dictionary from</span>
<span class="sd">         the fully qualified name of module to the AOBaseConfig that we want to apply to the module.</span>
<span class="sd">         Also has a special key: &quot;_default&quot;, if &quot;_default&quot; is present in the dictionary,</span>
<span class="sd">         the config for &quot;_default&quot; will be applied to all the remaining modules that does not have</span>
<span class="sd">         per module configuration specified.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">module_fqn_to_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AOBaseConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_module_fqn_to_config_handler</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">module_fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ModuleFqnToConfig</span>
<span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">module_fqn</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">:</span>
        <span class="c1"># Maybe: we can add module type specific config in the future, in needed</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">[</span><span class="n">module_fqn</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># fallback to use default if no module specific config is provided</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_default&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">module</span>


<span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">_int8_asymm_per_token_quant</span><span class="p">,</span>
            <span class="n">_int8_symm_per_token_reduced_range_quant</span><span class="p">,</span>
            <span class="n">_input_activation_quant_func_fp8</span><span class="p">,</span>
            <span class="n">_int4_symm_cutlass_quant</span><span class="p">,</span>
            <span class="n">_int8_symm_cutlass_quant</span><span class="p">,</span>
            <span class="n">_float8_cutlass_quant</span><span class="p">,</span>
            <span class="n">_float8_cutlass_quant_sparse</span><span class="p">,</span>
            <span class="n">Target</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
         <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
         <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
         <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>