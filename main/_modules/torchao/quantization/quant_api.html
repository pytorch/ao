

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.quantization.quant_api &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/design-tabs.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/quantization/quant_api';</script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/quantization/quant_api.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quick_start.html">
    Quick Start Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quantization_overview.html">
    Quantization Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../contributor_guide.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../sparsity.html">
    Sparsity Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../benchmarking_api_guide.html">
    Benchmarking API Guide
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../benchmarking_user_guide.html">
    Benchmarking User Guide
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_dtypes.html">
    torchao.dtypes
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_quantization.html">
    torchao.quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_qat.html">
    torchao.quantization.qat
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_sparsity.html">
    torchao.sparsity
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_float8.html">
    torchao.float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_utils.html">
    torchao.utils
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../pretraining.html">
    (Part 1) Pre-training with float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../finetuning.html">
    (Part 2) Fine-tuning with QAT, QLoRA, and float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../serving.html">
    (Part 3) Serving on vLLM, SGLang, ExecuTorch
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../torchao_vllm_integration.html">
    Integration with VLLM: Architecture and Usage Guide
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../torchao_hf_integration.html">
    Hugging Face Integration
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../serialization.html">
    Serialization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../static_quantization.html">
    Static Quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../subclass_basic.html">
    Writing Your Own Quantized Tensor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../subclass_advanced.html">
    Writing Your Own Quantized Tensor (advanced)
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_ptq.html">
    PyTorch 2 Export Post Training Quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_qat.html">
    PyTorch 2 Export Quantization-Aware Training (QAT)
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">
    PyTorch 2 Export Quantization with X86 Backend through Inductor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">
    PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">
    PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quantizer.html">
    How to Write a Quantizer for PyTorch 2 Export Quantization
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quick_start.html">
    Quick Start Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quantization_overview.html">
    Quantization Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../contributor_guide.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../sparsity.html">
    Sparsity Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../benchmarking_api_guide.html">
    Benchmarking API Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../benchmarking_user_guide.html">
    Benchmarking User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_dtypes.html">
    torchao.dtypes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_quantization.html">
    torchao.quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_qat.html">
    torchao.quantization.qat
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_sparsity.html">
    torchao.sparsity
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_float8.html">
    torchao.float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_utils.html">
    torchao.utils
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../pretraining.html">
    (Part 1) Pre-training with float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../finetuning.html">
    (Part 2) Fine-tuning with QAT, QLoRA, and float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../serving.html">
    (Part 3) Serving on vLLM, SGLang, ExecuTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../torchao_vllm_integration.html">
    Integration with VLLM: Architecture and Usage Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../torchao_hf_integration.html">
    Hugging Face Integration
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../serialization.html">
    Serialization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../static_quantization.html">
    Static Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../subclass_basic.html">
    Writing Your Own Quantized Tensor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../subclass_advanced.html">
    Writing Your Own Quantized Tensor (advanced)
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_ptq.html">
    PyTorch 2 Export Post Training Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_qat.html">
    PyTorch 2 Export Quantization-Aware Training (QAT)
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">
    PyTorch 2 Export Quantization with X86 Backend through Inductor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">
    PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">
    PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quantizer.html">
    How to Write a Quantizer for PyTorch 2 Export Quantization
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.quan...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="torchao.quantization.quant_api">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.quantization.quant_api</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># Copyright 2024-2025 Arm Limited and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Quantization APIs</span>

<span class="sd">Generally these APIs can be applied directly to any model</span>
<span class="sd">with Linear modules to obtain quantized linear ops. The intended</span>
<span class="sd">usage involves applying torch.compile to the model afterwards</span>
<span class="sd">both because primitives were designed based on the fusions that</span>
<span class="sd">come along with it and because that is how we access the intended quantized</span>
<span class="sd">and mixed GEMM kernels</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span> <span class="k">as</span> <span class="n">OrderedDictType</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.parametrize</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">parametrize</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torchao</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.core.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">AOBaseConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">CutlassInt4PackedLayout</span><span class="p">,</span>
    <span class="n">CutlassSemiSparseLayout</span><span class="p">,</span>
    <span class="n">Float8Layout</span><span class="p">,</span>
    <span class="n">Int4CPULayout</span><span class="p">,</span>
    <span class="n">Int4XPULayout</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">,</span>
    <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">,</span>
    <span class="n">PlainLayout</span><span class="p">,</span>
    <span class="n">QDQLayout</span><span class="p">,</span>
    <span class="n">SemiSparseLayout</span><span class="p">,</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">,</span>
    <span class="n">to_affine_quantized_floatx</span><span class="p">,</span>
    <span class="n">to_affine_quantized_floatx_static</span><span class="p">,</span>
    <span class="n">to_affine_quantized_intx</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.uintx.packed_linear_int8_dynamic_activation_intx_weight_layout</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Target</span><span class="p">,</span>
    <span class="n">make_packed_linear_int8_dynamic_activation_intx_weight_tensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Layout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">e4m3_dtype</span><span class="p">,</span> <span class="n">e5m2_dtype</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.float8_linear</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float8Linear</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.inference</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Float8MMConfig</span><span class="p">,</span>
    <span class="n">FP8Granularity</span><span class="p">,</span>
    <span class="n">_check_hardware_support</span><span class="p">,</span>
    <span class="n">_granularity_is_a_1_128_w_128_128</span><span class="p">,</span>
    <span class="n">_normalize_granularity</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># for BC, make sure to keep the `noqa: F401` comments to prevent</span>
<span class="c1"># ruff from removing &quot;unused imports&quot;</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.prototype.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Float8StaticActivationFloat8WeightConfig</span><span class="p">,</span>  <span class="c1"># noqa: F401</span>
    <span class="n">FPXWeightOnlyConfig</span><span class="p">,</span>  <span class="c1"># noqa: F401</span>
    <span class="n">GemliteUIntXWeightOnlyConfig</span><span class="p">,</span>  <span class="c1"># noqa: F401</span>
    <span class="n">Int4DynamicActivationInt4WeightConfig</span><span class="p">,</span>  <span class="c1"># noqa: F401</span>
    <span class="n">Int8DynamicActivationInt4WeightConfig</span><span class="p">,</span>  <span class="c1"># noqa: F401</span>
    <span class="n">UIntXWeightOnlyConfig</span><span class="p">,</span>  <span class="c1"># noqa: F401</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.linear_activation_weight_observed_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationWeightObservedTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.observer</span><span class="w"> </span><span class="kn">import</span> <span class="n">AffineQuantizedObserverBase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.common</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">KernelPreference</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.workflows</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Float8Tensor</span><span class="p">,</span>
    <span class="n">Int4ChooseQParamsAlgorithm</span><span class="p">,</span>
    <span class="n">Int4MarlinSparseTensor</span><span class="p">,</span>
    <span class="n">Int4PackingFormat</span><span class="p">,</span>
    <span class="n">Int4PlainInt32Tensor</span><span class="p">,</span>
    <span class="n">Int4PreshuffledTensor</span><span class="p">,</span>
    <span class="n">Int4Tensor</span><span class="p">,</span>
    <span class="n">Int4TilePackedTo4dTensor</span><span class="p">,</span>
    <span class="n">Int8Tensor</span><span class="p">,</span>
    <span class="n">IntxChooseQParamsAlgorithm</span><span class="p">,</span>
    <span class="n">IntxOpaqueTensor</span><span class="p">,</span>
    <span class="n">IntxPackingFormat</span><span class="p">,</span>
    <span class="n">IntxUnpackedToInt8Tensor</span><span class="p">,</span>
    <span class="n">QuantizeTensorToFloat8Kwargs</span><span class="p">,</span>
    <span class="n">QuantizeTensorToInt8Kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.transform_module</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">,</span>
    <span class="n">register_quantize_module_handler</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_fp8_mm_compat</span><span class="p">,</span>
    <span class="n">_linear_extra_repr</span><span class="p">,</span>
    <span class="n">_quantization_type</span><span class="p">,</span>
    <span class="n">get_block_size</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_MI300</span><span class="p">,</span>
    <span class="n">is_sm_at_least_89</span><span class="p">,</span>
    <span class="n">is_sm_at_least_90</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.autoquant</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">,</span> <span class="n">autoquant</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.GPTQ</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyGPTQQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Granularity</span><span class="p">,</span>
    <span class="n">PerAxis</span><span class="p">,</span>
    <span class="n">PerGroup</span><span class="p">,</span>
    <span class="n">PerRow</span><span class="p">,</span>
    <span class="n">PerTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.linear_activation_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationQuantizedTensor</span><span class="p">,</span>
    <span class="n">to_linear_activation_quantized</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.linear_quant_modules</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int4WeightOnlyQuantizer</span><span class="p">,</span>
    <span class="n">Int8DynActInt4WeightQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.qat</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">intx_quantization_aware_training</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">,</span>
    <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">ZeroPointDomain</span><span class="p">,</span>
    <span class="n">quantize_affine</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.unified</span><span class="w"> </span><span class="kn">import</span> <span class="n">Quantizer</span><span class="p">,</span> <span class="n">TwoStepQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_per_token_block_size</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># TODO: revisit this list?</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;swap_conv2d_1x1_to_linear&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Quantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TwoStepQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyGPTQQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;autoquant&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_get_subclass_inserter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_&quot;</span><span class="p">,</span>
    <span class="s2">&quot;intx_quantization_aware_training&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int8DynActInt4WeightQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Float8DynamicActivationFloat8SemiSparseWeightConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ModuleFqnToConfig&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">],</span>
    <span class="n">MarlinSparseLayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">],</span>
    <span class="n">Int4CPULayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">],</span>
    <span class="n">Int4XPULayout</span><span class="p">:</span> <span class="p">[</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">LAYOUT_TO_PRESERVE_ZEROS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">Int4CPULayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">Int4XPULayout</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">replacement_fn</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">,</span>
    <span class="n">cur_fqn</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively replaces each child module in `model` with the result of `replacement_fn(child)`</span>
<span class="sd">    if `filter_fn(child)` returns `True`.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model containing modules to be replaced.</span>
<span class="sd">        replacement_fn (Callable[[torch.nn.Module], torch.nn.Module]): The function to replace matching modules.</span>
<span class="sd">        filter_fn (Callable[[torch.nn.Module], bool]): The filter function to determine which modules to replace.</span>
<span class="sd">        cur_fqn (str, optional): The current fully qualified name of the module being processed. Defaults to &quot;&quot;.</span>
<span class="sd">        device (device, optional): Device to move the model to before applying `filter_fn`. Defaults to None.</span>
<span class="sd">        extra_args (Tuple[Any, ...], optional): optional extra args to pass to `replacement_fn`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">filter_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move to device before quantization</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">replacement_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">named_children_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">named_children_list</span><span class="p">:</span>
            <span class="n">new_child</span> <span class="o">=</span> <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span>
                <span class="n">replacement_fn</span><span class="p">,</span>
                <span class="n">filter_fn</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cur_fqn</span><span class="si">}{</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">device</span><span class="p">,</span>
                <span class="n">extra_args</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">child</span> <span class="ow">and</span> <span class="n">new_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_child</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move parent module to device</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_linear</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># avoid circular dependencies</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat.affine_fake_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">_AffineFakeQuantizedTensor</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># adding weight tensor subclass isinstance check to make sure the weight is only quantized once</span>
    <span class="c1"># when it is shared by multiple linear modules</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">_AffineFakeQuantizedTensor</span><span class="p">)</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">NonDynamicallyQuantizableLinear</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_subclass_inserter</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">enable_parametrization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a function which inserts the given subclass into all linear modules</span>
<span class="sd">    in the model. The inserted module will have its weight set to the result of</span>
<span class="sd">    `cls(mod.weight, **kwargs)`. If parametrization is enabled then this will be done using</span>
<span class="sd">    torch.nn.utils.parametrize instead of directly setting the attribute on the module.</span>

<span class="sd">    Args:</span>
<span class="sd">        cls (torch.Tensor): The class to insert as a child module.</span>
<span class="sd">        kwargs (Any): Any additional arguments for the constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">constructor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;constructor&quot;</span><span class="p">,</span> <span class="s2">&quot;subclass_constructor&quot;</span><span class="p">)</span>
    <span class="n">from_float</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;method&quot;</span><span class="p">,</span> <span class="s2">&quot;from_float&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">insert_subclass</span><span class="p">(</span><span class="n">lin</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">enable_parametrization</span><span class="p">:</span>
            <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
            <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
                <span class="n">lin</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">constructor</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="c1"># cls.from_float(...)</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">from_float</span><span class="p">)(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">lin</span>

    <span class="k">return</span> <span class="n">insert_subclass</span>


<span class="k">def</span><span class="w"> </span><span class="nf">swap_conv2d_1x1_to_linear</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Changes all conv2d 1x1 modules to equivalent linear modules so that they can then be quantized.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">PermuteSandwich</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="o">-</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replace_conv2d_1x1</span><span class="p">(</span><span class="n">conv</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">conv</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">lin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">conv</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">conv</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">PermuteSandwich</span><span class="p">(</span><span class="n">lin</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="n">mod</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">replace_conv2d_1x1</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="n">filter_fn</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">insert_observers_</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">input_observer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AffineQuantizedObserverBase</span><span class="p">],</span>
    <span class="n">weight_observer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AffineQuantizedObserverBase</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts the weight of a linear module to a LinearActivationWeightObservedTensor.</span>

<span class="sd">    This function wraps the weight of the given linear module with a LinearActivationWeightObservedTensor,</span>
<span class="sd">    which enables observation of both input and weight tensors during forward passes.</span>
<span class="sd">    The wrapped weight is then re-wrapped as a nn.Parameter to maintain compatibility</span>
<span class="sd">    with PyTorch&#39;s module system.</span>

<span class="sd">    Example::</span>

<span class="sd">    ```</span>
<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>
<span class="sd">        from torchao.quantization import PerTensor</span>
<span class="sd">        from torchao.quantization.linear_observer_tensor import insert_observers_</span>
<span class="sd">        from torchao.quantization.observer import (</span>
<span class="sd">            AffineQuantizedMinMaxObserver,</span>
<span class="sd">            MappingType</span>
<span class="sd">        )</span>

<span class="sd">        # Create observers</span>
<span class="sd">        input_observer = AffineQuantizedMinMaxObserver(</span>
<span class="sd">            MappingType.SYMMETRIC,</span>
<span class="sd">            torch.float8_e4m3fn,</span>
<span class="sd">            granularity_type=PerTensor(),</span>
<span class="sd">            eps=torch.finfo(torch.float32).eps,</span>
<span class="sd">            scale_dtype=torch.float,</span>
<span class="sd">            zero_point_dtype=torch.int,</span>
<span class="sd">            zero_point_domain=ZeroPointDomain.NONE,</span>
<span class="sd">        )</span>

<span class="sd">        # Create a linear module</span>
<span class="sd">        linear_module = nn.Linear(10, 20)</span>

<span class="sd">        # Convert the linear module&#39;s weight to an observed tensor</span>
<span class="sd">        insert_observers_(linear_module, input_observer, weight_observer=None)</span>

<span class="sd">        # The linear_module can now be used as usual, with observers calculating statistics</span>
<span class="sd">        output = linear_module(torch.randn(10, 10))</span>

<span class="sd">        # Get the scale and zero point of the input observer</span>
<span class="sd">        scale, zero_point = linear_module.weight.input_observer.calculate_qparams()</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        model (nn.Module): The nn.Module to convert.</span>
<span class="sd">        input_observer (Optional[AffineQuantizedObserverBase]): Observer for input tensor.</span>
<span class="sd">        weight_observer (Optional[AffineQuantizedObserverBase]): Observer for weight tensor.</span>
<span class="sd">        filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): Filter function to select which modules to convert.</span>
<span class="sd">            If not provided, all linear modules will be converted. This function should take a module and its fully qualified name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Linear: The modified linear module with its weight wrapped in a LinearActivationWeightObservedTensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">convert_to_linear_observer</span><span class="p">(</span><span class="n">linear_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="c1"># Wrap the weight with LinearActivationWeightObservedTensor and then with nn.Parameter</span>
        <span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">LinearActivationWeightObservedTensor</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
                <span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">input_observer</span><span class="o">=</span><span class="n">input_observer</span><span class="p">,</span>
                <span class="n">weight_observer</span><span class="o">=</span><span class="n">weight_observer</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">linear_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">linear_module</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">convert_to_linear_observer</span><span class="p">,</span>
        <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_embedding_extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;num_embeddings=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, embedding_dim=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_module_extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_extra_repr</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">):</span>
    <span class="n">module_torchao_extra_repr</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">original_extra_repr_str</span> <span class="o">=</span> <span class="n">original_extra_repr</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_extra_repr_str</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">module_torchao_extra_repr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">original_extra_repr_str</span><span class="p">)</span>

    <span class="n">module_torchao_extra_repr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">parameter_name</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">module_torchao_extra_repr</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_linear_subclass_inserter</span><span class="p">(</span>
    <span class="n">constructor</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">allow_requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">propagate_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to apply the constructor that quantizes the weight Tensor (with additional kwargs)</span>
<span class="sd">    to the weight of linear module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">insert_subclass</span><span class="p">(</span><span class="n">lin</span><span class="p">):</span>
        <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">allow_requires_grad</span> <span class="ow">and</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="k">if</span> <span class="n">propagate_bias</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">constructor</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span>
        <span class="p">)</span>
        <span class="n">lin</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">lin</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lin</span>

    <span class="k">return</span> <span class="n">insert_subclass</span>


<div class="viewcode-block" id="quantize_"><a class="viewcode-back" href="../../../generated/torchao.quantization.quantize_.html#torchao.quantization.quantize_">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">quantize_</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">AOBaseConfig</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="n">_is_linear</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the weight of linear modules in the model with `config`, model is modified inplace</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): input model</span>
<span class="sd">        config (AOBaseConfig): a workflow configuration object.</span>
<span class="sd">        filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): function that takes a nn.Module instance and fully qualified name of the module, returns True if we want to run `config` on</span>
<span class="sd">        the weight of the module</span>
<span class="sd">        device (device, optional): Device to move module to before applying `filter_fn`. This can be set to `&quot;cuda&quot;` to speed up quantization. The final model will be on the specified `device`.</span>
<span class="sd">            Defaults to None (do not change device).</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>
<span class="sd">        from torchao import quantize_</span>

<span class="sd">        # quantize with some predefined `config` method that corresponds to</span>
<span class="sd">        # optimized execution paths or kernels (e.g. int4 tinygemm kernel)</span>
<span class="sd">        # also customizable with arguments</span>
<span class="sd">        # currently options are</span>
<span class="sd">        # Int8DynamicActivationInt8WeightConfig (optimized with int8 mm op and torch.compile)</span>
<span class="sd">        # Int4WeightOnlyConfig (optimized with int4 tinygemm kernel and torch.compile)</span>
<span class="sd">        # Int8WeightOnlyConfig (optimized with int8 mm op and torch.compile</span>
<span class="sd">        from torchao.quantization.quant_api import Int4WeightOnlyConfig</span>

<span class="sd">        m = nn.Sequential(nn.Linear(32, 1024), nn.Linear(1024, 32))</span>
<span class="sd">        quantize_(m, Int4WeightOnlyConfig(group_size=32, version=1))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.quantize_&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">FqnToConfig</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Custom filter_fn and FqnToConfig were both specified. Only filter_fn=None is supported when FqnToConfig is specified.&quot;</span>
            <span class="p">)</span>
        <span class="n">named_modules</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">module_fqn</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">named_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">fqn_matches_fqn_config</span><span class="p">(</span><span class="n">module_fqn</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">_module_param_matches_fqn_config</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">module_fqn</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="s2">&quot;_default&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span> <span class="ow">and</span> <span class="n">_is_linear</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>
            <span class="p">):</span>
                <span class="n">replacement</span> <span class="o">=</span> <span class="n">_fqn_to_config_handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">module_fqn</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">replacement</span> <span class="o">=</span> <span class="n">replacement</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="c1"># handle module swap</span>
                <span class="k">if</span> <span class="n">replacement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">module</span> <span class="ow">and</span> <span class="n">module_fqn</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                    <span class="n">child_name</span> <span class="o">=</span> <span class="n">module_fqn</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">parent_fqn</span> <span class="o">=</span> <span class="n">module_fqn</span><span class="o">.</span><span class="n">removesuffix</span><span class="p">(</span><span class="n">child_name</span><span class="p">)</span><span class="o">.</span><span class="n">removesuffix</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                    <span class="n">parent_module</span> <span class="o">=</span> <span class="n">named_modules</span><span class="p">[</span><span class="n">parent_fqn</span><span class="p">]</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">replacement</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">AOBaseConfig</span><span class="p">):</span>
        <span class="n">filter_fn</span> <span class="o">=</span> <span class="n">_is_linear</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">filter_fn</span>
        <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)]</span>
        <span class="c1"># for each linear in the model, apply the transform if filtering passes</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">handler</span><span class="p">,</span>
            <span class="n">filter_fn</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">extra_args</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">,),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Passing a generic Callable to `quantize_` is no longer recommended and will be deprecated at a later release. Please see https://github.com/pytorch/ao/issues/1690 for instructions on how to pass in workflow configuration instead.&quot;&quot;&quot;</span>
        <span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_asymm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is defined here instead of local function to support serialization&quot;&quot;&quot;</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_uint8_asymm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">255</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>

    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationIntxWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for dynamically quantizing activations to torch.int8 and weights to torch.intx, with 1 &lt;= x &lt;= 8.</span>
<span class="sd">    More specifically, activations are dynamically quantized to 8-bits at a per-token granularity with scales/zeros.</span>
<span class="sd">    Weights are quantized with scales/zeros in a groupwise or channelwise manner using the number of bits specified by weight_dtype.</span>

<span class="sd">    This layout is identical to Int8DynamicActivationInt4WeightConfig when weight_dtype is torch.int4 and other args</span>
<span class="sd">    are the same.  However, this layout is more general and supports other weight dtypes.</span>

<span class="sd">    args:</span>
<span class="sd">        `weight_dtype`: The dtype to use for weight quantization.  Must be torch.intx, where 1 &lt;= x &lt;= 8.</span>
<span class="sd">       ` weight_granularity`: The granularity to use for weight quantization.  Must be PerGroup or PerAxis(axis=0).</span>
<span class="sd">        `weight_mapping_type`: The type of mapping to use for the weight quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.  MappingType.SYMMETRIC requires ZeroPointDomain.NONE</span>
<span class="sd">        `weight_scale_dtype`: The dtype to use for the weight scale.</span>
<span class="sd">        `act_mapping_type`: The type of mapping to use for the activation quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.</span>
<span class="sd">        `layout`: The layout to use for the packed weight tensor:</span>
<span class="sd">            - PackedLinearInt8DynamicActivationIntxWeightLayout: this layout is optimized for CPU performance.</span>
<span class="sd">            - QDQLayout: this layout represents the quantization with Q/DQ quant primitives, and is intended for</span>
<span class="sd">                export applications like ExecuTorch.</span>
<span class="sd">        `intx_packing_format`: The format to use for the packed weight tensor (version 2 only).</span>
<span class="sd">            - unpacked_to_int8: this format is the default and is intended for export applications like ExecuTorch.</span>
<span class="sd">            - opaque_torchao_auto: this format is optimized for CPU performance.</span>
<span class="sd">        `intx_choose_qparams_algorithm`: The algorithm to use for choosing the quantization parameters.</span>
<span class="sd">        `version`: version of the config to use, only subset of above args are valid based on version, see note for more details.</span>

<span class="sd">        Note:</span>

<span class="sd">        Current state for Int8DynamicActivationIntxWeightConfig is that it supports both v1 (legacy) and v2.</span>

<span class="sd">        * `intx_packing_format` is used for version 2.</span>
<span class="sd">        * `layout` is only used for version 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">weight_granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerGroup</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">weight_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="c1"># TODO: add weight_scale_dtype to Int8DynamicActivationInt4WeightConfig</span>
    <span class="n">weight_scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">QDQLayout</span><span class="p">()</span>
    <span class="n">intx_packing_format</span><span class="p">:</span> <span class="n">IntxPackingFormat</span> <span class="o">=</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span>
    <span class="n">intx_choose_qparams_algorithm</span><span class="p">:</span> <span class="n">IntxChooseQParamsAlgorithm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span>
    <span class="p">)</span>

    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int8DynamicActivationIntxWeightConfig&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;int</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_dtype must be torch.intx, where 1 &lt;= x &lt;= 8, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="p">(</span><span class="n">PerAxis</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">)),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_granularity must be PerAxis or PerGroup, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;axis must be 0, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC or MappingType.SYMMETRIC_NO_CLIPPING_ERR, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;act_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="p">(</span><span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">,</span> <span class="n">QDQLayout</span><span class="p">)</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;layout must be PackedLinearInt8DynamicActivationIntxWeightLayout or QDQLayout, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Target</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Target</span><span class="o">.</span><span class="n">KLEIDIAI</span><span class="p">,</span> <span class="n">Target</span><span class="o">.</span><span class="n">ATEN</span><span class="p">]:</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="p">):</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;When using layout PackedLinearInt8DynamicActivationIntxWeightLayout with target </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;the weight scale may be cast to bfloat16 by the kernel, but weight_scale_dtype is set to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_scale_dtype</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="s2">&quot;Explicitly set weight_scale_dtype to torch.bfloat16 to suppress this warning. &quot;</span>
                        <span class="s2">&quot;If you need weight_scale_dtype = torch.float32, use target=Target.UNIVERSAL instead.&quot;</span>
                    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_intx_weight_quantize_tensor</span><span class="p">(</span>
    <span class="n">weight</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">,</span>
    <span class="n">config</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_granularity</span>
    <span class="n">weight_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_mapping_type</span>
    <span class="n">weight_scale_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_scale_dtype</span>
    <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">intx_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_packing_format</span>
    <span class="n">intx_choose_qparams_algorithm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_choose_qparams_algorithm</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Int8DynamicActivationIntxWeightConfig only works for 2-d Tensor, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">):</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight_granularity</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="n">weight_granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_granularity must be PerGroup or PerAxis, got </span><span class="si">{</span><span class="n">weight_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
        <span class="n">opaque_formats</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_ATEN_KLEIDIAI</span><span class="p">,</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_TORCHAO_AUTO</span><span class="p">,</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_TORCHAO_KLEIDIAI</span><span class="p">,</span>
            <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">OPAQUE_TORCHAO_LOWBIT</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">intx_packing_format</span> <span class="o">==</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span>
            <span class="ow">or</span> <span class="n">intx_packing_format</span> <span class="ow">in</span> <span class="n">opaque_formats</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Unsupported packing format: </span><span class="si">{</span><span class="n">intx_packing_format</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">custom_zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">custom_zero_point</span> <span class="o">=</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="o">=</span><span class="n">weight_mapping_type</span><span class="p">,</span>
            <span class="n">activation_quantization</span><span class="o">=</span><span class="s2">&quot;int8_asym_per_token&quot;</span><span class="p">,</span>
            <span class="n">intx_choose_qparams_algorithm</span><span class="o">=</span><span class="n">intx_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
            <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_scale_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight_scale_dtype</span> <span class="o">!=</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="n">_adjust_scale_dtype_in_intx_unpacked_tensor</span><span class="p">(</span>
                <span class="n">new_weight</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">weight_scale_dtype</span>
            <span class="p">)</span>

        <span class="n">new_bias</span> <span class="o">=</span> <span class="n">bias</span>

        <span class="c1"># Create packed tensor</span>
        <span class="k">if</span> <span class="n">intx_packing_format</span> <span class="ow">in</span> <span class="n">opaque_formats</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">IntxOpaqueTensor</span><span class="o">.</span><span class="n">from_intx_unpacked_to_int8_tensor</span><span class="p">(</span>
                <span class="n">new_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">new_bias</span><span class="p">,</span> <span class="n">intx_packing_format</span><span class="o">=</span><span class="n">intx_packing_format</span>
            <span class="p">)</span>
            <span class="n">new_bias</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># bias is packed with weights</span>

        <span class="k">return</span> <span class="n">new_weight</span><span class="p">,</span> <span class="n">new_bias</span>

    <span class="c1"># Version 1</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">intx_choose_qparams_algorithm</span> <span class="o">==</span> <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;IntxChooseQParamsAlgorithm.AFFINE is the only supported algorithm for version 1&quot;</span>
    <span class="p">)</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Config Deprecation: version 1 of Int8DynamicActivationIntxWeightConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2967 for more details&quot;</span>
    <span class="p">)</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">weight_dtype</span><span class="p">]</span>

    <span class="c1"># We quantize with QDQLayout, and then construct the packed weight tensor later</span>
    <span class="c1"># set preserve_zero based on weight mapping type</span>
    <span class="n">preserve_zero</span> <span class="o">=</span> <span class="n">weight_mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">weight_mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">weight_scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">QDQLayout</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">QDQLayout</span><span class="p">):</span>
        <span class="c1"># TODO: _int8_asymm_per_token_quant uses scale_dtype=torch.float64, zero_point_dtype=torch.int64,</span>
        <span class="c1"># which is not great for export with QDQLayout.  It is also not consistent with _int8_symm_per_token_quant,</span>
        <span class="c1"># which uses scale_dtype=torch.float32, zero_point_dtype=torch.int32.</span>
        <span class="c1"># Maybe introduce new fp32/int32 versions of _int8_asymm_per_token_quant?</span>
        <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">:</span>
            <span class="n">activation_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>
        <span class="k">elif</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">activation_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_quant</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported activation mapping type: </span><span class="si">{</span><span class="n">act_mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">activation_quant_func</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">PackedLinearInt8DynamicActivationIntxWeightLayout</span><span class="p">):</span>
        <span class="c1"># PackedLinearInt8DynamicActivationIntxWeightLayout has dynamic activation quantization</span>
        <span class="c1"># fused with the kernel and it should not be applied separately</span>
        <span class="k">assert</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PackedLinearInt8DynamicActivationIntxWeightLayout requires act_mapping_type=MappingType.ASYMMETRIC&quot;</span>
        <span class="p">)</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
        <span class="n">groups_per_row</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">group_size</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups_per_row</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups_per_row</span><span class="p">)</span>
        <span class="n">has_weight_zeros</span> <span class="o">=</span> <span class="p">(</span><span class="n">zero_point</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">make_packed_linear_int8_dynamic_activation_intx_weight_tensor</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">scale</span><span class="p">,</span>
            <span class="n">zero_point</span> <span class="k">if</span> <span class="n">has_weight_zeros</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">layout</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
            <span class="n">validate_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># bias is packed with weights if present</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_intx_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationIntxWeightConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">new_weight</span><span class="p">,</span> <span class="n">new_bias</span> <span class="o">=</span> <span class="n">_int8_dynamic_activation_intx_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
        <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">new_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Int4WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int4WeightOnlyConfig.html#torchao.quantization.Int4WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for int4 weight only quantization, only groupwise quantization is supported</span>
<span class="sd">    right now, and we support version 1 and version 2, that are implemented differently although with</span>
<span class="sd">    same support. In version 2, different target are mainly distinguished by `packing_format` arg, and in version 1, mainly by `layout`.</span>

<span class="sd">    Args:</span>
<span class="sd">        `group_size`: parameter for quantization, controls the granularity of quantization, smaller</span>
<span class="sd">         size is more fine grained, choices are [256, 128, 64, 32], used in both version 1 and 2</span>
<span class="sd">        `int4_packing_format`: the packing format for int4 tensor, used in version 2 only</span>
<span class="sd">         `int4_choose_qparams_algorithm`: variants of choose qparams algorithm to use for int4,</span>
<span class="sd">         currently support TINYGEMM (&quot;tinygemm&quot;) and HQQ (&quot;hqq&quot;), used in version 2 only</span>
<span class="sd">        `layout`: layout type for quantized tensor, default is `TensorCoreTiledLayout(inner_k_tiles=8)`, used in version 1 only</span>
<span class="sd">        `use_hqq`: whether to use hqq or default quantization mode, default is False, used in version 1 only</span>
<span class="sd">        `zero_point_domain`: data type of zeros points, choices are [ZeroPointDomain.FLOAT, ZeroPointDomain.INT, ZeroPointDomain.NONE], used in version 1 only</span>
<span class="sd">        `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values. used in both version 1 and 2</span>
<span class="sd">        `preserve_zero`: whether to preserve zero, default is None. Will be set to True if zero_point_domain is ZeroPointDomain.INT, used in version 1 only</span>
<span class="sd">        `version`: version of the config to use, only subset of above args are valid for version 1, and subset of above args are valid for version 2, default is 2, see note for more details</span>

<span class="sd">    Note:</span>
<span class="sd">        Current state for Int4WeightOnlyConfig is that it supports both v1 (legacy) and v2</span>

<span class="sd">        For v2 (version = 2), only `group_size`, `int4_packing_format`, `int4_choose_qparams_algorithm` and `set_inductor_config` are valid, all other args will be ignored</span>
<span class="sd">        For v1 (version = 1), only `group_size`, `layout`, `use_hqq`, `zero_point_domain`, `preserve_zero` and `set_inductor_config` are valid, we plan to deprecate v1 in torchao 0.15 to make this config</span>
<span class="sd">        less confusing</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorCoreTiledLayout</span><span class="p">]</span> <span class="o">=</span> <span class="n">TensorCoreTiledLayout</span><span class="p">(</span><span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">use_hqq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ZeroPointDomain</span><span class="p">]</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">preserve_zero</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># only used in version &gt;= 2</span>
    <span class="n">int4_packing_format</span><span class="p">:</span> <span class="n">Int4PackingFormat</span> <span class="o">=</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PLAIN</span>
    <span class="n">int4_choose_qparams_algorithm</span><span class="p">:</span> <span class="n">Int4ChooseQParamsAlgorithm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">Int4ChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">TINYGEMM</span>
    <span class="p">)</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.Int4WeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_int4_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="c1"># TODO(future PR): perhaps move this logic to a different file, to keep the API</span>
    <span class="c1"># file clean of implementation details</span>

    <span class="c1"># for now, make these local variables to allow the rest of the function</span>
    <span class="c1"># to be a direct copy-paste</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">use_hqq</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_hqq</span>
    <span class="n">int4_choose_qparams_algorithm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">int4_choose_qparams_algorithm</span>
    <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">zero_point_domain</span>
    <span class="n">int4_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">int4_packing_format</span>

    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Skipping quantizing weight with int4 weight only quantization because the shape of weight </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not compatible with group_size </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">int4_choose_qparams_algorithm</span> <span class="o">==</span> <span class="n">Int4ChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">HQQ</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">TILE_PACKED_TO_4D</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Int4ChooseQParamsAlgorithm.HQQ is not supported by packing format </span><span class="si">{</span><span class="n">int4_packing_format</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;it&#39;s only supported by Int4PackingFormat.TILE_PACKED_TO_4D currently&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PRESHUFFLED</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4PreshuffledTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PLAIN</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">PLAIN_INT32</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4PlainInt32Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">MARLIN_SPARSE</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4MarlinSparseTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">elif</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="n">Int4PackingFormat</span><span class="o">.</span><span class="n">TILE_PACKED_TO_4D</span><span class="p">:</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4TilePackedTo4dTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">int4_choose_qparams_algorithm</span><span class="o">=</span><span class="n">int4_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported int4 packing format: </span><span class="si">{</span><span class="n">int4_packing_format</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span>

    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Config Deprecation: version 1 of Int4WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2948 for more details&quot;</span>
    <span class="p">)</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int4CPULayout</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="p">)</span>

    <span class="c1"># nonlocal zero_point_domain</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)</span> <span class="ow">in</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Only support layout: </span><span class="si">{</span><span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
        <span class="c1"># the first value is the default one</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="ow">in</span> <span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Layout only support </span><span class="si">{</span><span class="n">LAYOUT_TO_ZERO_POINT_DOMAIN</span><span class="p">[</span><span class="n">layout</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">Int4XPULayout</span><span class="p">):</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>

    <span class="n">preserve_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">config</span><span class="o">.</span><span class="n">preserve_zero</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">preserve_zero</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">LAYOUT_TO_PRESERVE_ZEROS</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">layout</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="c1"># Sparse Marlin only supports symmetric quantization.</span>
    <span class="c1"># NOTE: If we start having lots of layouts that require different configurations,</span>
    <span class="c1"># we should consider moving this logic somewhere else.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">):</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="k">assert</span> <span class="n">group_size</span> <span class="o">==</span> <span class="mi">128</span> <span class="ow">or</span> <span class="n">group_size</span> <span class="o">==</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;MarlinSparseLayout only supports 128 group size or per channel quantization, got </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int4WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int4_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Int4WeightOnlyConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int4_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Float8DynamicActivationInt4WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8DynamicActivationInt4WeightConfig.html#torchao.quantization.Float8DynamicActivationInt4WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration for apply float8 dynamic per row quantization and int4</span>
<span class="sd">    per group weight quantization to linear</span>
<span class="sd">    (only group_size 128 is supported right now since underlying kernel used only supports 128</span>
<span class="sd">    and above and no benefits of making it bigger)</span>

<span class="sd">    Args:</span>
<span class="sd">        `int4_packing_format`: how the weight is packed, only preshuffled is supported</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">int4_packing_format</span><span class="p">:</span> <span class="n">Int4PackingFormat</span> <span class="o">=</span> <span class="s2">&quot;preshuffled&quot;</span></div>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationInt4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_int4_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationInt4WeightConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">int4_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">int4_packing_format</span>

    <span class="k">assert</span> <span class="n">int4_packing_format</span> <span class="o">==</span> <span class="s2">&quot;preshuffled&quot;</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;only preshuffled int4_packing_format supported right now, got: </span><span class="si">{</span><span class="n">int4_packing_format</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int4PreshuffledTensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">activation_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="Int8WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8WeightOnlyConfig.html#torchao.quantization.Int8WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 weight-only symmetric per-channel quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        group_size (version 1) - Controls the granularity of quantization.</span>
<span class="sd">        If None, applies per-channel quantization. Otherwise, applies per-group quantization with the specified group size.</span>
<span class="sd">        granularity (version 2) - Quantization granularity.</span>
<span class="sd">            PerRow() for per-channel quantization, PerTensor() for per-tensor quantization.</span>
<span class="sd">        set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values</span>
<span class="sd">            for better performance with this quantization scheme.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Granularity</span><span class="p">]</span> <span class="o">=</span> <span class="n">PerRow</span><span class="p">()</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.Int8WeightOnlyConfig&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only support version 2 with group_size=None, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_weight_only_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Config Deprecation: version 1 of Int8WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2752 for more details&quot;</span>
        <span class="p">)</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">group_size</span>
        <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_size</span><span class="p">])</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unexpected version: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Int8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">granularity</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">granularity</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Int8WeightOnlyConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 weight only quant requires module to have </span><span class="si">{parameter_name}</span><span class="s2"> attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">_int8_weight_only_quantize_tensor</span><span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">parameter_name</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_tensor</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">_module_extra_repr</span><span class="p">,</span>
            <span class="n">original_extra_repr</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">,</span>
            <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">module</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_reduced_range_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_per_token_reduced_range_quant_noop_decode</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">127</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_symm_cutlass_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_int4_symm_cutlass_quant</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=-</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">CutlassInt4PackedLayout</span><span class="p">(),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_cutlass_quant</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_cutlass_quant_sparse</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">_get_per_token_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">CutlassSemiSparseLayout</span><span class="p">(),</span>
    <span class="p">)</span>


<div class="viewcode-block" id="Int8DynamicActivationInt8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.html#torchao.quantization.Int8DynamicActivationInt8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynamicActivationInt8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 dynamic symmetric per-token activation and int8 per-channel weight</span>
<span class="sd">    quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        layout: Optional[Layout] = PlainLayout() - Tensor layout for the quantized weights. Controls how the</span>
<span class="sd">            quantized data is stored and accessed.</span>
<span class="sd">        act_mapping_type: Optional[MappingType] = MappingType.SYMMETRIC - Mapping type for activation quantization.</span>
<span class="sd">            SYMMETRIC uses symmetric quantization around zero.</span>
<span class="sd">        weight_only_decode: bool = False - If True, only quantizes weights during forward pass and keeps activations</span>
<span class="sd">            in original precision during decode operations.</span>
<span class="sd">        set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values</span>
<span class="sd">            for better performance with this quantization scheme.</span>
<span class="sd">        version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Int8Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Layout</span><span class="p">]</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MappingType</span><span class="p">]</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">weight_only_decode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerRow</span><span class="p">()</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int8DynamicActivationInt8WeightConfig&quot;</span>
        <span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int8_weight_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
        <span class="n">act_mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span>
        <span class="n">weight_only_decode</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_only_decode</span>

        <span class="n">in_features</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># int8 dynamic quantization only has benefit when in_feature &gt; 16</span>
        <span class="k">if</span> <span class="n">in_features</span> <span class="o">&lt;=</span> <span class="mi">16</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Skipping applying Int8DynamicActivationInt8WeightConfig to weight of shape </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; because `in_feature` is &lt;= 16: </span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">weight</span>

        <span class="c1"># weight settings</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="n">weight_zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>

        <span class="k">if</span> <span class="n">weight_only_decode</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant_noop_decode</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># input settings</span>
            <span class="k">if</span> <span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
                <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_asymm_per_token_quant</span>

        <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_weight_block_size</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
            <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">weight_zero_point_domain</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span> <span class="ow">in</span> <span class="p">{</span><span class="n">PerRow</span><span class="p">(),</span> <span class="n">PerTensor</span><span class="p">()},</span> <span class="p">(</span>
            <span class="s2">&quot;Only PerRow and PerTensor are supported&quot;</span>
        <span class="p">)</span>
        <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
        <span class="n">act_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>

        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;asymmetric dynamic quant not supported currently&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unexpected version: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># TODO: Symmentric/Asymmetric choice for weight quantization</span>
        <span class="c1"># https://github.com/pytorch/ao/pull/3241#discussion_r2551515539</span>
        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">Int8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">granularity</span><span class="o">=</span><span class="n">weight_granularity</span><span class="p">,</span>
            <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">QuantizeTensorToInt8Kwargs</span><span class="p">(</span>
                <span class="n">granularity</span><span class="o">=</span><span class="n">act_granularity</span><span class="p">,</span>
                <span class="n">mapping_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">quantized_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8DynamicActivationInt8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_dynamic_activation_int8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying int8 dynamic activation int8 weight quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot;but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_int8_dynamic_activation_int8_weight_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">_module_extra_repr</span><span class="p">,</span>
            <span class="n">original_extra_repr</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">,</span>
            <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">module</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8StaticActivationInt8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying int8 static symmetric quantization to both activation and weight</span>

<span class="sd">    Args:</span>
<span class="sd">        scale (torch.Tensor): The scale tensor for activation quantization.</span>
<span class="sd">        granularity (Granularity): The granularity of quantization. PerRow() and PerTensor() are supported currently</span>
<span class="sd">        act_mapping_type (MappingType): The mapping type for activation quantization. only SYMMETRIC is supported currently</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">        version (int): the version of the config</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerRow</span><span class="p">()</span>
    <span class="n">act_mapping_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MappingType</span><span class="p">]</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Int8StaticActivationInt8WeightConfig&quot;</span>
        <span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Int8StaticActivationInt8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_int8_static_activation_int8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Int8StaticActivationInt8WeightConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span> <span class="ow">in</span> <span class="p">{</span><span class="n">PerRow</span><span class="p">(),</span> <span class="n">PerTensor</span><span class="p">()},</span> <span class="p">(</span>
        <span class="s2">&quot;Only PerRow and PerTensor is supported currently&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;asymmetric static quant not supported currently&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Expected module to have attribute `</span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2">` but not found&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="n">activation_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>

    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">Int8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span>
        <span class="n">granularity</span><span class="o">=</span><span class="n">weight_granularity</span><span class="p">,</span>
        <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">QuantizeTensorToInt8Kwargs</span><span class="p">(</span>
            <span class="n">granularity</span><span class="o">=</span><span class="n">activation_granularity</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_mapping_type</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">act_scale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="nb">setattr</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">parameter_name</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_tensor</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">_module_extra_repr</span><span class="p">,</span>
            <span class="n">original_extra_repr</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">,</span>
            <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">module</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">int8_dynamic_activation_int8_semi_sparse_weight</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies int8 dnynamic symmetric per-token activation and int8 per-channel weight</span>
<span class="sd">    quantization + 2:4 sparsity to linear layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;int8_dyanmic_activation_int8_semi_sparse_weight() will be deprecated at a later release. Please use the layout kwarg in Int8DynamicActivationInt8WeightConfig instead.</span>

<span class="sd">    from torchao.dtypes import SemiSparseLayout</span>
<span class="sd">    Int8DynamicActivationInt8WeightConfig(layout=SemiSparseLayout()&quot;&quot;&quot;</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">Int8DynamicActivationInt8WeightConfig</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">SemiSparseLayout</span><span class="p">())</span>


<div class="viewcode-block" id="Float8WeightOnlyConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8WeightOnlyConfig.html#torchao.quantization.Float8WeightOnlyConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 weight-only symmetric per-channel quantization to linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">        version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Float8Tensor (default)</span>

<span class="sd">    Note:</span>
<span class="sd">        The actual matmul will be computed in original precision of the weight tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.Float8WeightOnlyConfig&quot;</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_weight_only_quant_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Config Deprecation: version 1 of Float8WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2649 for more details&quot;</span>
        <span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_floatx</span>

        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unexpected version: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">Float8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">float8_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span> <span class="n">granularity</span><span class="o">=</span><span class="n">PerRow</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Float8WeightOnlyConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying float8 weight only quant requires module to have </span><span class="si">{parameter_name}</span><span class="s2"> attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">_unwrap_float8_linear</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">_float8_weight_only_quant_tensor</span><span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="n">config</span>
    <span class="p">)</span>

    <span class="nb">setattr</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">parameter_name</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_tensor</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">_module_extra_repr</span><span class="p">,</span>
            <span class="n">original_extra_repr</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">,</span>
            <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">module</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_input_activation_quant_func_fp8</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">activation_granularity</span><span class="p">:</span> <span class="n">FP8Granularity</span><span class="p">,</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function is used to quantize the input activation tensor for an aqt_float variant. If scale</span>
<span class="sd">    is not provided it will be dynamically calculate the scales otherwise it will use the provided scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Zero point is not supported for dynamic FP8 quantization&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerRow</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PerRow quantization only works for bfloat16 precision input activation&quot;</span>
        <span class="p">)</span>

    <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">activation_granularity</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Config is stored on weight</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerTensor</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Static quantization only supports PerTensor granularity&quot;</span>
        <span class="p">)</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx_static</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>  <span class="c1"># Config is stored on weight</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">activation</span>


<div class="viewcode-block" id="Float8DynamicActivationFloat8WeightConfig"><a class="viewcode-back" href="../../../generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.html#torchao.quantization.Float8DynamicActivationFloat8WeightConfig">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationFloat8WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for applying float8 dynamic symmetric quantization to both activations and weights of linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_dtype (torch.dtype): The target data type for activation quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn.</span>
<span class="sd">        granularity (Optional[Union[FP8Granularity, List[FP8Granularity]]]):</span>
<span class="sd">            The granularity for quantization. Can be either a single granularity (applied to both</span>
<span class="sd">            activations and weights) or a tuple of two granularities (one for activations, one for weights).</span>
<span class="sd">            If None, defaults to PerTensor for both. Currently both quantizations need to be the same type. And</span>
<span class="sd">            only PerTensor and PerRow are supported.</span>
<span class="sd">        mm_config (Float8MMConfig): Configuration for the matrix multiplication. Default uses fast accumulation.</span>
<span class="sd">        activation_value_lb (Optional[float]): the lower bound for activation value for calculating scale</span>
<span class="sd">        activation_value_ub (Optional[float]): the upper bound for activation value for calculating scale</span>
<span class="sd">        kernel_preference (KernelPreference): kernel preference for ops like matmul, grouped matmul etc. by defalut (KernelPreference.AUTO) it will be chosen for user based on hardware or other information, this only needs to be set in weight</span>
<span class="sd">        set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values.</span>
<span class="sd">        version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Float8Tensor (default)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">FP8Granularity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Float8MMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">activation_value_lb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">activation_value_ub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">kernel_preference</span><span class="p">:</span> <span class="n">KernelPreference</span> <span class="o">=</span> <span class="n">KernelPreference</span><span class="o">.</span><span class="n">AUTO</span>
    <span class="n">set_inductor_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Float8DynamicActivationFloat8WeightConfig&quot;</span>
        <span class="p">)</span>
        <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">_normalize_granularity</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span> <span class="o">=</span> <span class="p">[</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span><span class="p">]</span>

        <span class="n">default_use_fast_accum</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">_granularity_is_a_1_128_w_128_128</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_preference</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">KernelPreference</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                <span class="n">KernelPreference</span><span class="o">.</span><span class="n">TORCH</span><span class="p">,</span>
            <span class="p">),</span> <span class="s2">&quot;unimplemented&quot;</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;unimplemented&quot;</span>
            <span class="n">default_use_fast_accum</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mm_config</span> <span class="o">=</span> <span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="n">default_use_fast_accum</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_weight_quantize_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mm_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mm_config</span>
    <span class="n">activation_value_lb</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_value_lb</span>
    <span class="n">activation_value_ub</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_value_ub</span>
    <span class="n">kernel_preference</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">kernel_preference</span>

    <span class="c1"># Ensure works on device</span>
    <span class="n">_check_hardware_support</span><span class="p">(</span><span class="n">granularity</span><span class="p">)</span>
    <span class="n">activation_granularity</span><span class="p">,</span> <span class="n">weight_granularity</span> <span class="o">=</span> <span class="n">granularity</span>

    <span class="c1"># Note: right now we assume it&#39;s weights of conv2d and conv3d purely based</span>
    <span class="c1"># on the dimension of weight, currently there is no conflict with linear 2d</span>
    <span class="c1"># and moe weights 3d</span>
    <span class="c1"># if we need to support conv1d, which also has 3d weight, we may have to</span>
    <span class="c1"># pass around the module as well to distinguish between conv1d and 3d moe weight</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
        <span class="c1"># weights for conv2d or 3d</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation_granularity</span><span class="p">,</span> <span class="n">PerTensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerTensor</span>
        <span class="p">),</span> <span class="s2">&quot;4D/5D tensor only supports per tensor activation and weight quantization&quot;</span>

        <span class="c1"># conv3d weight dim: (C_out, C_in, K1, K2, K3)</span>
        <span class="c1"># conv2d weight dim: (C_out, C_in, K1, K2)</span>
        <span class="c1"># skip quantization when either C_out or C_in</span>
        <span class="c1"># is not a multiple of 16</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weight</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">_fp8_mm_compat</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="c1"># TODO(future PR): this should really throw an exception instead of silently</span>
        <span class="c1"># not doing what the user asked</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_granularity</span><span class="p">,</span> <span class="n">PerRow</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;PerRow quantization only works for bfloat16 precision input weight&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Config Deprecation: version 1 of Float8DynamicActivationFloat8WeightConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2649 for more details&quot;</span>
        <span class="p">)</span>

        <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_block_size</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">weight_granularity</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">block_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">block_size</span><span class="p">))</span>
        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
        <span class="n">input_quant_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="n">activation_granularity</span><span class="p">,</span>
            <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
            <span class="n">quantized_weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">,</span> <span class="n">quant_kwargs</span><span class="o">=</span><span class="n">input_quant_kwargs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unexpected version: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">act_quant_kwargs</span> <span class="o">=</span> <span class="n">QuantizeTensorToFloat8Kwargs</span><span class="p">(</span>
            <span class="n">activation_dtype</span><span class="p">,</span>
            <span class="n">activation_granularity</span><span class="p">,</span>
            <span class="n">hp_value_lb</span><span class="o">=</span><span class="n">activation_value_lb</span><span class="p">,</span>
            <span class="n">hp_value_ub</span><span class="o">=</span><span class="n">activation_value_ub</span><span class="p">,</span>
            <span class="n">kernel_preference</span><span class="o">=</span><span class="n">kernel_preference</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">Float8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">float8_dtype</span><span class="o">=</span><span class="n">weight_dtype</span><span class="p">,</span>
            <span class="n">granularity</span><span class="o">=</span><span class="n">weight_granularity</span><span class="p">,</span>
            <span class="n">mm_config</span><span class="o">=</span><span class="n">mm_config</span><span class="p">,</span>
            <span class="n">kernel_preference</span><span class="o">=</span><span class="n">kernel_preference</span><span class="p">,</span>
            <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">act_quant_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">quantized_weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_89</span><span class="p">()</span> <span class="ow">or</span> <span class="n">is_MI300</span><span class="p">(),</span> <span class="p">(</span>
        <span class="s2">&quot;Float8 dynamic activation quantization is only supported on CUDA&gt;=8.9 and MI300+&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;applying float8 dynamic activation quant requires module to have parameter </span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2"> attribute&quot;</span>
        <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot; but </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">_unwrap_float8_linear</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="n">quantized_tensor</span> <span class="o">=</span> <span class="n">_float8_dynamic_activation_float8_weight_quantize_tensor</span><span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">),</span> <span class="n">config</span>
    <span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">parameter_name</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_tensor</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">_module_extra_repr</span><span class="p">,</span>
            <span class="n">original_extra_repr</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">,</span>
            <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">module</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8DynamicActivationFloat8SemiSparseWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies float8 dynamic quantization to activations and float8 quantization followed by compression to sparse semi-structured tensor to weights of linear layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        `layout`: layout type for quantized weight tensor, only supports `CutlassSemiSparseLayout` at the moment.</span>
<span class="sd">        `activation_dtype`: data type for quantized activation tensor.</span>
<span class="sd">        `weight_dtype`: data type for quantized weight tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">CutlassSemiSparseLayout</span><span class="p">()</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e5m2_dtype</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">e4m3_dtype</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.Float8DynamicActivationFloat8SemiSparseWeightConfig&quot;</span>
        <span class="p">)</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">Float8DynamicActivationFloat8SemiSparseWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_float8_dynamic_activation_float8_semi_sparse_weight_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Float8DynamicActivationFloat8SemiSparseWeightConfig</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_sm_at_least_90</span><span class="p">(),</span> <span class="s2">&quot;Float8 quantization is only supported on CUDA&gt;=9.0&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Float8Linear</span><span class="p">):</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">_unwrap_float8_linear</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">activation_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">CutlassSemiSparseLayout</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Only CutlassSemiSparseLayout layout is supported. Received </span><span class="si">{</span><span class="n">layout</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">_float8_cutlass_quant_sparse</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_linear_activation_quantized</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant</span><span class="p">,</span>
        <span class="n">quant_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;target_dtype&quot;</span><span class="p">:</span> <span class="n">activation_dtype</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_adjust_scale_dtype_in_intx_unpacked_tensor</span><span class="p">(</span>
    <span class="n">intx_unpacked_tensor</span><span class="p">:</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="p">,</span>
    <span class="n">hp_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adjusts the scale_dtype on IntxUnpackedToInt8Tensor.</span>
<span class="sd">    Updating the scale dtype requires updating the qdata because qdata is calculated after the scale.</span>
<span class="sd">    This is used in IntxWeightOnlyConfig and Int8DynamicActivationIntxWeightConfig to make</span>
<span class="sd">    version=2 and version=1 numerically equivalent when the scale_dtype differs from the input dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">intx_unpacked_tensor</span><span class="p">,</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="p">)</span>
    <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">scale_dtype</span><span class="p">)</span>
    <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">]</span>
    <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">qdata</span> <span class="o">=</span> <span class="n">quantize_affine</span><span class="p">(</span>
        <span class="n">hp_tensor</span><span class="p">,</span>
        <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">intx_unpacked_tensor</span><span class="o">.</span><span class="n">zero_point</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">qmin</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">qmax</span><span class="p">,</span>
    <span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">IntxWeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for quantizing weights to torch.intx, with 1 &lt;= x &lt;= 8.</span>
<span class="sd">    Weights are quantized with scales/zeros in a groupwise or channelwise</span>
<span class="sd">    manner using the number of bits specified by weight_dtype.</span>
<span class="sd">    args:</span>
<span class="sd">        `weight_dtype`: The dtype to use for weight quantization.  Must be torch.intx, where 1 &lt;= x &lt;= 8.</span>
<span class="sd">        `granularity`: The granularity to use for weight quantization.  Must be PerGroup or PerAxis(0).</span>
<span class="sd">        `mapping_type`: The type of mapping to use for the weight quantization.</span>
<span class="sd">            Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC.</span>
<span class="sd">        `scale_dtype`: The dtype to use for the weight scale.</span>
<span class="sd">        `layout`: The layout to use for the packed weight tensor:</span>
<span class="sd">            - QDQLayout: this layout is designed for export to ExecuTorch.this layout represents the quantization with Q/DQ quant primitives,</span>
<span class="sd">                and is intended for export applications like ExecuTorch.</span>
<span class="sd">        `intx_packing_format`: The format to use for the packed weight tensor (version 2 only).</span>
<span class="sd">        `intx_choose_qparams_algorithm`: The algorithm to use for choosing the quantization parameters.</span>
<span class="sd">        `version`: version of the config to use, only subset of above args are valid based on version, see note for more details.</span>

<span class="sd">        Note:</span>

<span class="sd">        Current state for IntxWeightOnlyConfig is that it supports both v1 (legacy) and v2.</span>

<span class="sd">        * `intx_packing_format` is used for version 2.</span>
<span class="sd">        * `layout` is only used for version 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="n">Granularity</span> <span class="o">=</span> <span class="n">PerAxis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">QDQLayout</span><span class="p">()</span>
    <span class="n">intx_packing_format</span><span class="p">:</span> <span class="n">IntxPackingFormat</span> <span class="o">=</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span>
    <span class="n">intx_choose_qparams_algorithm</span><span class="p">:</span> <span class="n">IntxChooseQParamsAlgorithm</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span>
    <span class="p">)</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.IntxWeightOnlyConfig&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;int</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;weight_dtype must be torch.intx, where 1 &lt;= x &lt;= 8, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="p">(</span><span class="n">PerAxis</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">)),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;granularity must be PerAxis or PerGroup, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
            <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
        <span class="p">],</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;mapping_type must be MappingType.ASYMMETRIC, MappingType.SYMMETRIC, or MappingType.SYMMETRIC_NO_CLIPPING_ERR, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_intx_weight_only_quantize_tensor</span><span class="p">(</span>
    <span class="n">weight</span><span class="p">,</span>
    <span class="n">config</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">weight_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span>
    <span class="n">granularity</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">granularity</span>
    <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">mapping_type</span>
    <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_dtype</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layout</span>
    <span class="n">intx_packing_format</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_packing_format</span>
    <span class="n">intx_choose_qparams_algorithm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_choose_qparams_algorithm</span>

    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">elif</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="c1"># conv2d: N, C_in, H, W</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;IntxWeightOnlyConfig only works for 2-d and 4-d Tensors, got: </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerGroup</span><span class="p">):</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">granularity</span><span class="o">.</span><span class="n">group_size</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerAxis</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">granularity</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;axis must be 0 with PerAxis, but got </span><span class="si">{</span><span class="n">granularity</span><span class="o">.</span><span class="n">axis</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">input_dim</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;granularity must be PerGroup or PerAxis, got </span><span class="si">{</span><span class="n">granularity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># conv2d: N, C_in, H, W</span>
        <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_packing_format</span> <span class="o">==</span> <span class="n">IntxPackingFormat</span><span class="o">.</span><span class="n">UNPACKED_TO_INT8</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">custom_zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
                <span class="n">custom_zero_point</span> <span class="o">=</span> <span class="n">custom_zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
            <span class="n">new_weight</span> <span class="o">=</span> <span class="n">IntxUnpackedToInt8Tensor</span><span class="o">.</span><span class="n">from_hp</span><span class="p">(</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">block_size</span><span class="p">,</span>
                <span class="n">weight_dtype</span><span class="p">,</span>
                <span class="n">mapping_type</span><span class="o">=</span><span class="n">mapping_type</span><span class="p">,</span>
                <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
                <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
                <span class="n">intx_choose_qparams_algorithm</span><span class="o">=</span><span class="n">intx_choose_qparams_algorithm</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_dtype</span> <span class="o">!=</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="n">_adjust_scale_dtype_in_intx_unpacked_tensor</span><span class="p">(</span>
                    <span class="n">new_weight</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">scale_dtype</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="n">new_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported packing format: </span><span class="si">{</span><span class="n">intx_packing_format</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Version 1</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">intx_choose_qparams_algorithm</span> <span class="o">==</span> <span class="n">IntxChooseQParamsAlgorithm</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;version 1 only supports affine algorithm&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Config Deprecation: version 1 of IntxWeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2967 for more details&quot;</span>
    <span class="p">)</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">weight_dtype</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
        <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">=</span><span class="n">mapping_type</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="o">=</span><span class="p">(</span><span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">),</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">weight</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">IntxWeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_intx_weight_only_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">IntxWeightOnlyConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">custom_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;applying intx weight only quant requires module to have weight attribute&quot;</span>
        <span class="o">+</span> <span class="s2">&quot; but </span><span class="si">{module}</span><span class="s2"> does not have one&quot;</span>
    <span class="p">)</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">_intx_weight_only_quantize_tensor</span><span class="p">(</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">custom_scale</span><span class="o">=</span><span class="n">custom_scale</span><span class="p">,</span>
        <span class="n">custom_zero_point</span><span class="o">=</span><span class="n">custom_zero_point</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_embedding_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">module</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FqnToConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration class for applying different quantization configs to modules or parameters based on their fully qualified names (FQNs).</span>

<span class="sd">    Args:</span>
<span class="sd">        `fqn_to_config`: typing.OrderedDict[str, Optional[AOBaseConfig]]: an</span>
<span class="sd">         ordered dictionary from</span>
<span class="sd">             (1). fully qualified name (fqn) of module or parameter</span>
<span class="sd">             (2). regex of fully qualified name (in python `re` module regex format), should</span>
<span class="sd">                  start with prefix &quot;re:&quot; or</span>
<span class="sd">             (3). &quot;_default&quot;</span>
<span class="sd">         to the config that we want to apply to the module/param or None</span>

<span class="sd">         Config key ordered by precedence:</span>
<span class="sd">           * fully qualified parameter name, e.g. `language.layers.0.q_proj.weight`</span>
<span class="sd">           * fully qualified module name, e.g. `language.layers.0.q_proj`</span>
<span class="sd">           * regex for parameter names, must start with `re:`, e.g. `re:language\.layers\..+\.q_proj.weight`.</span>
<span class="sd">             The first regex that matches will be applied.</span>
<span class="sd">           * regex for module names, must start with `re:`, e.g. `re:language\.layers\..+\.q_proj`,</span>
<span class="sd">             whichever regex fully matches the module fqn first will be applied</span>
<span class="sd">             (order of keys for dictionary are kept consistent since we are using OrderedDict)</span>
<span class="sd">           * &quot;_default&quot;, fallback if no match for all previous keys</span>
<span class="sd">             (Note, when using `_default`, the config is applied to all modules, to apply</span>
<span class="sd">              it to only a subset of modules, e.g. with some types, it&#39;s better to filter</span>
<span class="sd">              the modules that we don&#39;t want to quantize before hand and configure them to</span>
<span class="sd">              None, e.g. `{&quot;re:.+norm.+&quot;: None, &quot;_default&quot;: linear_config}`) &quot;_default&quot; is not supported when filter_fn is not specified.</span>
<span class="sd">        `module_fqn_to_config`: typing.OrderedDict[str, Optional[AOBaseConfig]]: To maintain BC with ModuleFqnToConfig, to be deprecated later</span>
<span class="sd">        `version`: int: Version of config to use.</span>

<span class="sd">    Note:</span>
<span class="sd">        - The order of patterns in the OrderedDict may matter as only the first matching pattern is applied</span>
<span class="sd">        - &quot;_default&quot; is ignored for parameter replacement.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fqn_to_config</span><span class="p">:</span> <span class="n">OrderedDictType</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AOBaseConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">OrderedDict</span>
    <span class="p">)</span>
    <span class="c1"># to maintain BC, we keep the previous module_fqn_to_config field</span>
    <span class="n">module_fqn_to_config</span><span class="p">:</span> <span class="n">OrderedDictType</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AOBaseConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">OrderedDict</span>
    <span class="p">)</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.FqnToConfig&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_fqn_to_config</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`fqn_to_config` and `module_fqn_to_config` are both specified and are not equal!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># This code handles BC compatibility with `ModuleFqnToConfig`. It ensures that `self.module_fqn_to_config` and `self.fqn_to_config` share the same object.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_fqn_to_config</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_fqn_to_config</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_fqn_to_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span>

        <span class="c1"># TODO we plan to deprecate `_default later, so raise a warning if we find it passed in`</span>
        <span class="k">if</span> <span class="s2">&quot;_default&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Config Deprecation: _default is deprecated and will no longer be supported in a future release. Please see https://github.com/pytorch/ao/issues/3229 for more details.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="s2">&quot;FqnToConfig({&quot;</span><span class="p">,</span>
                <span class="o">*</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;  &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39;:</span><span class="se">\n</span><span class="s2">    </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">,&quot;</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">),</span>
                <span class="s2">&quot;})&quot;</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">)</span>


<span class="c1"># maintain BC</span>
<span class="n">ModuleFqnToConfig</span> <span class="o">=</span> <span class="n">FqnToConfig</span>

<span class="c1"># for now, we need to keep track of what configs support custom param quantization.</span>
<span class="c1"># Once we&#39;ve updated all the transform functions to take in a custom_param kwarg, we can delete this object and the subsequent check</span>
<span class="c1"># TODO see https://github.com/pytorch/ao/issues/3252 for more details</span>
<span class="n">CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">Float8DynamicActivationFloat8WeightConfig</span><span class="p">,</span>
    <span class="n">Float8WeightOnlyConfig</span><span class="p">,</span>
    <span class="n">Int8WeightOnlyConfig</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_fqn_to_config_handler</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">FqnToConfig</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function expects a module that either is specified in FqnToConfig or has a parameter that is specified in FqnToConfig.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (torch.nn.Module): The module to be processed.</span>
<span class="sd">        fqn (str): The fully qualified name of the module containing the parameters.</span>
<span class="sd">        config (FqnToConfig): Configuration object containing regex patterns / fqn mapped</span>
<span class="sd">            to quantization configurations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The modified module with quantized parameters.</span>

<span class="sd">    Raises:</span>
<span class="sd">        NotImplementedError: If the quantization configuration is not yet supported for parameter quantization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parameter_config_found</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">top_level_params</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">parameter_name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())):</span>
        <span class="k">if</span> <span class="n">parameter_name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">parameter_fqn</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">parameter_name</span>
            <span class="p">)</span>
            <span class="n">top_level_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">parameter_fqn</span><span class="p">))</span>

    <span class="c1"># First we see if any parameter fqn matches with FqnToConfig, if so, we apply the appropriate transform</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">parameter_fqn</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">top_level_params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">parameter_fqn</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
            <span class="n">parameter_config_found</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">[</span><span class="n">parameter_fqn</span><span class="p">]</span>
            <span class="c1"># if None, remove from subsequent regex check</span>
            <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">top_level_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="n">CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS</span><span class="p">:</span>
                    <span class="c1"># may be more than one param specified, so don&#39;t return prematurely</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="si">}</span><span class="s2"> does not yet support parameter quantization! Please see https://github.com/pytorch/ao/issues/3252 for more details&quot;</span>
                    <span class="p">)</span>

    <span class="c1"># then we see if we match module_fqn exactly</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">parameter_config_found</span> <span class="ow">and</span> <span class="n">fqn</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">module</span>

    <span class="c1"># Next try to match parameters on regex patterns</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">parameter_fqn</span> <span class="ow">in</span> <span class="n">top_level_params</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pattern</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;re:&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">re</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">pattern</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">parameter_fqn</span><span class="p">):</span>
                <span class="n">parameter_config_found</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">[</span><span class="n">pattern</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="n">CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS</span><span class="p">:</span>
                        <span class="c1"># may be more than one param specified, so don&#39;t return prematurely</span>
                        <span class="n">module</span> <span class="o">=</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">parameter_name</span><span class="o">=</span><span class="n">parameter_name</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="si">}</span><span class="s2"> does not yet support parameter quantization! Please see https://github.com/pytorch/ao/issues/3252 for more details&quot;</span>
                        <span class="p">)</span>

    <span class="c1"># try to match regex on module fqn</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">parameter_config_found</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
            <span class="c1"># we&#39;ll apply the config for first fully matched pattern</span>
            <span class="k">if</span> <span class="n">pattern</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;re:&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">re</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">pattern</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">fqn</span><span class="p">):</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">[</span><span class="n">pattern</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
                    <span class="k">return</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

    <span class="c1"># If no module_fqn or parameter_fqn matches, then we apply _default</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">parameter_config_found</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_default&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">handler</span> <span class="o">=</span> <span class="n">_QUANTIZE_CONFIG_HANDLER</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
            <span class="c1"># safe to return here as at most only one module will match</span>
            <span class="k">return</span> <span class="n">handler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span><span class="w"> </span><span class="nf">fqn_matches_fqn_config</span><span class="p">(</span>
    <span class="n">fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">FqnToConfig</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if a given fqn matches the exact fqn or regex pattern specified in FqnToConfig.</span>

<span class="sd">    Args:</span>
<span class="sd">        fqn (str): The fully qualified name of the module.</span>
<span class="sd">        config (FqnToConfig): Configuration object containing regex patterns or raw FQNs for quantization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if the fqn is specified in FqnToConfig. False otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">fqn</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">fqn</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;re:&quot;</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Error: Exact match but regex </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> specified.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">maybe_module_or_param_fqn_pattern</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">fqn_to_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">maybe_module_or_param_fqn_pattern</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;re:&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">re</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span>
                <span class="n">maybe_module_or_param_fqn_pattern</span><span class="p">[</span><span class="mi">3</span><span class="p">:],</span> <span class="n">fqn</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_module_param_matches_fqn_config</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">FqnToConfig</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if a given module contains top-level parameters that match the exact fqn or regex pattern specified in FqnToConfig.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): The module to be checked.</span>
<span class="sd">        fqn (str): The fully qualified name of the module.</span>
<span class="sd">        config (FqnToConfig): Configuration object containing regex patterns or raw FQNs for quantization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if the module contains top-level parameters that match the fqn or regex pattern specified in FqnTo</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">parameter_fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">name</span>
            <span class="k">if</span> <span class="n">fqn_matches_fqn_config</span><span class="p">(</span><span class="n">parameter_fqn</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_unwrap_float8_linear</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Float8Linear</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unwrap a torchao Float8Linear by returning a nn.Linear with the same weights and bias.</span>

<span class="sd">    Torchao inference quantization techniques are generally only applicable to nn.Linear</span>
<span class="sd">    layers, so this helper is useful for unwrapping models trained with torchao float8 training,</span>
<span class="sd">    which replaces nn.Linear layers with Float8Linear layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
        <span class="n">new_module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
    <span class="n">new_module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">new_module</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">new_module</span>


<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">_int8_asymm_per_token_quant</span><span class="p">,</span>
        <span class="n">_int8_symm_per_token_reduced_range_quant</span><span class="p">,</span>
        <span class="n">_input_activation_quant_func_fp8</span><span class="p">,</span>
        <span class="n">_int4_symm_cutlass_quant</span><span class="p">,</span>
        <span class="n">_int8_symm_cutlass_quant</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant</span><span class="p">,</span>
        <span class="n">_float8_cutlass_quant_sparse</span><span class="p">,</span>
        <span class="n">Target</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.quantization.quant_api",
       "headline": "torchao.quantization.quant_api",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/torchao/quantization/quant_api.html",
       "articleBody": "Source code for torchao.quantization.quant_api # Copyright (c) Meta Platforms, Inc. and affiliates. # Copyright 2024-2025 Arm Limited and affiliates. # All rights reserved. # This source code is licensed under the license found in the # LICENSE file in the root directory of this source tree. \"\"\" Quantization APIs Generally these APIs can be applied directly to any model with Linear modules to obtain quantized linear ops. The intended usage involves applying torch.compile to the model afterwards both because primitives were designed based on the fusions that come along with it and because that is how we access the intended quantized and mixed GEMM kernels \"\"\" import logging import re import types import warnings from collections import OrderedDict from dataclasses import dataclass, field from functools import partial from typing import Any, Callable, List, Optional, Tuple, Union from typing import OrderedDict as OrderedDictType import torch import torch.nn as nn import torch.nn.utils.parametrize as parametrize import torchao from torchao.core.config import AOBaseConfig from torchao.dtypes import ( AffineQuantizedTensor, CutlassInt4PackedLayout, CutlassSemiSparseLayout, Float8Layout, Int4CPULayout, Int4XPULayout, MarlinSparseLayout, PackedLinearInt8DynamicActivationIntxWeightLayout, PlainLayout, QDQLayout, SemiSparseLayout, TensorCoreTiledLayout, to_affine_quantized_floatx, to_affine_quantized_floatx_static, to_affine_quantized_intx, ) from torchao.dtypes.uintx.packed_linear_int8_dynamic_activation_intx_weight_layout import ( Target, make_packed_linear_int8_dynamic_activation_intx_weight_tensor, ) from torchao.dtypes.utils import Layout from torchao.float8.config import e4m3_dtype, e5m2_dtype from torchao.float8.float8_linear import Float8Linear from torchao.float8.inference import ( Float8MMConfig, FP8Granularity, _check_hardware_support, _granularity_is_a_1_128_w_128_128, _normalize_granularity, ) # for BC, make sure to keep the `noqa: F401` comments to prevent # ruff from removing \"unused imports\" from torchao.prototype.quantization.quant_api import ( Float8StaticActivationFloat8WeightConfig, # noqa: F401 FPXWeightOnlyConfig, # noqa: F401 GemliteUIntXWeightOnlyConfig, # noqa: F401 Int4DynamicActivationInt4WeightConfig, # noqa: F401 Int8DynamicActivationInt4WeightConfig, # noqa: F401 UIntXWeightOnlyConfig, # noqa: F401 ) from torchao.quantization.linear_activation_weight_observed_tensor import ( LinearActivationWeightObservedTensor, ) from torchao.quantization.observer import AffineQuantizedObserverBase from torchao.quantization.quantize_.common import ( KernelPreference, ) from torchao.quantization.quantize_.workflows import ( Float8Tensor, Int4ChooseQParamsAlgorithm, Int4MarlinSparseTensor, Int4PackingFormat, Int4PlainInt32Tensor, Int4PreshuffledTensor, Int4Tensor, Int4TilePackedTo4dTensor, Int8Tensor, IntxChooseQParamsAlgorithm, IntxOpaqueTensor, IntxPackingFormat, IntxUnpackedToInt8Tensor, QuantizeTensorToFloat8Kwargs, QuantizeTensorToInt8Kwargs, ) from torchao.quantization.transform_module import ( _QUANTIZE_CONFIG_HANDLER, register_quantize_module_handler, ) from torchao.quantization.utils import ( _fp8_mm_compat, _linear_extra_repr, _quantization_type, get_block_size, ) from torchao.utils import ( is_MI300, is_sm_at_least_89, is_sm_at_least_90, ) from .autoquant import AutoQuantizableLinearWeight, autoquant from .GPTQ import ( Int4WeightOnlyGPTQQuantizer, ) from .granularity import ( Granularity, PerAxis, PerGroup, PerRow, PerTensor, ) from .linear_activation_quantized_tensor import ( LinearActivationQuantizedTensor, to_linear_activation_quantized, ) from .linear_quant_modules import ( Int4WeightOnlyQuantizer, Int8DynActInt4WeightQuantizer, ) from .qat import ( intx_quantization_aware_training, ) from .quant_primitives import ( _DTYPE_TO_QVALUE_BOUNDS, MappingType, ZeroPointDomain, quantize_affine, ) from .unified import Quantizer, TwoStepQuantizer from .utils import _get_per_token_block_size logger = logging.getLogger(__name__) # TODO: revisit this list? __all__ = [ \"swap_conv2d_1x1_to_linear\", \"Quantizer\", \"TwoStepQuantizer\", \"Int4WeightOnlyGPTQQuantizer\", \"Int4WeightOnlyQuantizer\", \"autoquant\", \"_get_subclass_inserter\", \"quantize_\", \"intx_quantization_aware_training\", \"Int8DynActInt4WeightQuantizer\", \"Float8DynamicActivationFloat8SemiSparseWeightConfig\", \"ModuleFqnToConfig\", ] LAYOUT_TO_ZERO_POINT_DOMAIN = { TensorCoreTiledLayout: [ZeroPointDomain.FLOAT], MarlinSparseLayout: [ZeroPointDomain.INT], Int4CPULayout: [ZeroPointDomain.FLOAT], Int4XPULayout: [ZeroPointDomain.FLOAT, ZeroPointDomain.INT], } LAYOUT_TO_PRESERVE_ZEROS = { TensorCoreTiledLayout: False, MarlinSparseLayout: True, Int4CPULayout: False, Int4XPULayout: False, } def _replace_with_custom_fn_if_matches_filter( model, replacement_fn, filter_fn, cur_fqn=\"\", device=None, extra_args: Optional[Tuple[Any, ...]] = (), ) -\u003e None: \"\"\" Recursively replaces each child module in `model` with the result of `replacement_fn(child)` if `filter_fn(child)` returns `True`. Args: model (torch.nn.Module): The model containing modules to be replaced. replacement_fn (Callable[[torch.nn.Module], torch.nn.Module]): The function to replace matching modules. filter_fn (Callable[[torch.nn.Module], bool]): The filter function to determine which modules to replace. cur_fqn (str, optional): The current fully qualified name of the module being processed. Defaults to \"\". device (device, optional): Device to move the model to before applying `filter_fn`. Defaults to None. extra_args (Tuple[Any, ...], optional): optional extra args to pass to `replacement_fn`. Returns: None \"\"\" if filter_fn(model, cur_fqn[:-1]): if device is not None: model.to(device=device) # move to device before quantization model = replacement_fn(model, *extra_args) return model else: named_children_list = list(model.named_children()) for name, child in named_children_list: new_child = _replace_with_custom_fn_if_matches_filter( child, replacement_fn, filter_fn, f\"{cur_fqn}{name}.\", device, extra_args, ) if new_child is not child and new_child is not None: setattr(model, name, new_child) if device is not None: model.to(device=device) # move parent module to device return model def _is_linear(mod, *args): # avoid circular dependencies from torchao.quantization.qat.affine_fake_quantized_tensor import ( _AffineFakeQuantizedTensor, ) # adding weight tensor subclass isinstance check to make sure the weight is only quantized once # when it is shared by multiple linear modules return ( isinstance(mod, torch.nn.Linear) and hasattr(mod, \"weight\") and not isinstance(mod.weight, AutoQuantizableLinearWeight) and not isinstance(mod.weight, AffineQuantizedTensor) and not isinstance(mod.weight, LinearActivationQuantizedTensor) and not isinstance(mod.weight, _AffineFakeQuantizedTensor) and not isinstance(mod, nn.modules.linear.NonDynamicallyQuantizableLinear) ) def _get_subclass_inserter(cls, enable_parametrization=False, **kwargs): \"\"\" Returns a function which inserts the given subclass into all linear modules in the model. The inserted module will have its weight set to the result of `cls(mod.weight, **kwargs)`. If parametrization is enabled then this will be done using torch.nn.utils.parametrize instead of directly setting the attribute on the module. Args: cls (torch.Tensor): The class to insert as a child module. kwargs (Any): Any additional arguments for the constructor. \"\"\" constructor = kwargs.pop(\"constructor\", \"subclass_constructor\") from_float = kwargs.pop(\"method\", \"from_float\") def insert_subclass(lin): if enable_parametrization: lin.weight = torch.nn.Parameter( cls.from_float(lin.weight, **kwargs), requires_grad=False ) _, args = lin.weight.__tensor_flatten__() parametrize.register_parametrization( lin, \"weight\", getattr(cls, constructor)(*args) ) else: lin.weight = torch.nn.Parameter( # cls.from_float(...) getattr(cls, from_float)(lin.weight, **kwargs), requires_grad=False, ) return lin return insert_subclass def swap_conv2d_1x1_to_linear(model, filter_fn=None): \"\"\" Changes all conv2d 1x1 modules to equivalent linear modules so that they can then be quantized. \"\"\" class PermuteSandwich(torch.nn.Module): def __init__(self, mod): super().__init__() self.mod = mod def forward(self, *args): return self.mod(args[0].permute(0, 2, 3, 1)).permute(-0, 3, 1, 2) def replace_conv2d_1x1(conv): assert conv.kernel_size == (1, 1) lin = torch.nn.Linear( conv.in_channels, conv.out_channels, bias=(conv.bias is None) ) lin.weight = torch.nn.Parameter(conv.weight.squeeze(-1, -2)) lin.bias = conv.bias return PermuteSandwich(lin) if filter_fn is None: filter_fn = lambda mod, *args: isinstance( mod, torch.nn.Conv2d ) and mod.kernel_size == (1, 1) _replace_with_custom_fn_if_matches_filter( model, replace_conv2d_1x1, filter_fn=filter_fn ) def insert_observers_( model: nn.Module, input_observer: Optional[AffineQuantizedObserverBase], weight_observer: Optional[AffineQuantizedObserverBase], *, filter_fn: Optional[Callable[[torch.nn.Module, str], bool]] = None, ): \"\"\" Converts the weight of a linear module to a LinearActivationWeightObservedTensor. This function wraps the weight of the given linear module with a LinearActivationWeightObservedTensor, which enables observation of both input and weight tensors during forward passes. The wrapped weight is then re-wrapped as a nn.Parameter to maintain compatibility with PyTorch\u0027s module system. Example:: ``` import torch import torch.nn as nn from torchao.quantization import PerTensor from torchao.quantization.linear_observer_tensor import insert_observers_ from torchao.quantization.observer import ( AffineQuantizedMinMaxObserver, MappingType ) # Create observers input_observer = AffineQuantizedMinMaxObserver( MappingType.SYMMETRIC, torch.float8_e4m3fn, granularity_type=PerTensor(), eps=torch.finfo(torch.float32).eps, scale_dtype=torch.float, zero_point_dtype=torch.int, zero_point_domain=ZeroPointDomain.NONE, ) # Create a linear module linear_module = nn.Linear(10, 20) # Convert the linear module\u0027s weight to an observed tensor insert_observers_(linear_module, input_observer, weight_observer=None) # The linear_module can now be used as usual, with observers calculating statistics output = linear_module(torch.randn(10, 10)) # Get the scale and zero point of the input observer scale, zero_point = linear_module.weight.input_observer.calculate_qparams() ``` Args: model (nn.Module): The nn.Module to convert. input_observer (Optional[AffineQuantizedObserverBase]): Observer for input tensor. weight_observer (Optional[AffineQuantizedObserverBase]): Observer for weight tensor. filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): Filter function to select which modules to convert. If not provided, all linear modules will be converted. This function should take a module and its fully qualified name. Returns: nn.Linear: The modified linear module with its weight wrapped in a LinearActivationWeightObservedTensor. \"\"\" def convert_to_linear_observer(linear_module: nn.Linear): # Wrap the weight with LinearActivationWeightObservedTensor and then with nn.Parameter linear_module.weight = nn.Parameter( LinearActivationWeightObservedTensor.from_float( linear_module.weight, input_observer=input_observer, weight_observer=weight_observer, ), requires_grad=linear_module.weight.requires_grad, ) return linear_module _replace_with_custom_fn_if_matches_filter( model, convert_to_linear_observer, _is_linear if filter_fn is None else filter_fn, ) def _embedding_extra_repr(self): return f\"num_embeddings={self.weight.shape[0]}, embedding_dim={self.weight.shape[1]}, weight={_quantization_type(self.weight)}\" def _module_extra_repr(self, original_extra_repr, parameter_name): module_torchao_extra_repr = [] original_extra_repr_str = original_extra_repr() if len(original_extra_repr_str) \u003e 0: module_torchao_extra_repr.append(original_extra_repr_str) module_torchao_extra_repr.append( f\"{parameter_name}={_quantization_type(getattr(self, parameter_name))}\" ) return \", \".join(module_torchao_extra_repr) def _get_linear_subclass_inserter( constructor, *, allow_requires_grad=False, propagate_bias=False, **kwargs ): \"\"\"Helper function to apply the constructor that quantizes the weight Tensor (with additional kwargs) to the weight of linear module \"\"\" def insert_subclass(lin): requires_grad = allow_requires_grad and lin.weight.requires_grad if propagate_bias == True: kwargs[\"bias\"] = lin.bias lin.weight = torch.nn.Parameter( constructor(lin.weight, **kwargs), requires_grad=requires_grad ) lin.extra_repr = types.MethodType(_linear_extra_repr, lin) return lin return insert_subclass [docs]def quantize_( model: torch.nn.Module, config: AOBaseConfig, filter_fn: Optional[Callable[[torch.nn.Module, str], bool]] = _is_linear, device: Optional[torch.types.Device] = None, ): \"\"\"Convert the weight of linear modules in the model with `config`, model is modified inplace Args: model (torch.nn.Module): input model config (AOBaseConfig): a workflow configuration object. filter_fn (Optional[Callable[[torch.nn.Module, str], bool]]): function that takes a nn.Module instance and fully qualified name of the module, returns True if we want to run `config` on the weight of the module device (device, optional): Device to move module to before applying `filter_fn`. This can be set to `\"cuda\"` to speed up quantization. The final model will be on the specified `device`. Defaults to None (do not change device). Example:: import torch import torch.nn as nn from torchao import quantize_ # quantize with some predefined `config` method that corresponds to # optimized execution paths or kernels (e.g. int4 tinygemm kernel) # also customizable with arguments # currently options are # Int8DynamicActivationInt8WeightConfig (optimized with int8 mm op and torch.compile) # Int4WeightOnlyConfig (optimized with int4 tinygemm kernel and torch.compile) # Int8WeightOnlyConfig (optimized with int8 mm op and torch.compile from torchao.quantization.quant_api import Int4WeightOnlyConfig m = nn.Sequential(nn.Linear(32, 1024), nn.Linear(1024, 32)) quantize_(m, Int4WeightOnlyConfig(group_size=32, version=1)) \"\"\" torch._C._log_api_usage_once(\"torchao.quantization.quantize_\") if isinstance(config, FqnToConfig): if filter_fn is not None: raise ValueError( \"Custom filter_fn and FqnToConfig were both specified. Only filter_fn=None is supported when FqnToConfig is specified.\" ) named_modules = dict(model.named_modules()) for module_fqn, module in named_modules.items(): if ( fqn_matches_fqn_config(module_fqn, config) or _module_param_matches_fqn_config(module, module_fqn, config) or (\"_default\" in config.fqn_to_config and _is_linear(module)) ): replacement = _fqn_to_config_handler(module, module_fqn, config) if device is not None: replacement = replacement.to(device=device) # handle module swap if replacement is not module and module_fqn != \"\": child_name = module_fqn.split(\".\")[-1] parent_fqn = module_fqn.removesuffix(child_name).removesuffix(\".\") parent_module = named_modules[parent_fqn] setattr(parent_module, child_name, replacement) elif isinstance(config, AOBaseConfig): filter_fn = _is_linear if filter_fn is None else filter_fn handler = _QUANTIZE_CONFIG_HANDLER[type(config)] # for each linear in the model, apply the transform if filtering passes _replace_with_custom_fn_if_matches_filter( model, handler, filter_fn, device=device, extra_args=(config,), ) else: raise AssertionError( \"\"\"Passing a generic Callable to `quantize_` is no longer recommended and will be deprecated at a later release. Please see https://github.com/pytorch/ao/issues/1690 for instructions on how to pass in workflow configuration instead.\"\"\" ) def _int8_asymm_per_token_quant(x: torch.Tensor) -\u003e torch.Tensor: \"\"\"This is defined here instead of local function to support serialization\"\"\" mapping_type = MappingType.ASYMMETRIC target_dtype = torch.int8 scale_dtype = torch.float32 eps = torch.finfo(torch.float32).eps zero_point_dtype = torch.int8 return to_affine_quantized_intx( x, mapping_type, _get_per_token_block_size(x), target_dtype, eps=eps, scale_dtype=scale_dtype, zero_point_dtype=zero_point_dtype, ) def _uint8_asymm_per_token_quant(x: torch.Tensor) -\u003e torch.Tensor: mapping_type = MappingType.ASYMMETRIC target_dtype = torch.uint8 scale_dtype = torch.float32 eps = torch.finfo(torch.float32).eps zero_point_dtype = torch.int32 quant_min = 0 quant_max = 255 out = to_affine_quantized_intx( x, mapping_type, _get_per_token_block_size(x), target_dtype, quant_min=quant_min, quant_max=quant_max, eps=eps, scale_dtype=scale_dtype, zero_point_dtype=zero_point_dtype, ) return out def _int8_symm_per_token_quant(x: torch.Tensor) -\u003e torch.Tensor: mapping_type = MappingType.SYMMETRIC target_dtype = torch.int8 eps = 1e-5 quant_min = -127 quant_max = 127 return to_affine_quantized_intx( x, mapping_type, _get_per_token_block_size(x), target_dtype, eps=eps, quant_min=quant_min, quant_max=quant_max, scale_dtype=torch.float32, ) @dataclass class Int8DynamicActivationIntxWeightConfig(AOBaseConfig): \"\"\" Configuration for dynamically quantizing activations to torch.int8 and weights to torch.intx, with 1 \u003c= x \u003c= 8. More specifically, activations are dynamically quantized to 8-bits at a per-token granularity with scales/zeros. Weights are quantized with scales/zeros in a groupwise or channelwise manner using the number of bits specified by weight_dtype. This layout is identical to Int8DynamicActivationInt4WeightConfig when weight_dtype is torch.int4 and other args are the same. However, this layout is more general and supports other weight dtypes. args: `weight_dtype`: The dtype to use for weight quantization. Must be torch.intx, where 1 \u003c= x \u003c= 8. ` weight_granularity`: The granularity to use for weight quantization. Must be PerGroup or PerAxis(axis=0). `weight_mapping_type`: The type of mapping to use for the weight quantization. Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC. MappingType.SYMMETRIC requires ZeroPointDomain.NONE `weight_scale_dtype`: The dtype to use for the weight scale. `act_mapping_type`: The type of mapping to use for the activation quantization. Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC. `layout`: The layout to use for the packed weight tensor: - PackedLinearInt8DynamicActivationIntxWeightLayout: this layout is optimized for CPU performance. - QDQLayout: this layout represents the quantization with Q/DQ quant primitives, and is intended for export applications like ExecuTorch. `intx_packing_format`: The format to use for the packed weight tensor (version 2 only). - unpacked_to_int8: this format is the default and is intended for export applications like ExecuTorch. - opaque_torchao_auto: this format is optimized for CPU performance. `intx_choose_qparams_algorithm`: The algorithm to use for choosing the quantization parameters. `version`: version of the config to use, only subset of above args are valid based on version, see note for more details. Note: Current state for Int8DynamicActivationIntxWeightConfig is that it supports both v1 (legacy) and v2. * `intx_packing_format` is used for version 2. * `layout` is only used for version 1. \"\"\" weight_dtype: torch.dtype = torch.int8 weight_granularity: Granularity = PerGroup(32) weight_mapping_type: MappingType = MappingType.SYMMETRIC # TODO: add weight_scale_dtype to Int8DynamicActivationInt4WeightConfig weight_scale_dtype: Optional[torch.dtype] = None act_mapping_type: MappingType = MappingType.ASYMMETRIC layout: Layout = QDQLayout() intx_packing_format: IntxPackingFormat = IntxPackingFormat.UNPACKED_TO_INT8 intx_choose_qparams_algorithm: IntxChooseQParamsAlgorithm = ( IntxChooseQParamsAlgorithm.AFFINE ) version: int = 2 def __post_init__(self): torch._C._log_api_usage_once( \"torchao.quantization.Int8DynamicActivationIntxWeightConfig\" ) assert self.weight_dtype in [getattr(torch, f\"int{b}\") for b in range(1, 9)], ( f\"weight_dtype must be torch.intx, where 1 \u003c= x \u003c= 8, but got {self.weight_dtype}\" ) assert isinstance(self.weight_granularity, (PerAxis, PerGroup)), ( f\"weight_granularity must be PerAxis or PerGroup, but got {self.weight_granularity}\" ) if isinstance(self.weight_granularity, PerAxis): assert self.weight_granularity.axis == 0, ( f\"axis must be 0, but got {self.weight_granularity.axis}\" ) assert self.weight_mapping_type in [ MappingType.ASYMMETRIC, MappingType.SYMMETRIC, MappingType.SYMMETRIC_NO_CLIPPING_ERR, ], ( f\"weight_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC or MappingType.SYMMETRIC_NO_CLIPPING_ERR, but got {self.weight_mapping_type}\" ) assert self.act_mapping_type in [ MappingType.ASYMMETRIC, MappingType.SYMMETRIC, ], ( f\"act_mapping_type must be MappingType.ASYMMETRIC or MappingType.SYMMETRIC, but got {self.act_mapping_type}\" ) assert isinstance( self.layout, (PackedLinearInt8DynamicActivationIntxWeightLayout, QDQLayout) ), ( f\"layout must be PackedLinearInt8DynamicActivationIntxWeightLayout or QDQLayout, but got {self.layout}\" ) if isinstance(self.layout, PackedLinearInt8DynamicActivationIntxWeightLayout): if self.layout.target in [Target.AUTO, Target.KLEIDIAI, Target.ATEN]: if (self.weight_scale_dtype) is None or ( self.weight_scale_dtype != torch.bfloat16 ): logging.warning( f\"When using layout PackedLinearInt8DynamicActivationIntxWeightLayout with target {self.layout.target}, \" f\"the weight scale may be cast to bfloat16 by the kernel, but weight_scale_dtype is set to {self.weight_scale_dtype}. \" \"Explicitly set weight_scale_dtype to torch.bfloat16 to suppress this warning. \" \"If you need weight_scale_dtype = torch.float32, use target=Target.UNIVERSAL instead.\" ) def _int8_dynamic_activation_intx_weight_quantize_tensor( weight, bias, config, *, custom_scale: Optional[torch.Tensor] = None, custom_zero_point: Optional[torch.Tensor] = None, ): weight_dtype = config.weight_dtype weight_granularity = config.weight_granularity weight_mapping_type = config.weight_mapping_type weight_scale_dtype = config.weight_scale_dtype act_mapping_type = config.act_mapping_type layout = config.layout intx_packing_format = config.intx_packing_format intx_choose_qparams_algorithm = config.intx_choose_qparams_algorithm assert weight.dim() == 2, ( f\"Int8DynamicActivationIntxWeightConfig only works for 2-d Tensor, got: {weight.dim()}\" ) if isinstance(weight_granularity, PerGroup): group_size = weight_granularity.group_size elif isinstance(weight_granularity, PerAxis): assert weight_granularity.axis == 0, ( f\"axis must be 0 with PerAxis, but got {weight_granularity.axis}\" ) group_size = weight.shape[-1] else: raise ValueError( f\"weight_granularity must be PerGroup or PerAxis, got {weight_granularity}\" ) block_size = (1, group_size) if config.version == 2: assert act_mapping_type == MappingType.ASYMMETRIC opaque_formats = [ IntxPackingFormat.OPAQUE_ATEN_KLEIDIAI, IntxPackingFormat.OPAQUE_TORCHAO_AUTO, IntxPackingFormat.OPAQUE_TORCHAO_KLEIDIAI, IntxPackingFormat.OPAQUE_TORCHAO_LOWBIT, ] assert ( intx_packing_format == IntxPackingFormat.UNPACKED_TO_INT8 or intx_packing_format in opaque_formats ), f\"Unsupported packing format: {intx_packing_format}\" if custom_zero_point is not None and custom_zero_point.dtype == torch.int32: custom_zero_point = custom_zero_point.to(torch.int8) new_weight = IntxUnpackedToInt8Tensor.from_hp( weight, block_size, weight_dtype, mapping_type=weight_mapping_type, activation_quantization=\"int8_asym_per_token\", intx_choose_qparams_algorithm=intx_choose_qparams_algorithm, custom_scale=custom_scale, custom_zero_point=custom_zero_point, ) if weight_scale_dtype is not None and weight_scale_dtype != weight.dtype: _adjust_scale_dtype_in_intx_unpacked_tensor( new_weight, weight, weight_scale_dtype ) new_bias = bias # Create packed tensor if intx_packing_format in opaque_formats: new_weight = IntxOpaqueTensor.from_intx_unpacked_to_int8_tensor( new_weight, bias=new_bias, intx_packing_format=intx_packing_format ) new_bias = None # bias is packed with weights return new_weight, new_bias # Version 1 assert config.version == 1 assert intx_choose_qparams_algorithm == IntxChooseQParamsAlgorithm.AFFINE, ( \"IntxChooseQParamsAlgorithm.AFFINE is the only supported algorithm for version 1\" ) warnings.warn( \"Config Deprecation: version 1 of Int8DynamicActivationIntxWeightConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2967 for more details\" ) quant_min, quant_max = _DTYPE_TO_QVALUE_BOUNDS[weight_dtype] # We quantize with QDQLayout, and then construct the packed weight tensor later # set preserve_zero based on weight mapping type preserve_zero = weight_mapping_type in [ MappingType.SYMMETRIC, MappingType.SYMMETRIC_NO_CLIPPING_ERR, ] weight = to_affine_quantized_intx( input_float=weight, mapping_type=weight_mapping_type, block_size=(1, group_size), target_dtype=torch.int8, quant_min=quant_min, quant_max=quant_max, scale_dtype=weight_scale_dtype, zero_point_dtype=torch.int8, preserve_zero=preserve_zero, zero_point_domain=ZeroPointDomain.INT, _layout=QDQLayout(), ) if isinstance(layout, QDQLayout): # TODO: _int8_asymm_per_token_quant uses scale_dtype=torch.float64, zero_point_dtype=torch.int64, # which is not great for export with QDQLayout. It is also not consistent with _int8_symm_per_token_quant, # which uses scale_dtype=torch.float32, zero_point_dtype=torch.int32. # Maybe introduce new fp32/int32 versions of _int8_asymm_per_token_quant? if act_mapping_type == MappingType.ASYMMETRIC: activation_quant_func = _int8_asymm_per_token_quant elif act_mapping_type == MappingType.SYMMETRIC: activation_quant_func = _int8_symm_per_token_quant else: assert False, f\"Unsupported activation mapping type: {act_mapping_type}\" weight = to_linear_activation_quantized(weight, activation_quant_func) elif isinstance(layout, PackedLinearInt8DynamicActivationIntxWeightLayout): # PackedLinearInt8DynamicActivationIntxWeightLayout has dynamic activation quantization # fused with the kernel and it should not be applied separately assert act_mapping_type == MappingType.ASYMMETRIC, ( \"PackedLinearInt8DynamicActivationIntxWeightLayout requires act_mapping_type=MappingType.ASYMMETRIC\" ) data, scale, zero_point = weight.tensor_impl.get_plain() groups_per_row = weight.shape[-1] // group_size scale = scale.reshape(-1, groups_per_row) assert zero_point is not None zero_point = zero_point.reshape(-1, groups_per_row) has_weight_zeros = (zero_point != 0).any() weight = make_packed_linear_int8_dynamic_activation_intx_weight_tensor( data, scale, zero_point if has_weight_zeros else None, bias, weight_dtype, layout.target, validate_inputs=False, ) # bias is packed with weights if present bias = None return weight, bias @register_quantize_module_handler(Int8DynamicActivationIntxWeightConfig) def _int8_dynamic_activation_intx_weight_transform( module: torch.nn.Module, config: Int8DynamicActivationIntxWeightConfig, *, custom_scale: Optional[torch.Tensor] = None, custom_zero_point: Optional[torch.Tensor] = None, ) -\u003e torch.nn.Module: new_weight, new_bias = _int8_dynamic_activation_intx_weight_quantize_tensor( module.weight, module.bias, config, custom_scale=custom_scale, custom_zero_point=custom_zero_point, ) module.weight = torch.nn.Parameter(new_weight, requires_grad=False) if new_bias is None: module.bias = None if isinstance(module, nn.Linear): module.extra_repr = types.MethodType(_linear_extra_repr, module) return module [docs]@dataclass class Int4WeightOnlyConfig(AOBaseConfig): \"\"\" Configuration for int4 weight only quantization, only groupwise quantization is supported right now, and we support version 1 and version 2, that are implemented differently although with same support. In version 2, different target are mainly distinguished by `packing_format` arg, and in version 1, mainly by `layout`. Args: `group_size`: parameter for quantization, controls the granularity of quantization, smaller size is more fine grained, choices are [256, 128, 64, 32], used in both version 1 and 2 `int4_packing_format`: the packing format for int4 tensor, used in version 2 only `int4_choose_qparams_algorithm`: variants of choose qparams algorithm to use for int4, currently support TINYGEMM (\"tinygemm\") and HQQ (\"hqq\"), used in version 2 only `layout`: layout type for quantized tensor, default is `TensorCoreTiledLayout(inner_k_tiles=8)`, used in version 1 only `use_hqq`: whether to use hqq or default quantization mode, default is False, used in version 1 only `zero_point_domain`: data type of zeros points, choices are [ZeroPointDomain.FLOAT, ZeroPointDomain.INT, ZeroPointDomain.NONE], used in version 1 only `set_inductor_config`: if True, adjusts `torchinductor` settings to recommended values. used in both version 1 and 2 `preserve_zero`: whether to preserve zero, default is None. Will be set to True if zero_point_domain is ZeroPointDomain.INT, used in version 1 only `version`: version of the config to use, only subset of above args are valid for version 1, and subset of above args are valid for version 2, default is 2, see note for more details Note: Current state for Int4WeightOnlyConfig is that it supports both v1 (legacy) and v2 For v2 (version = 2), only `group_size`, `int4_packing_format`, `int4_choose_qparams_algorithm` and `set_inductor_config` are valid, all other args will be ignored For v1 (version = 1), only `group_size`, `layout`, `use_hqq`, `zero_point_domain`, `preserve_zero` and `set_inductor_config` are valid, we plan to deprecate v1 in torchao 0.15 to make this config less confusing \"\"\" group_size: int = 128 layout: Optional[TensorCoreTiledLayout] = TensorCoreTiledLayout(inner_k_tiles=8) use_hqq: bool = False zero_point_domain: Optional[ZeroPointDomain] = ZeroPointDomain.NONE set_inductor_config: bool = True preserve_zero: Optional[bool] = None # only used in version \u003e= 2 int4_packing_format: Int4PackingFormat = Int4PackingFormat.PLAIN int4_choose_qparams_algorithm: Int4ChooseQParamsAlgorithm = ( Int4ChooseQParamsAlgorithm.TINYGEMM ) version: int = 2 def __post_init__(self): torch._C._log_api_usage_once(\"torchao.quantization.Int4WeightOnlyConfig\") def _int4_weight_only_quantize_tensor(weight, config): # TODO(future PR): perhaps move this logic to a different file, to keep the API # file clean of implementation details # for now, make these local variables to allow the rest of the function # to be a direct copy-paste group_size = config.group_size layout = config.layout use_hqq = config.use_hqq int4_choose_qparams_algorithm = config.int4_choose_qparams_algorithm zero_point_domain = config.zero_point_domain int4_packing_format = config.int4_packing_format if weight.shape[-1] % group_size != 0: logger.info( f\"Skipping quantizing weight with int4 weight only quantization because the shape of weight {weight.shape} is not compatible with group_size {group_size}\" ) return weight block_size = tuple([1 for _ in range(weight.ndim - 1)] + [group_size]) if config.version == 2: block_size = list(block_size) if int4_choose_qparams_algorithm == Int4ChooseQParamsAlgorithm.HQQ: assert int4_packing_format == Int4PackingFormat.TILE_PACKED_TO_4D, ( f\"Int4ChooseQParamsAlgorithm.HQQ is not supported by packing format {int4_packing_format}, \" f\"it\u0027s only supported by Int4PackingFormat.TILE_PACKED_TO_4D currently\" ) if int4_packing_format == Int4PackingFormat.PRESHUFFLED: new_weight = Int4PreshuffledTensor.from_hp( weight, block_size, activation_dtype=torch.bfloat16, ) return new_weight elif int4_packing_format == Int4PackingFormat.PLAIN: new_weight = Int4Tensor.from_hp( weight, block_size, ) return new_weight elif int4_packing_format == Int4PackingFormat.PLAIN_INT32: new_weight = Int4PlainInt32Tensor.from_hp( weight, block_size, ) return new_weight elif int4_packing_format == Int4PackingFormat.MARLIN_SPARSE: new_weight = Int4MarlinSparseTensor.from_hp( weight, block_size, ) return new_weight elif int4_packing_format == Int4PackingFormat.TILE_PACKED_TO_4D: new_weight = Int4TilePackedTo4dTensor.from_hp( weight, block_size, int4_choose_qparams_algorithm=int4_choose_qparams_algorithm, ) return new_weight else: raise ValueError(f\"Unsupported int4 packing format: {int4_packing_format}\") assert config.version == 1 warnings.warn( \"Config Deprecation: version 1 of Int4WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2948 for more details\" ) mapping_type = MappingType.ASYMMETRIC target_dtype = torch.int32 quant_min = 0 quant_max = 15 eps = 1e-6 zero_point_dtype = ( weight.dtype if isinstance(layout, Int4CPULayout) else torch.bfloat16 ) # nonlocal zero_point_domain assert type(layout) in LAYOUT_TO_ZERO_POINT_DOMAIN.keys(), ( f\"Only support layout: {LAYOUT_TO_ZERO_POINT_DOMAIN.keys()}\" ) if zero_point_domain == ZeroPointDomain.NONE: # the first value is the default one zero_point_domain = LAYOUT_TO_ZERO_POINT_DOMAIN[type(layout)][0] else: assert zero_point_domain in LAYOUT_TO_ZERO_POINT_DOMAIN[type(layout)], ( f\"Layout only support {LAYOUT_TO_ZERO_POINT_DOMAIN[layout]}\" ) if zero_point_domain == ZeroPointDomain.INT and isinstance(layout, Int4XPULayout): zero_point_dtype = torch.int32 preserve_zero = ( config.preserve_zero if config.preserve_zero is not None else LAYOUT_TO_PRESERVE_ZEROS[type(layout)] ) # Sparse Marlin only supports symmetric quantization. # NOTE: If we start having lots of layouts that require different configurations, # we should consider moving this logic somewhere else. if isinstance(layout, MarlinSparseLayout): mapping_type = MappingType.SYMMETRIC assert group_size == 128 or group_size == weight.shape[-1], ( f\"MarlinSparseLayout only supports 128 group size or per channel quantization, got {group_size}\" ) new_weight = to_affine_quantized_intx( weight, mapping_type, block_size, target_dtype, quant_min, quant_max, eps, zero_point_dtype=zero_point_dtype, preserve_zero=preserve_zero, zero_point_domain=zero_point_domain, _layout=layout, use_hqq=use_hqq, ) return new_weight @register_quantize_module_handler(Int4WeightOnlyConfig) def _int4_weight_only_transform( module: torch.nn.Module, config: Int4WeightOnlyConfig ) -\u003e torch.nn.Module: if config.set_inductor_config: torchao.quantization.utils.recommended_inductor_config_setter() assert hasattr(module, \"weight\"), ( \"applying int8 weight only quant requires module to have weight attribute\" + \" but {module} does not have one\" ) new_weight = _int4_weight_only_quantize_tensor(module.weight, config) module.weight = torch.nn.Parameter(new_weight, requires_grad=False) module.extra_repr = types.MethodType(_linear_extra_repr, module) return module [docs]@dataclass class Float8DynamicActivationInt4WeightConfig(AOBaseConfig): \"\"\"Configuration for apply float8 dynamic per row quantization and int4 per group weight quantization to linear (only group_size 128 is supported right now since underlying kernel used only supports 128 and above and no benefits of making it bigger) Args: `int4_packing_format`: how the weight is packed, only preshuffled is supported \"\"\" int4_packing_format: Int4PackingFormat = \"preshuffled\" @register_quantize_module_handler(Float8DynamicActivationInt4WeightConfig) def _float8_dynamic_activation_int4_weight_transform( module: torch.nn.Module, config: Float8DynamicActivationInt4WeightConfig ) -\u003e torch.nn.Module: assert hasattr(module, \"weight\"), ( \"applying int8 weight only quant requires module to have weight attribute\" + \" but {module} does not have one\" ) int4_packing_format = config.int4_packing_format assert int4_packing_format == \"preshuffled\", ( f\"only preshuffled int4_packing_format supported right now, got: {int4_packing_format}\" ) weight = module.weight group_size = 128 block_size = tuple([1 for _ in range(weight.ndim - 1)] + [group_size]) new_weight = Int4PreshuffledTensor.from_hp( module.weight, block_size, activation_dtype=torch.float8_e4m3fn, ) module.weight = torch.nn.Parameter(new_weight, requires_grad=False) module.extra_repr = types.MethodType(_linear_extra_repr, module) return module [docs]@dataclass class Int8WeightOnlyConfig(AOBaseConfig): \"\"\" Configuration for applying int8 weight-only symmetric per-channel quantization to linear layers. Args: group_size (version 1) - Controls the granularity of quantization. If None, applies per-channel quantization. Otherwise, applies per-group quantization with the specified group size. granularity (version 2) - Quantization granularity. PerRow() for per-channel quantization, PerTensor() for per-tensor quantization. set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values for better performance with this quantization scheme. \"\"\" group_size: Optional[int] = None granularity: Optional[Granularity] = PerRow() set_inductor_config: bool = True version: int = 1 def __post_init__(self): torch._C._log_api_usage_once(\"torchao.quantization.Int8WeightOnlyConfig\") if self.version == 2: assert self.group_size is None, ( f\"Only support version 2 with group_size=None, got {self.group_size}\" ) def _int8_weight_only_quantize_tensor(weight, config): if config.version == 1: warnings.warn( \"Config Deprecation: version 1 of Int8WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2752 for more details\" ) mapping_type = MappingType.SYMMETRIC target_dtype = torch.int8 eps = torch.finfo(torch.float32).eps zero_point_dtype = torch.int64 group_size = config.group_size if group_size is None: group_size = weight.shape[-1] block_size = tuple([1 for x in range(weight.dim() - 1)] + [group_size]) new_weight = to_affine_quantized_intx( weight, mapping_type, block_size, target_dtype, eps=eps, zero_point_dtype=zero_point_dtype, ) else: assert config.version == 2, f\"Unexpected version: {config.version}\" new_weight = Int8Tensor.from_hp(weight, granularity=config.granularity) return new_weight @register_quantize_module_handler(Int8WeightOnlyConfig) def _int8_weight_only_transform( module: torch.nn.Module, config: Int8WeightOnlyConfig, *, parameter_name: str = \"weight\", ): if config.set_inductor_config: torchao.quantization.utils.recommended_inductor_config_setter() assert hasattr(module, parameter_name), ( \"applying int8 weight only quant requires module to have {parameter_name} attribute\" + \" but {module} does not have one\" ) quantized_tensor = _int8_weight_only_quantize_tensor( getattr(module, parameter_name), config ) setattr( module, parameter_name, torch.nn.Parameter(quantized_tensor, requires_grad=False), ) module.extra_repr = types.MethodType( partial( _module_extra_repr, original_extra_repr=module.extra_repr, parameter_name=parameter_name, ), module, ) return module def _int8_symm_per_token_reduced_range_quant(x: torch.Tensor) -\u003e torch.Tensor: mapping_type = MappingType.SYMMETRIC target_dtype = torch.int8 eps = 1e-5 quant_min = -127 quant_max = 127 return to_affine_quantized_intx( x, mapping_type, _get_per_token_block_size(x), target_dtype, eps=eps, quant_min=quant_min, quant_max=quant_max, scale_dtype=torch.float32 if x.dtype == torch.float16 else None, ) def _int8_symm_per_token_reduced_range_quant_noop_decode( x: torch.Tensor, ) -\u003e torch.Tensor: mapping_type = MappingType.SYMMETRIC target_dtype = torch.int8 eps = 1e-5 quant_min = -127 quant_max = 127 if x.shape[1] == 1: return x else: return to_affine_quantized_intx( x, mapping_type, _get_per_token_block_size(x), target_dtype, eps=eps, quant_min=quant_min, quant_max=quant_max, scale_dtype=torch.float32 if x.dtype == torch.float16 else None, ) def _int8_symm_cutlass_quant(x: torch.Tensor) -\u003e torch.Tensor: return to_affine_quantized_intx( x, mapping_type=MappingType.SYMMETRIC, block_size=_get_per_token_block_size(x), target_dtype=torch.int8, scale_dtype=torch.float32, eps=torch.finfo(torch.float32).eps, zero_point_domain=ZeroPointDomain.NONE, ) def _int4_symm_cutlass_quant(x: torch.Tensor) -\u003e torch.Tensor: return to_affine_quantized_intx( x, mapping_type=MappingType.SYMMETRIC, block_size=_get_per_token_block_size(x), target_dtype=torch.int8, quant_min=-8, quant_max=7, scale_dtype=torch.float32, eps=torch.finfo(torch.float32).eps, zero_point_domain=ZeroPointDomain.NONE, _layout=CutlassInt4PackedLayout(), ) def _float8_cutlass_quant( x: torch.Tensor, target_dtype: torch.dtype, ) -\u003e torch.Tensor: return to_affine_quantized_floatx( x, block_size=_get_per_token_block_size(x), scale_dtype=torch.float32, target_dtype=target_dtype, _layout=Float8Layout(mm_config=None), ) def _float8_cutlass_quant_sparse( x: torch.Tensor, target_dtype: torch.dtype, ) -\u003e (torch.Tensor, torch.Tensor): return to_affine_quantized_floatx( x, block_size=_get_per_token_block_size(x), scale_dtype=torch.float32, target_dtype=target_dtype, _layout=CutlassSemiSparseLayout(), ) [docs]@dataclass class Int8DynamicActivationInt8WeightConfig(AOBaseConfig): \"\"\" Configuration for applying int8 dynamic symmetric per-token activation and int8 per-channel weight quantization to linear layers. Args: layout: Optional[Layout] = PlainLayout() - Tensor layout for the quantized weights. Controls how the quantized data is stored and accessed. act_mapping_type: Optional[MappingType] = MappingType.SYMMETRIC - Mapping type for activation quantization. SYMMETRIC uses symmetric quantization around zero. weight_only_decode: bool = False - If True, only quantizes weights during forward pass and keeps activations in original precision during decode operations. set_inductor_config: bool = True - If True, adjusts `torchinductor` settings to recommended values for better performance with this quantization scheme. version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Int8Tensor \"\"\" layout: Optional[Layout] = PlainLayout() act_mapping_type: Optional[MappingType] = MappingType.SYMMETRIC weight_only_decode: bool = False granularity: Granularity = PerRow() set_inductor_config: bool = True version: int = 1 def __post_init__(self): torch._C._log_api_usage_once( \"torchao.quantization.Int8DynamicActivationInt8WeightConfig\" ) def _int8_dynamic_activation_int8_weight_quantize_tensor(weight, config): if config.version == 1: layout = config.layout act_mapping_type = config.act_mapping_type weight_only_decode = config.weight_only_decode in_features = weight.shape[-1] # int8 dynamic quantization only has benefit when in_feature \u003e 16 if in_features \u003c= 16: logger.info( f\"Skipping applying Int8DynamicActivationInt8WeightConfig to weight of shape {weight.shape}\" f\" because `in_feature` is \u003c= 16: {in_features}\" ) return weight # weight settings mapping_type = MappingType.SYMMETRIC weight_zero_point_domain = ZeroPointDomain.NONE def get_weight_block_size(x): return tuple([1 for _ in range(x.dim() - 1)] + [x.shape[-1]]) target_dtype = torch.int8 eps = torch.finfo(torch.float32).eps zero_point_dtype = torch.int64 if weight_only_decode: input_quant_func = _int8_symm_per_token_reduced_range_quant_noop_decode else: # input settings if act_mapping_type == MappingType.SYMMETRIC: input_quant_func = _int8_symm_per_token_reduced_range_quant else: input_quant_func = _int8_asymm_per_token_quant block_size = get_weight_block_size(weight) new_weight = to_affine_quantized_intx( weight, mapping_type, block_size, target_dtype, eps=eps, zero_point_dtype=zero_point_dtype, _layout=layout, zero_point_domain=weight_zero_point_domain, ) quantized_weight = to_linear_activation_quantized(new_weight, input_quant_func) else: assert config.granularity in {PerRow(), PerTensor()}, ( \"Only PerRow and PerTensor are supported\" ) weight_granularity = config.granularity act_granularity = config.granularity assert config.act_mapping_type == MappingType.SYMMETRIC, ( \"asymmetric dynamic quant not supported currently\" ) assert config.version == 2, f\"Unexpected version: {config.version}\" # TODO: Symmentric/Asymmetric choice for weight quantization # https://github.com/pytorch/ao/pull/3241#discussion_r2551515539 quantized_weight = Int8Tensor.from_hp( weight, granularity=weight_granularity, act_quant_kwargs=QuantizeTensorToInt8Kwargs( granularity=act_granularity, mapping_type=config.act_mapping_type, ), ) return quantized_weight @register_quantize_module_handler(Int8DynamicActivationInt8WeightConfig) def _int8_dynamic_activation_int8_weight_transform( module: torch.nn.Module, config: Int8DynamicActivationInt8WeightConfig, *, parameter_name=\"weight\", ) -\u003e torch.nn.Module: if config.set_inductor_config: torchao.quantization.utils.recommended_inductor_config_setter() assert hasattr(module, \"weight\"), ( \"applying int8 dynamic activation int8 weight quant requires module to have weight attribute\" + \"but {module} does not have one\" ) new_weight = _int8_dynamic_activation_int8_weight_quantize_tensor( module.weight, config ) module.weight = torch.nn.Parameter(new_weight, requires_grad=False) module.extra_repr = types.MethodType( partial( _module_extra_repr, original_extra_repr=module.extra_repr, parameter_name=parameter_name, ), module, ) return module @dataclass class Int8StaticActivationInt8WeightConfig(AOBaseConfig): \"\"\" Configuration for applying int8 static symmetric quantization to both activation and weight Args: scale (torch.Tensor): The scale tensor for activation quantization. granularity (Granularity): The granularity of quantization. PerRow() and PerTensor() are supported currently act_mapping_type (MappingType): The mapping type for activation quantization. only SYMMETRIC is supported currently set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values. version (int): the version of the config \"\"\" scale: torch.Tensor granularity: Granularity = PerRow() act_mapping_type: Optional[MappingType] = MappingType.SYMMETRIC set_inductor_config: bool = True version: int = 1 def __post_init__(self): torch._C._log_api_usage_once( \"torchao.quantization.Int8StaticActivationInt8WeightConfig\" ) @register_quantize_module_handler(Int8StaticActivationInt8WeightConfig) def _int8_static_activation_int8_weight_transform( module: torch.nn.Module, config: Int8StaticActivationInt8WeightConfig, *, parameter_name=\"weight\", ): assert config.granularity in {PerRow(), PerTensor()}, ( \"Only PerRow and PerTensor is supported currently\" ) assert config.act_mapping_type == MappingType.SYMMETRIC, ( \"asymmetric static quant not supported currently\" ) assert hasattr(module, parameter_name), ( f\"Expected module to have attribute `{parameter_name}` but not found\" ) if config.set_inductor_config: torchao.quantization.utils.recommended_inductor_config_setter() activation_granularity = config.granularity weight_granularity = config.granularity quantized_tensor = Int8Tensor.from_hp( getattr(module, parameter_name), granularity=weight_granularity, act_quant_kwargs=QuantizeTensorToInt8Kwargs( granularity=activation_granularity, mapping_type=config.act_mapping_type, ), act_scale=config.scale.detach(), ) setattr( module, parameter_name, torch.nn.Parameter(quantized_tensor, requires_grad=False), ) module.extra_repr = types.MethodType( partial( _module_extra_repr, original_extra_repr=module.extra_repr, parameter_name=parameter_name, ), module, ) return module def int8_dynamic_activation_int8_semi_sparse_weight(): \"\"\" Applies int8 dnynamic symmetric per-token activation and int8 per-channel weight quantization + 2:4 sparsity to linear layers. \"\"\" warnings.warn( \"\"\"int8_dyanmic_activation_int8_semi_sparse_weight() will be deprecated at a later release. Please use the layout kwarg in Int8DynamicActivationInt8WeightConfig instead. from torchao.dtypes import SemiSparseLayout Int8DynamicActivationInt8WeightConfig(layout=SemiSparseLayout()\"\"\" ) return Int8DynamicActivationInt8WeightConfig(layout=SemiSparseLayout()) [docs]@dataclass class Float8WeightOnlyConfig(AOBaseConfig): \"\"\" Configuration for applying float8 weight-only symmetric per-channel quantization to linear layers. Args: weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn. set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values. version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Float8Tensor (default) Note: The actual matmul will be computed in original precision of the weight tensor. \"\"\" weight_dtype: torch.dtype = e4m3_dtype set_inductor_config: bool = True version: int = 2 def __post_init__(self): torch._C._log_api_usage_once(\"torchao.quantization.Float8WeightOnlyConfig\") def _float8_weight_only_quant_tensor(weight, config): if config.version == 1: warnings.warn( \"Config Deprecation: version 1 of Float8WeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2649 for more details\" ) from torchao.dtypes import to_affine_quantized_floatx block_size = tuple([1 for _ in range(weight.dim() - 1)] + [weight.shape[-1]]) new_weight = to_affine_quantized_floatx( input_float=weight, block_size=block_size, target_dtype=config.weight_dtype, scale_dtype=None, _layout=Float8Layout(mm_config=None), ) else: assert config.version == 2, f\"Unexpected version: {config.version}\" weight_dtype = config.weight_dtype new_weight = Float8Tensor.from_hp( weight, float8_dtype=weight_dtype, granularity=PerRow() ) return new_weight @register_quantize_module_handler(Float8WeightOnlyConfig) def _float8_weight_only_transform( module: torch.nn.Module, config: Float8WeightOnlyConfig, *, parameter_name: str = \"weight\", ) -\u003e torch.nn.Module: if config.set_inductor_config: torchao.quantization.utils.recommended_inductor_config_setter() assert hasattr(module, parameter_name), ( \"applying float8 weight only quant requires module to have {parameter_name} attribute\" + \" but {module} does not have one\" ) if isinstance(module, Float8Linear): module = _unwrap_float8_linear(module) quantized_tensor = _float8_weight_only_quant_tensor( getattr(module, parameter_name), config ) setattr( module, parameter_name, torch.nn.Parameter(quantized_tensor, requires_grad=False), ) module.extra_repr = types.MethodType( partial( _module_extra_repr, original_extra_repr=module.extra_repr, parameter_name=parameter_name, ), module, ) return module def _input_activation_quant_func_fp8( x: torch.Tensor, activation_granularity: FP8Granularity, activation_dtype: torch.dtype, scale: Optional[torch.Tensor] = None, zero_point: Optional[torch.Tensor] = None, ): \"\"\"This function is used to quantize the input activation tensor for an aqt_float variant. If scale is not provided it will be dynamically calculate the scales otherwise it will use the provided scale. \"\"\" assert zero_point is None, ( \"Zero point is not supported for dynamic FP8 quantization\" ) if isinstance(activation_granularity, PerRow): assert x.dtype == torch.bfloat16, ( \"PerRow quantization only works for bfloat16 precision input activation\" ) block_size = get_block_size(x.shape, activation_granularity) if scale is None: activation = to_affine_quantized_floatx( input_float=x, block_size=block_size, target_dtype=activation_dtype, scale_dtype=torch.float32, _layout=Float8Layout(mm_config=None), # Config is stored on weight ) else: assert isinstance(activation_granularity, PerTensor), ( \"Static quantization only supports PerTensor granularity\" ) activation = to_affine_quantized_floatx_static( input_float=x, block_size=block_size, scale=scale, target_dtype=activation_dtype, _layout=Float8Layout(mm_config=None), # Config is stored on weight ) return activation [docs]@dataclass class Float8DynamicActivationFloat8WeightConfig(AOBaseConfig): \"\"\" Configuration for applying float8 dynamic symmetric quantization to both activations and weights of linear layers. Args: activation_dtype (torch.dtype): The target data type for activation quantization. Default is torch.float8_e4m3fn. weight_dtype (torch.dtype): The target data type for weight quantization. Default is torch.float8_e4m3fn. granularity (Optional[Union[FP8Granularity, List[FP8Granularity]]]): The granularity for quantization. Can be either a single granularity (applied to both activations and weights) or a tuple of two granularities (one for activations, one for weights). If None, defaults to PerTensor for both. Currently both quantizations need to be the same type. And only PerTensor and PerRow are supported. mm_config (Float8MMConfig): Configuration for the matrix multiplication. Default uses fast accumulation. activation_value_lb (Optional[float]): the lower bound for activation value for calculating scale activation_value_ub (Optional[float]): the upper bound for activation value for calculating scale kernel_preference (KernelPreference): kernel preference for ops like matmul, grouped matmul etc. by defalut (KernelPreference.AUTO) it will be chosen for user based on hardware or other information, this only needs to be set in weight set_inductor_config (bool): if True, adjusts `torchinductor` settings to recommended values. version (int): the version of the config, version 1 is using AffineQuantizedTensor that we plan to deprecate/split, version 2 is using Float8Tensor (default) \"\"\" activation_dtype: torch.dtype = e4m3_dtype weight_dtype: torch.dtype = e4m3_dtype granularity: Optional[Union[FP8Granularity, List[FP8Granularity]]] = None mm_config: Optional[Float8MMConfig] = None activation_value_lb: Optional[float] = None activation_value_ub: Optional[float] = None kernel_preference: KernelPreference = KernelPreference.AUTO set_inductor_config: bool = True version: int = 2 def __post_init__(self): torch._C._log_api_usage_once( \"torchao.quantization.Float8DynamicActivationFloat8WeightConfig\" ) activation_granularity, weight_granularity = _normalize_granularity( self.granularity ) self.granularity = [activation_granularity, weight_granularity] default_use_fast_accum = True if _granularity_is_a_1_128_w_128_128(self.granularity): assert self.kernel_preference in ( KernelPreference.AUTO, KernelPreference.TORCH, ), \"unimplemented\" assert self.version \u003e= 2, \"unimplemented\" default_use_fast_accum = False if self.mm_config is None: self.mm_config = Float8MMConfig(use_fast_accum=default_use_fast_accum) def _float8_dynamic_activation_float8_weight_quantize_tensor(weight, config): activation_dtype = config.activation_dtype weight_dtype = config.weight_dtype granularity = config.granularity mm_config = config.mm_config activation_value_lb = config.activation_value_lb activation_value_ub = config.activation_value_ub kernel_preference = config.kernel_preference # Ensure works on device _check_hardware_support(granularity) activation_granularity, weight_granularity = granularity # Note: right now we assume it\u0027s weights of conv2d and conv3d purely based # on the dimension of weight, currently there is no conflict with linear 2d # and moe weights 3d # if we need to support conv1d, which also has 3d weight, we may have to # pass around the module as well to distinguish between conv1d and 3d moe weight if weight.dim() in [4, 5]: # weights for conv2d or 3d assert isinstance(activation_granularity, PerTensor) and isinstance( weight_granularity, PerTensor ), \"4D/5D tensor only supports per tensor activation and weight quantization\" # conv3d weight dim: (C_out, C_in, K1, K2, K3) # conv2d weight dim: (C_out, C_in, K1, K2) # skip quantization when either C_out or C_in # is not a multiple of 16 if weight.shape[0] % 16 != 0 or weight.shape[1] % 16 != 0: return weight elif not _fp8_mm_compat(weight): # TODO(future PR): this should really throw an exception instead of silently # not doing what the user asked return weight if isinstance(weight_granularity, PerRow): assert weight.dtype == torch.bfloat16, ( \"PerRow quantization only works for bfloat16 precision input weight\" ) if config.version == 1: warnings.warn( \"Config Deprecation: version 1 of Float8DynamicActivationFloat8WeightConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2649 for more details\" ) block_size = get_block_size(weight.shape[-2:], weight_granularity) if weight.dim() == 3: block_size = tuple([1] + list(block_size)) quantized_weight = to_affine_quantized_floatx( input_float=weight, block_size=block_size, target_dtype=weight_dtype, scale_dtype=torch.float32, _layout=Float8Layout(mm_config=mm_config), ) input_quant_func = _input_activation_quant_func_fp8 input_quant_kwargs = { \"activation_granularity\": activation_granularity, \"activation_dtype\": activation_dtype, } quantized_weight = to_linear_activation_quantized( quantized_weight, input_quant_func, quant_kwargs=input_quant_kwargs ) else: assert config.version == 2, f\"Unexpected version: {config.version}\" act_quant_kwargs = QuantizeTensorToFloat8Kwargs( activation_dtype, activation_granularity, hp_value_lb=activation_value_lb, hp_value_ub=activation_value_ub, kernel_preference=kernel_preference, ) quantized_weight = Float8Tensor.from_hp( weight, float8_dtype=weight_dtype, granularity=weight_granularity, mm_config=mm_config, kernel_preference=kernel_preference, act_quant_kwargs=act_quant_kwargs, ) return quantized_weight @register_quantize_module_handler(Float8DynamicActivationFloat8WeightConfig) def _float8_dynamic_activation_float8_weight_transform( module: torch.nn.Module, config: Float8DynamicActivationFloat8WeightConfig, *, parameter_name: str = \"weight\", ): assert is_sm_at_least_89() or is_MI300(), ( \"Float8 dynamic activation quantization is only supported on CUDA\u003e=8.9 and MI300+\" ) if config.set_inductor_config: torchao.quantization.utils.recommended_inductor_config_setter() assert hasattr(module, parameter_name), ( f\"applying float8 dynamic activation quant requires module to have parameter {parameter_name} attribute\" + f\" but {module} does not have one\" ) if isinstance(module, Float8Linear): module = _unwrap_float8_linear(module) quantized_tensor = _float8_dynamic_activation_float8_weight_quantize_tensor( getattr(module, parameter_name), config ) setattr( module, parameter_name, torch.nn.Parameter(quantized_tensor, requires_grad=False), ) module.extra_repr = types.MethodType( partial( _module_extra_repr, original_extra_repr=module.extra_repr, parameter_name=parameter_name, ), module, ) return module @dataclass class Float8DynamicActivationFloat8SemiSparseWeightConfig(AOBaseConfig): \"\"\" Applies float8 dynamic quantization to activations and float8 quantization followed by compression to sparse semi-structured tensor to weights of linear layers. Args: `layout`: layout type for quantized weight tensor, only supports `CutlassSemiSparseLayout` at the moment. `activation_dtype`: data type for quantized activation tensor. `weight_dtype`: data type for quantized weight tensor. \"\"\" layout: Layout = CutlassSemiSparseLayout() activation_dtype: torch.dtype = e5m2_dtype weight_dtype: torch.dtype = e4m3_dtype def __post_init__(self): torch._C._log_api_usage_once( \"torchao.quantization.Float8DynamicActivationFloat8SemiSparseWeightConfig\" ) @register_quantize_module_handler(Float8DynamicActivationFloat8SemiSparseWeightConfig) def _float8_dynamic_activation_float8_semi_sparse_weight_transform( module: torch.nn.Module, config: Float8DynamicActivationFloat8SemiSparseWeightConfig ): assert is_sm_at_least_90(), \"Float8 quantization is only supported on CUDA\u003e=9.0\" if isinstance(module, Float8Linear): module = _unwrap_float8_linear(module) weight = module.weight weight_dtype = config.weight_dtype activation_dtype = config.activation_dtype layout = config.layout if not isinstance(layout, CutlassSemiSparseLayout): raise NotImplementedError( f\"Only CutlassSemiSparseLayout layout is supported. Received {layout}.\" ) weight = _float8_cutlass_quant_sparse(weight, weight_dtype) weight = to_linear_activation_quantized( weight, _float8_cutlass_quant, quant_kwargs={\"target_dtype\": activation_dtype}, ) module.weight = torch.nn.Parameter(weight, requires_grad=False) module.extra_repr = types.MethodType(_linear_extra_repr, module) return module def _adjust_scale_dtype_in_intx_unpacked_tensor( intx_unpacked_tensor: IntxUnpackedToInt8Tensor, hp_tensor: torch.Tensor, scale_dtype: torch.dtype, ) -\u003e None: \"\"\" Adjusts the scale_dtype on IntxUnpackedToInt8Tensor. Updating the scale dtype requires updating the qdata because qdata is calculated after the scale. This is used in IntxWeightOnlyConfig and Int8DynamicActivationIntxWeightConfig to make version=2 and version=1 numerically equivalent when the scale_dtype differs from the input dtype \"\"\" assert isinstance(intx_unpacked_tensor, IntxUnpackedToInt8Tensor) intx_unpacked_tensor.scale = intx_unpacked_tensor.scale.to(scale_dtype) qmin, qmax = _DTYPE_TO_QVALUE_BOUNDS[intx_unpacked_tensor.target_dtype] intx_unpacked_tensor.qdata = quantize_affine( hp_tensor, intx_unpacked_tensor.block_size, intx_unpacked_tensor.scale, intx_unpacked_tensor.zero_point, output_dtype=torch.int8, quant_min=qmin, quant_max=qmax, ) @dataclass class IntxWeightOnlyConfig(AOBaseConfig): \"\"\" Configuration for quantizing weights to torch.intx, with 1 \u003c= x \u003c= 8. Weights are quantized with scales/zeros in a groupwise or channelwise manner using the number of bits specified by weight_dtype. args: `weight_dtype`: The dtype to use for weight quantization. Must be torch.intx, where 1 \u003c= x \u003c= 8. `granularity`: The granularity to use for weight quantization. Must be PerGroup or PerAxis(0). `mapping_type`: The type of mapping to use for the weight quantization. Must be one of MappingType.ASYMMETRIC or MappingType.SYMMETRIC. `scale_dtype`: The dtype to use for the weight scale. `layout`: The layout to use for the packed weight tensor: - QDQLayout: this layout is designed for export to ExecuTorch.this layout represents the quantization with Q/DQ quant primitives, and is intended for export applications like ExecuTorch. `intx_packing_format`: The format to use for the packed weight tensor (version 2 only). `intx_choose_qparams_algorithm`: The algorithm to use for choosing the quantization parameters. `version`: version of the config to use, only subset of above args are valid based on version, see note for more details. Note: Current state for IntxWeightOnlyConfig is that it supports both v1 (legacy) and v2. * `intx_packing_format` is used for version 2. * `layout` is only used for version 1. \"\"\" weight_dtype: torch.dtype = torch.int8 granularity: Granularity = PerAxis(0) mapping_type: MappingType = MappingType.SYMMETRIC scale_dtype: Optional[torch.dtype] = None layout: Layout = QDQLayout() intx_packing_format: IntxPackingFormat = IntxPackingFormat.UNPACKED_TO_INT8 intx_choose_qparams_algorithm: IntxChooseQParamsAlgorithm = ( IntxChooseQParamsAlgorithm.AFFINE ) version: int = 2 def __post_init__(self): torch._C._log_api_usage_once(\"torchao.quantization.IntxWeightOnlyConfig\") assert self.weight_dtype in [getattr(torch, f\"int{b}\") for b in range(1, 9)], ( f\"weight_dtype must be torch.intx, where 1 \u003c= x \u003c= 8, but got {self.weight_dtype}\" ) assert isinstance(self.granularity, (PerAxis, PerGroup)), ( f\"granularity must be PerAxis or PerGroup, but got {self.granularity}\" ) if isinstance(self.granularity, PerAxis): assert self.granularity.axis == 0, ( f\"axis must be 0 with PerAxis, but got {self.granularity.axis}\" ) assert self.mapping_type in [ MappingType.ASYMMETRIC, MappingType.SYMMETRIC, MappingType.SYMMETRIC_NO_CLIPPING_ERR, ], ( f\"mapping_type must be MappingType.ASYMMETRIC, MappingType.SYMMETRIC, or MappingType.SYMMETRIC_NO_CLIPPING_ERR, but got {self.mapping_type}\" ) def _intx_weight_only_quantize_tensor( weight, config, *, custom_scale: Optional[torch.Tensor] = None, custom_zero_point: Optional[torch.Tensor] = None, ): weight_dtype = config.weight_dtype granularity = config.granularity mapping_type = config.mapping_type scale_dtype = config.scale_dtype layout = config.layout intx_packing_format = config.intx_packing_format intx_choose_qparams_algorithm = config.intx_choose_qparams_algorithm if weight.dim() == 2: input_dim = -1 elif weight.dim() == 4: # conv2d: N, C_in, H, W input_dim = 1 else: raise ValueError( f\"IntxWeightOnlyConfig only works for 2-d and 4-d Tensors, got: {weight.dim()}\" ) if isinstance(granularity, PerGroup): group_size = granularity.group_size elif isinstance(granularity, PerAxis): assert granularity.axis == 0, ( f\"axis must be 0 with PerAxis, but got {granularity.axis}\" ) group_size = weight.shape[input_dim] else: raise ValueError(f\"granularity must be PerGroup or PerAxis, got {granularity}\") if weight.dim() == 2: block_size = (1, group_size) else: # conv2d: N, C_in, H, W assert weight.dim() == 4 block_size = (1, group_size, 1, 1) if config.version == 2: if config.intx_packing_format == IntxPackingFormat.UNPACKED_TO_INT8: if custom_zero_point is not None and custom_zero_point.dtype == torch.int32: custom_zero_point = custom_zero_point.to(torch.int8) new_weight = IntxUnpackedToInt8Tensor.from_hp( weight, block_size, weight_dtype, mapping_type=mapping_type, custom_scale=custom_scale, custom_zero_point=custom_zero_point, intx_choose_qparams_algorithm=intx_choose_qparams_algorithm, ) if scale_dtype is not None and scale_dtype != weight.dtype: _adjust_scale_dtype_in_intx_unpacked_tensor( new_weight, weight, scale_dtype ) return new_weight else: raise ValueError(f\"Unsupported packing format: {intx_packing_format}\") # Version 1 assert config.intx_choose_qparams_algorithm == IntxChooseQParamsAlgorithm.AFFINE, ( \"version 1 only supports affine algorithm\" ) assert config.version == 1 warnings.warn( \"Config Deprecation: version 1 of IntxWeightOnlyConfig is deprecated and will no longer be supported in a future release, please use version 2, see https://github.com/pytorch/ao/issues/2967 for more details\" ) quant_min, quant_max = _DTYPE_TO_QVALUE_BOUNDS[weight_dtype] weight = to_affine_quantized_intx( input_float=weight, mapping_type=mapping_type, block_size=block_size, target_dtype=torch.int8, quant_min=quant_min, quant_max=quant_max, scale_dtype=scale_dtype, zero_point_dtype=torch.int8, preserve_zero=(mapping_type == MappingType.SYMMETRIC), zero_point_domain=ZeroPointDomain.INT, _layout=layout, ) return weight @register_quantize_module_handler(IntxWeightOnlyConfig) def _intx_weight_only_transform( module: torch.nn.Module, config: IntxWeightOnlyConfig, *, custom_scale: Optional[torch.Tensor] = None, custom_zero_point: Optional[torch.Tensor] = None, ) -\u003e torch.nn.Module: assert hasattr(module, \"weight\"), ( \"applying intx weight only quant requires module to have weight attribute\" + \" but {module} does not have one\" ) new_weight = _intx_weight_only_quantize_tensor( module.weight, config, custom_scale=custom_scale, custom_zero_point=custom_zero_point, ) module.weight = torch.nn.Parameter(new_weight, requires_grad=False) if isinstance(module, nn.Linear): module.extra_repr = types.MethodType(_linear_extra_repr, module) elif isinstance(module, nn.Embedding): module.extra_repr = types.MethodType(_embedding_extra_repr, module) return module @dataclass class FqnToConfig(AOBaseConfig): \"\"\"Configuration class for applying different quantization configs to modules or parameters based on their fully qualified names (FQNs). Args: `fqn_to_config`: typing.OrderedDict[str, Optional[AOBaseConfig]]: an ordered dictionary from (1). fully qualified name (fqn) of module or parameter (2). regex of fully qualified name (in python `re` module regex format), should start with prefix \"re:\" or (3). \"_default\" to the config that we want to apply to the module/param or None Config key ordered by precedence: * fully qualified parameter name, e.g. `language.layers.0.q_proj.weight` * fully qualified module name, e.g. `language.layers.0.q_proj` * regex for parameter names, must start with `re:`, e.g. `re:language\\.layers\\..+\\.q_proj.weight`. The first regex that matches will be applied. * regex for module names, must start with `re:`, e.g. `re:language\\.layers\\..+\\.q_proj`, whichever regex fully matches the module fqn first will be applied (order of keys for dictionary are kept consistent since we are using OrderedDict) * \"_default\", fallback if no match for all previous keys (Note, when using `_default`, the config is applied to all modules, to apply it to only a subset of modules, e.g. with some types, it\u0027s better to filter the modules that we don\u0027t want to quantize before hand and configure them to None, e.g. `{\"re:.+norm.+\": None, \"_default\": linear_config}`) \"_default\" is not supported when filter_fn is not specified. `module_fqn_to_config`: typing.OrderedDict[str, Optional[AOBaseConfig]]: To maintain BC with ModuleFqnToConfig, to be deprecated later `version`: int: Version of config to use. Note: - The order of patterns in the OrderedDict may matter as only the first matching pattern is applied - \"_default\" is ignored for parameter replacement. \"\"\" fqn_to_config: OrderedDictType[str, Optional[AOBaseConfig]] = field( default_factory=OrderedDict ) # to maintain BC, we keep the previous module_fqn_to_config field module_fqn_to_config: OrderedDictType[str, Optional[AOBaseConfig]] = field( default_factory=OrderedDict ) version: int = 1 def __post_init__(self): torch._C._log_api_usage_once(\"torchao.quantization.FqnToConfig\") if ( len(self.fqn_to_config) \u003e 0 and len(self.module_fqn_to_config) \u003e 0 and self.fqn_to_config != self.module_fqn_to_config ): raise ValueError( \"`fqn_to_config` and `module_fqn_to_config` are both specified and are not equal!\" ) # This code handles BC compatibility with `ModuleFqnToConfig`. It ensures that `self.module_fqn_to_config` and `self.fqn_to_config` share the same object. if len(self.module_fqn_to_config) \u003e 0 and len(self.fqn_to_config) == 0: self.fqn_to_config = self.module_fqn_to_config if len(self.fqn_to_config) \u003e 0 and len(self.module_fqn_to_config) == 0: self.module_fqn_to_config = self.fqn_to_config # TODO we plan to deprecate `_default later, so raise a warning if we find it passed in` if \"_default\" in self.fqn_to_config: warnings.warn( \"Config Deprecation: _default is deprecated and will no longer be supported in a future release. Please see https://github.com/pytorch/ao/issues/3229 for more details.\" ) def __str__(self): return \"\\n\".join( [ \"FqnToConfig({\", *( f\" \u0027{key}\u0027:\\n {value},\" for key, value in self.fqn_to_config.items() ), \"})\", ] ) # maintain BC ModuleFqnToConfig = FqnToConfig # for now, we need to keep track of what configs support custom param quantization. # Once we\u0027ve updated all the transform functions to take in a custom_param kwarg, we can delete this object and the subsequent check # TODO see https://github.com/pytorch/ao/issues/3252 for more details CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS = { Float8DynamicActivationFloat8WeightConfig, Float8WeightOnlyConfig, Int8WeightOnlyConfig, } def _fqn_to_config_handler( module: torch.nn.Module, fqn: str, config: FqnToConfig, ): \"\"\"This function expects a module that either is specified in FqnToConfig or has a parameter that is specified in FqnToConfig. Args: module (torch.nn.Module): The module to be processed. fqn (str): The fully qualified name of the module containing the parameters. config (FqnToConfig): Configuration object containing regex patterns / fqn mapped to quantization configurations. Returns: torch.nn.Module: The modified module with quantized parameters. Raises: NotImplementedError: If the quantization configuration is not yet supported for parameter quantization. \"\"\" parameter_config_found = False top_level_params = [] for i, (parameter_name, param) in enumerate(list(module.named_parameters())): if parameter_name in dir(module): parameter_fqn = ( f\"{fqn}.{parameter_name}\" if len(fqn) \u003e 0 else parameter_name ) top_level_params.append((i, parameter_name, param, parameter_fqn)) # First we see if any parameter fqn matches with FqnToConfig, if so, we apply the appropriate transform for i, parameter_name, param, parameter_fqn in list(top_level_params): if parameter_fqn in config.fqn_to_config: parameter_config_found = True c = config.fqn_to_config[parameter_fqn] # if None, remove from subsequent regex check if c is None: top_level_params.pop(i) else: handler = _QUANTIZE_CONFIG_HANDLER[type(c)] if type(c) in CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS: # may be more than one param specified, so don\u0027t return prematurely module = handler(module, c, parameter_name=parameter_name) else: raise NotImplementedError( f\"{type(c)} does not yet support parameter quantization! Please see https://github.com/pytorch/ao/issues/3252 for more details\" ) # then we see if we match module_fqn exactly if not parameter_config_found and fqn in config.fqn_to_config: c = config.fqn_to_config[fqn] if c is not None: handler = _QUANTIZE_CONFIG_HANDLER[type(c)] return handler(module, c) else: return module # Next try to match parameters on regex patterns for i, parameter_name, param, parameter_fqn in top_level_params: for pattern in config.fqn_to_config: if pattern.startswith(\"re:\") and re.fullmatch(pattern[3:], parameter_fqn): parameter_config_found = True c = config.fqn_to_config[pattern] if c is not None: handler = _QUANTIZE_CONFIG_HANDLER[type(c)] if type(c) in CUSTOM_PARAM_QUANTIZATION_SUPPORTED_CONFIGS: # may be more than one param specified, so don\u0027t return prematurely module = handler(module, c, parameter_name=parameter_name) else: raise NotImplementedError( f\"{type(c)} does not yet support parameter quantization! Please see https://github.com/pytorch/ao/issues/3252 for more details\" ) # try to match regex on module fqn if not parameter_config_found: for pattern in config.fqn_to_config: # we\u0027ll apply the config for first fully matched pattern if pattern.startswith(\"re:\") and re.fullmatch(pattern[3:], fqn): c = config.fqn_to_config[pattern] if c is not None: handler = _QUANTIZE_CONFIG_HANDLER[type(c)] return handler(module, c) # If no module_fqn or parameter_fqn matches, then we apply _default if not parameter_config_found: c = config.fqn_to_config.get(\"_default\", None) if c is not None: handler = _QUANTIZE_CONFIG_HANDLER[type(c)] # safe to return here as at most only one module will match return handler(module, c) return module def fqn_matches_fqn_config( fqn: str, config: FqnToConfig, ): \"\"\"Check if a given fqn matches the exact fqn or regex pattern specified in FqnToConfig. Args: fqn (str): The fully qualified name of the module. config (FqnToConfig): Configuration object containing regex patterns or raw FQNs for quantization. Returns: bool: True if the fqn is specified in FqnToConfig. False otherwise. \"\"\" if fqn in config.fqn_to_config: assert not fqn.startswith(\"re:\"), ( f\"Error: Exact match but regex {fqn} specified.\" ) return True else: for maybe_module_or_param_fqn_pattern in config.fqn_to_config: if maybe_module_or_param_fqn_pattern.startswith(\"re:\") and re.fullmatch( maybe_module_or_param_fqn_pattern[3:], fqn ): return True return False def _module_param_matches_fqn_config( module: nn.Module, fqn: str, config: FqnToConfig, ): \"\"\"Check if a given module contains top-level parameters that match the exact fqn or regex pattern specified in FqnToConfig. Args: module (nn.Module): The module to be checked. fqn (str): The fully qualified name of the module. config (FqnToConfig): Configuration object containing regex patterns or raw FQNs for quantization. Returns: bool: True if the module contains top-level parameters that match the fqn or regex pattern specified in FqnTo \"\"\" for name, param in module.named_parameters(): if name in dir(module): parameter_fqn = f\"{fqn}.{name}\" if len(fqn) \u003e 0 else name if fqn_matches_fqn_config(parameter_fqn, config): return True return False def _unwrap_float8_linear(module: Float8Linear) -\u003e nn.Linear: \"\"\" Unwrap a torchao Float8Linear by returning a nn.Linear with the same weights and bias. Torchao inference quantization techniques are generally only applicable to nn.Linear layers, so this helper is useful for unwrapping models trained with torchao float8 training, which replaces nn.Linear layers with Float8Linear layers. \"\"\" with torch.device(\"meta\"): new_module = nn.Linear(module.in_features, module.out_features) new_module.weight = module.weight new_module.bias = module.bias return new_module torch.serialization.add_safe_globals( [ _int8_asymm_per_token_quant, _int8_symm_per_token_reduced_range_quant, _input_activation_quant_func_fp8, _int4_symm_cutlass_quant, _int8_symm_cutlass_quant, _float8_cutlass_quant, _float8_cutlass_quant_sparse, Target, ] )",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/quantization/quant_api.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>