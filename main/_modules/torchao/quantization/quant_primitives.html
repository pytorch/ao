

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.quantization.quant_primitives &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/design-tabs.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/quantization/quant_primitives';</script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/quantization/quant_primitives.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_reference/index.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../eager_quantization/index.html">
    Eager Quantization Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../developer_notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../pt2e_quantization/index.html">
    PT2E Quantization Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_reference/index.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../eager_quantization/index.html">
    Eager Quantization Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../developer_notes/index.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../pt2e_quantization/index.html">
    PT2E Quantization Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.quan...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="torchao.quantization.quant_primitives">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.quantization.quant_primitives</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>

<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">auto</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.prototype.custom_fp_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_f32_to_floatx_unpacked</span><span class="p">,</span>
    <span class="n">_floatx_unpacked_to_f32</span><span class="p">,</span>
    <span class="n">_n_ones</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_register_custom_op</span><span class="p">,</span>
    <span class="n">_register_meta_op</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;choose_qparams_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_affine_with_min_max&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MappingType&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ZeroPointDomain&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TorchAODType&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_affine_tinygemm&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_affine_dont_preserve_zero&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_affine_floatx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_and_quantize_affine_hqq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_and_quantize_scale_only_hqq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_and_quantize_scale_only_sinq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_scale_float8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_choose_qparams_gguf&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_quantize_affine_no_zero_point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_quantize_affine_tinygemm&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_quantize_affine_floatx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_quantize_affine_float8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_quantize_gguf&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_dequantize_affine_no_zero_point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_dequantize_affine_tinygemm&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_dequantize_affine_floatx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_dequantize_affine_float8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_dequantize_gguf&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_fake_quantize_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_fake_quantize_affine_cachemask&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="MappingType"><a class="viewcode-back" href="../../../api_reference/generated/torchao.quantization.MappingType.html#torchao.quantization.MappingType">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">MappingType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;How floating point number is mapped to integer number</span>

<span class="sd">    symmetric mapping means floating point range is symmetrically mapped to integer range</span>
<span class="sd">    let&#39;s say we have floating point range (-3.5, 10.2) and integer range (-8, 7) (int4)</span>
<span class="sd">    we&#39;ll use (-10.2, 10.2) as the range for floating point and map that to (-8, 7)</span>
<span class="sd">    e.g. scale = (10.2 - (-10.2)) / (7 - (-8))</span>

<span class="sd">    SYMMETRIC_NO_CLIPPING_ERR is a variant of symmetric mapping, where the scale is the max of smin</span>
<span class="sd">    and smax, where smin = min_val_neg / quant_min, and smax = max_val_pos / quant_max. By calculating</span>
<span class="sd">    smin and smax individually, there can be less round error on negative values, and no out-of-range</span>
<span class="sd">    of all floating point values.</span>

<span class="sd">    asymmetric mapping means we just directly map the floating point range to integer range,</span>
<span class="sd">    for the above example, we will map (-3.5, 10.2) to (-8, 7) and calculate quantization parameter</span>
<span class="sd">    based on this mapping</span>
<span class="sd">    e.g. scale = (10.2 - (-3.5)) / (7 - (-8))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SYMMETRIC</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SYMMETRIC_NO_CLIPPING_ERR</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">ASYMMETRIC</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">ZeroPointDomain</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Enum that indicate whether zero_point is in integer domain or floating point domain</span>

<span class="sd">    integer domain: quantized_val = (float_val / scale) (integer) + zero_point (integer)</span>
<span class="sd">    float domain: quantized_val = (float_val - (zero_point (float) - scale * mid_point)) / scale</span>
<span class="sd">    none domain: quantized_val = (float_val / scale)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">INT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">FLOAT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<div class="viewcode-block" id="TorchAODType"><a class="viewcode-back" href="../../../api_reference/generated/torchao.quantization.TorchAODType.html#torchao.quantization.TorchAODType">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TorchAODType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Placeholder for dtypes that do not exist in PyTorch core yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># torch.int1 to torch.int7 will be added to PyTorch 2.6</span>
    <span class="c1"># These will remain here for BC with older PyTorch versions</span>
    <span class="n">INT1</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT2</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT3</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT4</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT5</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT6</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT7</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">MappingType</span><span class="p">,</span> <span class="n">ZeroPointDomain</span><span class="p">])</span>

<span class="n">FP8_TYPES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">,</span>
<span class="p">}</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Map from dtype to the bound value of integers</span>
<span class="sd">TODO: maybe can replace this with call to torch.iinfo</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">15</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">15</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT2</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT3</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT5</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT6</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT7</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_SUB_BYTE_INT_BOUNDS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT1</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT2</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT3</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT4</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT5</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT6</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT7</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">6</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">_SUB_BYTE_UINT_BOUNDS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint1</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint2</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint3</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint5</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint6</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint7</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">7</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint2</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint3</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint5</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint6</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint7</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">_SUB_BYTE_INT_BOUNDS</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int1</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int2</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int3</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int5</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int6</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int7</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">6</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int2</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int3</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int5</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int6</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int7</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">)</span>
<span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_SUB_BYTE_INT_BOUNDS</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="o">==</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

<span class="n">_GGUF_QK_K</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">_ONES_TABLE</span> <span class="o">=</span> <span class="p">[</span><span class="n">_n_ones</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)]</span>

<span class="n">quant_lib</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">Library</span><span class="p">(</span><span class="s2">&quot;torchao&quot;</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>

<span class="n">register_custom_op</span> <span class="o">=</span> <span class="n">_register_custom_op</span><span class="p">(</span><span class="n">quant_lib</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_Round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of generic round operation with backward STE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">gy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gy</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_RoundToFloat8</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of `tensor.to(float8_dtype)` with backward STE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">float8_dtype</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">gy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gy</span><span class="p">,</span> <span class="kc">None</span>


<span class="c1"># TODO: decide on if we want to allow custom quant_min/quant_max here</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get quant_min and quant_max args based on dtype and also verify bounds.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: Target quantization dtype (e.g., torch.uint8, torch.int8, or FP8 types)</span>
<span class="sd">        quant_min: Minimum quantized value, or None to use dtype default</span>
<span class="sd">        quant_max: Maximum quantized value, or None to use dtype default</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[int/float, int/float]: Validated (quant_min, quant_max) values</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If dtype is unsupported</span>
<span class="sd">        AssertionError: If quant_min/quant_max are out of bounds for dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">FP8_TYPES</span><span class="p">:</span>
        <span class="n">quant_min_lower_bound</span><span class="p">,</span> <span class="n">quant_max_upper_bound</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported dtype: </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quant_min_lower_bound</span><span class="p">,</span> <span class="n">quant_max_upper_bound</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">dtype</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">quant_min</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">quant_min</span> <span class="o">=</span> <span class="n">quant_min_lower_bound</span>
    <span class="k">if</span> <span class="n">quant_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">quant_max</span> <span class="o">=</span> <span class="n">quant_max_upper_bound</span>

    <span class="k">assert</span> <span class="n">quant_min</span> <span class="o">&gt;=</span> <span class="n">quant_min_lower_bound</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;quant_min out of bound for dtype, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;quant_min_lower_bound: </span><span class="si">{</span><span class="n">quant_min_lower_bound</span><span class="si">}</span><span class="s2"> quant_min: </span><span class="si">{</span><span class="n">quant_min</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="n">quant_max</span> <span class="o">&lt;=</span> <span class="n">quant_max_upper_bound</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;quant_max out of bound for dtype, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;quant_max_upper_bound: </span><span class="si">{</span><span class="n">quant_max_upper_bound</span><span class="si">}</span><span class="s2"> quant_max: </span><span class="si">{</span><span class="n">quant_max</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_reduction_params</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given block_size and input size find the parameters for reduction:</span>

<span class="sd">    Output:</span>
<span class="sd">        shape_for_reduction: the shape we use to `view` input to prepare it for reduction</span>
<span class="sd">        reduction_dims: the dims we&#39;ll do reduction over</span>

<span class="sd">    Example::</span>
<span class="sd">        Input:</span>
<span class="sd">          block_size: (3, 3, 2, 10)</span>
<span class="sd">          input_size: (3, 3, 10, 10)</span>

<span class="sd">        Output:</span>
<span class="sd">          shape_for_reduction: (3, 3, 5, 2, 10)</span>
<span class="sd">          reduction_dim: [0, 1, 3, 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
    <span class="n">shape_for_reduction</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">reduction_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cur_dim</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">and</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expecting input size at </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> dimension: </span><span class="si">{</span><span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> to be divisible by block_size at </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> dimension: </span><span class="si">{</span><span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="c1"># reduce over the block_size[i] dim</span>
            <span class="n">reduction_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_dim</span> <span class="o">+=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># block_size[i] == input_size[i] or block_size[i] == 1</span>
            <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="c1"># we only need to reduce over the dimension if block_size is greater than 1</span>
            <span class="c1"># otherwise it&#39;s already the same as reduced dimension</span>
            <span class="k">if</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">reduction_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_dim</span><span class="p">)</span>
            <span class="n">cur_dim</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span>


<div class="viewcode-block" id="quantize_affine"><a class="viewcode-back" href="../../../api_reference/generated/torchao.quantization.quantize_affine.html#torchao.quantization.quantize_affine">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): original float32, float16 or bfloat16 Tensor</span>
<span class="sd">      block_size: (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">           e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (float): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (int): quantization parameter for affine quantization</span>
<span class="sd">      output_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype</span>

<span class="sd">    Note:</span>
<span class="sd">      How can block_size represent different granularities?</span>
<span class="sd">      let&#39;s say we have a Tensor of size: (3, 3, 10, 10), here is the table showing how block_size represents different</span>
<span class="sd">      granularities:</span>

<span class="sd">       granularity type       |     block_size</span>
<span class="sd">         per_tensor           |    (3, 3, 10, 10)</span>
<span class="sd">         per_axis (axis=0)    |    (1, 3, 10, 10)</span>
<span class="sd">         per_axis (axis=1)    |    (3, 1, 10, 10)</span>
<span class="sd">     per_group (groupsize=2)  |    (3, 3, 10, 2)</span>
<span class="sd">     per_group (groupsize=2) for axis = 3 | (3, 3, 2, 10)</span>


<span class="sd">    Output:</span>
<span class="sd">      quantized tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize tensor using affine quantization with integer zero point domain.</span>

<span class="sd">    Op definition that has compatible signatures with custom op library.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        output_dtype: Target quantized dtype (e.g., torch.uint8, torch.int8)</span>
<span class="sd">        quant_min: Minimum quantized value, derived from dtype if None</span>
<span class="sd">        quant_max: Maximum quantized value, derived from dtype if None</span>

<span class="sd">    Returns:</span>
<span class="sd">        Quantized tensor with requested dtype</span>

<span class="sd">    Note:</span>
<span class="sd">        zero_point_domain is pre-defined as INT, meaning:</span>
<span class="sd">        quantized_val = (float_val / scale) (integer) + zero_point (integer)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="c1"># workaround for uintx dtypes, since we don&#39;t have native Uintx dtype connected with</span>
    <span class="c1"># torch.uintx dtypes yet</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">return</span> <span class="n">_quantize_affine_no_dtype_cast</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_no_dtype_cast</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize tensor using affine quantization without dtype casting.</span>

<span class="sd">    Performs quantization with integer zero point domain without casting to target dtype.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        quant_min: Minimum quantized value</span>
<span class="sd">        quant_max: Maximum quantized value</span>

<span class="sd">    Returns:</span>
<span class="sd">        Quantized tensor without dtype casting</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Quantize the input based on the quantization parameters scale and zero_point with zero_point_domain = INT</span>
<span class="sd">    3. Reshape the quantized result to original shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validations</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported input dtype: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># in some cases zero_point being a non-value shows as a tensor</span>
        <span class="c1"># with numel=0 which we handle by unifying the two</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span><span class="p">))</span> <span class="o">+</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span>
    <span class="p">)</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quant</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_tinygemm</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize tensor using affine quantization with float zero point domain for tinygemm.</span>

<span class="sd">    Specialized quantization for tinygemm int4mm kernel where zero point is in floating point domain.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        output_dtype: Target quantized dtype (e.g., torch.uint8, torch.int8)</span>
<span class="sd">        quant_min: Minimum quantized value, derived from dtype if None</span>
<span class="sd">        quant_max: Maximum quantized value, derived from dtype if None</span>

<span class="sd">    Returns:</span>
<span class="sd">        Quantized tensor with requested dtype</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Quantize the input based on the quantization parameters scale and zero_point with zero_point_domain = FLOAT</span>
<span class="sd">    3. Reshape the quantized result to original shape</span>

<span class="sd">    Note:</span>
<span class="sd">        zero_point_domain is pre-defined as FLOAT, meaning:</span>
<span class="sd">        quantized_val = (float_val - (zero_point (float) - scale * mid_point)) / scale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="c1"># workaround for uintx dtypes, since we don&#39;t have native Uintx dtype connected with</span>
    <span class="c1"># torch.uintx dtypes yet</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">return</span> <span class="n">_quantize_affine_tinygemm_no_dtype_cast</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_tinygemm_no_dtype_cast</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize tensor using affine quantization with float zero point domain without dtype casting.</span>

<span class="sd">    Specialized quantization for tinygemm int4mm kernel where zero point is in floating point domain.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        quant_min: Minimum quantized value</span>
<span class="sd">        quant_max: Maximum quantized value</span>

<span class="sd">    Returns:</span>
<span class="sd">        Quantized tensor without dtype casting</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Quantize the input based on the quantization parameters scale and zero_point with zero_point_domain = FLOAT</span>
<span class="sd">    3. Reshape the quantized result to original shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validations</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported input dtype: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># in some cases zero_point being a non-value shows as a tensor</span>
        <span class="c1"># with numel=0 which we handle by unifying the two</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">zero_point</span> <span class="o">-</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">mid_point</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">((</span><span class="nb">input</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span><span class="p">),</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quant</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_no_zero_point</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize tensor using affine quantization without zero point.</span>

<span class="sd">    Specialized quantization for cases where zero point is not needed (e.g., floatx quantization).</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (ignored, should be None)</span>
<span class="sd">        output_dtype: Target quantized dtype (e.g., torch.uint8, torch.int8)</span>
<span class="sd">        quant_min: Minimum quantized value, derived from dtype if None</span>
<span class="sd">        quant_max: Maximum quantized value, derived from dtype if None</span>

<span class="sd">    Returns:</span>
<span class="sd">        Quantized tensor with requested dtype</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Quantize the input based on the quantization parameters scale with zero_point_domain = NONE</span>
<span class="sd">    3. Reshape the quantized result to original shape</span>

<span class="sd">    Note:</span>
<span class="sd">        zero_point_domain is pre-defined as NONE, meaning:</span>
<span class="sd">        quantized_val = (float_val / scale) | This is primarily used for floatx quantization</span>
<span class="sd">        where we do not want to round values to nearest integer and instead scale and cast.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="c1"># workaround for uintx dtypes, since we don&#39;t have native Uintx dtype connected with</span>
    <span class="c1"># torch.uintx dtypes yet</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">return</span> <span class="n">_quantize_affine_no_zero_point_no_dtype_cast</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_no_zero_point_no_dtype_cast</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize tensor using affine quantization without zero point and without dtype casting.</span>

<span class="sd">    Specialized quantization for cases where zero point is not needed without casting to target dtype.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (ignored, should be None)</span>
<span class="sd">        quant_min: Minimum quantized value</span>
<span class="sd">        quant_max: Maximum quantized value</span>

<span class="sd">    Returns:</span>
<span class="sd">        Quantized tensor without dtype casting</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Quantize the input based on the quantization parameters scale with zero_point_domain = NONE</span>
<span class="sd">    3. Reshape the quantized result to original shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validations</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported input dtype: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># in some cases zero_point being a non-value shows as a tensor</span>
        <span class="c1"># with numel=0 which we handle by unifying the two</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)),</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quant</span>


<div class="viewcode-block" id="dequantize_affine"><a class="viewcode-back" href="../../../api_reference/generated/torchao.quantization.dequantize_affine.html#torchao.quantization.dequantize_affine">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument</span>
<span class="sd">      block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">                               e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for input Tensor</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for input Tensor</span>
<span class="sd">      output_dtype (torch.dtype): dtype for output Tensor, default is fp32</span>

<span class="sd">      Default value for zero_point is in integer domain, zero point is added to the quantized integer value during quantization</span>

<span class="sd">    Output:</span>
<span class="sd">      dequantized Tensor, with requested dtype or fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_dequantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">input_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dequantize tensor using affine dequantization with integer zero point domain.</span>

<span class="sd">    Op definition that has compatible signatures with custom op library.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Quantized tensor to dequantize</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        input_dtype: Expected dtype of input tensor (e.g., torch.uint8, torch.int8)</span>
<span class="sd">        quant_min: Minimum quantized value for input tensor</span>
<span class="sd">        quant_max: Maximum quantized value for input tensor</span>
<span class="sd">        output_dtype: Target output dtype (default: torch.float32)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dequantized tensor with requested output dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">, got: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported output dtype: </span><span class="si">{</span><span class="n">output_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_no_dtype_check</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_no_dtype_check</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dequantize tensor using affine dequantization without dtype checking.</span>

<span class="sd">    Converts quantized tensors to their high precision floating point representation.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Quantized tensor to dequantize</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        quant_min: Minimum quantized value for input tensor</span>
<span class="sd">        quant_max: Maximum quantized value for input tensor</span>
<span class="sd">        output_dtype: Target output dtype (default: torch.float32)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dequantized tensor with requested output dtype</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Dequantize the input based on the quantization parameters scale and zero_point</span>
<span class="sd">    3. Reshape the quantized result to original shape and change dtype to the output_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># Force a copy to avoid input modification due</span>
    <span class="c1"># to upcoming in-place operations.</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span> <span class="o">-</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="k">return</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_no_zero_point_no_dtype_check</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dequantize tensor using affine dequantization without zero point and without dtype checking.</span>

<span class="sd">    Converts quantized tensors to their high precision floating point representation without zero point.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Quantized tensor to dequantize</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (ignored, should be None)</span>
<span class="sd">        quant_min: Minimum quantized value for input tensor</span>
<span class="sd">        quant_max: Maximum quantized value for input tensor</span>
<span class="sd">        output_dtype: Target output dtype (default: torch.float32)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dequantized tensor with requested output dtype</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. Figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. Dequantize the input based on the quantization parameters scale (no zero point)</span>
<span class="sd">    3. Reshape the quantized result to original shape and change dtype to the output_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;zero_point should be None for _dequantize_affine_no_zero_point&quot;</span>
    <span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="k">return</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_no_zero_point</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument</span>
<span class="sd">      block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">                               e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (Tensor): quantization parameter for affine quantization, no zero point is used for this op</span>
<span class="sd">      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for input Tensor</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for input Tensor</span>
<span class="sd">      output_dtype (torch.dtype): dtype for output Tensor, default is fp32</span>

<span class="sd">      Default value for zero_point is in integer domain, zero point is added to the quantized integer value during quantization</span>

<span class="sd">    Output:</span>
<span class="sd">      dequantized Tensor, with requested dtype or fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">, got: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported output dtype: </span><span class="si">{</span><span class="n">output_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_no_zero_point_no_dtype_check</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_tinygemm_no_dtype_check</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function converts AQT tensors to their high precision floating point representation</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. dequantize the input based on the quantization parameters scale and zero_point and args like zero_point_domain</span>
<span class="sd">    3. reshape the quantized result to origianl shape and change dtype to the output_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># TODO: this seems to be a detail for tinygemm (converting from uint to int, probably need to refactor this)</span>
    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="c1"># This should allocate new memory and avoid input modification</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">mid_point</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">*=</span> <span class="n">scale</span>
    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dequant</span> <span class="o">+=</span> <span class="n">zero_point</span>

    <span class="k">return</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_tinygemm</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument</span>
<span class="sd">      block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">                               e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for input Tensor</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for input Tensor</span>
<span class="sd">      output_dtype (torch.dtype): dtype for output Tensor, default is fp32</span>

<span class="sd">      Default value for zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)</span>

<span class="sd">    Output:</span>
<span class="sd">      dequantized Tensor, with requested dtype or fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">, got: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported output dtype: </span><span class="si">{</span><span class="n">output_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_tinygemm_no_dtype_check</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_fake_quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General fake quantize op for quantization-aware training (QAT).</span>
<span class="sd">    This is equivalent to calling `quantize_affine` + `dequantize_affine`</span>
<span class="sd">    but without the dtype casts.</span>

<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): original float32, float16 or bfloat16 Tensor</span>
<span class="sd">      block_size: (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">           e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (float): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (int): quantization parameter for affine quantization</span>
<span class="sd">      quant_dtype (torch.dtype): desired quantized dtype for determining and validating quant_min and quant_max values.</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype</span>
<span class="sd">      zero_point_domain (ZeroPointDomain): the domain that zero_point is in, should be either integer or float</span>
<span class="sd">        if zero_point is in integer domain, zero point is added to the quantized integer value during</span>
<span class="sd">        quantization</span>
<span class="sd">        if zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)</span>
<span class="sd">        value during quantization</span>
<span class="sd">        default is ZeroPointDomain.INT</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please use ZeroPointDomain.NONE instead of None&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span> <span class="ow">and</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;zero_point should be None when zero_point_domain is NONE&quot;</span><span class="p">)</span>
    <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">fq</span><span class="p">)</span> <span class="o">=</span> <span class="n">_do_fake_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fq</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_fake_quantize_affine_cachemask</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General fake quantize op for quantization-aware training (QAT).</span>
<span class="sd">    This is equivalent to calling `quantize_affine` + `dequantize_affine`</span>
<span class="sd">    but without the dtype casts.</span>

<span class="sd">    Note: Compared to :func:`~torchao.quantization.quant_primitives._fake_quantize_affine`,</span>
<span class="sd">    this consumes more memory and returns an additional outlier mask for</span>
<span class="sd">    intermediate quantized values.</span>

<span class="sd">    Args:</span>
<span class="sd">      Same as :func:`~torchao.quantization.quant_primitives._fake_quantize_affine`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 2-tuple of (</span>
<span class="sd">          final fake quantized values,</span>
<span class="sd">          outlier mask for intermediate quantized values</span>
<span class="sd">      )</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please use ZeroPointDomain.NONE instead of None&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;zero_point should be None when zero_point_domain is NONE&quot;</span><span class="p">)</span>
    <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">dq</span><span class="p">)</span> <span class="o">=</span> <span class="n">_do_fake_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">((</span><span class="n">q</span> <span class="o">&gt;=</span> <span class="n">quant_min</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span> <span class="o">&lt;=</span> <span class="n">quant_max</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">dq</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_do_fake_quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function for fake quantization that returns both intermediate and final values.</span>

<span class="sd">    Performs quantization followed by dequantization without dtype casting, returning both</span>
<span class="sd">    the intermediate quantized values and the final dequantized values.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor to fake quantize (float32, float16, or bfloat16)</span>
<span class="sd">        block_size: Granularity of quantization - size of tensor elements sharing same qparam</span>
<span class="sd">        scale: Quantization scale parameter</span>
<span class="sd">        zero_point: Quantization zero point parameter (optional)</span>
<span class="sd">        quant_dtype: Target quantized dtype for determining quant_min/quant_max</span>
<span class="sd">        quant_min: Minimum quantized value, derived from dtype if None</span>
<span class="sd">        quant_max: Maximum quantized value, derived from dtype if None</span>
<span class="sd">        zero_point_domain: Domain of zero point (INT, FLOAT, or NONE)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of (intermediate quantized values, final dequantized values)</span>

<span class="sd">    Helper function for `_fake_quantize_affine` that returns both the</span>
<span class="sd">    intermediate quantized values and the final dequantized values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">quant_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">:</span>
        <span class="n">_quantize_affine</span> <span class="o">=</span> <span class="n">_quantize_affine_no_dtype_cast</span>
        <span class="n">_dequantize_affine</span> <span class="o">=</span> <span class="n">_dequantize_affine_no_dtype_check</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">:</span>
        <span class="n">_quantize_affine</span> <span class="o">=</span> <span class="n">_quantize_affine_tinygemm_no_dtype_cast</span>
        <span class="n">_dequantize_affine</span> <span class="o">=</span> <span class="n">_dequantize_affine_tinygemm_no_dtype_check</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
        <span class="n">_quantize_affine</span> <span class="o">=</span> <span class="n">_quantize_affine_no_zero_point_no_dtype_cast</span>
        <span class="n">_dequantize_affine</span> <span class="o">=</span> <span class="n">_dequantize_affine_no_zero_point_no_dtype_check</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unrecognized zero point domain: </span><span class="si">{</span><span class="n">zero_point_domain</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dq</span> <span class="o">=</span> <span class="n">_dequantize_affine</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">input_dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">dq</span><span class="p">)</span>


<div class="viewcode-block" id="choose_qparams_affine"><a class="viewcode-back" href="../../../api_reference/generated/torchao.quantization.choose_qparams_affine.html#torchao.quantization.choose_qparams_affine">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
    <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): fp32, bf16, fp16 input Tensor</span>
<span class="sd">        mapping_type (MappingType): determines how the qparams are calculated, symmetric or asymmetric</span>
<span class="sd">        block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">          e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">        target_dtype (torch.dtype): dtype for target quantized Tensor</span>
<span class="sd">        quant_min (Optional[int]): minimum quantized value for target quantized Tensor</span>
<span class="sd">        quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor</span>
<span class="sd">        eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype</span>
<span class="sd">        scale_dtype (torch.dtype): dtype for scale Tensor</span>
<span class="sd">        zero_point_dtype (torch.dtype): dtype for zero_point Tensor, defaults to torch.int32</span>
<span class="sd">        keepdim (bool): whether to keep dimensions with size 1 in output (aligned with _choose_scale_float8)</span>
<span class="sd">        Now removed params:</span>
<span class="sd">            zero_point_domain (ZeroPointDomain): the domain that zero_point is in, defaults to Integer or None</span>
<span class="sd">            preserve_zero (bool): whether to preserve zero in the quantized Tensor, defaults to True</span>

<span class="sd">    Output:</span>
<span class="sd">        Tuple of scales and zero_points Tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_choose_qparams_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="c1"># TODO: lower this op to custom op library</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_affine_tinygemm</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specialized version of choose_qparams_affine</span>

<span class="sd">    This is used for tinygemm int4mm kernel where zero point is in floating point domain</span>
<span class="sd">    and zero does not have to be exactly representable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): fp32, bf16, fp16 input Tensor</span>
<span class="sd">        mapping_type (MappingType): determines how the qparams are calculated, symmetric or asymmetric</span>
<span class="sd">        block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">        target_dtype (torch.dtype): dtype for target quantized Tensor</span>
<span class="sd">        quant_min (Optional[int]): minimum quantized value for target quantized Tensor</span>
<span class="sd">        quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor</span>
<span class="sd">        eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype</span>
<span class="sd">        scale_dtype (torch.dtype): dtype for scale Tensor</span>
<span class="sd">        zero_point_dtype (torch.dtype): dtype for zero_point Tensor</span>

<span class="sd">    Output:</span>
<span class="sd">        Tuple of scales and zero_points Tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="ow">is</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># For preserve_zero=False, we don&#39;t ensure zero is exactly representable</span>
    <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">min_val</span>
    <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">max_val</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

    <span class="c1"># For zero_point_domain=FLOAT in asymmetric quantization</span>
    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="c1"># this is not preserving zero_point, this is converting to TensorCoreTiledFormat</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">mid_point</span>

    <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>

    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span>


<span class="c1"># TODO: lower this op to custom op library</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_affine_dont_preserve_zero</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Specialized version of choose_qparams_affine with zero_point_domain=ZeroPointDomain.INT and preserve_zero=False.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): fp32, bf16, fp16 input Tensor</span>
<span class="sd">        mapping_type (MappingType): determines how the qparams are calculated, asymmetric only</span>
<span class="sd">        block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">        target_dtype (torch.dtype): dtype for target quantized Tensor</span>
<span class="sd">        quant_min (Optional[int]): minimum quantized value for target quantized Tensor</span>
<span class="sd">        quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor</span>
<span class="sd">        eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype</span>
<span class="sd">        scale_dtype (torch.dtype): dtype for scale Tensor</span>
<span class="sd">        zero_point_dtype (torch.dtype): dtype for zero_point Tensor</span>
<span class="sd">        Now removed params default values:</span>
<span class="sd">            zero_point_domain (ZeroPointDomain): the domain that zero_point is in, defaults to Integer</span>
<span class="sd">            preserve_zero (bool): whether to preserve zero in the quantized Tensor, defaults to False</span>

<span class="sd">    Output:</span>
<span class="sd">        Tuple of scales and zero_points Tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># For no preserve zero, we don&#39;t ensure zero is exactly representable</span>
    <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">min_val</span>
    <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">max_val</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="c1"># Zero point is int</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span>
    <span class="p">)</span>


<span class="c1"># TODO: lower this op to custom op library</span>
<div class="viewcode-block" id="choose_qparams_affine_with_min_max"><a class="viewcode-back" href="../../../api_reference/generated/torchao.quantization.choose_qparams_affine_with_min_max.html#torchao.quantization.choose_qparams_affine_with_min_max">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine_with_min_max</span><span class="p">(</span>
    <span class="n">min_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">max_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">preserve_zero</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A variant of :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`</span>
<span class="sd">    operator that pass in min_val and max_val directly instead of deriving these from a single input.</span>
<span class="sd">    This is used for observers in static quantization where min_val and max_val may be obtained through</span>
<span class="sd">    tracking all the data in calibration data set.</span>

<span class="sd">    Args:</span>
<span class="sd">      Mostly same as :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`. with one</span>
<span class="sd">      difference: instead of passing in `input` Tensor and use that to calculate min_val/max_val</span>
<span class="sd">      and then scale/zero_point, we pass in min_val/max_val directly</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please use ZeroPointDomain.NONE instead of None&quot;</span><span class="p">)</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">assert</span> <span class="n">min_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">max_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Need to provide `min_val` and `max_val`, got: {min_val, max_val}&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">max_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Expecting `min_val` and `max_val` to have the same dtype, got: {min_val.dtype, max_val.dtype}&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="n">scale_device</span> <span class="o">=</span> <span class="n">min_val</span><span class="o">.</span><span class="n">device</span>

    <span class="k">if</span> <span class="n">preserve_zero</span><span class="p">:</span>
        <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">min_val</span><span class="p">))</span>
        <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">max_val</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">min_val</span>
        <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">max_val</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="ow">or</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span>
    <span class="p">):</span>
        <span class="c1"># scales</span>
        <span class="k">if</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="n">min_val_neg</span><span class="p">,</span> <span class="n">max_val_pos</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span>
            <span class="c1"># calculate smin and smax individually and choose the larger one. For example, if quant_min = -8 and</span>
            <span class="c1"># quant_max = 7.</span>
            <span class="c1"># - If smin is bigger: There would be coverage on negative values down to -8, and less rounding</span>
            <span class="c1"># error than the existing SYMMETRIC case.</span>
            <span class="c1"># - If smax is bigger: it covers the positive values up to 7. The round</span>
            <span class="c1"># error may be bigger than the existing SYMMETRIC case. Either way, there&#39;s no out-of-range fp values after</span>
            <span class="c1"># quantization.</span>
            <span class="n">smin</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_min</span><span class="p">)</span>
            <span class="n">smax</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">smin</span> <span class="o">&gt;</span> <span class="n">smax</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">smin</span><span class="p">,</span> <span class="n">smax</span><span class="p">)</span>
        <span class="c1"># zeros</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">preserve_zero</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;preserve_zero == False is not supported for symmetric quantization&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">:</span>
            <span class="c1"># TODO INT should not be a valid ZeroPointDomain for symmetric quantization since</span>
            <span class="c1"># symmetric quant doesn&#39;t have a zero_point</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;zero_point_domain should be ZeroPointDomain.INT or ZeroPointDomain.NONE for symmetric quantization&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scale_device</span>
        <span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;zero_point must be in FLOAT/INT/None domain for asymmetric quantization&quot;</span>
            <span class="p">)</span>
            <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="c1"># this is not preserving zero_point, this is converting to TensorCoreTiledFormat</span>
            <span class="c1"># TODO move the conversion of zero_point out of quant_primitives</span>
            <span class="c1"># and into TensorCoreTiledLayout.from_plain</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">mid_point</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">min_val</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span></div>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;op definition that has compatible signatures with custom op library</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size</span>
<span class="sd">    2. find min_val/max_val based on the dimension for reduction</span>
<span class="sd">    3. calculate quantization parameters based on min_val/max_val based on args like `preserve_zero`</span>
<span class="sd">       and `zero_point_domain`</span>

<span class="sd">    Note:</span>
<span class="sd">        Set keepdim=True to align with _choose_scale_float8 behavior. This ensures</span>
<span class="sd">        scale/zero_point maintain the same rank as input, making it easier to handle downstream.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="c1"># Save original input size before reshaping for later use</span>
    <span class="n">original_input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>

    <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">min_val</span><span class="p">))</span>
    <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">max_val</span><span class="p">))</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="o">.</span><span class="n">name</span>
        <span class="ow">or</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="o">.</span><span class="n">name</span>
    <span class="p">):</span>
        <span class="c1"># scales</span>
        <span class="k">if</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
            <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="n">min_val_neg</span><span class="p">,</span> <span class="n">max_val_pos</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="o">.</span><span class="n">name</span>
            <span class="c1"># calculate smin and smax individually and choose the larger one. For example, if quant_min = -8 and</span>
            <span class="c1"># quant_max = 7.</span>
            <span class="c1"># - If smin is bigger: There would be coverage on negative values down to -8, and less rounding</span>
            <span class="c1"># error than the existing SYMMETRIC case.</span>
            <span class="c1"># - If smax is bigger: it covers the positive values up to 7. The round</span>
            <span class="c1"># error may be bigger than the existing SYMMETRIC case. Either way, there&#39;s no out-of-range fp values after</span>
            <span class="c1"># quantization.</span>
            <span class="n">smin</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_min</span><span class="p">)</span>
            <span class="n">smax</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">smin</span> <span class="o">&gt;</span> <span class="n">smax</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">smin</span><span class="p">,</span> <span class="n">smax</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="o">.</span><span class="n">name</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>

    <span class="c1"># Reshape scale and zero_point to match expected output shape</span>
    <span class="c1"># This aligns with _choose_scale_float8 behavior</span>
    <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">original_input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_gguf</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    There are two sets of qparams: quantized_block_scale, quantized_block_min and super_block_scale_scale and super_block_min_scale</span>
<span class="sd">    the relationship is the following:</span>
<span class="sd">    block_scale = quantized_block_scale * super_block_sclae</span>
<span class="sd">    block_min = quantized_block_min * super_block_min</span>
<span class="sd">    quantized_val = (float_val - block_min) / block_scale + quant_min</span>
<span class="sd">    first we calculate block_scale and block_min</span>
<span class="sd">    then we calculate super_block_scale_scale and super_block_min_scale</span>
<span class="sd">    after that we can calculate quantized_block_scale and quantized_min_scale</span>
<span class="sd">    the returned values are: super_block_scale_scale, super_block_min_scale, quantized_block_scale</span>
<span class="sd">    and quantized_min_scale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>

    <span class="c1"># 1. get block_scale block_min</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># asymmetric quant to fully utilize the range</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">max_val</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">min_val</span>

    <span class="c1"># 2. get super_block_scale_scale and super_block_min_scale</span>
    <span class="k">assert</span> <span class="n">_GGUF_QK_K</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">super_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_GGUF_QK_K</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">super_block_size</span><span class="p">,</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">block_scale_absmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">block_scale</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">block_min_absmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">block_min</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="c1"># 2. get super_block_scale_scale and super_block_min_scale</span>
    <span class="c1"># TODO: make this configurable</span>
    <span class="c1"># we also quantize the quantization parameters (scale and min) for each block to 6 bit</span>
    <span class="c1"># for Q4_K</span>
    <span class="n">qparam_quant_max</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">qparam_quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">super_block_scale_scale</span> <span class="o">=</span> <span class="n">block_scale_absmax</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">qparam_quant_max</span> <span class="o">-</span> <span class="n">qparam_quant_min</span>
    <span class="p">)</span>
    <span class="n">super_block_min_scale</span> <span class="o">=</span> <span class="n">block_min_absmax</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">qparam_quant_max</span> <span class="o">-</span> <span class="n">qparam_quant_min</span>
    <span class="p">)</span>
    <span class="n">super_block_scale_scale_view</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="n">super_block_min_scale_view</span> <span class="o">=</span> <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># 3. quantize block scale and min are stored in 6 bits using super_block_scale_scale and super_block_min_scale</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">block_scale</span> <span class="o">/</span> <span class="n">super_block_scale_scale_view</span><span class="p">,</span> <span class="n">qparam_quant_min</span><span class="p">,</span> <span class="n">qparam_quant_max</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">block_min</span> <span class="o">/</span> <span class="n">super_block_min_scale_view</span><span class="p">,</span> <span class="n">qparam_quant_min</span><span class="p">,</span> <span class="n">qparam_quant_max</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_gguf</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">super_block_scale_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">super_block_min_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_min</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">target_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span>

    <span class="c1"># step 1: first order quantization</span>
    <span class="c1"># just going through shape calculation for block_scale and block_min to get the correct shape</span>
    <span class="n">input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="n">input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_shape_for_reduction</span><span class="p">)</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># step 2: second order quantization, recover unquantized block_scale and block_min</span>
    <span class="n">super_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_GGUF_QK_K</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">super_block_input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">super_block_size</span><span class="p">,</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">super_block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">super_block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_scale_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_min_scale</span> <span class="o">=</span> <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>

    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span> <span class="o">*</span> <span class="n">quantized_block_scale</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">super_block_min_scale</span> <span class="o">*</span> <span class="n">quantized_block_min</span>

    <span class="c1"># step 3: quantization with the unquantized block_scale and block_min</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">int_data</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">block_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">block_scale</span>
    <span class="n">int_data</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">int_data</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_gguf</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">super_block_scale_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">super_block_min_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_min</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># step 1. reshape input and quantized block scale and min to the shape</span>
    <span class="c1"># after first quantization</span>
    <span class="n">input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="n">input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_shape_for_reduction</span><span class="p">)</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># step 2. calculate and reshape block_qparams for second quantization step</span>
    <span class="n">super_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_GGUF_QK_K</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">super_block_input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">super_block_size</span><span class="p">,</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">super_block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">super_block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_scale_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_min_scale</span> <span class="o">=</span> <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>

    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span> <span class="o">*</span> <span class="n">quantized_block_scale</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">super_block_min_scale</span> <span class="o">*</span> <span class="n">quantized_block_min</span>

    <span class="c1"># step 3. dequantize with block_scale and block_min</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">block_scale</span> <span class="o">+</span> <span class="n">block_min</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dequant</span>


<span class="c1"># HQQ</span>
<span class="c1">############################################################################</span>
<span class="c1"># Shrinking operator (proximal operator for the lp norm)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_shrink_lp_op</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">lp_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">lp_norm</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lp_norm</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>


<span class="c1"># Proximal solver || W - dequantize(quantize(W))||_p^p</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimize_weights_proximal_legacy</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">min_max</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">opt_params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;lp_norm&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">1e1</span><span class="p">,</span>
        <span class="s2">&quot;kappa&quot;</span><span class="p">:</span> <span class="mf">1.01</span><span class="p">,</span>
        <span class="s2">&quot;iters&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s2">&quot;early_stop&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="n">lp_norm</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">kappa</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">early_stop</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;lp_norm&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;kappa&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;iters&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;early_stop&quot;</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="p">(</span><span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>

    <span class="n">W_f</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="n">best_error</span> <span class="o">=</span> <span class="mf">1e4</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">W_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">W_f</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">min_max</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">min_max</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">W_r</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_q</span> <span class="o">-</span> <span class="n">zero</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>
        <span class="n">W_e</span> <span class="o">=</span> <span class="n">_shrink_lp_op</span><span class="p">(</span><span class="n">W_f</span> <span class="o">-</span> <span class="n">W_r</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">lp_norm</span><span class="p">)</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">W_q</span> <span class="o">-</span> <span class="p">(</span><span class="n">W_f</span> <span class="o">-</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">*=</span> <span class="n">kappa</span>

        <span class="n">current_error</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">W_f</span> <span class="o">-</span> <span class="n">W_r</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iter &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">&quot; | Error: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">current_error</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">early_stop</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">current_error</span> <span class="o">&lt;</span> <span class="n">best_error</span><span class="p">:</span>
                <span class="n">best_error</span> <span class="o">=</span> <span class="n">current_error</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">W_f</span><span class="p">,</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">W_r</span><span class="p">,</span> <span class="n">W_e</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="n">W_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tensor</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">min_max</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">min_max</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span>


<span class="c1"># Mainly used to check if the group-size is divisible by numel()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_is_divisible</span><span class="p">(</span><span class="n">val1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">val2</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">val2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">val1</span> <span class="o">/</span> <span class="n">val2</span><span class="p">))</span> <span class="o">==</span> <span class="n">val1</span>


<span class="c1"># Converts hqq format W_dequant = (W_q - zero)*scale into affinequantized format: (W_q - mid_point)*scale_ao + zero_ao</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_convert_to_affinequantized_format</span><span class="p">(</span>
    <span class="n">W_q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">nbits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">nbits</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">zero_ao</span> <span class="o">=</span> <span class="p">((</span><span class="n">mid_point</span> <span class="o">-</span> <span class="n">zero</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">scale_ao</span> <span class="o">=</span> <span class="n">scale</span>
    <span class="n">W_q_ao</span> <span class="o">=</span> <span class="n">W_q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W_q_ao</span><span class="p">,</span> <span class="n">scale_ao</span><span class="p">,</span> <span class="n">zero_ao</span>


<span class="c1"># Main hqq quantizer function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_and_quantize_affine_hqq</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">nbits</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">optimize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">compute_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># to check the optimizer error</span>
    <span class="n">raw_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># If True, it will return the quant params in hqq lib format</span>
    <span class="n">optimize_weights</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">optimize_weights_proximal_legacy</span><span class="p">,</span>  <span class="c1"># weights proximal optimizer function</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Choose quantization parameters and quantize tensor using HQQ (Half-Quadratic Quantization).</span>

<span class="sd">    Performs quantization using HQQ method with optional weight optimization via proximal solver.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        nbits: Number of bits for quantization (default: 4)</span>
<span class="sd">        group_size: Size of quantization groups (default: 64)</span>
<span class="sd">        optimize: Whether to optimize weights using proximal solver (default: True)</span>
<span class="sd">        axis: Axis along which to perform quantization (0 or 1, default: 1)</span>
<span class="sd">        compute_dtype: Target compute dtype (default: torch.float16)</span>
<span class="sd">        device: Target device for computation (default: &quot;cuda&quot;)</span>
<span class="sd">        verbose: Whether to print optimization error information (default: False)</span>
<span class="sd">        raw_output: If True, return params in HQQ library format (default: False)</span>
<span class="sd">        optimize_weights: Weight optimization function (default: optimize_weights_proximal_legacy)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of (quantized_weights, scale, zero_point, original_shape)</span>

<span class="sd">    Note:</span>
<span class="sd">        Uses proximal solver to minimize ||W - dequantize(quantize(W))||_p^p for weight optimization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;axis should be either 0 or 1&quot;</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">_is_divisible</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">group_size</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;group_size should be divisble by the total tensor dimensions. shape: &quot;</span>
            <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="o">+</span> <span class="s2">&quot;, group_size: &quot;</span>
            <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># It&#39;s better to work with float32 here</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Reshape for grouping</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="n">W</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">group_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get min/max values</span>
    <span class="n">_min</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">_max</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">max_v</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">nbits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">min_v</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">min_max</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_v</span><span class="p">,</span> <span class="n">max_v</span><span class="p">]</span>

    <span class="c1"># Clamp to avoid fp16 issues</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_v</span> <span class="o">/</span> <span class="p">(</span><span class="n">_max</span> <span class="o">-</span> <span class="n">_min</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="mf">2e4</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="o">-</span><span class="n">_min</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># Round zero as in: https://github.com/casper-hansen/AutoAWQ/blob/main/awq/quantize/quantizer.py#L42C9-L42C14</span>
    <span class="k">if</span> <span class="n">nbits</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">zero</span><span class="p">)</span>

    <span class="c1"># Fine-tune weights</span>
    <span class="k">if</span> <span class="n">optimize</span><span class="p">:</span>
        <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span> <span class="o">=</span> <span class="n">optimize_weights</span><span class="p">(</span>
            <span class="n">tensor</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">zero</span><span class="o">=</span><span class="n">zero</span><span class="p">,</span>
            <span class="n">min_max</span><span class="o">=</span><span class="n">min_max</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="n">W_q</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">min_max</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">min_max</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Store meta-data (we invert the scale for dequantization)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span>

    <span class="c1"># Convert to TensorCoreTiled format</span>
    <span class="c1"># TODO move the conversion of zero_point out of quant_primitives</span>
    <span class="c1"># and into TensorCoreTiledLayout.from_plain and rename this</span>
    <span class="c1"># helper function correctly.</span>
    <span class="k">if</span> <span class="n">raw_output</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span> <span class="o">=</span> <span class="n">_convert_to_affinequantized_format</span><span class="p">(</span>
            <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">shape</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># this path was not used before, the way hqq sets up scale/zero is transposed</span>
        <span class="c1"># compared to the rest of our utils so we need to reshape them acccordingly.</span>
        <span class="n">W_q</span> <span class="o">=</span> <span class="n">W_q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Make sure all the weights are in the right compute_dtype/device</span>
    <span class="n">W_q</span> <span class="o">=</span> <span class="n">W_q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># cleanup</span>
    <span class="k">del</span> <span class="n">W</span><span class="p">,</span> <span class="n">_min</span><span class="p">,</span> <span class="n">_max</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">shape</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_and_quantize_scale_only_hqq</span><span class="p">(</span>
    <span class="n">hp_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">qmin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">qmax</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">stochastic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">early_stop_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Half-Quadratic Quantization (scale-only, symmetric) for 2D weights with row-wise blocks.</span>
<span class="sd">    - hp_tensor: [out, in] (bf16/fp16/fp32 accepted; promoted to fp32 internally)</span>
<span class="sd">    - block_size: must be [1, group_size]; groups along the last dim</span>
<span class="sd">    - qmin, qmax: integer range (e.g., -8, 7 for signed 4-bit)</span>
<span class="sd">    Returns:</span>
<span class="sd">      qdata: int32, same shape as hp_tensor</span>
<span class="sd">      scale: hp_tensor.dtype, shape [out, in // group_size] (one scale per row-wise block)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- strict interface guarantees ---</span>
    <span class="k">assert</span> <span class="n">hp_tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;hp_tensor must be 2D [out, in]&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;block_size must be a 2-element list/tuple&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;block_size must be [1, group_size] with group_size &gt;= 1&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">qmin</span> <span class="o">&lt;</span> <span class="n">qmax</span><span class="p">,</span> <span class="s2">&quot;qmin must be &lt; qmax&quot;</span>

    <span class="c1"># Promote to fp32 for stable math</span>
    <span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">compute_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">hp_tensor</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">k</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> must be divisible by group_size=</span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">round_det</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># ties-to-even; fine for PTQ</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">round_stoch</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># unbiased stochastic rounding</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">_r</span> <span class="o">=</span> <span class="n">round_stoch</span> <span class="k">if</span> <span class="n">stochastic</span> <span class="k">else</span> <span class="n">round_det</span>

    <span class="c1"># Reshape Wg into [n, n_groups, group_size]</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">hp_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">n_groups</span> <span class="o">=</span> <span class="n">k</span> <span class="o">//</span> <span class="n">group_size</span>
    <span class="n">Wg</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="c1"># Initialize per-block scales as max-abs / qabs</span>
    <span class="c1"># scale.shape = [n, n_groups]</span>
    <span class="n">qabs</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">qmin</span><span class="p">),</span> <span class="nb">abs</span><span class="p">(</span><span class="n">qmax</span><span class="p">))</span> <span class="ow">or</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">Wg</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">qabs</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">compute_eps</span><span class="p">)</span>
    <span class="n">prev_scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

    <span class="c1"># Iterate HQQ updates</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iters</span><span class="p">)):</span>
        <span class="c1"># Quantize using current scale</span>
        <span class="c1"># Qg.shape = [n, n_groups, group_size]</span>
        <span class="n">Qg</span> <span class="o">=</span> <span class="n">_r</span><span class="p">(</span><span class="n">Wg</span> <span class="o">/</span> <span class="n">scale</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>

        <span class="c1"># Solve least-square problem min_{s} ||Wg - s * Qg||^2 and project</span>
        <span class="c1"># solution onto positive space, or take previous value</span>
        <span class="n">num</span> <span class="o">=</span> <span class="p">(</span><span class="n">Wg</span> <span class="o">*</span> <span class="n">Qg</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># [n, n_groups]</span>
        <span class="n">den</span> <span class="o">=</span> <span class="p">(</span><span class="n">Qg</span> <span class="o">*</span> <span class="n">Qg</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># [n, n_groups]</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">den</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span><span class="p">,</span> <span class="n">prev_scale</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span>
            <span class="n">compute_eps</span>
        <span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>  <span class="c1"># project LS solution onto [eps, inf]</span>

        <span class="n">rel</span> <span class="o">=</span> <span class="p">((</span><span class="n">scale</span> <span class="o">-</span> <span class="n">prev_scale</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">/</span> <span class="n">prev_scale</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">compute_eps</span><span class="p">))</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">rel</span> <span class="o">&lt;</span> <span class="n">early_stop_tol</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">prev_scale</span> <span class="o">=</span> <span class="n">scale</span>

    <span class="c1"># Quantize using final scale</span>
    <span class="n">Qg</span> <span class="o">=</span> <span class="n">_r</span><span class="p">(</span><span class="n">Wg</span> <span class="o">/</span> <span class="n">scale</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>

    <span class="c1"># Restore shapes</span>
    <span class="n">qdata</span> <span class="o">=</span> <span class="n">Qg</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">out_dtype</span> <span class="o">=</span> <span class="n">hp_tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">out_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">qdata</span><span class="p">,</span> <span class="n">scale</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_and_quantize_scale_only_sinq</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">qmin</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">qmax</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">niter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">compute_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SINQ: Sinkhorn-Normalized Quantization (https://www.arxiv.org/abs/2509.22944)</span>

<span class="sd">    Iteratively normalizes row and column standard deviations to minimize</span>
<span class="sd">    matrix imbalance before quantization with dual scales.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor: Input weight tensor</span>
<span class="sd">        group_size: Quantization group size (default: 64)</span>
<span class="sd">        niter: Number of Sinkhorn iterations (default: 20)</span>
<span class="sd">        compute_dtype: Target compute dtype (default: torch.float16)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of (qdata, scale_row, scale_col)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">_is_divisible</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">group_size</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;group_size must divide tensor elements. shape: </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, group_size: </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Reshape for 1D tiling</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>  <span class="c1"># [N*num_groups, group_size]</span>

    <span class="c1"># Algorithm 1: Sinkhorn Normalization</span>
    <span class="n">q_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">W</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">q_min</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">q_min</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="n">W_hat</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">scale_col_sinkhorn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">W</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">)</span>
    <span class="n">scale_row_sinkhorn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">W</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="c1"># Normalize columns (dim=0)</span>
        <span class="n">q_col</span> <span class="o">=</span> <span class="n">W_hat</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_min</span>
        <span class="n">q_col</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">q_col</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
        <span class="n">W_hat</span> <span class="o">=</span> <span class="n">W_hat</span> <span class="o">/</span> <span class="n">q_col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">scale_col_sinkhorn</span> <span class="o">=</span> <span class="n">scale_col_sinkhorn</span> <span class="o">*</span> <span class="n">q_col</span>

        <span class="c1"># Normalize rows (dim=1)</span>
        <span class="n">q_row</span> <span class="o">=</span> <span class="n">W_hat</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_min</span>
        <span class="n">q_row</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">q_row</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
        <span class="n">W_hat</span> <span class="o">=</span> <span class="n">W_hat</span> <span class="o">/</span> <span class="n">q_row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scale_row_sinkhorn</span> <span class="o">=</span> <span class="n">scale_row_sinkhorn</span> <span class="o">*</span> <span class="n">q_row</span>

    <span class="c1"># INT8 symmetric quantization</span>
    <span class="c1"># TODO: Consider custom bitwidth for SIMD acceleration like vadd4</span>
    <span class="n">scale_s</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_hat</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">qmax</span><span class="p">))</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">)</span>
    <span class="c1"># TODO: Find better rounding strategy like stochastic rounding</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_hat</span> <span class="o">/</span> <span class="n">scale_s</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>
    <span class="c1"># TODO: PERF test for scale factor dtype (FP16 vs. INT8)</span>
    <span class="c1"># Although FP16 has high accuracy, FP16×INT8 can&#39;t be computed</span>
    <span class="c1"># in Tensor Core directly, requiring INT8 to FP16 ops.</span>
    <span class="n">qdata</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

    <span class="c1"># Combine RTN scale with row Sinkhorn factor</span>
    <span class="n">scale_row</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">scale_s</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_row_sinkhorn</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">group_size</span>
    <span class="n">scale_col</span> <span class="o">=</span> <span class="n">scale_col_sinkhorn</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">num_groups</span><span class="p">)[:</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">qdata</span><span class="p">,</span> <span class="n">scale_row</span><span class="p">,</span> <span class="n">scale_col</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_affine_floatx</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Choose quantization parameters for floatx quantization.</span>

<span class="sd">    Calculates scale parameter for quantizing to custom floating point format.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor: Input tensor to quantize (float32, float16, or bfloat16)</span>
<span class="sd">        ebits: Number of exponent bits in target floatx format</span>
<span class="sd">        mbits: Number of mantissa bits in target floatx format</span>

<span class="sd">    Returns:</span>
<span class="sd">        Scale tensor for floatx quantization</span>

<span class="sd">    Note:</span>
<span class="sd">        Uses global lookup table as workaround for torch.compile() compatibility</span>
<span class="sd">        since _n_ones() is not compatible due to &lt;&lt; operator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># _n_ones() is not compatible with torch.compile() due to &lt;&lt; operator</span>
    <span class="c1"># https://github.com/pytorch/pytorch/issues/119152</span>
    <span class="c1"># exp_bias = _n_ones(ebits - 1)</span>
    <span class="c1"># max_normal = 2 ** (_n_ones(ebits) - exp_bias) * (_n_ones(mbits + 1) / (2 ** mbits))</span>

    <span class="c1"># workaround: global lookup table</span>
    <span class="n">exp_bias</span> <span class="o">=</span> <span class="n">_ONES_TABLE</span><span class="p">[</span><span class="n">ebits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">max_normal</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">_ONES_TABLE</span><span class="p">[</span><span class="n">ebits</span><span class="p">]</span> <span class="o">-</span> <span class="n">exp_bias</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">_ONES_TABLE</span><span class="p">[</span><span class="n">mbits</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">mbits</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span> <span class="o">/</span> <span class="n">max_normal</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_floatx</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantizes the float32 high precision floating point tensor to low precision floating point number and</span>
<span class="sd">    converts the result to unpacked floating point format with the format of 00SEEEMM (for fp6_e3m2) where S means sign bit, e means exponent bit and m means mantissa bit</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">tensor_floatx</span> <span class="o">=</span> <span class="n">_f32_to_floatx_unpacked</span><span class="p">(</span><span class="n">tensor</span> <span class="o">/</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_floatx</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_floatx</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">_floatx_unpacked_to_f32</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_scale_float8</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">hp_value_lb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hp_value_ub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates float8 scaling factor for the given high precision tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (torch.Tensor): Input tensor to be quantized.</span>
<span class="sd">        float8_dtype (torch.dtype): Data type of the quantized tensor (e.g., torch.float8_e4m3fn, torch.float8_e5m2).</span>
<span class="sd">        scale_dtype (torch.dtype): Data type of the scaling factor (e.g., torch.float32).</span>
<span class="sd">        block_size (Tuple[int, ...]): Block size for block-wise quantization. For per-tensor scaling, use block_size == tensor.shape.</span>
<span class="sd">        hp_value_lb (Optional[float]): the lower bound for high precision floating point value for calculating scale</span>
<span class="sd">        hp_value_ub (Optional[float]): the upper bound for high precision floating point value for calculating scale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">float8_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
    <span class="p">)</span>
    <span class="n">tensor_reshaped</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">max_abs</span> <span class="o">=</span> <span class="n">tensor_reshaped</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hp_value_lb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">hp_value_ub</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">max_abs</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">hp_value_lb</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">hp_value_ub</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">max_abs</span> <span class="o">/</span> <span class="n">quant_max</span>
    <span class="c1"># Reshape scale back to match the expected output shape</span>
    <span class="c1"># The scale tensor should have the same shape as the input divided by block_size</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">input_size</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">input_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="c1"># Shielding for Version &gt; 2.8</span>
        <span class="k">assert</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e8m0fnu</span><span class="p">,</span> <span class="s2">&quot;Only float8_e8m0fnuz is supported&quot;</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp2</span><span class="p">(</span><span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">scale</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_maybe_expand_scale_to_tensor_shape</span><span class="p">(</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expand a scale tensor to match the target tensor shape for block-wise quantization.</span>
<span class="sd">    If this is rowwise quantization, however, just return the scale as is.</span>

<span class="sd">    Args:</span>
<span class="sd">        scale (torch.Tensor): Scale tensor with shape corresponding to block structure</span>
<span class="sd">        target_shape (torch.Size): Target tensor shape to expand to</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Scale tensor expanded to match target_shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target_shape</span><span class="p">:</span>
        <span class="c1"># Scale already matches target shape</span>
        <span class="k">return</span> <span class="n">scale</span>

    <span class="k">if</span> <span class="n">scale</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Scalar scale - can broadcast naturally</span>
        <span class="k">return</span> <span class="n">scale</span>

    <span class="c1"># If the scale can be broadcast as is, then we don&#39;t need to expand it</span>
    <span class="c1"># E.g. for rowwise quantization, scale = [256, 1] and target_shape = [256, 512]</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">scale</span>

    <span class="c1"># Calculate block sizes from shape difference</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Scale tensor has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dimensions but target has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">block_sizes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
        <span class="n">target_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">//</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="c1"># Verify that target_shape is evenly divisible by scale.shape</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">target_dim</span><span class="p">,</span> <span class="n">scale_dim</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">target_shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">block_sizes</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">target_dim</span> <span class="o">!=</span> <span class="n">scale_dim</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Dimension </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: target size </span><span class="si">{</span><span class="n">target_dim</span><span class="si">}</span><span class="s2"> is not evenly divisible &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;by scale size </span><span class="si">{</span><span class="n">scale_dim</span><span class="si">}</span><span class="s2"> (block size would be </span><span class="si">{</span><span class="n">target_dim</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">scale_dim</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>

    <span class="c1"># Expand scale using repeat_interleave</span>
    <span class="n">expanded_scale</span> <span class="o">=</span> <span class="n">scale</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">block_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">block_sizes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">block_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">expanded_scale</span> <span class="o">=</span> <span class="n">expanded_scale</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">expanded_scale</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_float8</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizes the high precision floating point tensor to a float8 tensor, using the given scaling factor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_fp32</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Expand scale to match tensor dimensions for block-wise quantization</span>
    <span class="n">scale_expanded</span> <span class="o">=</span> <span class="n">_maybe_expand_scale_to_tensor_shape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">tensor_scaled</span> <span class="o">=</span> <span class="n">tensor_fp32</span> <span class="o">/</span> <span class="n">scale_expanded</span>
    <span class="n">max_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">float8_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
    <span class="n">tensor_clamped</span> <span class="o">=</span> <span class="n">tensor_scaled</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="n">max_value</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">max_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_RoundToFloat8</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tensor_clamped</span><span class="p">,</span> <span class="n">float8_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_float8</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dequantizes the float8 tensor to high precision tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fp8_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Expand scale to match tensor dimensions for block-wise quantization</span>
    <span class="n">scale_expanded</span> <span class="o">=</span> <span class="n">_maybe_expand_scale_to_tensor_shape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">hp_tensor</span> <span class="o">=</span> <span class="n">fp8_tensor</span> <span class="o">*</span> <span class="n">scale_expanded</span>
    <span class="k">return</span> <span class="n">hp_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="nd">@_register_custom_op</span><span class="p">(</span><span class="n">quant_lib</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_float8_non_decomposed</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizes the high precision floating point tensor to a float8 tensor, using the given scaling factor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_quantize_affine_float8</span><span class="p">(</span>
        <span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">float8_dtype</span><span class="o">=</span><span class="n">float8_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="nd">@_register_meta_op</span><span class="p">(</span><span class="n">quant_lib</span><span class="p">,</span> <span class="s2">&quot;quantize_affine_float8_non_decomposed&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_float8_meta</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float8_dtype</span><span class="p">)</span>


<span class="nd">@_register_custom_op</span><span class="p">(</span><span class="n">quant_lib</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_float8_non_decomposed</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dequantizes the float8 tensor to high precision tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_float8</span><span class="p">(</span>
        <span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="nd">@_register_meta_op</span><span class="p">(</span><span class="n">quant_lib</span><span class="p">,</span> <span class="s2">&quot;dequantize_affine_float8_non_decomposed&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_float8_meta</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">)</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.quantization.quant_primitives",
       "headline": "torchao.quantization.quant_primitives",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/torchao/quantization/quant_primitives.html",
       "articleBody": "Source code for torchao.quantization.quant_primitives # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # This source code is licensed under the license found in the # LICENSE file in the root directory of this source tree. import math from enum import Enum, auto from typing import Callable, Dict, List, Optional, Tuple, Union import torch from torchao.prototype.custom_fp_utils import ( _f32_to_floatx_unpacked, _floatx_unpacked_to_f32, _n_ones, ) from torchao.utils import ( _register_custom_op, _register_meta_op, ) __all__ = [ \"choose_qparams_affine\", \"choose_qparams_affine_with_min_max\", \"quantize_affine\", \"dequantize_affine\", \"MappingType\", \"ZeroPointDomain\", \"TorchAODType\", \"_choose_qparams_affine_tinygemm\", \"_choose_qparams_affine_dont_preserve_zero\", \"_choose_qparams_affine_floatx\", \"_choose_qparams_and_quantize_affine_hqq\", \"_choose_qparams_and_quantize_scale_only_hqq\", \"_choose_qparams_and_quantize_scale_only_sinq\", \"_choose_scale_float8\", \"_choose_qparams_gguf\", \"_quantize_affine_no_zero_point\", \"_quantize_affine_tinygemm\", \"_quantize_affine_floatx\", \"_quantize_affine_float8\", \"_quantize_gguf\", \"_dequantize_affine_no_zero_point\", \"_dequantize_affine_tinygemm\", \"_dequantize_affine_floatx\", \"_dequantize_affine_float8\", \"_dequantize_gguf\", \"_fake_quantize_affine\", \"_fake_quantize_affine_cachemask\", ] [docs]class MappingType(Enum): \"\"\"How floating point number is mapped to integer number symmetric mapping means floating point range is symmetrically mapped to integer range let\u0027s say we have floating point range (-3.5, 10.2) and integer range (-8, 7) (int4) we\u0027ll use (-10.2, 10.2) as the range for floating point and map that to (-8, 7) e.g. scale = (10.2 - (-10.2)) / (7 - (-8)) SYMMETRIC_NO_CLIPPING_ERR is a variant of symmetric mapping, where the scale is the max of smin and smax, where smin = min_val_neg / quant_min, and smax = max_val_pos / quant_max. By calculating smin and smax individually, there can be less round error on negative values, and no out-of-range of all floating point values. asymmetric mapping means we just directly map the floating point range to integer range, for the above example, we will map (-3.5, 10.2) to (-8, 7) and calculate quantization parameter based on this mapping e.g. scale = (10.2 - (-3.5)) / (7 - (-8)) \"\"\" SYMMETRIC = auto() SYMMETRIC_NO_CLIPPING_ERR = auto() ASYMMETRIC = auto() class ZeroPointDomain(Enum): \"\"\"Enum that indicate whether zero_point is in integer domain or floating point domain integer domain: quantized_val = (float_val / scale) (integer) + zero_point (integer) float domain: quantized_val = (float_val - (zero_point (float) - scale * mid_point)) / scale none domain: quantized_val = (float_val / scale) \"\"\" INT = auto() FLOAT = auto() NONE = auto() [docs]class TorchAODType(Enum): \"\"\" Placeholder for dtypes that do not exist in PyTorch core yet. \"\"\" # torch.int1 to torch.int7 will be added to PyTorch 2.6 # These will remain here for BC with older PyTorch versions INT1 = auto() INT2 = auto() INT3 = auto() INT4 = auto() INT5 = auto() INT6 = auto() INT7 = auto() torch.serialization.add_safe_globals([MappingType, ZeroPointDomain]) FP8_TYPES = { torch.float8_e4m3fn, torch.float8_e5m2, torch.float8_e4m3fnuz, torch.float8_e5m2fnuz, } \"\"\" Map from dtype to the bound value of integers TODO: maybe can replace this with call to torch.iinfo \"\"\" _DTYPE_TO_QVALUE_BOUNDS: Dict[Union[torch.dtype, TorchAODType], Tuple[int, int]] = { torch.uint8: (0, 255), torch.int8: (-128, 127), torch.int16: (-(2**15), 2**15 - 1), torch.int32: (-(2**31), 2**31 - 1), } _DTYPE_TO_BIT_WIDTH: Dict[Union[torch.dtype, TorchAODType], Tuple[int, int]] = { TorchAODType.INT1: 1, TorchAODType.INT2: 2, TorchAODType.INT3: 3, TorchAODType.INT4: 4, TorchAODType.INT5: 5, TorchAODType.INT6: 6, TorchAODType.INT7: 7, torch.uint8: 8, torch.int8: 8, torch.int16: 16, torch.int32: 32, } _SUB_BYTE_UINT_BOUNDS: Dict[Union[torch.dtype, TorchAODType], Tuple[int, int]] = {} _SUB_BYTE_INT_BOUNDS: Dict[Union[torch.dtype, TorchAODType], Tuple[int, int]] = { TorchAODType.INT1: (-(2**0), 2**0 - 1), TorchAODType.INT2: (-(2**1), 2**1 - 1), TorchAODType.INT3: (-(2**2), 2**2 - 1), TorchAODType.INT4: (-(2**3), 2**3 - 1), TorchAODType.INT5: (-(2**4), 2**4 - 1), TorchAODType.INT6: (-(2**5), 2**5 - 1), TorchAODType.INT7: (-(2**6), 2**6 - 1), } _SUB_BYTE_UINT_BOUNDS = { torch.uint1: (0, 2**1 - 1), torch.uint2: (0, 2**2 - 1), torch.uint3: (0, 2**3 - 1), torch.uint4: (0, 2**4 - 1), torch.uint5: (0, 2**5 - 1), torch.uint6: (0, 2**6 - 1), torch.uint7: (0, 2**7 - 1), } _DTYPE_TO_BIT_WIDTH.update( { torch.uint1: 1, torch.uint2: 2, torch.uint3: 3, torch.uint4: 4, torch.uint5: 5, torch.uint6: 6, torch.uint7: 7, } ) _SUB_BYTE_INT_BOUNDS.update( { torch.int1: (-(2**0), 2**0 - 1), torch.int2: (-(2**1), 2**1 - 1), torch.int3: (-(2**2), 2**2 - 1), torch.int4: (-(2**3), 2**3 - 1), torch.int5: (-(2**4), 2**4 - 1), torch.int6: (-(2**5), 2**5 - 1), torch.int7: (-(2**6), 2**6 - 1), } ) _DTYPE_TO_BIT_WIDTH.update( { torch.int1: 1, torch.int2: 2, torch.int3: 3, torch.int4: 4, torch.int5: 5, torch.int6: 6, torch.int7: 7, } ) _DTYPE_TO_QVALUE_BOUNDS.update(_SUB_BYTE_UINT_BOUNDS) _DTYPE_TO_QVALUE_BOUNDS.update(_SUB_BYTE_INT_BOUNDS) assert _DTYPE_TO_BIT_WIDTH.keys() == _DTYPE_TO_QVALUE_BOUNDS.keys() _GGUF_QK_K = 256 _ONES_TABLE = [_n_ones(i) for i in range(8)] quant_lib = torch.library.Library(\"torchao\", \"FRAGMENT\") register_custom_op = _register_custom_op(quant_lib) class _Round(torch.autograd.Function): \"\"\" Implementation of generic round operation with backward STE. \"\"\" @staticmethod def forward(ctx, x: torch.Tensor) -\u003e torch.Tensor: return torch.round(x) @staticmethod def backward(ctx, gy: torch.Tensor) -\u003e torch.Tensor: return gy class _RoundToFloat8(torch.autograd.Function): \"\"\" Implementation of `tensor.to(float8_dtype)` with backward STE. \"\"\" @staticmethod def forward(ctx, x: torch.Tensor, float8_dtype: torch.dtype) -\u003e torch.Tensor: return x.to(float8_dtype) @staticmethod def backward(ctx, gy: torch.Tensor) -\u003e torch.Tensor: return gy, None # TODO: decide on if we want to allow custom quant_min/quant_max here def _get_and_check_qmin_qmax(dtype, quant_min, quant_max): \"\"\"Get quant_min and quant_max args based on dtype and also verify bounds. Args: dtype: Target quantization dtype (e.g., torch.uint8, torch.int8, or FP8 types) quant_min: Minimum quantized value, or None to use dtype default quant_max: Maximum quantized value, or None to use dtype default Returns: Tuple[int/float, int/float]: Validated (quant_min, quant_max) values Raises: ValueError: If dtype is unsupported AssertionError: If quant_min/quant_max are out of bounds for dtype \"\"\" if dtype in FP8_TYPES: quant_min_lower_bound, quant_max_upper_bound = ( torch.finfo(dtype).min, torch.finfo(dtype).max, ) elif dtype not in _DTYPE_TO_QVALUE_BOUNDS: raise ValueError(f\"Unsupported dtype: {dtype}\") else: quant_min_lower_bound, quant_max_upper_bound = _DTYPE_TO_QVALUE_BOUNDS[dtype] if quant_min is None: quant_min = quant_min_lower_bound if quant_max is None: quant_max = quant_max_upper_bound assert quant_min \u003e= quant_min_lower_bound, ( \"quant_min out of bound for dtype, \" f\"quant_min_lower_bound: {quant_min_lower_bound} quant_min: {quant_min}\" ) assert quant_max \u003c= quant_max_upper_bound, ( \"quant_max out of bound for dtype, \" f\"quant_max_upper_bound: {quant_max_upper_bound} quant_max: {quant_max}\" ) return quant_min, quant_max def _get_reduction_params(block_size, input_size): \"\"\"Given block_size and input size find the parameters for reduction: Output: shape_for_reduction: the shape we use to `view` input to prepare it for reduction reduction_dims: the dims we\u0027ll do reduction over Example:: Input: block_size: (3, 3, 2, 10) input_size: (3, 3, 10, 10) Output: shape_for_reduction: (3, 3, 5, 2, 10) reduction_dim: [0, 1, 3, 4] \"\"\" assert len(block_size) == len(input_size) shape_for_reduction = [] reduction_dims = [] cur_dim = 0 for i in range(len(block_size)): if block_size[i] != input_size[i] and block_size[i] \u003e 1: assert input_size[i] % block_size[i] == 0, ( f\"Expecting input size at {i} dimension: {input_size[i]} to be divisible by block_size at {i} dimension: {block_size[i]}\" ) shape_for_reduction.append(input_size[i] // block_size[i]) shape_for_reduction.append(block_size[i]) # reduce over the block_size[i] dim reduction_dims.append(cur_dim + 1) cur_dim += 2 else: # block_size[i] == input_size[i] or block_size[i] == 1 shape_for_reduction.append(input_size[i]) # we only need to reduce over the dimension if block_size is greater than 1 # otherwise it\u0027s already the same as reduced dimension if block_size[i] != 1: reduction_dims.append(cur_dim) cur_dim += 1 return shape_for_reduction, reduction_dims [docs]@torch.no_grad() def quantize_affine( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], output_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, ) -\u003e torch.Tensor: \"\"\" Args: input (torch.Tensor): original float32, float16 or bfloat16 Tensor block_size: (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam e.g. when size is the same as the input tensor dimension, we are using per tensor quantization scale (float): quantization parameter for affine quantization zero_point (int): quantization parameter for affine quantization output_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype Note: How can block_size represent different granularities? let\u0027s say we have a Tensor of size: (3, 3, 10, 10), here is the table showing how block_size represents different granularities: granularity type | block_size per_tensor | (3, 3, 10, 10) per_axis (axis=0) | (1, 3, 10, 10) per_axis (axis=1) | (3, 1, 10, 10) per_group (groupsize=2) | (3, 3, 10, 2) per_group (groupsize=2) for axis = 3 | (3, 3, 2, 10) Output: quantized tensor with requested dtype \"\"\" return _quantize_affine( input, block_size, scale, zero_point, output_dtype, quant_min, quant_max, ) @register_custom_op def _quantize_affine( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], output_dtype: torch.dtype, quant_min: Optional[Union[int, float, bool]] = None, quant_max: Optional[Union[int, float, bool]] = None, ) -\u003e torch.Tensor: \"\"\"Quantize tensor using affine quantization with integer zero point domain. Op definition that has compatible signatures with custom op library. Args: input: Input tensor to quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) output_dtype: Target quantized dtype (e.g., torch.uint8, torch.int8) quant_min: Minimum quantized value, derived from dtype if None quant_max: Maximum quantized value, derived from dtype if None Returns: Quantized tensor with requested dtype Note: zero_point_domain is pre-defined as INT, meaning: quantized_val = (float_val / scale) (integer) + zero_point (integer) \"\"\" quant_min, quant_max = _get_and_check_qmin_qmax(output_dtype, quant_min, quant_max) # workaround for uintx dtypes, since we don\u0027t have native Uintx dtype connected with # torch.uintx dtypes yet if output_dtype in _SUB_BYTE_UINT_BOUNDS: output_dtype = torch.uint8 return _quantize_affine_no_dtype_cast( input, block_size, scale, zero_point, quant_min, quant_max, ).to(output_dtype) def _quantize_affine_no_dtype_cast( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_min: Union[int, float], quant_max: Union[int, float], ) -\u003e torch.Tensor: \"\"\"Quantize tensor using affine quantization without dtype casting. Performs quantization with integer zero point domain without casting to target dtype. Args: input: Input tensor to quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) quant_min: Minimum quantized value quant_max: Maximum quantized value Returns: Quantized tensor without dtype casting The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Quantize the input based on the quantization parameters scale and zero_point with zero_point_domain = INT 3. Reshape the quantized result to original shape \"\"\" # TODO: validations # TODO: validate scale/zero_point dimensions are compatible with block_size assert input.dtype in [ torch.float32, torch.float16, torch.bfloat16, ], f\"Unsupported input dtype: {input.dtype}\" assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) original_shape = input.shape input = input.view(shape_for_reduction) shape_after_reduction = shape_for_reduction for i in reduction_dims: shape_after_reduction[i] = 1 scale = scale.view(shape_after_reduction) if zero_point is not None and zero_point.numel() \u003e 0: zero_point = zero_point.view(shape_after_reduction) else: # in some cases zero_point being a non-value shows as a tensor # with numel=0 which we handle by unifying the two zero_point = None quant = torch.clamp( _Round.apply(input * (1.0 / scale)) + zero_point, quant_min, quant_max ) quant = quant.view(original_shape) return quant def _quantize_affine_tinygemm( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], output_dtype: torch.dtype, quant_min: Optional[Union[int, float, bool]] = None, quant_max: Optional[Union[int, float, bool]] = None, ) -\u003e torch.Tensor: \"\"\"Quantize tensor using affine quantization with float zero point domain for tinygemm. Specialized quantization for tinygemm int4mm kernel where zero point is in floating point domain. Args: input: Input tensor to quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) output_dtype: Target quantized dtype (e.g., torch.uint8, torch.int8) quant_min: Minimum quantized value, derived from dtype if None quant_max: Maximum quantized value, derived from dtype if None Returns: Quantized tensor with requested dtype The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Quantize the input based on the quantization parameters scale and zero_point with zero_point_domain = FLOAT 3. Reshape the quantized result to original shape Note: zero_point_domain is pre-defined as FLOAT, meaning: quantized_val = (float_val - (zero_point (float) - scale * mid_point)) / scale \"\"\" quant_min, quant_max = _get_and_check_qmin_qmax(output_dtype, quant_min, quant_max) # workaround for uintx dtypes, since we don\u0027t have native Uintx dtype connected with # torch.uintx dtypes yet if output_dtype in _SUB_BYTE_UINT_BOUNDS: output_dtype = torch.uint8 return _quantize_affine_tinygemm_no_dtype_cast( input, block_size, scale, zero_point, quant_min, quant_max, ).to(output_dtype) def _quantize_affine_tinygemm_no_dtype_cast( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, ) -\u003e torch.Tensor: \"\"\"Quantize tensor using affine quantization with float zero point domain without dtype casting. Specialized quantization for tinygemm int4mm kernel where zero point is in floating point domain. Args: input: Input tensor to quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) quant_min: Minimum quantized value quant_max: Maximum quantized value Returns: Quantized tensor without dtype casting The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Quantize the input based on the quantization parameters scale and zero_point with zero_point_domain = FLOAT 3. Reshape the quantized result to original shape \"\"\" # TODO: validations # TODO: validate scale/zero_point dimensions are compatible with block_size assert input.dtype in [ torch.float32, torch.float16, torch.bfloat16, ], f\"Unsupported input dtype: {input.dtype}\" assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) original_shape = input.shape input = input.view(shape_for_reduction) shape_after_reduction = shape_for_reduction for i in reduction_dims: shape_after_reduction[i] = 1 scale = scale.view(shape_after_reduction) if zero_point is not None and zero_point.numel() \u003e 0: zero_point = zero_point.view(shape_after_reduction) else: # in some cases zero_point being a non-value shows as a tensor # with numel=0 which we handle by unifying the two zero_point = None mid_point = (quant_max + quant_min + 1) / 2 min_val = zero_point - scale * mid_point quant = torch.clamp(_Round.apply((input - min_val) / scale), quant_min, quant_max) quant = quant.view(original_shape) return quant def _quantize_affine_no_zero_point( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], output_dtype: torch.dtype, quant_min: Optional[Union[int, float, bool]] = None, quant_max: Optional[Union[int, float, bool]] = None, ) -\u003e torch.Tensor: \"\"\"Quantize tensor using affine quantization without zero point. Specialized quantization for cases where zero point is not needed (e.g., floatx quantization). Args: input: Input tensor to quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (ignored, should be None) output_dtype: Target quantized dtype (e.g., torch.uint8, torch.int8) quant_min: Minimum quantized value, derived from dtype if None quant_max: Maximum quantized value, derived from dtype if None Returns: Quantized tensor with requested dtype The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Quantize the input based on the quantization parameters scale with zero_point_domain = NONE 3. Reshape the quantized result to original shape Note: zero_point_domain is pre-defined as NONE, meaning: quantized_val = (float_val / scale) | This is primarily used for floatx quantization where we do not want to round values to nearest integer and instead scale and cast. \"\"\" quant_min, quant_max = _get_and_check_qmin_qmax(output_dtype, quant_min, quant_max) # workaround for uintx dtypes, since we don\u0027t have native Uintx dtype connected with # torch.uintx dtypes yet if output_dtype in _SUB_BYTE_UINT_BOUNDS: output_dtype = torch.uint8 return _quantize_affine_no_zero_point_no_dtype_cast( input, block_size, scale, zero_point, quant_min, quant_max, ).to(output_dtype) def _quantize_affine_no_zero_point_no_dtype_cast( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, ) -\u003e torch.Tensor: \"\"\"Quantize tensor using affine quantization without zero point and without dtype casting. Specialized quantization for cases where zero point is not needed without casting to target dtype. Args: input: Input tensor to quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (ignored, should be None) quant_min: Minimum quantized value quant_max: Maximum quantized value Returns: Quantized tensor without dtype casting The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Quantize the input based on the quantization parameters scale with zero_point_domain = NONE 3. Reshape the quantized result to original shape \"\"\" # TODO: validations # TODO: validate scale/zero_point dimensions are compatible with block_size assert input.dtype in [ torch.float32, torch.float16, torch.bfloat16, ], f\"Unsupported input dtype: {input.dtype}\" assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) original_shape = input.shape input = input.view(shape_for_reduction) shape_after_reduction = shape_for_reduction for i in reduction_dims: shape_after_reduction[i] = 1 scale = scale.view(shape_after_reduction) if zero_point is not None and zero_point.numel() \u003e 0: zero_point = zero_point.view(shape_after_reduction) else: # in some cases zero_point being a non-value shows as a tensor # with numel=0 which we handle by unifying the two zero_point = None quant = torch.clamp(_Round.apply(input * (1.0 / scale)), quant_min, quant_max) quant = quant.view(original_shape) return quant [docs]def dequantize_affine( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], input_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, *, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\" Args: input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam e.g. when size is the same as the input tensor dimension, we are using per tensor quantization scale (Tensor): quantization parameter for affine quantization zero_point (Tensor): quantization parameter for affine quantization input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor quant_min (Optional[int]): minimum quantized value for input Tensor quant_max (Optional[int]): maximum quantized value for input Tensor output_dtype (torch.dtype): dtype for output Tensor, default is fp32 Default value for zero_point is in integer domain, zero point is added to the quantized integer value during quantization Output: dequantized Tensor, with requested dtype or fp32 \"\"\" return _dequantize_affine( input, block_size, scale, zero_point, input_dtype, quant_min, quant_max, output_dtype=output_dtype, ) @register_custom_op def _dequantize_affine( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], input_dtype: torch.dtype, quant_min: Optional[Union[int, float, bool]] = None, quant_max: Optional[Union[int, float, bool]] = None, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\"Dequantize tensor using affine dequantization with integer zero point domain. Op definition that has compatible signatures with custom op library. Args: input: Quantized tensor to dequantize block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) input_dtype: Expected dtype of input tensor (e.g., torch.uint8, torch.int8) quant_min: Minimum quantized value for input tensor quant_max: Maximum quantized value for input tensor output_dtype: Target output dtype (default: torch.float32) Returns: Dequantized tensor with requested output dtype \"\"\" # TODO: validate scale/zero_point dimensions are compatible with block_size if input_dtype not in _SUB_BYTE_UINT_BOUNDS: assert input.dtype == input_dtype, ( f\"Expected: {input_dtype}, got: {input.dtype}\" ) assert output_dtype in [ torch.float32, torch.float16, torch.bfloat16, ], f\"Unsupported output dtype: {output_dtype}\" quant_min, quant_max = _get_and_check_qmin_qmax(input_dtype, quant_min, quant_max) return _dequantize_affine_no_dtype_check( input, block_size, scale, zero_point, quant_min, quant_max, output_dtype, ) def _dequantize_affine_no_dtype_check( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_min: Union[int, float], quant_max: Union[int, float], output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\"Dequantize tensor using affine dequantization without dtype checking. Converts quantized tensors to their high precision floating point representation. Args: input: Quantized tensor to dequantize block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) quant_min: Minimum quantized value for input tensor quant_max: Maximum quantized value for input tensor output_dtype: Target output dtype (default: torch.float32) Returns: Dequantized tensor with requested output dtype The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Dequantize the input based on the quantization parameters scale and zero_point 3. Reshape the quantized result to original shape and change dtype to the output_dtype \"\"\" assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) original_shape = input.shape input = input.view(shape_for_reduction) shape_after_reduction = shape_for_reduction for i in reduction_dims: shape_after_reduction[i] = 1 scale = scale.view(shape_after_reduction) if zero_point is not None: zero_point = zero_point.view(shape_after_reduction) # Force a copy to avoid input modification due # to upcoming in-place operations. dequant = input.to(output_dtype, copy=True) if zero_point is not None: dequant = dequant - zero_point.to(output_dtype) dequant = dequant * scale return dequant.view(original_shape).to(output_dtype) def _dequantize_affine_no_zero_point_no_dtype_check( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_min: Union[int, float], quant_max: Union[int, float], output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\"Dequantize tensor using affine dequantization without zero point and without dtype checking. Converts quantized tensors to their high precision floating point representation without zero point. Args: input: Quantized tensor to dequantize block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (ignored, should be None) quant_min: Minimum quantized value for input tensor quant_max: Maximum quantized value for input tensor output_dtype: Target output dtype (default: torch.float32) Returns: Dequantized tensor with requested output dtype The op does the following: 1. Figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. Dequantize the input based on the quantization parameters scale (no zero point) 3. Reshape the quantized result to original shape and change dtype to the output_dtype \"\"\" assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) original_shape = input.shape input = input.view(shape_for_reduction) shape_after_reduction = shape_for_reduction for i in reduction_dims: shape_after_reduction[i] = 1 scale = scale.view(shape_after_reduction) assert zero_point is None, ( \"zero_point should be None for _dequantize_affine_no_zero_point\" ) dequant = input.to(output_dtype) dequant = dequant * scale return dequant.view(original_shape).to(output_dtype) def _dequantize_affine_no_zero_point( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], input_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, *, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\" Args: input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam e.g. when size is the same as the input tensor dimension, we are using per tensor quantization scale (Tensor): quantization parameter for affine quantization zero_point (Tensor): quantization parameter for affine quantization, no zero point is used for this op input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor quant_min (Optional[int]): minimum quantized value for input Tensor quant_max (Optional[int]): maximum quantized value for input Tensor output_dtype (torch.dtype): dtype for output Tensor, default is fp32 Default value for zero_point is in integer domain, zero point is added to the quantized integer value during quantization Output: dequantized Tensor, with requested dtype or fp32 \"\"\" # TODO: validate scale/zero_point dimensions are compatible with block_size if input_dtype not in _SUB_BYTE_UINT_BOUNDS: assert input.dtype == input_dtype, ( f\"Expected: {input_dtype}, got: {input.dtype}\" ) assert output_dtype in [ torch.float32, torch.float16, torch.bfloat16, ], f\"Unsupported output dtype: {output_dtype}\" quant_min, quant_max = _get_and_check_qmin_qmax(input_dtype, quant_min, quant_max) return _dequantize_affine_no_zero_point_no_dtype_check( input, block_size, scale, zero_point, quant_min, quant_max, output_dtype, ) def _dequantize_affine_tinygemm_no_dtype_check( input: torch.Tensor, block_size: List[int], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_min: Union[int, float], quant_max: Union[int, float], output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\"This function converts AQT tensors to their high precision floating point representation The op does the following: 1. figure out the dimension for reduction based on block_size, also reshape the input to align with the shape after reduction 2. dequantize the input based on the quantization parameters scale and zero_point and args like zero_point_domain 3. reshape the quantized result to origianl shape and change dtype to the output_dtype \"\"\" assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) original_shape = input.shape input = input.view(shape_for_reduction) shape_after_reduction = shape_for_reduction for i in reduction_dims: shape_after_reduction[i] = 1 scale = scale.view(shape_after_reduction) if zero_point is not None: zero_point = zero_point.view(shape_after_reduction) # TODO: this seems to be a detail for tinygemm (converting from uint to int, probably need to refactor this) mid_point = (quant_max + quant_min + 1) / 2 # This should allocate new memory and avoid input modification dequant = input - mid_point dequant = dequant.to(output_dtype) dequant *= scale if zero_point is not None: dequant += zero_point return dequant.view(original_shape).to(output_dtype) def _dequantize_affine_tinygemm( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], input_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, *, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\" Args: input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam e.g. when size is the same as the input tensor dimension, we are using per tensor quantization scale (Tensor): quantization parameter for affine quantization zero_point (Tensor): quantization parameter for affine quantization input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor quant_min (Optional[int]): minimum quantized value for input Tensor quant_max (Optional[int]): maximum quantized value for input Tensor output_dtype (torch.dtype): dtype for output Tensor, default is fp32 Default value for zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized) Output: dequantized Tensor, with requested dtype or fp32 \"\"\" # TODO: validate scale/zero_point dimensions are compatible with block_size if input_dtype not in _SUB_BYTE_UINT_BOUNDS: assert input.dtype == input_dtype, ( f\"Expected: {input_dtype}, got: {input.dtype}\" ) assert output_dtype in [ torch.float32, torch.float16, torch.bfloat16, ], f\"Unsupported output dtype: {output_dtype}\" quant_min, quant_max = _get_and_check_qmin_qmax(input_dtype, quant_min, quant_max) return _dequantize_affine_tinygemm_no_dtype_check( input, block_size, scale, zero_point, quant_min, quant_max, output_dtype, ) def _fake_quantize_affine( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, zero_point_domain: ZeroPointDomain = ZeroPointDomain.INT, ) -\u003e torch.Tensor: \"\"\" General fake quantize op for quantization-aware training (QAT). This is equivalent to calling `quantize_affine` + `dequantize_affine` but without the dtype casts. Args: input (torch.Tensor): original float32, float16 or bfloat16 Tensor block_size: (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam e.g. when size is the same as the input tensor dimension, we are using per tensor quantization scale (float): quantization parameter for affine quantization zero_point (int): quantization parameter for affine quantization quant_dtype (torch.dtype): desired quantized dtype for determining and validating quant_min and quant_max values. quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype zero_point_domain (ZeroPointDomain): the domain that zero_point is in, should be either integer or float if zero_point is in integer domain, zero point is added to the quantized integer value during quantization if zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized) value during quantization default is ZeroPointDomain.INT \"\"\" if zero_point_domain is None: raise ValueError(\"Please use ZeroPointDomain.NONE instead of None\") elif zero_point_domain is ZeroPointDomain.NONE and zero_point is not None: raise ValueError(\"zero_point should be None when zero_point_domain is NONE\") (_, fq) = _do_fake_quantize_affine( input, block_size, scale, zero_point, quant_dtype, quant_min, quant_max, zero_point_domain, ) return fq def _fake_quantize_affine_cachemask( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, zero_point_domain: ZeroPointDomain = ZeroPointDomain.INT, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\" General fake quantize op for quantization-aware training (QAT). This is equivalent to calling `quantize_affine` + `dequantize_affine` but without the dtype casts. Note: Compared to :func:`~torchao.quantization.quant_primitives._fake_quantize_affine`, this consumes more memory and returns an additional outlier mask for intermediate quantized values. Args: Same as :func:`~torchao.quantization.quant_primitives._fake_quantize_affine`. Returns: A 2-tuple of ( final fake quantized values, outlier mask for intermediate quantized values ) \"\"\" if zero_point_domain is None: raise ValueError(\"Please use ZeroPointDomain.NONE instead of None\") elif zero_point_domain is None and zero_point is not None: raise ValueError(\"zero_point should be None when zero_point_domain is NONE\") (q, dq) = _do_fake_quantize_affine( input, block_size, scale, zero_point, quant_dtype, quant_min, quant_max, zero_point_domain, ) mask = torch.logical_and((q \u003e= quant_min), (q \u003c= quant_max)) return (dq, mask) def _do_fake_quantize_affine( input: torch.Tensor, block_size: Tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], quant_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, zero_point_domain: ZeroPointDomain = ZeroPointDomain.INT, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\"Helper function for fake quantization that returns both intermediate and final values. Performs quantization followed by dequantization without dtype casting, returning both the intermediate quantized values and the final dequantized values. Args: input: Input tensor to fake quantize (float32, float16, or bfloat16) block_size: Granularity of quantization - size of tensor elements sharing same qparam scale: Quantization scale parameter zero_point: Quantization zero point parameter (optional) quant_dtype: Target quantized dtype for determining quant_min/quant_max quant_min: Minimum quantized value, derived from dtype if None quant_max: Maximum quantized value, derived from dtype if None zero_point_domain: Domain of zero point (INT, FLOAT, or NONE) Returns: Tuple of (intermediate quantized values, final dequantized values) Helper function for `_fake_quantize_affine` that returns both the intermediate quantized values and the final dequantized values. \"\"\" input_dtype = input.dtype quant_min, quant_max = _get_and_check_qmin_qmax(quant_dtype, quant_min, quant_max) if zero_point_domain == ZeroPointDomain.INT: _quantize_affine = _quantize_affine_no_dtype_cast _dequantize_affine = _dequantize_affine_no_dtype_check elif zero_point_domain == ZeroPointDomain.FLOAT: _quantize_affine = _quantize_affine_tinygemm_no_dtype_cast _dequantize_affine = _dequantize_affine_tinygemm_no_dtype_check elif zero_point_domain == ZeroPointDomain.NONE: _quantize_affine = _quantize_affine_no_zero_point_no_dtype_cast _dequantize_affine = _dequantize_affine_no_zero_point_no_dtype_check else: raise ValueError(f\"Unrecognized zero point domain: {zero_point_domain}\") q = _quantize_affine( input, block_size, scale, zero_point, quant_min, quant_max, ) dq = _dequantize_affine( q, block_size, scale, zero_point, quant_min, quant_max, output_dtype=input_dtype, ) return (q, dq) [docs]@torch.no_grad() def choose_qparams_affine( input: torch.Tensor, mapping_type: MappingType, block_size: Tuple[int], target_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, eps: Optional[float] = None, scale_dtype: Optional[torch.dtype] = None, zero_point_dtype: Optional[torch.dtype] = torch.int32, keepdim: bool = False, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\" Args: input (torch.Tensor): fp32, bf16, fp16 input Tensor mapping_type (MappingType): determines how the qparams are calculated, symmetric or asymmetric block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam e.g. when size is the same as the input tensor dimension, we are using per tensor quantization target_dtype (torch.dtype): dtype for target quantized Tensor quant_min (Optional[int]): minimum quantized value for target quantized Tensor quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype scale_dtype (torch.dtype): dtype for scale Tensor zero_point_dtype (torch.dtype): dtype for zero_point Tensor, defaults to torch.int32 keepdim (bool): whether to keep dimensions with size 1 in output (aligned with _choose_scale_float8) Now removed params: zero_point_domain (ZeroPointDomain): the domain that zero_point is in, defaults to Integer or None preserve_zero (bool): whether to preserve zero in the quantized Tensor, defaults to True Output: Tuple of scales and zero_points Tensor with requested dtype \"\"\" return _choose_qparams_affine( input, mapping_type.name, block_size, target_dtype, quant_min, quant_max, eps, scale_dtype, zero_point_dtype, keepdim, ) # TODO: lower this op to custom op library @torch.no_grad() def _choose_qparams_affine_tinygemm( input: torch.Tensor, mapping_type: MappingType, block_size: Tuple[int], target_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = None, quant_max: Optional[Union[int, float]] = None, eps: Optional[float] = None, scale_dtype: Optional[torch.dtype] = None, zero_point_dtype: Optional[torch.dtype] = None, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\" Specialized version of choose_qparams_affine This is used for tinygemm int4mm kernel where zero point is in floating point domain and zero does not have to be exactly representable. Args: input (torch.Tensor): fp32, bf16, fp16 input Tensor mapping_type (MappingType): determines how the qparams are calculated, symmetric or asymmetric block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam target_dtype (torch.dtype): dtype for target quantized Tensor quant_min (Optional[int]): minimum quantized value for target quantized Tensor quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype scale_dtype (torch.dtype): dtype for scale Tensor zero_point_dtype (torch.dtype): dtype for zero_point Tensor Output: Tuple of scales and zero_points Tensor with requested dtype \"\"\" quant_min, quant_max = _get_and_check_qmin_qmax(target_dtype, quant_min, quant_max) assert mapping_type is MappingType.ASYMMETRIC, ( f\"Unsupported mapping type: {mapping_type}\" ) if scale_dtype is None: scale_dtype = input.dtype if eps is None: eps = torch.finfo(input.dtype).eps assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) input = input.view(shape_for_reduction) min_val = torch.amin(input, dim=reduction_dims, keepdim=False) max_val = torch.amax(input, dim=reduction_dims, keepdim=False) # For preserve_zero=False, we don\u0027t ensure zero is exactly representable min_val_neg = min_val max_val_pos = max_val scale = (max_val_pos - min_val_neg) / float(quant_max - quant_min) scale = torch.clamp(scale, min=eps) # For zero_point_domain=FLOAT in asymmetric quantization mid_point = (quant_max + quant_min + 1) / 2 # this is not preserving zero_point, this is converting to TensorCoreTiledFormat zero_point = min_val_neg + scale * mid_point if zero_point_dtype is None: zero_point_dtype = input.dtype zero_point = zero_point.to(dtype=zero_point_dtype) return scale.to(dtype=scale_dtype, device=input.device), zero_point # TODO: lower this op to custom op library def _choose_qparams_affine_dont_preserve_zero( input: torch.Tensor, mapping_type: MappingType, block_size: Tuple[int], target_dtype: torch.dtype, quant_min: Optional[Union[int, float, bool]] = None, quant_max: Optional[Union[int, float, bool]] = None, eps: Optional[float] = None, scale_dtype: Optional[torch.dtype] = None, zero_point_dtype: Optional[torch.dtype] = None, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\"Specialized version of choose_qparams_affine with zero_point_domain=ZeroPointDomain.INT and preserve_zero=False. Args: input (torch.Tensor): fp32, bf16, fp16 input Tensor mapping_type (MappingType): determines how the qparams are calculated, asymmetric only block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that\u0027s sharing the same qparam target_dtype (torch.dtype): dtype for target quantized Tensor quant_min (Optional[int]): minimum quantized value for target quantized Tensor quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype scale_dtype (torch.dtype): dtype for scale Tensor zero_point_dtype (torch.dtype): dtype for zero_point Tensor Now removed params default values: zero_point_domain (ZeroPointDomain): the domain that zero_point is in, defaults to Integer preserve_zero (bool): whether to preserve zero in the quantized Tensor, defaults to False Output: Tuple of scales and zero_points Tensor with requested dtype \"\"\" quant_min, quant_max = _get_and_check_qmin_qmax(target_dtype, quant_min, quant_max) assert mapping_type == MappingType.ASYMMETRIC, ( f\"Unsupported mapping type: {mapping_type}\" ) if scale_dtype is None: scale_dtype = input.dtype if eps is None: eps = torch.finfo(input.dtype).eps assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) input = input.view(shape_for_reduction) min_val = torch.amin(input, dim=reduction_dims, keepdim=False) max_val = torch.amax(input, dim=reduction_dims, keepdim=False) # For no preserve zero, we don\u0027t ensure zero is exactly representable min_val_neg = min_val max_val_pos = max_val scale = (max_val_pos - min_val_neg) / float(quant_max - quant_min) scale = torch.clamp(scale, min=eps) # Zero point is int zero_point = quant_min - _Round.apply(min_val_neg / scale) zero_point = torch.clamp(zero_point, quant_min, quant_max) if zero_point_dtype is None: zero_point_dtype = torch.int32 return scale.to(dtype=scale_dtype, device=input.device), zero_point.to( dtype=zero_point_dtype ) # TODO: lower this op to custom op library [docs]def choose_qparams_affine_with_min_max( min_val: torch.Tensor, max_val: torch.Tensor, mapping_type: MappingType, block_size: Tuple[int, ...], target_dtype: torch.dtype, quant_min: Optional[int] = None, quant_max: Optional[int] = None, eps: Optional[float] = None, scale_dtype: Optional[torch.dtype] = None, zero_point_dtype: Optional[torch.dtype] = None, preserve_zero: bool = True, zero_point_domain: ZeroPointDomain = ZeroPointDomain.INT, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\"A variant of :func:`~torchao.quantization.quant_primitives.choose_qparams_affine` operator that pass in min_val and max_val directly instead of deriving these from a single input. This is used for observers in static quantization where min_val and max_val may be obtained through tracking all the data in calibration data set. Args: Mostly same as :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`. with one difference: instead of passing in `input` Tensor and use that to calculate min_val/max_val and then scale/zero_point, we pass in min_val/max_val directly \"\"\" if zero_point_domain is None: raise ValueError(\"Please use ZeroPointDomain.NONE instead of None\") quant_min, quant_max = _get_and_check_qmin_qmax(target_dtype, quant_min, quant_max) assert mapping_type in [ MappingType.SYMMETRIC, MappingType.SYMMETRIC_NO_CLIPPING_ERR, MappingType.ASYMMETRIC, ], f\"Unsupported mapping type: {mapping_type}\" assert min_val is not None and max_val is not None, ( \"Need to provide `min_val` and `max_val`, got: {min_val, max_val}\" ) assert min_val.dtype == max_val.dtype, ( \"Expecting `min_val` and `max_val` to have the same dtype, got: {min_val.dtype, max_val.dtype}\" ) if scale_dtype is None: scale_dtype = min_val.dtype if eps is None: eps = torch.finfo(min_val.dtype).eps scale_device = min_val.device if preserve_zero: min_val_neg = torch.min(min_val, torch.zeros_like(min_val)) max_val_pos = torch.max(max_val, torch.zeros_like(max_val)) else: min_val_neg = min_val max_val_pos = max_val if ( mapping_type == MappingType.SYMMETRIC or mapping_type == MappingType.SYMMETRIC_NO_CLIPPING_ERR ): # scales if mapping_type == MappingType.SYMMETRIC: max_val_pos = torch.max(-min_val_neg, max_val_pos) scale = max_val_pos / (float(quant_max - quant_min) / 2) else: assert mapping_type == MappingType.SYMMETRIC_NO_CLIPPING_ERR # calculate smin and smax individually and choose the larger one. For example, if quant_min = -8 and # quant_max = 7. # - If smin is bigger: There would be coverage on negative values down to -8, and less rounding # error than the existing SYMMETRIC case. # - If smax is bigger: it covers the positive values up to 7. The round # error may be bigger than the existing SYMMETRIC case. Either way, there\u0027s no out-of-range fp values after # quantization. smin = min_val_neg / float(quant_min) smax = max_val_pos / float(quant_max) mask = smin \u003e smax scale = torch.where(mask, smin, smax) # zeros if not preserve_zero: raise ValueError( \"preserve_zero == False is not supported for symmetric quantization\" ) if zero_point_domain == ZeroPointDomain.FLOAT: # TODO INT should not be a valid ZeroPointDomain for symmetric quantization since # symmetric quant doesn\u0027t have a zero_point raise ValueError( \"zero_point_domain should be ZeroPointDomain.INT or ZeroPointDomain.NONE for symmetric quantization\" ) if zero_point_domain == ZeroPointDomain.NONE: zero_point = None else: zero_point = torch.full_like(scale, int((quant_max + quant_min + 1) / 2)) scale = torch.clamp(scale, min=eps) else: assert mapping_type == MappingType.ASYMMETRIC scale = (max_val_pos - min_val_neg) / torch.tensor( float(quant_max - quant_min), dtype=scale_dtype, device=scale_device ) scale = torch.clamp(scale, min=eps) if zero_point_domain == ZeroPointDomain.NONE: zero_point = None elif zero_point_domain == ZeroPointDomain.INT: zero_point = quant_min - _Round.apply(min_val_neg / scale) zero_point = torch.clamp(zero_point, quant_min, quant_max) if zero_point_dtype is None: zero_point_dtype = torch.int32 else: assert zero_point_domain == ZeroPointDomain.FLOAT, ( \"zero_point must be in FLOAT/INT/None domain for asymmetric quantization\" ) mid_point = (quant_max + quant_min + 1) / 2 # this is not preserving zero_point, this is converting to TensorCoreTiledFormat # TODO move the conversion of zero_point out of quant_primitives # and into TensorCoreTiledLayout.from_plain zero_point = min_val_neg + scale * mid_point if zero_point is not None: zero_point = zero_point.to(dtype=zero_point_dtype) return scale.to(dtype=scale_dtype, device=min_val.device), zero_point @register_custom_op def _choose_qparams_affine( input: Optional[torch.Tensor], mapping_type: str, block_size: List[int], target_dtype: torch.dtype, quant_min: Optional[Union[int, float, bool]] = None, quant_max: Optional[Union[int, float, bool]] = None, eps: Optional[float] = None, scale_dtype: Optional[torch.dtype] = None, zero_point_dtype: Optional[torch.dtype] = None, keepdim: bool = False, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\"op definition that has compatible signatures with custom op library The op does the following: 1. figure out the dimension for reduction based on block_size 2. find min_val/max_val based on the dimension for reduction 3. calculate quantization parameters based on min_val/max_val based on args like `preserve_zero` and `zero_point_domain` Note: Set keepdim=True to align with _choose_scale_float8 behavior. This ensures scale/zero_point maintain the same rank as input, making it easier to handle downstream. \"\"\" quant_min, quant_max = _get_and_check_qmin_qmax(target_dtype, quant_min, quant_max) assert mapping_type in [ MappingType.SYMMETRIC.name, MappingType.SYMMETRIC_NO_CLIPPING_ERR.name, MappingType.ASYMMETRIC.name, ], f\"Unsupported mapping type: {mapping_type}\" if scale_dtype is None: scale_dtype = input.dtype if eps is None: eps = torch.finfo(input.dtype).eps assert len(block_size) == input.dim(), ( f\"Got input dim:{input.dim()}, block_size: {block_size}\" ) # Save original input size before reshaping for later use original_input_size = input.size() shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) input = input.view(shape_for_reduction) min_val = torch.amin(input, dim=reduction_dims, keepdim=keepdim) max_val = torch.amax(input, dim=reduction_dims, keepdim=keepdim) min_val_neg = torch.min(min_val, torch.zeros_like(min_val)) max_val_pos = torch.max(max_val, torch.zeros_like(max_val)) if ( mapping_type == MappingType.SYMMETRIC.name or mapping_type == MappingType.SYMMETRIC_NO_CLIPPING_ERR.name ): # scales if mapping_type == MappingType.SYMMETRIC.name: max_val_pos = torch.max(-min_val_neg, max_val_pos) scale = max_val_pos / (float(quant_max - quant_min) / 2) else: assert mapping_type == MappingType.SYMMETRIC_NO_CLIPPING_ERR.name # calculate smin and smax individually and choose the larger one. For example, if quant_min = -8 and # quant_max = 7. # - If smin is bigger: There would be coverage on negative values down to -8, and less rounding # error than the existing SYMMETRIC case. # - If smax is bigger: it covers the positive values up to 7. The round # error may be bigger than the existing SYMMETRIC case. Either way, there\u0027s no out-of-range fp values after # quantization. smin = min_val_neg / float(quant_min) smax = max_val_pos / float(quant_max) mask = smin \u003e smax scale = torch.where(mask, smin, smax) zero_point = torch.full_like(scale, int((quant_max + quant_min + 1) / 2)) scale = torch.clamp(scale, min=eps) else: assert mapping_type == MappingType.ASYMMETRIC.name scale = (max_val_pos - min_val_neg) / float(quant_max - quant_min) scale = torch.clamp(scale, min=eps) zero_point = quant_min - _Round.apply(min_val_neg / scale) zero_point = torch.clamp(zero_point, quant_min, quant_max) if zero_point_dtype is None: zero_point_dtype = torch.int32 # Reshape scale and zero_point to match expected output shape # This aligns with _choose_scale_float8 behavior if keepdim: output_shape = [ original_input_size[i] // block_size[i] for i in range(len(block_size)) ] scale = scale.reshape(output_shape) zero_point = zero_point.reshape(output_shape) return scale.to(dtype=scale_dtype, device=input.device), zero_point.to( dtype=zero_point_dtype ) def _choose_qparams_gguf( input: Optional[torch.Tensor], block_size: List[int], target_dtype: torch.dtype, ) -\u003e Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: \"\"\" There are two sets of qparams: quantized_block_scale, quantized_block_min and super_block_scale_scale and super_block_min_scale the relationship is the following: block_scale = quantized_block_scale * super_block_sclae block_min = quantized_block_min * super_block_min quantized_val = (float_val - block_min) / block_scale + quant_min first we calculate block_scale and block_min then we calculate super_block_scale_scale and super_block_min_scale after that we can calculate quantized_block_scale and quantized_min_scale the returned values are: super_block_scale_scale, super_block_min_scale, quantized_block_scale and quantized_min_scale \"\"\" dtype = input.dtype # 1. get block_scale block_min shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) input = input.view(shape_for_reduction) min_val = torch.amin(input, dim=reduction_dims, keepdim=False) max_val = torch.amax(input, dim=reduction_dims, keepdim=False) quant_max = 15 quant_min = 0 # asymmetric quant to fully utilize the range block_scale = max_val / (float(quant_max - quant_min) / 2) block_scale = (max_val - min_val) / float(quant_max - quant_min) block_min = min_val # 2. get super_block_scale_scale and super_block_min_scale assert _GGUF_QK_K % block_size[-1] == 0 super_block_size = (1, _GGUF_QK_K // block_size[-1]) shape_for_reduction, reduction_dims = _get_reduction_params( super_block_size, block_scale.size() ) block_scale = block_scale.view(shape_for_reduction) block_min = block_min.view(shape_for_reduction) shape_after_reduction = shape_for_reduction.copy() for i in reduction_dims: shape_after_reduction[i] = 1 block_scale_absmax = torch.amax( torch.abs(block_scale), dim=reduction_dims, keepdim=False ) block_min_absmax = torch.amax( torch.abs(block_min), dim=reduction_dims, keepdim=False ) # 2. get super_block_scale_scale and super_block_min_scale # TODO: make this configurable # we also quantize the quantization parameters (scale and min) for each block to 6 bit # for Q4_K qparam_quant_max = 2**6 - 1 qparam_quant_min = 0 super_block_scale_scale = block_scale_absmax / float( qparam_quant_max - qparam_quant_min ) super_block_min_scale = block_min_absmax / float( qparam_quant_max - qparam_quant_min ) super_block_scale_scale_view = super_block_scale_scale.view(shape_after_reduction) super_block_min_scale_view = super_block_min_scale.view(shape_after_reduction) # 3. quantize block scale and min are stored in 6 bits using super_block_scale_scale and super_block_min_scale quantized_block_scale = torch.clamp( block_scale / super_block_scale_scale_view, qparam_quant_min, qparam_quant_max ) quantized_block_min = torch.clamp( block_min / super_block_min_scale_view, qparam_quant_min, qparam_quant_max ) return ( super_block_scale_scale.to(dtype), super_block_min_scale.to(dtype), quantized_block_scale.to(dtype), quantized_block_min.to(dtype), ) def _quantize_gguf( input: torch.Tensor, block_size: List[int], target_dtype: torch.dtype, super_block_scale_scale: torch.Tensor, super_block_min_scale: torch.Tensor, quantized_block_scale: torch.Tensor, quantized_block_min: torch.Tensor, ) -\u003e torch.Tensor: assert target_dtype == torch.uint4 # step 1: first order quantization # just going through shape calculation for block_scale and block_min to get the correct shape input_shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) block_qparam_shape_after_reduction = input_shape_for_reduction.copy() for i in reduction_dims: block_qparam_shape_after_reduction[i] = 1 original_shape = input.shape input = input.view(input_shape_for_reduction) quantized_block_scale = quantized_block_scale.view( block_qparam_shape_after_reduction ) quantized_block_min = quantized_block_min.view(block_qparam_shape_after_reduction) # step 2: second order quantization, recover unquantized block_scale and block_min super_block_size = (1, _GGUF_QK_K // block_size[-1], 1) super_block_input_shape_for_reduction, reduction_dims = _get_reduction_params( super_block_size, quantized_block_scale.size() ) super_block_qparam_shape_after_reduction = ( super_block_input_shape_for_reduction.copy() ) for i in reduction_dims: super_block_qparam_shape_after_reduction[i] = 1 quantized_block_scale = quantized_block_scale.view( super_block_input_shape_for_reduction ) quantized_block_min = quantized_block_min.view( super_block_input_shape_for_reduction ) super_block_scale_scale = super_block_scale_scale.view( super_block_qparam_shape_after_reduction ) super_block_min_scale = super_block_min_scale.view( super_block_qparam_shape_after_reduction ) block_scale = super_block_scale_scale * quantized_block_scale block_min = super_block_min_scale * quantized_block_min # step 3: quantization with the unquantized block_scale and block_min block_scale = block_scale.view(block_qparam_shape_after_reduction) block_min = block_min.view(block_qparam_shape_after_reduction) int_data = (input - block_min) / block_scale int_data = int_data.view(original_shape) return int_data def _dequantize_gguf( input: torch.Tensor, block_size: List[int], target_dtype: torch.dtype, super_block_scale_scale: torch.Tensor, super_block_min_scale: torch.Tensor, quantized_block_scale: torch.Tensor, quantized_block_min: torch.Tensor, output_dtype: Optional[torch.dtype] = None, ) -\u003e torch.Tensor: # step 1. reshape input and quantized block scale and min to the shape # after first quantization input_shape_for_reduction, reduction_dims = _get_reduction_params( block_size, input.size() ) block_qparam_shape_after_reduction = input_shape_for_reduction.copy() for i in reduction_dims: block_qparam_shape_after_reduction[i] = 1 original_shape = input.shape input = input.view(input_shape_for_reduction) quantized_block_scale = quantized_block_scale.view( block_qparam_shape_after_reduction ) quantized_block_min = quantized_block_min.view(block_qparam_shape_after_reduction) # step 2. calculate and reshape block_qparams for second quantization step super_block_size = (1, _GGUF_QK_K // block_size[-1], 1) super_block_input_shape_for_reduction, reduction_dims = _get_reduction_params( super_block_size, quantized_block_scale.size() ) super_block_qparam_shape_after_reduction = ( super_block_input_shape_for_reduction.copy() ) for i in reduction_dims: super_block_qparam_shape_after_reduction[i] = 1 quantized_block_scale = quantized_block_scale.view( super_block_input_shape_for_reduction ) quantized_block_min = quantized_block_min.view( super_block_input_shape_for_reduction ) super_block_scale_scale = super_block_scale_scale.view( super_block_qparam_shape_after_reduction ) super_block_min_scale = super_block_min_scale.view( super_block_qparam_shape_after_reduction ) block_scale = super_block_scale_scale * quantized_block_scale block_min = super_block_min_scale * quantized_block_min # step 3. dequantize with block_scale and block_min block_scale = block_scale.view(block_qparam_shape_after_reduction) block_min = block_min.view(block_qparam_shape_after_reduction) dequant = input * block_scale + block_min dequant = dequant.view(original_shape) if output_dtype is not None: dequant = dequant.to(output_dtype) return dequant # HQQ ############################################################################ # Shrinking operator (proximal operator for the lp norm) def _shrink_lp_op(x: torch.Tensor, beta: float, lp_norm: float) -\u003e torch.Tensor: if lp_norm == 1: return torch.sign(x) * torch.nn.functional.relu(torch.abs(x) - 1.0 / beta) else: return torch.sign(x) * torch.nn.functional.relu( torch.abs(x) - (1.0 / beta) * torch.pow(torch.abs(x), lp_norm - 1) ) # Proximal solver || W - dequantize(quantize(W))||_p^p @torch.inference_mode() def optimize_weights_proximal_legacy( tensor: torch.Tensor, scale: torch.Tensor, zero: torch.Tensor, min_max: list, axis: int = 0, dtype: Union[torch.dtype, None] = None, device: Union[str, None] = None, verbose: bool = False, opt_params: dict = { \"lp_norm\": 0.7, \"beta\": 1e1, \"kappa\": 1.01, \"iters\": 20, \"early_stop\": True, }, ) -\u003e tuple: lp_norm, beta, kappa, iters, early_stop = ( opt_params[\"lp_norm\"], opt_params[\"beta\"], opt_params[\"kappa\"], opt_params[\"iters\"], opt_params[\"early_stop\"], ) device = tensor.device if (device is None) else torch.device(device) if dtype is None: dtype = torch.float16 if (device.type == \"cuda\") else torch.float32 W_f = tensor.to(dtype=dtype, device=device) scale = scale.to(dtype=dtype, device=device) zero = zero.to(dtype=dtype, device=device) best_error = 1e4 for i in range(iters): W_q = torch.round(W_f * scale + zero).clamp(min_max[0], min_max[1]) W_r = (W_q - zero) / scale W_e = _shrink_lp_op(W_f - W_r, beta, lp_norm) zero = torch.mean(W_q - (W_f - W_e) * scale, axis=axis, keepdim=True) beta *= kappa current_error = float(torch.abs(W_f - W_r).mean()) if verbose: print(\"Iter \" + str(i + 1), \" | Error: \" + str(current_error)) if early_stop: if current_error \u003c best_error: best_error = current_error else: break scale = scale.to(tensor.device) zero = zero.to(tensor.device) del W_f, W_q, W_r, W_e torch.cuda.empty_cache() W_q = torch.round(tensor * scale + zero).clamp(min_max[0], min_max[1]) return W_q, scale, zero # Mainly used to check if the group-size is divisible by numel() def _is_divisible(val1: int, val2: int) -\u003e bool: return int(val2 * math.ceil(val1 / val2)) == val1 # Converts hqq format W_dequant = (W_q - zero)*scale into affinequantized format: (W_q - mid_point)*scale_ao + zero_ao def _convert_to_affinequantized_format( W_q: torch.Tensor, scale: torch.Tensor, zero: torch.Tensor, nbits: int, shape: Union[List, Tuple, torch.Size], ) -\u003e Tuple: quant_min = 0 quant_max = 2**nbits - 1 mid_point = (quant_max + quant_min + 1) / 2 zero_ao = ((mid_point - zero.float()) * scale.float()).to(zero.dtype) scale_ao = scale W_q_ao = W_q.view(shape) return W_q_ao, scale_ao, zero_ao # Main hqq quantizer function def _choose_qparams_and_quantize_affine_hqq( tensor: torch.Tensor, nbits: float = 4, group_size: int = 64, optimize: bool = True, axis: int = 1, compute_dtype: torch.dtype = torch.float16, device: str = \"cuda\", verbose: bool = False, # to check the optimizer error raw_output: bool = False, # If True, it will return the quant params in hqq lib format optimize_weights: Callable = optimize_weights_proximal_legacy, # weights proximal optimizer function ) -\u003e tuple: \"\"\"Choose quantization parameters and quantize tensor using HQQ (Half-Quadratic Quantization). Performs quantization using HQQ method with optional weight optimization via proximal solver. Args: tensor: Input tensor to quantize (float32, float16, or bfloat16) nbits: Number of bits for quantization (default: 4) group_size: Size of quantization groups (default: 64) optimize: Whether to optimize weights using proximal solver (default: True) axis: Axis along which to perform quantization (0 or 1, default: 1) compute_dtype: Target compute dtype (default: torch.float16) device: Target device for computation (default: \"cuda\") verbose: Whether to print optimization error information (default: False) raw_output: If True, return params in HQQ library format (default: False) optimize_weights: Weight optimization function (default: optimize_weights_proximal_legacy) Returns: Tuple of (quantized_weights, scale, zero_point, original_shape) Note: Uses proximal solver to minimize ||W - dequantize(quantize(W))||_p^p for weight optimization. \"\"\" assert axis in [0, 1], \"axis should be either 0 or 1\" if group_size is not None: assert _is_divisible(tensor.numel(), group_size), ( \"group_size should be divisble by the total tensor dimensions. shape: \" + str(tensor.shape) + \", group_size: \" + str(group_size) ) # It\u0027s better to work with float32 here W = tensor.to(device=device, dtype=torch.float32) shape = W.shape # Reshape for grouping if group_size is not None: W = W.reshape([-1, group_size]) if (axis == 1) else W.reshape([group_size, -1]) # Get min/max values _min = W.min(axis=axis, keepdim=True)[0] _max = W.max(axis=axis, keepdim=True)[0] max_v = round(2**nbits - 1) min_v = 0 min_max = [min_v, max_v] # Clamp to avoid fp16 issues scale = (max_v / (_max - _min)).clamp(max=2e4) zero = -_min * scale # Round zero as in: https://github.com/casper-hansen/AutoAWQ/blob/main/awq/quantize/quantizer.py#L42C9-L42C14 if nbits in [4]: zero = _Round.apply(zero) # Fine-tune weights if optimize: W_q, scale, zero = optimize_weights( tensor=W, scale=scale, zero=zero, min_max=min_max, axis=axis, device=device, verbose=verbose, ) else: zero = zero.to(compute_dtype) scale = scale.to(compute_dtype) W_q = _Round.apply(W * scale + zero).clamp(min_max[0], min_max[1]) # Store meta-data (we invert the scale for dequantization) scale = 1.0 / scale # Convert to TensorCoreTiled format # TODO move the conversion of zero_point out of quant_primitives # and into TensorCoreTiledLayout.from_plain and rename this # helper function correctly. if raw_output is False: W_q, scale, zero = _convert_to_affinequantized_format( W_q, scale, zero, nbits, shape ) else: # this path was not used before, the way hqq sets up scale/zero is transposed # compared to the rest of our utils so we need to reshape them acccordingly. W_q = W_q.reshape(shape) if axis == 1: scale = scale.reshape(shape[0], -1) zero = zero.reshape(shape[0], -1) else: scale = scale.reshape(-1, shape[-1]) zero = zero.reshape(-1, shape[-1]) # Make sure all the weights are in the right compute_dtype/device W_q = W_q.to(dtype=torch.uint8, device=device) scale = scale.to(dtype=compute_dtype, device=device) zero = zero.to(dtype=compute_dtype, device=device) # cleanup del W, _min, _max torch.cuda.empty_cache() return W_q, scale, zero, shape @torch.no_grad() def _choose_qparams_and_quantize_scale_only_hqq( hp_tensor: torch.Tensor, block_size: List[int], qmin: int, qmax: int, *, iters: int = 20, stochastic: bool = False, early_stop_tol: float = 1e-5, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\" Half-Quadratic Quantization (scale-only, symmetric) for 2D weights with row-wise blocks. - hp_tensor: [out, in] (bf16/fp16/fp32 accepted; promoted to fp32 internally) - block_size: must be [1, group_size]; groups along the last dim - qmin, qmax: integer range (e.g., -8, 7 for signed 4-bit) Returns: qdata: int32, same shape as hp_tensor scale: hp_tensor.dtype, shape [out, in // group_size] (one scale per row-wise block) \"\"\" # --- strict interface guarantees --- assert hp_tensor.ndim == 2, \"hp_tensor must be 2D [out, in]\" assert isinstance(block_size, (list, tuple)) and len(block_size) == 2, ( \"block_size must be a 2-element list/tuple\" ) assert block_size[0] == 1 and block_size[1] \u003e= 1, ( \"block_size must be [1, group_size] with group_size \u003e= 1\" ) assert qmin \u003c qmax, \"qmin must be \u003c qmax\" # Promote to fp32 for stable math compute_dtype = torch.float32 compute_eps = torch.finfo(compute_dtype).eps n, k = hp_tensor.shape group_size = int(block_size[1]) assert k % group_size == 0, ( f\"in_features={k} must be divisible by group_size={group_size}\" ) def round_det(x: torch.Tensor) -\u003e torch.Tensor: # ties-to-even; fine for PTQ return x.round() def round_stoch(x: torch.Tensor) -\u003e torch.Tensor: # unbiased stochastic rounding return torch.floor(x + torch.rand_like(x)) _r = round_stoch if stochastic else round_det # Reshape Wg into [n, n_groups, group_size] W = hp_tensor.to(compute_dtype).contiguous() n_groups = k // group_size Wg = W.view(n, n_groups, group_size) # Initialize per-block scales as max-abs / qabs # scale.shape = [n, n_groups] qabs = max(abs(qmin), abs(qmax)) or 1 scale = (Wg.abs().amax(dim=2) / qabs).clamp_min(compute_eps) prev_scale = scale.clone() # Iterate HQQ updates for _ in range(max(1, iters)): # Quantize using current scale # Qg.shape = [n, n_groups, group_size] Qg = _r(Wg / scale.unsqueeze(-1)).clamp(qmin, qmax) # Solve least-square problem min_{s} ||Wg - s * Qg||^2 and project # solution onto positive space, or take previous value num = (Wg * Qg).sum(dim=2, dtype=torch.float32) # [n, n_groups] den = (Qg * Qg).sum(dim=2, dtype=torch.float32) # [n, n_groups] scale = torch.where(den \u003e 0, num / den, prev_scale) scale = scale.clamp_min( compute_eps ).abs() # project LS solution onto [eps, inf] rel = ((scale - prev_scale).abs() / prev_scale.clamp_min(compute_eps)).max() if rel \u003c early_stop_tol: break prev_scale = scale # Quantize using final scale Qg = _r(Wg / scale.unsqueeze(-1)).clamp(qmin, qmax) # Restore shapes qdata = Qg.view(n, k).contiguous().to(torch.int32) out_dtype = hp_tensor.dtype scale = scale.to(out_dtype) return qdata, scale def _choose_qparams_and_quantize_scale_only_sinq( tensor: torch.Tensor, qmin: int = -(2 ** (4 - 1)), qmax: int = 2 ** (4 - 1) - 1, group_size: int = 64, niter: int = 20, compute_dtype: torch.dtype = torch.float16, ) -\u003e tuple: \"\"\" SINQ: Sinkhorn-Normalized Quantization (https://www.arxiv.org/abs/2509.22944) Iteratively normalizes row and column standard deviations to minimize matrix imbalance before quantization with dual scales. Args: tensor: Input weight tensor group_size: Quantization group size (default: 64) niter: Number of Sinkhorn iterations (default: 20) compute_dtype: Target compute dtype (default: torch.float16) Returns: Tuple of (qdata, scale_row, scale_col) \"\"\" if group_size is not None: assert _is_divisible(tensor.numel(), group_size), ( f\"group_size must divide tensor elements. shape: {tensor.shape}, group_size: {group_size}\" ) W = tensor.to(dtype=compute_dtype) shape = W.shape # Reshape for 1D tiling W = W.reshape(-1, group_size) # [N*num_groups, group_size] # Algorithm 1: Sinkhorn Normalization q_min = min(W.std(dim=0).min().item(), W.std(dim=1).min().item()) q_min = max(q_min, 1e-8) W_hat = W.clone() scale_col_sinkhorn = torch.ones(W.shape[1], device=W.device, dtype=compute_dtype) scale_row_sinkhorn = torch.ones(W.shape[0], device=W.device, dtype=compute_dtype) for _ in range(niter): # Normalize columns (dim=0) q_col = W_hat.std(dim=0) / q_min q_col = torch.clamp(q_col, min=1e-8) W_hat = W_hat / q_col.unsqueeze(0) scale_col_sinkhorn = scale_col_sinkhorn * q_col # Normalize rows (dim=1) q_row = W_hat.std(dim=1) / q_min q_row = torch.clamp(q_row, min=1e-8) W_hat = W_hat / q_row.unsqueeze(1) scale_row_sinkhorn = scale_row_sinkhorn * q_row # INT8 symmetric quantization # TODO: Consider custom bitwidth for SIMD acceleration like vadd4 scale_s = (W_hat.abs().amax(dim=1, keepdim=True) / float(qmax)).clamp_min(1e-8) # TODO: Find better rounding strategy like stochastic rounding Q = _Round.apply(W_hat / scale_s).clamp(qmin, qmax) # TODO: PERF test for scale factor dtype (FP16 vs. INT8) # Although FP16 has high accuracy, FP16\u00d7INT8 can\u0027t be computed # in Tensor Core directly, requiring INT8 to FP16 ops. qdata = Q.view(shape).contiguous().to(torch.int8) # Combine RTN scale with row Sinkhorn factor scale_row = ( (scale_s.view(-1) * scale_row_sinkhorn).view(shape[0], -1).to(compute_dtype) ) num_groups = shape[1] // group_size scale_col = scale_col_sinkhorn.repeat(num_groups)[: shape[1]].to(compute_dtype) return qdata, scale_row, scale_col def _choose_qparams_affine_floatx( tensor: torch.Tensor, ebits: int, mbits: int ) -\u003e torch.Tensor: \"\"\"Choose quantization parameters for floatx quantization. Calculates scale parameter for quantizing to custom floating point format. Args: tensor: Input tensor to quantize (float32, float16, or bfloat16) ebits: Number of exponent bits in target floatx format mbits: Number of mantissa bits in target floatx format Returns: Scale tensor for floatx quantization Note: Uses global lookup table as workaround for torch.compile() compatibility since _n_ones() is not compatible due to \u003c\u003c operator. \"\"\" # _n_ones() is not compatible with torch.compile() due to \u003c\u003c operator # https://github.com/pytorch/pytorch/issues/119152 # exp_bias = _n_ones(ebits - 1) # max_normal = 2 ** (_n_ones(ebits) - exp_bias) * (_n_ones(mbits + 1) / (2 ** mbits)) # workaround: global lookup table exp_bias = _ONES_TABLE[ebits - 1] max_normal = 2 ** (_ONES_TABLE[ebits] - exp_bias) * ( _ONES_TABLE[mbits + 1] / (2**mbits) ) dtype = tensor.dtype tensor = tensor.float() scale = tensor.abs().amax(1).clamp(min=1e-12) / max_normal return scale.to(dtype) def _quantize_affine_floatx( tensor: torch.Tensor, scale: torch.Tensor, ebits: int, mbits: int ) -\u003e torch.Tensor: \"\"\"Quantizes the float32 high precision floating point tensor to low precision floating point number and converts the result to unpacked floating point format with the format of 00SEEEMM (for fp6_e3m2) where S means sign bit, e means exponent bit and m means mantissa bit \"\"\" tensor = tensor.float() tensor_floatx = _f32_to_floatx_unpacked(tensor / scale.view(-1, 1), ebits, mbits) return tensor_floatx def _dequantize_affine_floatx( tensor: torch.Tensor, scale: torch.Tensor, ebits: int, mbits: int, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: tensor = _floatx_unpacked_to_f32(tensor, ebits, mbits) tensor = tensor * scale.float().view(-1, 1) tensor = tensor.to(dtype=output_dtype) return tensor @register_custom_op def _choose_scale_float8( tensor: torch.Tensor, block_size: List[int], float8_dtype: torch.dtype = torch.float8_e4m3fn, scale_dtype: torch.dtype = torch.float32, hp_value_lb: Optional[float] = None, hp_value_ub: Optional[float] = None, ) -\u003e torch.Tensor: \"\"\" Calculates float8 scaling factor for the given high precision tensor. Args: tensor (torch.Tensor): Input tensor to be quantized. float8_dtype (torch.dtype): Data type of the quantized tensor (e.g., torch.float8_e4m3fn, torch.float8_e5m2). scale_dtype (torch.dtype): Data type of the scaling factor (e.g., torch.float32). block_size (Tuple[int, ...]): Block size for block-wise quantization. For per-tensor scaling, use block_size == tensor.shape. hp_value_lb (Optional[float]): the lower bound for high precision floating point value for calculating scale hp_value_ub (Optional[float]): the upper bound for high precision floating point value for calculating scale \"\"\" quant_max = torch.finfo(float8_dtype).max shape_for_reduction, reduction_dims = _get_reduction_params( block_size, tensor.shape ) tensor_reshaped = tensor.view(shape_for_reduction) max_abs = tensor_reshaped.abs().amax(dim=reduction_dims, keepdim=True) if hp_value_lb is not None or hp_value_ub is not None: max_abs = torch.clamp(max_abs, min=hp_value_lb, max=hp_value_ub) scale = max_abs / quant_max # Reshape scale back to match the expected output shape # The scale tensor should have the same shape as the input divided by block_size output_shape = [ input_size // block_size[i] for i, input_size in enumerate(tensor.shape) ] scale = scale.reshape(output_shape) if scale_dtype is not torch.float32: # Shielding for Version \u003e 2.8 assert scale_dtype is torch.float8_e8m0fnu, \"Only float8_e8m0fnuz is supported\" scale = torch.exp2(_Round.apply(torch.log2(scale))) return scale.to(dtype=torch.float32) def _maybe_expand_scale_to_tensor_shape( scale: torch.Tensor, target_shape: torch.Size ) -\u003e torch.Tensor: \"\"\" Expand a scale tensor to match the target tensor shape for block-wise quantization. If this is rowwise quantization, however, just return the scale as is. Args: scale (torch.Tensor): Scale tensor with shape corresponding to block structure target_shape (torch.Size): Target tensor shape to expand to Returns: torch.Tensor: Scale tensor expanded to match target_shape \"\"\" if scale.shape == target_shape: # Scale already matches target shape return scale if scale.numel() == 1: # Scalar scale - can broadcast naturally return scale # If the scale can be broadcast as is, then we don\u0027t need to expand it # E.g. for rowwise quantization, scale = [256, 1] and target_shape = [256, 512] if all(a == b or a == 1 for a, b in zip(scale.shape, target_shape)): return scale # Calculate block sizes from shape difference if len(scale.shape) != len(target_shape): raise ValueError( f\"Scale tensor has {len(scale.shape)} dimensions but target has {len(target_shape)}\" ) block_sizes = tuple( target_shape[i] // scale.shape[i] for i in range(len(target_shape)) ) # Verify that target_shape is evenly divisible by scale.shape for i, (target_dim, scale_dim, block_size) in enumerate( zip(target_shape, scale.shape, block_sizes) ): if target_dim != scale_dim * block_size: raise ValueError( f\"Dimension {i}: target size {target_dim} is not evenly divisible \" f\"by scale size {scale_dim} (block size would be {target_dim / scale_dim})\" ) # Expand scale using repeat_interleave expanded_scale = scale for i, block_size in enumerate(block_sizes): if block_size \u003e 1: expanded_scale = expanded_scale.repeat_interleave(block_size, dim=i) return expanded_scale def _quantize_affine_float8( tensor: torch.Tensor, scale: torch.Tensor, float8_dtype: torch.dtype = torch.float8_e4m3fn, ) -\u003e torch.Tensor: \"\"\" Quantizes the high precision floating point tensor to a float8 tensor, using the given scaling factor. \"\"\" tensor_fp32 = tensor.to(torch.float32) # Expand scale to match tensor dimensions for block-wise quantization scale_expanded = _maybe_expand_scale_to_tensor_shape(scale, tensor.shape) tensor_scaled = tensor_fp32 / scale_expanded max_value = torch.finfo(float8_dtype).max tensor_clamped = tensor_scaled.clamp(min=-max_value, max=max_value) return _RoundToFloat8.apply(tensor_clamped, float8_dtype) def _dequantize_affine_float8( tensor: torch.Tensor, scale: torch.Tensor, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\" Dequantizes the float8 tensor to high precision tensor. \"\"\" fp8_tensor = tensor.to(torch.float32) # Expand scale to match tensor dimensions for block-wise quantization scale_expanded = _maybe_expand_scale_to_tensor_shape(scale, tensor.shape) hp_tensor = fp8_tensor * scale_expanded return hp_tensor.to(output_dtype) @_register_custom_op(quant_lib, False) def _quantize_affine_float8_non_decomposed( tensor: torch.Tensor, scale: torch.Tensor, float8_dtype: torch.dtype = torch.float8_e4m3fn, ) -\u003e torch.Tensor: \"\"\" Quantizes the high precision floating point tensor to a float8 tensor, using the given scaling factor. \"\"\" return _quantize_affine_float8( tensor=tensor, scale=scale, float8_dtype=float8_dtype, ) @_register_meta_op(quant_lib, \"quantize_affine_float8_non_decomposed\") def _quantize_affine_float8_meta( tensor: torch.Tensor, scale: torch.Tensor, float8_dtype: torch.dtype = torch.float8_e4m3fn, ) -\u003e torch.Tensor: return torch.empty_like(tensor, dtype=float8_dtype) @_register_custom_op(quant_lib, False) def _dequantize_affine_float8_non_decomposed( tensor: torch.Tensor, scale: torch.Tensor, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: \"\"\" Dequantizes the float8 tensor to high precision tensor. \"\"\" return _dequantize_affine_float8( tensor=tensor, scale=scale, output_dtype=output_dtype, ) @_register_meta_op(quant_lib, \"dequantize_affine_float8_non_decomposed\") def _dequantize_affine_float8_meta( tensor: torch.Tensor, scale: torch.Tensor, output_dtype: torch.dtype = torch.float32, ) -\u003e torch.Tensor: return torch.empty_like(tensor, dtype=output_dtype)",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/quantization/quant_primitives.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>