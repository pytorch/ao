


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.quantization.quant_primitives &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributor_guide.html">Contributor Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.quantization.quant_primitives</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.quantization.quant_primitives</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>

<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">auto</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.prototype.custom_fp_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_f32_to_floatx_unpacked</span><span class="p">,</span>
    <span class="n">_floatx_unpacked_to_f32</span><span class="p">,</span>
    <span class="n">_n_ones</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_3</span><span class="p">,</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">,</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">,</span>
    <span class="n">_register_custom_op</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;choose_qparams_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_affine_tinygemm&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_affine_dont_preserve_zero&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_affine_with_min_max&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_affine_floatx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_affine_no_zero_point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_affine_float_zero_point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine_no_zero_point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine_float_zero_point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_affine_floatx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine_floatx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fake_quantize_affine&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fake_quantize_affine_cachemask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_and_quantize_affine_hqq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_and_quantize_affine_qqq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine_qqq&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MappingType&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ZeroPointDomain&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TorchAODType&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_affine_float8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_affine_float8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_affine_float8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;choose_qparams_gguf&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_gguf&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dequantize_gguf&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="MappingType"><a class="viewcode-back" href="../../../generated/torchao.quantization.MappingType.html#torchao.quantization.MappingType">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">MappingType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;How floating point number is mapped to integer number</span>

<span class="sd">    symmetric mapping means floating point range is symmetrically mapped to integer range</span>
<span class="sd">    let&#39;s say we have floating point range (-3.5, 10.2) and integer range (-8, 7) (int4)</span>
<span class="sd">    we&#39;ll use (-10.2, 10.2) as the range for floating point and map that to (-8, 7)</span>
<span class="sd">    e.g. scale = (10.2 - (-10.2)) / (7 - (-8))</span>

<span class="sd">    SYMMETRIC_NO_CLIPPING_ERR is a variant of symmetric mapping, where the scale is the max of smin</span>
<span class="sd">    and smax, where smin = min_val_neg / quant_min, and smax = max_val_pos / quant_max. By calculating</span>
<span class="sd">    smin and smax individually, there can be less round error on negative values, and no out-of-range</span>
<span class="sd">    of all floating point values.</span>

<span class="sd">    asymmetric mapping means we just directly map the floating point range to integer range,</span>
<span class="sd">    for the above example, we will map (-3.5, 10.2) to (-8, 7) and calculate quantization parameter</span>
<span class="sd">    based on this mapping</span>
<span class="sd">    e.g. scale = (10.2 - (-3.5)) / (7 - (-8))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SYMMETRIC</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SYMMETRIC_NO_CLIPPING_ERR</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">ASYMMETRIC</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<div class="viewcode-block" id="ZeroPointDomain"><a class="viewcode-back" href="../../../generated/torchao.quantization.ZeroPointDomain.html#torchao.quantization.ZeroPointDomain">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ZeroPointDomain</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Enum that indicate whether zero_point is in integer domain or floating point domain</span>

<span class="sd">    integer domain: quantized_val = (float_val / scale) (integer) + zero_point (integer)</span>
<span class="sd">    float domain: quantized_val = (float_val - (zero_point (float) - scale * mid_point)) / scale</span>
<span class="sd">    none domain: quantized_val = (float_val / scale)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">INT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">FLOAT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<div class="viewcode-block" id="TorchAODType"><a class="viewcode-back" href="../../../generated/torchao.quantization.TorchAODType.html#torchao.quantization.TorchAODType">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TorchAODType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Placeholder for dtypes that do not exist in PyTorch core yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># torch.int1 to torch.int7 will be added to PyTorch 2.6</span>
    <span class="c1"># These will remain here for BC with older PyTorch versions</span>
    <span class="n">INT1</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT2</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT3</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT4</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT5</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT6</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">INT7</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">MappingType</span><span class="p">,</span> <span class="n">ZeroPointDomain</span><span class="p">])</span>

<span class="n">FP8_TYPES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">,</span>
<span class="p">}</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Map from dtype to the bound value of integers</span>
<span class="sd">TODO: maybe can replace this with call to torch.iinfo</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">15</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">15</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT2</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT3</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT5</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT6</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT7</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_SUB_BYTE_INT_BOUNDS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">TorchAODType</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT1</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT2</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT3</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT4</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT5</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT6</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT7</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">6</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">}</span>

<span class="c1"># torch.uintX available only in PyTorch 2.3+</span>
<span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_3</span><span class="p">:</span>
    <span class="n">_SUB_BYTE_UINT_BOUNDS</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint1</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint2</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint3</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint5</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint6</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint7</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">7</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint2</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint3</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint5</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint6</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint7</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>

<span class="c1"># torch.intX available only in PyTorch 2.6+</span>
<span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_6</span><span class="p">:</span>
    <span class="n">_SUB_BYTE_INT_BOUNDS</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int1</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int2</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int3</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int5</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int6</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">5</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int7</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">6</span><span class="p">),</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int2</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int3</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int5</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int6</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int7</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>

<span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">)</span>
<span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_SUB_BYTE_INT_BOUNDS</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">_DTYPE_TO_BIT_WIDTH</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="o">==</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

<span class="n">_GGUF_QK_K</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">_ONES_TABLE</span> <span class="o">=</span> <span class="p">[</span><span class="n">_n_ones</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)]</span>

<span class="n">quant_lib</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">Library</span><span class="p">(</span><span class="s2">&quot;torchao&quot;</span><span class="p">,</span> <span class="s2">&quot;FRAGMENT&quot;</span><span class="p">)</span>

<span class="n">register_custom_op</span> <span class="o">=</span> <span class="n">_register_custom_op</span><span class="p">(</span><span class="n">quant_lib</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_Round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of generic round operation with backward STE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">gy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gy</span>


<span class="c1"># TODO: decide on if we want to allow custom quant_min/quant_max here</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get quant_min and quant_max args based on dtype and also</span>
<span class="sd">    verify that they are within the range of possible quant_min/quant_max</span>
<span class="sd">    for dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">FP8_TYPES</span><span class="p">:</span>
        <span class="n">quant_min_lower_bound</span><span class="p">,</span> <span class="n">quant_max_upper_bound</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported dtype: </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quant_min_lower_bound</span><span class="p">,</span> <span class="n">quant_max_upper_bound</span> <span class="o">=</span> <span class="n">_DTYPE_TO_QVALUE_BOUNDS</span><span class="p">[</span><span class="n">dtype</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">quant_min</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">quant_min</span> <span class="o">=</span> <span class="n">quant_min_lower_bound</span>
    <span class="k">if</span> <span class="n">quant_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">quant_max</span> <span class="o">=</span> <span class="n">quant_max_upper_bound</span>

    <span class="k">assert</span> <span class="n">quant_min</span> <span class="o">&gt;=</span> <span class="n">quant_min_lower_bound</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;quant_min out of bound for dtype, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;quant_min_lower_bound: </span><span class="si">{</span><span class="n">quant_min_lower_bound</span><span class="si">}</span><span class="s2"> quant_min: </span><span class="si">{</span><span class="n">quant_min</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="n">quant_max</span> <span class="o">&lt;=</span> <span class="n">quant_max_upper_bound</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;quant_max out of bound for dtype, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;quant_max_upper_bound: </span><span class="si">{</span><span class="n">quant_max_upper_bound</span><span class="si">}</span><span class="s2"> quant_max: </span><span class="si">{</span><span class="n">quant_max</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_reduction_params</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given block_size and input size find the parameters for reduction:</span>

<span class="sd">    Output:</span>
<span class="sd">        shape_for_reduction: the shape we use to `view` input to prepare it for reduction</span>
<span class="sd">        reduction_dims: the dims we&#39;ll do reduction over</span>

<span class="sd">    Example::</span>
<span class="sd">        Input:</span>
<span class="sd">          block_size: (3, 3, 2, 10)</span>
<span class="sd">          input_size: (3, 3, 10, 10)</span>

<span class="sd">        Output:</span>
<span class="sd">          shape_for_reduction: (3, 3, 5, 2, 10)</span>
<span class="sd">          reduction_dim: [0, 1, 3, 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
    <span class="n">shape_for_reduction</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">reduction_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cur_dim</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">and</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expecting input size at </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> dimension: </span><span class="si">{</span><span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> to be divisible by block_size at </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> dimension: </span><span class="si">{</span><span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="c1"># reduce over the block_size[i] dim</span>
            <span class="n">reduction_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_dim</span> <span class="o">+=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># block_size[i] == input_size[i] or block_size[i] == 1</span>
            <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="c1"># we only need to reduce over the dimension if block_size is greater than 1</span>
            <span class="c1"># otherwise it&#39;s already the same as reduced dimension</span>
            <span class="k">if</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">reduction_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_dim</span><span class="p">)</span>
            <span class="n">cur_dim</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span>


<div class="viewcode-block" id="quantize_affine"><a class="viewcode-back" href="../../../generated/torchao.quantization.quantize_affine.html#torchao.quantization.quantize_affine">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): original float32, float16 or bfloat16 Tensor</span>
<span class="sd">      block_size: (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">           e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (float): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (int): quantization parameter for affine quantization</span>
<span class="sd">      output_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype</span>

<span class="sd">    Note:</span>
<span class="sd">      How can block_size represent different granularities?</span>
<span class="sd">      let&#39;s say we have a Tensor of size: (3, 3, 10, 10), here is the table showing how block_size represents different</span>
<span class="sd">      granularities:</span>

<span class="sd">       granularity type       |     block_size</span>
<span class="sd">         per_tensor           |    (3, 3, 10, 10)</span>
<span class="sd">         per_axis (axis=0)    |    (1, 3, 10, 10)</span>
<span class="sd">         per_axis (axis=1)    |    (3, 1, 10, 10)</span>
<span class="sd">     per_group (groupsize=2)  |    (3, 3, 10, 2)</span>
<span class="sd">     per_group (groupsize=2) for axis = 3 | (3, 3, 2, 10)</span>


<span class="sd">    Output:</span>
<span class="sd">      quantized tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;op definition that has compatible signatures with custom op library</span>

<span class="sd">    Note:</span>
<span class="sd">        zero_point_domain is pre-defined specifies how we quantize the floating point to quantized data:</span>
<span class="sd">        INT: quantized_val = (float_val / scale) (integer) + zero_point (integer)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="c1"># workaround for uintx dtypes, since we don&#39;t have native Uintx dtype connected with</span>
    <span class="c1"># torch.uintx dtypes yet</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">return</span> <span class="n">_quantize_affine_no_dtype_cast</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_no_dtype_cast</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. quantize the input based on the quantization parameters scale and zero_point and zero_point_domain = INT</span>
<span class="sd">    3. reshape the quantized result to origianl shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validations</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported input dtype: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># in some cases zero_point being a non-value shows as a tensor</span>
        <span class="c1"># with numel=0 which we handle by unifying the two</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span><span class="p">))</span> <span class="o">+</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span>
    <span class="p">)</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quant</span>


<span class="k">def</span><span class="w"> </span><span class="nf">quantize_affine_float_zero_point</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. quantize the input based on the quantization parameters scale and zero_point and zero_point_domain = FLOAT</span>
<span class="sd">    3. reshape the quantized result to origianl shape</span>

<span class="sd">    Note:</span>
<span class="sd">        zero_point_domain is pre-defined specifies how we quantize the floating point to quantized data:</span>
<span class="sd">        FLOAT: quantized_val = (float_val - (zero_point (float) - scale * mid_point)) / scale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="c1"># workaround for uintx dtypes, since we don&#39;t have native Uintx dtype connected with</span>
    <span class="c1"># torch.uintx dtypes yet</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">return</span> <span class="n">_quantize_affine_float_zero_point_no_dtype_cast</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_float_zero_point_no_dtype_cast</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. quantize the input based on the quantization parameters scale and zero_point and zero_point_domain = FLOAT</span>
<span class="sd">    3. reshape the quantized result to origianl shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validations</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported input dtype: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># in some cases zero_point being a non-value shows as a tensor</span>
        <span class="c1"># with numel=0 which we handle by unifying the two</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">zero_point</span> <span class="o">-</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">mid_point</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">((</span><span class="nb">input</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span><span class="p">),</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quant</span>


<span class="k">def</span><span class="w"> </span><span class="nf">quantize_affine_no_zero_point</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. quantize the input based on the quantization parameters scale and zero_point and zero_point_domain = NONE</span>
<span class="sd">    3. reshape the quantized result to origianl shape</span>

<span class="sd">    Note:</span>
<span class="sd">        zero_point_domain is pre-defined specifies how we quantize the floating point to quantized data:</span>
<span class="sd">        None: quantized_val = (float_val / scale) | this is primarily used for floatx quantization</span>
<span class="sd">            Where we do not want to round values to nearest integer and instead scale and cast.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="c1"># workaround for uintx dtypes, since we don&#39;t have native Uintx dtype connected with</span>
    <span class="c1"># torch.uintx dtypes yet</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">return</span> <span class="n">_quantize_affine_no_zero_point_no_dtype_cast</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_quantize_affine_no_zero_point_no_dtype_cast</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. quantize the input based on the quantization parameters scale and zero_point and zero_point_domain = NONE</span>
<span class="sd">    3. reshape the quantized result to origianl shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validations</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported input dtype: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># in some cases zero_point being a non-value shows as a tensor</span>
        <span class="c1"># with numel=0 which we handle by unifying the two</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)),</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="n">quant</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quant</span>


<div class="viewcode-block" id="dequantize_affine"><a class="viewcode-back" href="../../../generated/torchao.quantization.dequantize_affine.html#torchao.quantization.dequantize_affine">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument</span>
<span class="sd">      block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">                               e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for input Tensor</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for input Tensor</span>
<span class="sd">      output_dtype (torch.dtype): dtype for output Tensor, default is fp32</span>

<span class="sd">      Default value for zero_point is in integer domain, zero point is added to the quantized integer value during quantization</span>

<span class="sd">    Output:</span>
<span class="sd">      dequantized Tensor, with requested dtype or fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_dequantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">input_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;op definition that has compatible signatures with custom op library&quot;&quot;&quot;</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">, got: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported output dtype: </span><span class="si">{</span><span class="n">output_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_no_dtype_check</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_no_dtype_check</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function converts AQT tensors to their high precision floating point representation</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. dequantize the input based on the quantization parameters scale and zero_point and args like zero_point_domain</span>
<span class="sd">    3. reshape the quantized result to origianl shape and change dtype to the output_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># Force a copy to avoid input modification due</span>
    <span class="c1"># to upcoming in-place operations.</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span> <span class="o">-</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="k">return</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_no_zero_point_no_dtype_check</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function converts AQT tensors to their high precision floating point representation</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. dequantize the input based on the quantization parameters scale and zero_point and args like zero_point_domain</span>
<span class="sd">    3. reshape the quantized result to origianl shape and change dtype to the output_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;zero_point should be None for dequantize_affine_no_zero_point&quot;</span>
    <span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="k">return</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine_no_zero_point</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument</span>
<span class="sd">      block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">                               e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (Tensor): quantization parameter for affine quantization, no zero point is used for this op</span>
<span class="sd">      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for input Tensor</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for input Tensor</span>
<span class="sd">      output_dtype (torch.dtype): dtype for output Tensor, default is fp32</span>

<span class="sd">      Default value for zero_point is in integer domain, zero point is added to the quantized integer value during quantization</span>

<span class="sd">    Output:</span>
<span class="sd">      dequantized Tensor, with requested dtype or fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">, got: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported output dtype: </span><span class="si">{</span><span class="n">output_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_no_zero_point_no_dtype_check</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dequantize_affine_float_zero_point_no_dtype_check</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function converts AQT tensors to their high precision floating point representation</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size, also reshape the input to align with</span>
<span class="sd">       the shape after reduction</span>
<span class="sd">    2. dequantize the input based on the quantization parameters scale and zero_point and args like zero_point_domain</span>
<span class="sd">    3. reshape the quantized result to origianl shape and change dtype to the output_dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># TODO: this seems to be a detail for tinygemm (converting from uint to int, probably need to refactor this)</span>
    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="c1"># This should allocate new memory and avoid input modification</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">mid_point</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">*=</span> <span class="n">scale</span>
    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dequant</span> <span class="o">+=</span> <span class="n">zero_point</span>

    <span class="k">return</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine_float_zero_point</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">input_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument</span>
<span class="sd">      block_size: (List[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">                               e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (Tensor): quantization parameter for affine quantization</span>
<span class="sd">      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for input Tensor</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for input Tensor</span>
<span class="sd">      output_dtype (torch.dtype): dtype for output Tensor, default is fp32</span>

<span class="sd">      Default value for zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)</span>

<span class="sd">    Output:</span>
<span class="sd">      dequantized Tensor, with requested dtype or fp32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: validate scale/zero_point dimensions are compatible with block_size</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUB_BYTE_UINT_BOUNDS</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">, got: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported output dtype: </span><span class="si">{</span><span class="n">output_dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_dequantize_affine_float_zero_point_no_dtype_check</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="fake_quantize_affine"><a class="viewcode-back" href="../../../generated/torchao.quantization.fake_quantize_affine.html#torchao.quantization.fake_quantize_affine">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">fake_quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General fake quantize op for quantization-aware training (QAT).</span>
<span class="sd">    This is equivalent to calling `quantize_affine` + `dequantize_affine`</span>
<span class="sd">    but without the dtype casts.</span>

<span class="sd">    Args:</span>
<span class="sd">      input (torch.Tensor): original float32, float16 or bfloat16 Tensor</span>
<span class="sd">      block_size: (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">           e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      scale (float): quantization parameter for affine quantization</span>
<span class="sd">      zero_point (int): quantization parameter for affine quantization</span>
<span class="sd">      quant_dtype (torch.dtype): desired quantized dtype for determining and validating quant_min and quant_max values.</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype</span>
<span class="sd">      zero_point_domain (ZeroPointDomain): the domain that zero_point is in, should be either integer or float</span>
<span class="sd">        if zero_point is in integer domain, zero point is added to the quantized integer value during</span>
<span class="sd">        quantization</span>
<span class="sd">        if zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)</span>
<span class="sd">        value during quantization</span>
<span class="sd">        default is ZeroPointDomain.INT</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please use ZeroPointDomain.NONE instead of None&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span> <span class="ow">and</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;zero_point should be None when zero_point_domain is NONE&quot;</span><span class="p">)</span>
    <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">fq</span><span class="p">)</span> <span class="o">=</span> <span class="n">_do_fake_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fq</span></div>


<div class="viewcode-block" id="fake_quantize_affine_cachemask"><a class="viewcode-back" href="../../../generated/torchao.quantization.fake_quantize_affine_cachemask.html#torchao.quantization.fake_quantize_affine_cachemask">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">fake_quantize_affine_cachemask</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General fake quantize op for quantization-aware training (QAT).</span>
<span class="sd">    This is equivalent to calling `quantize_affine` + `dequantize_affine`</span>
<span class="sd">    but without the dtype casts.</span>

<span class="sd">    Note: Compared to :func:`~torchao.quantization.quant_primitives.fake_quantize_affine`,</span>
<span class="sd">    this consumes more memory and returns an additional outlier mask for</span>
<span class="sd">    intermediate quantized values.</span>

<span class="sd">    Args:</span>
<span class="sd">      Same as :func:`~torchao.quantization.quant_primitives.fake_quantize_affine`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 2-tuple of (</span>
<span class="sd">          final fake quantized values,</span>
<span class="sd">          outlier mask for intermediate quantized values</span>
<span class="sd">      )</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please use ZeroPointDomain.NONE instead of None&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;zero_point should be None when zero_point_domain is NONE&quot;</span><span class="p">)</span>
    <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">dq</span><span class="p">)</span> <span class="o">=</span> <span class="n">_do_fake_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">((</span><span class="n">q</span> <span class="o">&gt;=</span> <span class="n">quant_min</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span> <span class="o">&lt;=</span> <span class="n">quant_max</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">dq</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_do_fake_quantize_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">quant_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for `fake_quantize_affine` that returns both the</span>
<span class="sd">    intermediate quantized values and the final dequantized values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">quant_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">:</span>
        <span class="n">_quantize_affine</span> <span class="o">=</span> <span class="n">_quantize_affine_no_dtype_cast</span>
        <span class="n">_dequantize_affine</span> <span class="o">=</span> <span class="n">_dequantize_affine_no_dtype_check</span>
    <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">:</span>
        <span class="n">_quantize_affine</span> <span class="o">=</span> <span class="n">_quantize_affine_float_zero_point_no_dtype_cast</span>
        <span class="n">_dequantize_affine</span> <span class="o">=</span> <span class="n">_dequantize_affine_float_zero_point_no_dtype_check</span>
    <span class="k">elif</span> <span class="n">ZeroPointDomain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
        <span class="n">_quantize_affine</span> <span class="o">=</span> <span class="n">_quantize_affine_no_zero_point_no_dtype_cast</span>
        <span class="n">_dequantize_affine</span> <span class="o">=</span> <span class="n">_dequantize_affine_no_zero_point_no_dtype_check</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unrecognized zero point domain: </span><span class="si">{</span><span class="n">zero_point_domain</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">_quantize_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dq</span> <span class="o">=</span> <span class="n">_dequantize_affine</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">input_dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">dq</span><span class="p">)</span>


<div class="viewcode-block" id="choose_qparams_affine"><a class="viewcode-back" href="../../../generated/torchao.quantization.choose_qparams_affine.html#torchao.quantization.choose_qparams_affine">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): fp32, bf16, fp16 input Tensor</span>
<span class="sd">        mapping_type (MappingType): determines how the qparams are calculated, symmetric or asymmetric</span>
<span class="sd">        block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">          e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">        target_dtype (torch.dtype): dtype for target quantized Tensor</span>
<span class="sd">        quant_min (Optional[int]): minimum quantized value for target quantized Tensor</span>
<span class="sd">        quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor</span>
<span class="sd">        eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype</span>
<span class="sd">        scale_dtype (torch.dtype): dtype for scale Tensor</span>
<span class="sd">        zero_point_dtype (torch.dtype): dtype for zero_point Tensor, defaults to torch.int32</span>
<span class="sd">        Now removed params:</span>
<span class="sd">            zero_point_domain (ZeroPointDomain): the domain that zero_point is in, defaults to Integer or None</span>
<span class="sd">            preserve_zero (bool): whether to preserve zero in the quantized Tensor, defaults to True</span>

<span class="sd">    Output:</span>
<span class="sd">        Tuple of scales and zero_points Tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_choose_qparams_affine</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">target_dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="c1"># TODO: lower this op to custom op library</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine_tinygemm</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specialized version of choose_qparams_affine</span>

<span class="sd">    This is used for tinygemm int4mm kernel where zero point is in floating point domain</span>
<span class="sd">    and zero does not have to be exactly representable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): fp32, bf16, fp16 input Tensor</span>
<span class="sd">        mapping_type (MappingType): determines how the qparams are calculated, symmetric or asymmetric</span>
<span class="sd">        block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">        target_dtype (torch.dtype): dtype for target quantized Tensor</span>
<span class="sd">        quant_min (Optional[int]): minimum quantized value for target quantized Tensor</span>
<span class="sd">        quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor</span>
<span class="sd">        eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype</span>
<span class="sd">        scale_dtype (torch.dtype): dtype for scale Tensor</span>
<span class="sd">        zero_point_dtype (torch.dtype): dtype for zero_point Tensor</span>

<span class="sd">    Output:</span>
<span class="sd">        Tuple of scales and zero_points Tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="ow">is</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># For preserve_zero=False, we don&#39;t ensure zero is exactly representable</span>
    <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">min_val</span>
    <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">max_val</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

    <span class="c1"># For zero_point_domain=FLOAT in asymmetric quantization</span>
    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="c1"># this is not preserving zero_point, this is converting to TensorCoreTiledFormat</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">mid_point</span>

    <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>

    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span>


<span class="c1"># TODO: lower this op to custom op library</span>
<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine_dont_preserve_zero</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Specialized version of choose_qparams_affine with zero_point_domain=ZeroPointDomain.INT and preserve_zero=False.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): fp32, bf16, fp16 input Tensor</span>
<span class="sd">        mapping_type (MappingType): determines how the qparams are calculated, asymmetric only</span>
<span class="sd">        block_size: (Tuple[int]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">        target_dtype (torch.dtype): dtype for target quantized Tensor</span>
<span class="sd">        quant_min (Optional[int]): minimum quantized value for target quantized Tensor</span>
<span class="sd">        quant_max (Optioanl[int]): maximum quantized value for target quantized Tensor</span>
<span class="sd">        eps (Optional[float]): minimum scale, if not provided, default to eps of input.dtype</span>
<span class="sd">        scale_dtype (torch.dtype): dtype for scale Tensor</span>
<span class="sd">        zero_point_dtype (torch.dtype): dtype for zero_point Tensor</span>
<span class="sd">        Now removed params default values:</span>
<span class="sd">            zero_point_domain (ZeroPointDomain): the domain that zero_point is in, defaults to Integer</span>
<span class="sd">            preserve_zero (bool): whether to preserve zero in the quantized Tensor, defaults to False</span>

<span class="sd">    Output:</span>
<span class="sd">        Tuple of scales and zero_points Tensor with requested dtype</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># For no preserve zero, we don&#39;t ensure zero is exactly representable</span>
    <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">min_val</span>
    <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">max_val</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="c1"># Zero point is int</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span>
    <span class="p">)</span>


<span class="c1"># TODO: lower this op to custom op library</span>
<div class="viewcode-block" id="choose_qparams_affine_with_min_max"><a class="viewcode-back" href="../../../generated/torchao.quantization.choose_qparams_affine_with_min_max.html#torchao.quantization.choose_qparams_affine_with_min_max">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine_with_min_max</span><span class="p">(</span>
    <span class="n">min_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">max_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">preserve_zero</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A variant of :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`</span>
<span class="sd">    operator that pass in min_val and max_val directly instead of deriving these from a single input.</span>
<span class="sd">    This is used for observers in static quantization where min_val and max_val may be obtained through</span>
<span class="sd">    tracking all the data in calibration data set.</span>

<span class="sd">    Args:</span>
<span class="sd">      Mostly same as :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`. with one</span>
<span class="sd">      difference: instead of passing in `input` Tensor and use that to calculate min_val/max_val</span>
<span class="sd">      and then scale/zero_point, we pass in min_val/max_val directly</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please use ZeroPointDomain.NONE instead of None&quot;</span><span class="p">)</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">assert</span> <span class="n">min_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">max_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Need to provide `min_val` and `max_val`, got: {min_val, max_val}&quot;</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">max_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;Expecting `min_val` and `max_val` to have the same dtype, got: {min_val.dtype, max_val.dtype}&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="n">scale_device</span> <span class="o">=</span> <span class="n">min_val</span><span class="o">.</span><span class="n">device</span>

    <span class="k">if</span> <span class="n">preserve_zero</span><span class="p">:</span>
        <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">min_val</span><span class="p">))</span>
        <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">max_val</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">min_val</span>
        <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">max_val</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="ow">or</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span>
    <span class="p">):</span>
        <span class="c1"># scales</span>
        <span class="k">if</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">:</span>
            <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="n">min_val_neg</span><span class="p">,</span> <span class="n">max_val_pos</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span>
            <span class="c1"># calculate smin and smax individually and choose the larger one. For example, if quant_min = -8 and</span>
            <span class="c1"># quant_max = 7.</span>
            <span class="c1"># - If smin is bigger: There would be coverage on negative values down to -8, and less rounding</span>
            <span class="c1"># error than the existing SYMMETRIC case.</span>
            <span class="c1"># - If smax is bigger: it covers the positive values up to 7. The round</span>
            <span class="c1"># error may be bigger than the existing SYMMETRIC case. Either way, there&#39;s no out-of-range fp values after</span>
            <span class="c1"># quantization.</span>
            <span class="n">smin</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_min</span><span class="p">)</span>
            <span class="n">smax</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">smin</span> <span class="o">&gt;</span> <span class="n">smax</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">smin</span><span class="p">,</span> <span class="n">smax</span><span class="p">)</span>
        <span class="c1"># zeros</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">preserve_zero</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;preserve_zero == False is not supported for symmetric quantization&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">:</span>
            <span class="c1"># TODO INT should not be a valid ZeroPointDomain for symmetric quantization since</span>
            <span class="c1"># symmetric quant doesn&#39;t have a zero_point</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;zero_point_domain should be ZeroPointDomain.INT or ZeroPointDomain.NONE for symmetric quantization&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scale_device</span>
        <span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">:</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;zero_point must be in FLOAT/INT/None domain for asymmetric quantization&quot;</span>
            <span class="p">)</span>
            <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="c1"># this is not preserving zero_point, this is converting to TensorCoreTiledFormat</span>
            <span class="c1"># TODO move the conversion of zero_point out of quant_primitives</span>
            <span class="c1"># and into TensorCoreTiledLayout.from_plain</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">mid_point</span>

    <span class="k">if</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">min_val</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span></div>


<span class="nd">@register_custom_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_choose_qparams_affine</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">mapping_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;op definition that has compatible signatures with custom op library</span>

<span class="sd">    The op does the following:</span>
<span class="sd">    1. figure out the dimension for reduction based on block_size</span>
<span class="sd">    2. find min_val/max_val based on the dimension for reduction</span>
<span class="sd">    3. calculate quantization parameters based on min_val/max_val based on args like `preserve_zero`</span>
<span class="sd">       and `zero_point_domain`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="n">_get_and_check_qmin_qmax</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">mapping_type</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported mapping type: </span><span class="si">{</span><span class="n">mapping_type</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got input dim:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">, block_size: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">min_val</span><span class="p">))</span>
    <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">max_val</span><span class="p">))</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="o">.</span><span class="n">name</span>
        <span class="ow">or</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="o">.</span><span class="n">name</span>
    <span class="p">):</span>
        <span class="c1"># scales</span>
        <span class="k">if</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
            <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="n">min_val_neg</span><span class="p">,</span> <span class="n">max_val_pos</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC_NO_CLIPPING_ERR</span><span class="o">.</span><span class="n">name</span>
            <span class="c1"># calculate smin and smax individually and choose the larger one. For example, if quant_min = -8 and</span>
            <span class="c1"># quant_max = 7.</span>
            <span class="c1"># - If smin is bigger: There would be coverage on negative values down to -8, and less rounding</span>
            <span class="c1"># error than the existing SYMMETRIC case.</span>
            <span class="c1"># - If smax is bigger: it covers the positive values up to 7. The round</span>
            <span class="c1"># error may be bigger than the existing SYMMETRIC case. Either way, there&#39;s no out-of-range fp values after</span>
            <span class="c1"># quantization.</span>
            <span class="n">smin</span> <span class="o">=</span> <span class="n">min_val_neg</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_min</span><span class="p">)</span>
            <span class="n">smax</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">smin</span> <span class="o">&gt;</span> <span class="n">smax</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">smin</span><span class="p">,</span> <span class="n">smax</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span><span class="o">.</span><span class="n">name</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>

    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_and_quantize_affine_qqq</span><span class="p">(</span>
    <span class="n">w</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">assert</span> <span class="n">num_bits</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported num_bits = </span><span class="si">{</span><span class="n">num_bits</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">size_n</span><span class="p">,</span> <span class="n">size_k</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">group_size</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">size_k</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported groupsize = </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">orig_device</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">device</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">size_k</span>

    <span class="k">if</span> <span class="n">group_size</span> <span class="o">&lt;</span> <span class="n">size_k</span><span class="p">:</span>
        <span class="c1"># Reshape to [-1, group_size]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">))</span>

        <span class="n">max_q_val</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">half_q_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_q_val</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="c1"># Compute scale for each group</span>
        <span class="n">s_group</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">s_group</span> <span class="o">*=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">max_q_val</span>  <span class="c1"># 2 =&gt; symmetric</span>

        <span class="c1"># Quantize</span>
        <span class="n">q_w</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">w</span> <span class="o">/</span> <span class="n">s_group</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">q_w</span> <span class="o">+=</span> <span class="n">half_q_val</span>
        <span class="n">q_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">q_w</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_q_val</span><span class="p">)</span>
        <span class="c1"># Compute ref (dequantized)</span>
        <span class="n">w_ref</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_w</span> <span class="o">-</span> <span class="n">half_q_val</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="o">*</span> <span class="n">s_group</span>

        <span class="c1"># Restore original shapes</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">reshape_w</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">size_n</span><span class="p">,</span> <span class="n">size_k</span><span class="p">))</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">w</span>

        <span class="n">q_w</span> <span class="o">=</span> <span class="n">reshape_w</span><span class="p">(</span><span class="n">q_w</span><span class="p">)</span>
        <span class="n">w_ref</span> <span class="o">=</span> <span class="n">reshape_w</span><span class="p">(</span><span class="n">w_ref</span><span class="p">)</span>

        <span class="c1"># Compute int8 quantization scale for each channel</span>
        <span class="n">s_channel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w_ref</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">s_channel</span> <span class="o">/=</span> <span class="mf">127.0</span>
        <span class="n">t_int8</span> <span class="o">=</span> <span class="p">(</span><span class="n">w_ref</span> <span class="o">/</span> <span class="n">s_channel</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="n">w_ref</span> <span class="o">=</span> <span class="n">t_int8</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="o">*</span> <span class="n">s_channel</span>
        <span class="n">s_channel</span> <span class="o">=</span> <span class="n">s_channel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

        <span class="c1"># Fuse scales</span>
        <span class="n">s_group</span> <span class="o">=</span> <span class="p">(</span><span class="n">s_group</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size_n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="o">/</span> <span class="n">s_channel</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_q_val</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Compute scale for each channel</span>
        <span class="n">s_channel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">s_channel</span> <span class="o">/=</span> <span class="n">max_q_val</span>

        <span class="c1"># Quantize</span>
        <span class="n">q_w</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">w</span> <span class="o">/</span> <span class="n">s_channel</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">q_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">q_w</span><span class="p">,</span> <span class="o">-</span><span class="n">max_q_val</span><span class="p">,</span> <span class="n">max_q_val</span><span class="p">)</span>
        <span class="c1"># Compute ref (dequantized)</span>
        <span class="n">w_ref</span> <span class="o">=</span> <span class="n">q_w</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="o">*</span> <span class="n">s_channel</span>

        <span class="n">s_group</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">orig_device</span><span class="p">)</span>
        <span class="c1"># div 2 ** (8 - self.bits)) to offset right shift in unpacking</span>
        <span class="n">s_channel</span> <span class="o">/=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mi">8</span> <span class="o">-</span> <span class="n">num_bits</span><span class="p">)</span>
        <span class="n">s_channel</span> <span class="o">=</span> <span class="n">s_channel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size_n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">q_w</span><span class="p">,</span> <span class="n">s_group</span><span class="p">,</span> <span class="n">s_channel</span><span class="p">,</span> <span class="n">w_ref</span>


<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_gguf</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    There are two sets of qparams: quantized_block_scale, quantized_block_min and super_block_scale_scale and super_block_min_scale</span>
<span class="sd">    the relationship is the following:</span>
<span class="sd">    block_scale = quantized_block_scale * super_block_sclae</span>
<span class="sd">    block_min = quantized_block_min * super_block_min</span>
<span class="sd">    quantized_val = (float_val - block_min) / block_scale + quant_min</span>
<span class="sd">    first we calculate block_scale and block_min</span>
<span class="sd">    then we calculate super_block_scale_scale and super_block_min_scale</span>
<span class="sd">    after that we can calculate quantized_block_scale and quantized_min_scale</span>
<span class="sd">    the returned values are: super_block_scale_scale, super_block_min_scale, quantized_block_scale</span>
<span class="sd">    and quantized_min_scale</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>

    <span class="c1"># 1. get block_scale block_min</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># asymmetric quant to fully utilize the range</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">max_val</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">min_val</span>

    <span class="c1"># 2. get super_block_scale_scale and super_block_min_scale</span>
    <span class="k">assert</span> <span class="n">_GGUF_QK_K</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">super_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_GGUF_QK_K</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">super_block_size</span><span class="p">,</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>

    <span class="n">shape_after_reduction</span> <span class="o">=</span> <span class="n">shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">block_scale_absmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">block_scale</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">block_min_absmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">block_min</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="c1"># 2. get super_block_scale_scale and super_block_min_scale</span>
    <span class="c1"># TODO: make this configurable</span>
    <span class="c1"># we also quantize the quantization parameters (scale and min) for each block to 6 bit</span>
    <span class="c1"># for Q4_K</span>
    <span class="n">qparam_quant_max</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">6</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">qparam_quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">super_block_scale_scale</span> <span class="o">=</span> <span class="n">block_scale_absmax</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">qparam_quant_max</span> <span class="o">-</span> <span class="n">qparam_quant_min</span>
    <span class="p">)</span>
    <span class="n">super_block_min_scale</span> <span class="o">=</span> <span class="n">block_min_absmax</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">qparam_quant_max</span> <span class="o">-</span> <span class="n">qparam_quant_min</span>
    <span class="p">)</span>
    <span class="n">super_block_scale_scale_view</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>
    <span class="n">super_block_min_scale_view</span> <span class="o">=</span> <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># 3. quantize block scale and min are stored in 6 bits using super_block_scale_scale and super_block_min_scale</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">block_scale</span> <span class="o">/</span> <span class="n">super_block_scale_scale_view</span><span class="p">,</span> <span class="n">qparam_quant_min</span><span class="p">,</span> <span class="n">qparam_quant_max</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">block_min</span> <span class="o">/</span> <span class="n">super_block_min_scale_view</span><span class="p">,</span> <span class="n">qparam_quant_min</span><span class="p">,</span> <span class="n">qparam_quant_max</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">quantize_gguf</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">super_block_scale_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">super_block_min_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_min</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">target_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint4</span>

    <span class="c1"># step 1: first order quantization</span>
    <span class="c1"># just going through shape calculation for block_scale and block_min to get the correct shape</span>
    <span class="n">input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="n">input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_shape_for_reduction</span><span class="p">)</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># step 2: second order quantization, recover unquantized block_scale and block_min</span>
    <span class="n">super_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_GGUF_QK_K</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">super_block_input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">super_block_size</span><span class="p">,</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">super_block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">super_block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_scale_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_min_scale</span> <span class="o">=</span> <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>

    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span> <span class="o">*</span> <span class="n">quantized_block_scale</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">super_block_min_scale</span> <span class="o">*</span> <span class="n">quantized_block_min</span>

    <span class="c1"># step 3: quantization with the unquantized block_scale and block_min</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">int_data</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">block_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">block_scale</span>
    <span class="n">int_data</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">int_data</span>


<span class="k">def</span><span class="w"> </span><span class="nf">dequantize_gguf</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">super_block_scale_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">super_block_min_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_block_min</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># step 1. reshape input and quantized block scale and min to the shape</span>
    <span class="c1"># after first quantization</span>
    <span class="n">input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="n">input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">original_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_shape_for_reduction</span><span class="p">)</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>

    <span class="c1"># step 2. calculate and reshape block_qparams for second quantization step</span>
    <span class="n">super_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_GGUF_QK_K</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">super_block_input_shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
        <span class="n">super_block_size</span><span class="p">,</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">super_block_qparam_shape_after_reduction</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
        <span class="n">super_block_qparam_shape_after_reduction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">quantized_block_scale</span> <span class="o">=</span> <span class="n">quantized_block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">quantized_block_min</span> <span class="o">=</span> <span class="n">quantized_block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_input_shape_for_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_scale_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>
    <span class="n">super_block_min_scale</span> <span class="o">=</span> <span class="n">super_block_min_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">super_block_qparam_shape_after_reduction</span>
    <span class="p">)</span>

    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">super_block_scale_scale</span> <span class="o">*</span> <span class="n">quantized_block_scale</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">super_block_min_scale</span> <span class="o">*</span> <span class="n">quantized_block_min</span>

    <span class="c1"># step 3. dequantize with block_scale and block_min</span>
    <span class="n">block_scale</span> <span class="o">=</span> <span class="n">block_scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">block_min</span> <span class="o">=</span> <span class="n">block_min</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">block_qparam_shape_after_reduction</span><span class="p">)</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">block_scale</span> <span class="o">+</span> <span class="n">block_min</span>
    <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dequant</span> <span class="o">=</span> <span class="n">dequant</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dequant</span>


<span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine_qqq</span><span class="p">(</span>
    <span class="n">w</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">s_group</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">s_channel</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">num_bits</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Unsupported num_bits = </span><span class="si">{</span><span class="n">num_bits</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">size_n</span><span class="p">,</span> <span class="n">size_k</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">group_size</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">size_k</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Unsupported groupsize = </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">size_k</span>

    <span class="k">if</span> <span class="n">group_size</span> <span class="o">&lt;</span> <span class="n">size_k</span><span class="p">:</span>
        <span class="c1"># Reshape to [-1, group_size]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">))</span>

        <span class="n">max_q_val</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">half_q_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_q_val</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="n">s_group</span> <span class="o">=</span> <span class="n">s_group</span> <span class="o">*</span> <span class="n">s_channel</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="n">w_dq</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">-</span> <span class="n">half_q_val</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="o">*</span> <span class="n">s_group</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Restore original shapes</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">reshape_w</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">size_n</span><span class="p">,</span> <span class="n">size_k</span><span class="p">))</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">w</span>

        <span class="n">w_dq</span> <span class="o">=</span> <span class="n">reshape_w</span><span class="p">(</span><span class="n">w_dq</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">s_channel</span> <span class="o">=</span> <span class="n">s_channel</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mi">8</span> <span class="o">-</span> <span class="n">num_bits</span><span class="p">))</span>
        <span class="n">w_dq</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="o">*</span> <span class="n">s_channel</span>

    <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_dq</span> <span class="o">=</span> <span class="n">w_dq</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w_dq</span> <span class="o">=</span> <span class="n">w_dq</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w_dq</span>


<span class="c1"># HQQ</span>
<span class="c1">############################################################################</span>
<span class="c1"># Shrinking operator (proximal operator for the lp norm)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_shrink_lp_op</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">lp_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">lp_norm</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lp_norm</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>


<span class="c1"># Proximal solver || W - dequantize(quantize(W))||_p^p</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimize_weights_proximal_legacy</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">min_max</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">opt_params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;lp_norm&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">1e1</span><span class="p">,</span>
        <span class="s2">&quot;kappa&quot;</span><span class="p">:</span> <span class="mf">1.01</span><span class="p">,</span>
        <span class="s2">&quot;iters&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s2">&quot;early_stop&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="n">lp_norm</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">kappa</span><span class="p">,</span> <span class="n">iters</span><span class="p">,</span> <span class="n">early_stop</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;lp_norm&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;kappa&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;iters&quot;</span><span class="p">],</span>
        <span class="n">opt_params</span><span class="p">[</span><span class="s2">&quot;early_stop&quot;</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="p">(</span><span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>

    <span class="n">W_f</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="n">best_error</span> <span class="o">=</span> <span class="mf">1e4</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">W_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">W_f</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">min_max</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">min_max</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">W_r</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_q</span> <span class="o">-</span> <span class="n">zero</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>
        <span class="n">W_e</span> <span class="o">=</span> <span class="n">_shrink_lp_op</span><span class="p">(</span><span class="n">W_f</span> <span class="o">-</span> <span class="n">W_r</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">lp_norm</span><span class="p">)</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">W_q</span> <span class="o">-</span> <span class="p">(</span><span class="n">W_f</span> <span class="o">-</span> <span class="n">W_e</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">*=</span> <span class="n">kappa</span>

        <span class="n">current_error</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">W_f</span> <span class="o">-</span> <span class="n">W_r</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iter &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">&quot; | Error: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">current_error</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">early_stop</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">current_error</span> <span class="o">&lt;</span> <span class="n">best_error</span><span class="p">:</span>
                <span class="n">best_error</span> <span class="o">=</span> <span class="n">current_error</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">W_f</span><span class="p">,</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">W_r</span><span class="p">,</span> <span class="n">W_e</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="n">W_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tensor</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">min_max</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">min_max</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span>


<span class="c1"># Mainly used to check if the group-size is divisible by numel()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_is_divisible</span><span class="p">(</span><span class="n">val1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">val2</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">val2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">val1</span> <span class="o">/</span> <span class="n">val2</span><span class="p">))</span> <span class="o">==</span> <span class="n">val1</span>


<span class="c1"># Converts hqq format W_dequant = (W_q - zero)*scale into affinequantized format: (W_q - mid_point)*scale_ao + zero_ao</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_convert_to_affinequantized_format</span><span class="p">(</span>
    <span class="n">W_q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zero</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">nbits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">nbits</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">mid_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="n">quant_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">zero_ao</span> <span class="o">=</span> <span class="p">((</span><span class="n">mid_point</span> <span class="o">-</span> <span class="n">zero</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">scale_ao</span> <span class="o">=</span> <span class="n">scale</span>
    <span class="n">W_q_ao</span> <span class="o">=</span> <span class="n">W_q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W_q_ao</span><span class="p">,</span> <span class="n">scale_ao</span><span class="p">,</span> <span class="n">zero_ao</span>


<span class="c1"># Main hqq quantizer function</span>
<div class="viewcode-block" id="choose_qparams_and_quantize_affine_hqq"><a class="viewcode-back" href="../../../generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq.html#torchao.quantization.choose_qparams_and_quantize_affine_hqq">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_and_quantize_affine_hqq</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">nbits</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">optimize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">compute_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># to check the optimizer error</span>
    <span class="n">raw_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># If True, it will return the quant params in hqq lib format</span>
    <span class="n">optimize_weights</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">optimize_weights_proximal_legacy</span><span class="p">,</span>  <span class="c1"># weights proximal optimizer function</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;axis should be either 0 or 1&quot;</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">_is_divisible</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">group_size</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;group_size should be divisble by the total tensor dimensions. shape: &quot;</span>
            <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="o">+</span> <span class="s2">&quot;, group_size: &quot;</span>
            <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># It&#39;s better to work with float32 here</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Reshape for grouping</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="n">W</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">group_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get min/max values</span>
    <span class="n">_min</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">_max</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">max_v</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">nbits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">min_v</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">min_max</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_v</span><span class="p">,</span> <span class="n">max_v</span><span class="p">]</span>

    <span class="c1"># Clamp to avoid fp16 issues</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_v</span> <span class="o">/</span> <span class="p">(</span><span class="n">_max</span> <span class="o">-</span> <span class="n">_min</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="mf">2e4</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="o">-</span><span class="n">_min</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># Round zero as in: https://github.com/casper-hansen/AutoAWQ/blob/main/awq/quantize/quantizer.py#L42C9-L42C14</span>
    <span class="k">if</span> <span class="n">nbits</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">zero</span><span class="p">)</span>

    <span class="c1"># Fine-tune weights</span>
    <span class="k">if</span> <span class="n">optimize</span><span class="p">:</span>
        <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span> <span class="o">=</span> <span class="n">optimize_weights</span><span class="p">(</span>
            <span class="n">tensor</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">zero</span><span class="o">=</span><span class="n">zero</span><span class="p">,</span>
            <span class="n">min_max</span><span class="o">=</span><span class="n">min_max</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="n">W_q</span> <span class="o">=</span> <span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">min_max</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">min_max</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Store meta-data (we invert the scale for dequantization)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span>

    <span class="c1"># Convert to TensorCoreTiled format</span>
    <span class="c1"># TODO move the conversion of zero_point out of quant_primitives</span>
    <span class="c1"># and into TensorCoreTiledLayout.from_plain and rename this</span>
    <span class="c1"># helper function correctly.</span>
    <span class="k">if</span> <span class="n">raw_output</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span> <span class="o">=</span> <span class="n">_convert_to_affinequantized_format</span><span class="p">(</span>
            <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">nbits</span><span class="p">,</span> <span class="n">shape</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># this path was not used before, the way hqq sets up scale/zero is transposed</span>
        <span class="c1"># compared to the rest of our utils so we need to reshape them acccordingly.</span>
        <span class="n">W_q</span> <span class="o">=</span> <span class="n">W_q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Make sure all the weights are in the right compute_dtype/device</span>
    <span class="n">W_q</span> <span class="o">=</span> <span class="n">W_q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># cleanup</span>
    <span class="k">del</span> <span class="n">W</span><span class="p">,</span> <span class="n">_min</span><span class="p">,</span> <span class="n">_max</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">shape</span></div>


<div class="viewcode-block" id="choose_qparams_affine_floatx"><a class="viewcode-back" href="../../../generated/torchao.quantization.choose_qparams_affine_floatx.html#torchao.quantization.choose_qparams_affine_floatx">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine_floatx</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># _n_ones() is not compatible with torch.compile() due to &lt;&lt; operator</span>
    <span class="c1"># https://github.com/pytorch/pytorch/issues/119152</span>
    <span class="c1"># exp_bias = _n_ones(ebits - 1)</span>
    <span class="c1"># max_normal = 2 ** (_n_ones(ebits) - exp_bias) * (_n_ones(mbits + 1) / (2 ** mbits))</span>

    <span class="c1"># workaround: global lookup table</span>
    <span class="n">exp_bias</span> <span class="o">=</span> <span class="n">_ONES_TABLE</span><span class="p">[</span><span class="n">ebits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">max_normal</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">_ONES_TABLE</span><span class="p">[</span><span class="n">ebits</span><span class="p">]</span> <span class="o">-</span> <span class="n">exp_bias</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">_ONES_TABLE</span><span class="p">[</span><span class="n">mbits</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">mbits</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span> <span class="o">/</span> <span class="n">max_normal</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="quantize_affine_floatx"><a class="viewcode-back" href="../../../generated/torchao.quantization.quantize_affine_floatx.html#torchao.quantization.quantize_affine_floatx">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">quantize_affine_floatx</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantizes the float32 high precision floating point tensor to low precision floating point number and</span>
<span class="sd">    converts the result to unpacked floating point format with the format of 00SEEEMM (for fp6_e3m2) where S means sign bit, e means exponent bit and m means mantissa bit</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">tensor_floatx</span> <span class="o">=</span> <span class="n">_f32_to_floatx_unpacked</span><span class="p">(</span><span class="n">tensor</span> <span class="o">/</span> <span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_floatx</span></div>


<div class="viewcode-block" id="dequantize_affine_floatx"><a class="viewcode-back" href="../../../generated/torchao.quantization.dequantize_affine_floatx.html#torchao.quantization.dequantize_affine_floatx">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine_floatx</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">ebits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mbits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">_floatx_unpacked_to_f32</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">choose_qparams_affine_float8</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
    <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates float8 scaling factor for the given high precision tensor, using tensorwise granularity.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (torch.Tensor): Input tensor to be quantized.</span>
<span class="sd">        float8_dtype (torch.dtype): Data type of the quantized tensor (e.g., torch.float8_e4m3fn, torch.float8_e5m2).</span>
<span class="sd">        scale_dtype (torch.dtype): Data type of the scaling factor (e.g., torch.float32).</span>
<span class="sd">        block_size (Optional[Tuple[int, ...]]): Block size for block-wise quantization. If None, tensorwise quantization is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">float8_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
    <span class="c1"># only tensorwise scaling is supported for now:</span>
    <span class="k">if</span> <span class="n">block_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_abs</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">max_abs</span> <span class="o">/</span> <span class="n">quant_max</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_for_reduction</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span>
            <span class="n">block_size</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>
        <span class="n">tensor_reshaped</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape_for_reduction</span><span class="p">)</span>
        <span class="n">max_abs</span> <span class="o">=</span> <span class="n">tensor_reshaped</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">reduction_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">scale</span> <span class="o">=</span> <span class="n">max_abs</span> <span class="o">/</span> <span class="n">quant_max</span>
        <span class="c1"># Reshape scale back to match the expected output shape</span>
        <span class="c1"># The scale tensor should have the same shape as the input divided by block_size</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">input_size</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">input_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="c1"># Shielding for Version &gt; 2.8</span>
        <span class="k">assert</span> <span class="n">scale_dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e8m0fnu</span><span class="p">,</span> <span class="s2">&quot;Only float8_e8m0fnuz is supported&quot;</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp2</span><span class="p">(</span><span class="n">_Round</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">scale</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_expand_scale_to_tensor_shape</span><span class="p">(</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expand a scale tensor to match the target tensor shape for block-wise quantization.</span>

<span class="sd">    Args:</span>
<span class="sd">        scale (torch.Tensor): Scale tensor with shape corresponding to block structure</span>
<span class="sd">        target_shape (torch.Size): Target tensor shape to expand to</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Scale tensor expanded to match target_shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target_shape</span><span class="p">:</span>
        <span class="c1"># Scale already matches target shape</span>
        <span class="k">return</span> <span class="n">scale</span>

    <span class="k">if</span> <span class="n">scale</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Scalar scale - can broadcast naturally</span>
        <span class="k">return</span> <span class="n">scale</span>

    <span class="c1"># Calculate block sizes from shape difference</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Scale tensor has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> dimensions but target has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">block_sizes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
        <span class="n">target_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">//</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="c1"># Verify that target_shape is evenly divisible by scale.shape</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">target_dim</span><span class="p">,</span> <span class="n">scale_dim</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">target_shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">block_sizes</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">target_dim</span> <span class="o">!=</span> <span class="n">scale_dim</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Dimension </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: target size </span><span class="si">{</span><span class="n">target_dim</span><span class="si">}</span><span class="s2"> is not evenly divisible &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;by scale size </span><span class="si">{</span><span class="n">scale_dim</span><span class="si">}</span><span class="s2"> (block size would be </span><span class="si">{</span><span class="n">target_dim</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">scale_dim</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>

    <span class="c1"># Expand scale using repeat_interleave</span>
    <span class="n">expanded_scale</span> <span class="o">=</span> <span class="n">scale</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">block_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">block_sizes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">block_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">expanded_scale</span> <span class="o">=</span> <span class="n">expanded_scale</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">expanded_scale</span>


<span class="k">def</span><span class="w"> </span><span class="nf">quantize_affine_float8</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">float8_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizes the high precision floating point tensor to a float8 tensor, using the given scaling factor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_fp32</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Expand scale to match tensor dimensions for block-wise quantization</span>
    <span class="n">scale_expanded</span> <span class="o">=</span> <span class="n">_expand_scale_to_tensor_shape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">tensor_scaled</span> <span class="o">=</span> <span class="n">tensor_fp32</span> <span class="o">/</span> <span class="n">scale_expanded</span>
    <span class="n">max_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">float8_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
    <span class="n">tensor_clamped</span> <span class="o">=</span> <span class="n">tensor_scaled</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="n">max_value</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">max_value</span><span class="p">)</span>
    <span class="n">fp8_tensor</span> <span class="o">=</span> <span class="n">tensor_clamped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">float8_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fp8_tensor</span>


<span class="k">def</span><span class="w"> </span><span class="nf">dequantize_affine_float8</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dequantizes the float8 tensor to high precision tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fp8_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Expand scale to match tensor dimensions for block-wise quantization</span>
    <span class="n">scale_expanded</span> <span class="o">=</span> <span class="n">_expand_scale_to_tensor_shape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">hp_tensor</span> <span class="o">=</span> <span class="n">fp8_tensor</span> <span class="o">*</span> <span class="n">scale_expanded</span>
    <span class="k">return</span> <span class="n">hp_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>