


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.quantization.autoquant &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization_overview.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributor_guide.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_api_guide.html">Benchmarking API Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_user_guide.html">Benchmarking User Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_qat.html">torchao.quantization.qat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_float8.html">torchao.float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_utils.html">torchao.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_utils.html#torchao-quantization-quantize-common">torchao.quantization.quantize_.common</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Eager Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../pretraining.html">(Part 1) Pre-training with float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../finetuning.html">(Part 2) Fine-tuning with QAT, QLoRA, and float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serving.html">(Part 3) Serving on vLLM, SGLang, ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchao_vllm_integration.html">Integration with VLLM: Architecture and Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../static_quantization.html">Static Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PT2E Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_ptq.html">PyTorch 2 Export Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_qat.html">PyTorch 2 Export Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">PyTorch 2 Export Quantization with X86 Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">PyTorch 2 Export Quantization with Intel GPU Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">PyTorch 2 Export Quantization for OpenVINO torch.compile Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials_source/pt2e_quantizer.html">How to Write a <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code> for PyTorch 2 Export Quantization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.quantization.autoquant</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.quantization.autoquant</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils._python_dispatch</span><span class="w"> </span><span class="kn">import</span> <span class="n">return_and_correct_aliasing</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torchao</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">Float8Layout</span><span class="p">,</span>
    <span class="n">MarlinSparseLayout</span><span class="p">,</span>
    <span class="n">PlainLayout</span><span class="p">,</span>
    <span class="n">SemiSparseLayout</span><span class="p">,</span>
    <span class="n">TensorCoreTiledLayout</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Layout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.float8.inference</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float8MMConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.kernel</span><span class="w"> </span><span class="kn">import</span> <span class="n">safe_int_mm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.linear_activation_quantized_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearActivationQuantizedTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">ZeroPointDomain</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_quantize_activation_per_token_absmax</span><span class="p">,</span>
    <span class="n">compute_error</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TorchAOBaseTensor</span><span class="p">,</span>
    <span class="n">is_sm_at_least_89</span><span class="p">,</span>
    <span class="n">is_sm_at_least_90</span><span class="p">,</span>
    <span class="n">torch_version_at_least</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">PerRow</span><span class="p">,</span>
    <span class="n">PerTensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.subclass</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa</span>
    <span class="n">Int8DynamicallyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">Int8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">QuantizedLinearWeightBase</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;AutoQuantizableLinearWeight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;autoquant&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DEFAULT_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DEFAULT_INT4_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
    <span class="s2">&quot;GEMLITE_INT4_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DEFAULT_FLOAT_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DEFAULT_SPARSE_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OTHER_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ALL_AUTOQUANT_CLASS_LIST&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="n">_AUTOQUANT_CACHE</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_check_cache</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_AUTOQUANT_CACHE</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="bp">cls</span><span class="p">,)</span> <span class="o">+</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_update_cache</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="n">res</span><span class="p">):</span>
    <span class="n">_AUTOQUANT_CACHE</span><span class="p">[(</span><span class="bp">cls</span><span class="p">,)</span> <span class="o">+</span> <span class="n">shapes_and_dtype</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>


<span class="c1"># TODO: Document the methods</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AutoQuantizableLinearWeight</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A subclass of torch.Tensor that, when run, finds the best type of quantization for itself and swaps</span>
<span class="sd">    its data with the quantized version.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (torch.Tensor): The initial weight tensor.</span>
<span class="sd">        qtensor_class_list (list): A list of tensor classes to be considered for quantization.</span>
<span class="sd">        *args: Additional positional arguments.</span>
<span class="sd">        mode (list, optional): A list containing mode settings for quantization. The first element is the mode type</span>
<span class="sd">                               (e.g., &quot;relu&quot;), and the second element is the mode value (e.g., None). Defaults to [&quot;relu&quot;, None].</span>
<span class="sd">        **kwargs: Additional keyword arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">qtensor_class_list</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="n">min_sqnr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">qtensor_class_list</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="n">min_sqnr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qtensor_class_list</span> <span class="o">=</span> <span class="n">qtensor_class_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logged_data</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_sqnr</span> <span class="o">=</span> <span class="n">min_sqnr</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(data=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="si">}</span><span class="s2">, shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, qtensor_class_list=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qtensor_class_list</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_shape</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_autoquant</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="n">act_mat</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">logged_dtype</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">logged_shapes</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">w_autoquant</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="kc">None</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">shapes_and_dtype</span> <span class="o">=</span> <span class="n">logged_shapes</span> <span class="o">+</span> <span class="p">(</span><span class="n">logged_dtype</span><span class="p">,)</span>
        <span class="n">w_autoquant</span><span class="o">.</span><span class="n">logged_data</span><span class="p">[</span><span class="n">shapes_and_dtype</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">w_autoquant</span><span class="o">.</span><span class="n">logged_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">q_cls</span> <span class="ow">in</span> <span class="n">w_autoquant</span><span class="o">.</span><span class="n">qtensor_class_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_check_cache</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_update_cache</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">tune_autoquant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="n">best_time</span><span class="p">):</span>
        <span class="n">act_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">bias_shape</span><span class="p">,</span> <span class="n">act_dtype</span> <span class="o">=</span> <span class="n">shapes_and_dtype</span>
        <span class="k">if</span> <span class="n">_check_cache</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">act_mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">act_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">act_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="kc">None</span>
                    <span class="k">if</span> <span class="n">bias_shape</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">act_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">ref_output</span> <span class="o">=</span> <span class="n">AQDefaultLinearWeight</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span>
                        <span class="n">act_mat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span>
                    <span class="p">)</span>
                    <span class="n">q_output</span> <span class="o">=</span> <span class="n">q_cls</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span>
                        <span class="n">act_mat</span><span class="p">,</span> <span class="n">q_cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span> <span class="n">bias</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">min_sqnr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                        <span class="ow">and</span> <span class="p">(</span><span class="n">sqnr</span> <span class="o">:=</span> <span class="n">compute_error</span><span class="p">(</span><span class="n">q_output</span><span class="p">,</span> <span class="n">ref_output</span><span class="p">))</span>
                        <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_sqnr</span>
                    <span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;skipping q_cls: </span><span class="si">{</span><span class="n">q_cls</span><span class="si">}</span><span class="s2"> because the sqnr is too small, minimum expected sqnr: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">min_sqnr</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">sqnr</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">res</span> <span class="o">=</span> <span class="n">q_cls</span><span class="o">.</span><span class="n">_autoquant_test</span><span class="p">(</span>
                            <span class="n">act_mat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">best_time</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span>
                        <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;warning: failed to autoquant </span><span class="si">{</span><span class="n">q_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> for shape: </span><span class="si">{</span><span class="n">shapes_and_dtype</span><span class="si">}</span><span class="s2"> due to </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                    <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
                <span class="n">_update_cache</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_quantized</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error_on_unseen</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">error_on_unseen</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">logged_data</span> <span class="o">==</span> <span class="p">{}:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;must run module normally to get shape, dtype info for autoquant&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_data</span> <span class="o">==</span> <span class="p">{})</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">error_on_unseen</span><span class="p">:</span>
            <span class="c1"># default back to non-quantized weight if not seen</span>
            <span class="bp">self</span> <span class="o">=</span> <span class="n">AQDefaultLinearWeight</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="c1"># only want to print shape (at start) and final result (at end)</span>
        <span class="c1"># once per shape+quantization subclass combination.</span>
        <span class="n">ran_new_benchmarks</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">print_shape_once</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">count_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">do_print</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">differe_shape_count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="n">times_seen</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">logged_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">differe_shape_count</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">do_print</span><span class="p">:</span>
                    <span class="n">act_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">shapes_and_dtype</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;activation_shapes: </span><span class="si">{</span><span class="n">act_shape</span><span class="si">}</span><span class="s2">, times_seen: </span><span class="si">{</span><span class="n">times_seen</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">do_print</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;weight_shape: </span><span class="si">{</span><span class="n">weight_shape</span><span class="si">}</span><span class="s2">, dtype: </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, bias_shape: </span><span class="si">{</span><span class="n">bias_shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">differe_shape_count</span>

        <span class="c1"># check each class</span>
        <span class="n">best_time</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">best_cls</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">q_cls</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">qtensor_class_list</span><span class="p">:</span>
            <span class="c1"># for each logged shape+dtype, benchmark</span>
            <span class="n">cur_time</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total_seen</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">shape_count</span> <span class="o">=</span> <span class="n">count_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">do_print</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="n">times_seen</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">logged_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">_check_cache</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># only print shapes once</span>
                    <span class="k">if</span> <span class="n">print_shape_once</span><span class="p">:</span>
                        <span class="n">print_shape_once</span> <span class="o">=</span> <span class="kc">False</span>
                        <span class="n">count_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">do_print</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                    <span class="n">time_for_best_shape</span> <span class="o">=</span> <span class="n">_check_cache</span><span class="p">(</span><span class="n">best_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">)</span>
                    <span class="n">time_for_best_shape</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
                        <span class="k">if</span> <span class="n">time_for_best_shape</span> <span class="ow">is</span> <span class="kc">None</span>
                        <span class="k">else</span> <span class="n">time_for_best_shape</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">tune_autoquant</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">,</span> <span class="n">time_for_best_shape</span><span class="p">)</span>
                    <span class="n">ran_new_benchmarks</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
                <span class="n">cur_time</span> <span class="o">+=</span> <span class="n">_check_cache</span><span class="p">(</span><span class="n">q_cls</span><span class="p">,</span> <span class="n">shapes_and_dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">times_seen</span>
                <span class="n">total_seen</span> <span class="o">+=</span> <span class="n">times_seen</span>
            <span class="n">cur_time</span> <span class="o">=</span> <span class="n">cur_time</span> <span class="o">/</span> <span class="n">total_seen</span>
            <span class="c1"># print aggregated time if there were multiple shapes to aggregate and some new benchmarking was done</span>
            <span class="k">if</span> <span class="n">shape_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape_count</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">ran_new_benchmarks</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;&gt;time (all shapes): </span><span class="si">{</span><span class="n">cur_time</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">ms for </span><span class="si">{</span><span class="n">q_cls</span><span class="si">}</span><span class="s2">, prev_best: </span><span class="si">{</span><span class="n">best_time</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">ms&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">cur_time</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span> <span class="ow">and</span> <span class="n">best_time</span> <span class="o">&gt;=</span> <span class="n">cur_time</span><span class="p">:</span>
                <span class="n">best_time</span> <span class="o">=</span> <span class="n">cur_time</span>
                <span class="n">best_cls</span> <span class="o">=</span> <span class="n">q_cls</span>
        <span class="c1"># if no new benchmarking was done, don&#39;t print the final result, it will be the same as for another layer</span>
        <span class="k">if</span> <span class="n">ran_new_benchmarks</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best_cls=</span><span class="si">{</span><span class="n">best_cls</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">best_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">best_cls</span> <span class="o">=</span> <span class="n">AQDefaultLinearWeight</span>

        <span class="c1"># TODO handle random cls args/kwargs? or should they be curried?</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">best_cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qtensor_class_list</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">min_sqnr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_sqnr</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">],</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qtensor_class_list</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_sqnr</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">outer_stride</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span>
        <span class="n">qtensor_class_list</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">min_sqnr</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_attributes</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">qtensor_class_list</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">min_sqnr</span><span class="o">=</span><span class="n">min_sqnr</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span> <span class="k">if</span> <span class="n">outer_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">outer_size</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">strides</span><span class="o">=</span><span class="n">outer_stride</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">qtensor_class_list</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">qtensor_class_list</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">:</span>
            <span class="n">mat1</span><span class="p">,</span> <span class="n">w_autoquant</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">log_shape</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">w_autoquant</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">w_autoquant</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERR: subclass doesn&#39;t implement </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">do_autoquant_bench</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    runs benchmark op(*args, **kwargs) avoiding torch.compile overhead</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch._inductor.runtime.benchmarking</span><span class="w"> </span><span class="kn">import</span> <span class="n">benchmarker</span>

    <span class="n">rep</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;rep&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">warmup</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;warmup&quot;</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
        <span class="n">stream</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
            <span class="n">op</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">stream</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">):</span>
            <span class="n">op</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.9.0.dev&quot;</span><span class="p">):</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">statistics</span><span class="w"> </span><span class="kn">import</span> <span class="n">median</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">benchmarker</span><span class="o">.</span><span class="n">benchmark_gpu</span><span class="p">(</span>
                <span class="k">lambda</span><span class="p">:</span> <span class="n">graph</span><span class="o">.</span><span class="n">replay</span><span class="p">(),</span> <span class="n">warmup</span><span class="o">=</span><span class="n">warmup</span><span class="p">,</span> <span class="n">rep</span><span class="o">=</span><span class="n">rep</span><span class="p">,</span> <span class="n">return_mode</span><span class="o">=</span><span class="s2">&quot;all&quot;</span>
            <span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">median</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">benchmarker</span><span class="o">.</span><span class="n">benchmark_gpu</span><span class="p">(</span>
                <span class="k">lambda</span><span class="p">:</span> <span class="n">graph</span><span class="o">.</span><span class="n">replay</span><span class="p">(),</span> <span class="n">warmup</span><span class="o">=</span><span class="n">warmup</span><span class="p">,</span> <span class="n">rep</span><span class="o">=</span><span class="n">rep</span><span class="p">,</span> <span class="n">return_mode</span><span class="o">=</span><span class="s2">&quot;median&quot;</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_interpolate_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">mode</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;interpolate&quot;</span>
        <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">float</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_to_float16</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_to_bfloat16</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_identity</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tests and benchmarks the autoquantization process for the given activation matrix, weight, and bias.</span>

<span class="sd">    Args:</span>
<span class="sd">        act_mat (torch.Tensor): The activation matrix.</span>
<span class="sd">        weight (torch.Tensor): The weight tensor.</span>
<span class="sd">        bias (torch.Tensor or None): The bias tensor.</span>
<span class="sd">        best_time (float): The best time to beat for the quantization process.</span>
<span class="sd">        mode (list, optional): A list containing mode settings for quantization. The first element is the mode type</span>
<span class="sd">                                (e.g., &quot;relu&quot;), and the second element is the mode value (e.g., None). Defaults to [&quot;relu&quot;, None].</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The benchmarked time for the autoquantization process.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_autoquant_test</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">act_mat</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">best_time</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]):</span>
        <span class="n">w_qtensor</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_is_interpolate_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">):</span>
            <span class="n">q_c_op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max-autotune-no-cudagraphs&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
            <span class="n">q_c_op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max-autotune-no-cudagraphs&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">do_autoquant_bench</span><span class="p">(</span><span class="n">q_c_op</span><span class="p">,</span> <span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">rep</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">res</span> <span class="o">&lt;</span> <span class="n">best_time</span> <span class="o">*</span> <span class="mf">1.1</span><span class="p">:</span>
            <span class="n">res2</span> <span class="o">=</span> <span class="n">do_autoquant_bench</span><span class="p">(</span>
                <span class="n">q_c_op</span><span class="p">,</span> <span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">rep</span><span class="o">=</span><span class="mi">900</span>
            <span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">res2</span> <span class="o">*</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="n">res</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&gt;&gt;time: </span><span class="si">{</span><span class="n">res</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">ms for </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2">, to_beat: </span><span class="si">{</span><span class="n">best_time</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">ms &quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt8DynamicallyQuantizedLinearWeight</span><span class="p">(</span><span class="n">AQMixin</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Int8DynamicallyQuantizedLinearWeight</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">aq_layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weight</span>

        <span class="c1"># TODO test if this is valid</span>
        <span class="c1"># in_features = weight.shape[1]</span>
        <span class="c1"># int8 dynamic quantization only has benefit when in_feature &gt; 16</span>
        <span class="c1"># if in_features &lt;= 16:</span>
        <span class="c1">#     return weight</span>

        <span class="c1"># avoid circular dep</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_intx</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
            <span class="n">_int8_symm_per_token_reduced_range_quant</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># input settings</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_int8_symm_per_token_reduced_range_quant</span>

        <span class="c1"># weight settings</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">aq_layout</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_weight_block_size</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQInt8DynamicallyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_autoquant_test</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">act_mat</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">best_time</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tests and benchmarks the autoquantization process with special handling for interpolate mode.</span>

<span class="sd">        Args:</span>
<span class="sd">            act_mat (torch.Tensor): The activation matrix.</span>
<span class="sd">            weight (torch.Tensor): The weight tensor.</span>
<span class="sd">            bias (torch.Tensor or None): The bias tensor.</span>
<span class="sd">            best_time (float): The best time to beat for the quantization process.</span>
<span class="sd">            mode (list, optional): A list containing mode settings for quantization. The first element is the mode type</span>
<span class="sd">                                   (e.g., &quot;relu&quot;), and the second element is the mode value (e.g., None). Defaults to [&quot;relu&quot;, None].</span>

<span class="sd">        Returns:</span>
<span class="sd">            float: The benchmarked time for the autoquantization process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_interpolate_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_autoquant_test</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">best_time</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

        <span class="c1"># SAM best is between .8 and 1, SDXL also performs best in this range</span>
        <span class="n">INTERPOLATION_CONSTANT</span> <span class="o">=</span> <span class="n">mode</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w_qtensor</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x_vals_int8</span><span class="p">,</span> <span class="n">x_scales</span> <span class="o">=</span> <span class="n">_quantize_activation_per_token_absmax</span><span class="p">(</span>
            <span class="n">act_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="n">quantized_matmul</span> <span class="o">=</span> <span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x_vals_int8</span><span class="p">,</span> <span class="n">x_scales</span><span class="p">,</span> <span class="n">w_vals_int8</span><span class="p">:</span> <span class="n">safe_int_mm</span><span class="p">(</span>
                <span class="n">x_vals_int8</span><span class="p">,</span> <span class="n">w_vals_int8</span>
            <span class="p">)</span>
            <span class="o">*</span> <span class="n">x_scales</span>
        <span class="p">)</span>
        <span class="n">q_c_matmul</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">quantized_matmul</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max-autotune-no-cudagraphs&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">w_vals_int8</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">w_qtensor</span><span class="o">.</span><span class="n">original_weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">res_matmul</span> <span class="o">=</span> <span class="n">do_autoquant_bench</span><span class="p">(</span>
                <span class="n">q_c_matmul</span><span class="p">,</span> <span class="n">x_vals_int8</span><span class="p">,</span> <span class="n">x_scales</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">w_vals_int8</span>
            <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&gt;&gt;time: </span><span class="si">{</span><span class="n">res_matmul</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">ms for </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2"> matmul, to_beat: </span><span class="si">{</span><span class="n">best_time</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">ms&quot;</span>
        <span class="p">)</span>

        <span class="c1"># if the (much faster) matmul kernel is already beat, don&#39;t bother benchmarking full op</span>
        <span class="k">if</span> <span class="n">res_matmul</span> <span class="o">&gt;=</span> <span class="n">best_time</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">res_matmul</span>

        <span class="c1"># calculate what time full op needs to beat for dynamic quant to be best given INTERPOLATION_CONSTANT</span>
        <span class="n">to_beat</span> <span class="o">=</span> <span class="n">best_time</span> <span class="o">+</span> <span class="n">INTERPOLATION_CONSTANT</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">INTERPOLATION_CONSTANT</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">best_time</span> <span class="o">-</span> <span class="n">res_matmul</span>
        <span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_autoquant_test</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">to_beat</span><span class="p">)</span>
        <span class="n">max_int_const_win</span> <span class="o">=</span> <span class="p">(</span><span class="n">best_time</span> <span class="o">-</span> <span class="n">res_matmul</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">res</span> <span class="o">-</span> <span class="n">res_matmul</span><span class="p">)</span>
        <span class="n">res_f</span> <span class="o">=</span> <span class="n">INTERPOLATION_CONSTANT</span> <span class="o">*</span> <span class="n">res</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">INTERPOLATION_CONSTANT</span><span class="p">)</span> <span class="o">*</span> <span class="n">res_matmul</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&gt;&gt;time: </span><span class="si">{</span><span class="n">res_f</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">ms for </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2"> interpolated, breakeven constant: </span><span class="si">{</span><span class="n">max_int_const_win</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">res_f</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt8DynamicallyQuantizedSemiSparseLinearWeight</span><span class="p">(</span>
    <span class="n">AQInt8DynamicallyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">aq_layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">SemiSparseLayout</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_autoquant_test</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">act_mat</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">best_time</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_autoquant_test</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">best_time</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt8WeightOnlyQuantizedLinearWeight</span><span class="p">(</span><span class="n">AffineQuantizedTensor</span><span class="p">,</span> <span class="n">AQMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Int8WeightOnlyQuantizedLinearWeight</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQInt8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_hp_to_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt8WeightOnlyQuantizedLinearWeight2</span><span class="p">(</span>
    <span class="n">AQInt8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="n">AQMixin</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Int8WeightOnlyQuantizedLinearWeight that</span>
<span class="sd">    uses a different kernel</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the quantized linear operations</span>

<span class="sd">        Args:</span>
<span class="sd">            act_mat (torch.Tensor): The activation matrix.</span>
<span class="sd">            w_qtensor (torch.Tensor): The quantized weight tensor.</span>
<span class="sd">            bias (torch.Tensor or None): The bias tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The result of the quantized operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">orig_shape</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">act_mat</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">act_mat</span> <span class="o">*</span> <span class="n">w_qtensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">orig_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">w_qtensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span>
        <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">orig_dtype</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_autoquant_test</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">act_mat</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># if act_mat has batchsize&gt;2 don&#39;t use this kernel</span>
        <span class="k">if</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">32</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_autoquant_test</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt8WeightOnlyQuantizedLinearWeight3</span><span class="p">(</span>
    <span class="n">AQInt8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="n">AQMixin</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Int8WeightOnlyQuantizedLinearWeight that</span>
<span class="sd">    uses a different kernel</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="n">orig_shape</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span>
            <span class="n">act_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">orig_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">w_qtensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">*</span> <span class="n">w_qtensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">orig_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt4G32WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">LinearActivationQuantizedTensor</span><span class="p">,</span> <span class="n">AQMixin</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of int4_weight_only</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="c1"># can&#39;t override the `layout` attribute</span>
    <span class="n">aq_layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">TensorCoreTiledLayout</span><span class="p">(</span><span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_intx</span>

        <span class="n">group_size</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">group_size</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">aq_layout</span>

        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weight</span>

        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># NOTE: we only convert activation dtype and weight dtype here</span>
        <span class="c1"># because the kernel implementation for both TensorCoreTiledLayout and MarlinSparseLayout</span>
        <span class="c1"># can work with multiple bias dtypes (by converting bias to the dtype of activation)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">TensorCoreTiledLayout</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
        <span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_to_bfloat16</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">)</span> <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_to_float16</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_identity</span>

        <span class="n">use_hqq</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>
        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
        <span class="n">preserve_zero</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">zero_point_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">):</span>
            <span class="n">mapping_type</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
            <span class="n">preserve_zero</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span>
            <span class="n">use_hqq</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">mapping_type</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">eps</span><span class="p">,</span>
            <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">zero_point_dtype</span><span class="p">,</span>
            <span class="n">preserve_zero</span><span class="o">=</span><span class="n">preserve_zero</span><span class="p">,</span>
            <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
            <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQInt4G32WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt4G64WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt4G128WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt4G256WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQInt4G128WeightOnlyQuantizedMarlinSparseLinearWeight</span><span class="p">(</span>
    <span class="n">AQInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">aq_layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">MarlinSparseLayout</span><span class="p">()</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQGemliteInt4G32WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">LinearActivationQuantizedTensor</span><span class="p">,</span> <span class="n">AQMixin</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_intx</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.uintx.gemlite_layout</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_gemlite_aqt_kwargs</span>

        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

        <span class="n">bit_width</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="n">packing_bitwidth</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;weight_only&quot;</span>
        <span class="n">use_hqq</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">aqt_kwargs</span> <span class="o">=</span> <span class="n">get_gemlite_aqt_kwargs</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">group_size</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
            <span class="n">bit_width</span><span class="o">=</span><span class="n">bit_width</span><span class="p">,</span>
            <span class="n">packing_bitwidth</span><span class="o">=</span><span class="n">packing_bitwidth</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">use_hqq</span><span class="o">=</span><span class="n">use_hqq</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="o">**</span><span class="n">aqt_kwargs</span><span class="p">)</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_to_float16</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQGemliteInt4G32WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQGemliteInt4G64WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQGemliteInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQGemliteInt4G128WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQGemliteInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQGemliteInt4G256WeightOnlyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQGemliteInt4G32WeightOnlyQuantizedLinearWeight</span>
<span class="p">):</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQDefaultLinearWeight</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AQMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to be used in concert with AutoQuantizableLinearWeight to provide a</span>
<span class="sd">    default/non-quantized option. Only implements the bare minimum needed to work with the</span>
<span class="sd">    AutoQuantizableLinearWeight class using the same interfaces that would normally be</span>
<span class="sd">    used by QTensor subclasses but for a default linear op instead. Result of from_float</span>
<span class="sd">    is not a tensor subclass, but rather the float tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">weight</span>


<span class="c1"># TODO: remove skip_weight_conversion arg</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float32Tensor</span><span class="p">(</span><span class="n">TorchAOBaseTensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tensor subclass tensor for fp32 dtype&quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="k">if</span> <span class="n">skip_weight_conversion</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="n">_DTYPE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span>
            <span class="n">act_mat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_DTYPE</span><span class="p">),</span>
            <span class="n">w_qtensor</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_DTYPE</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">orig_dtype</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>


<span class="nd">@Float32Tensor</span><span class="o">.</span><span class="n">implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">aten</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">default</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="nd">@Float32Tensor</span><span class="o">.</span><span class="n">implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
    <span class="p">)</span>


<span class="nd">@Float32Tensor</span><span class="o">.</span><span class="n">implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">)</span>
    <span class="p">)</span>


<span class="nd">@Float32Tensor</span><span class="o">.</span><span class="n">implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span>
        <span class="n">args</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">,</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">BFloat16Tensor</span><span class="p">(</span><span class="n">Float32Tensor</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="k">if</span> <span class="n">skip_weight_conversion</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="n">_DTYPE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
        <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span>
            <span class="n">act_mat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_DTYPE</span><span class="p">),</span>
            <span class="n">w_qtensor</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_DTYPE</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">orig_dtype</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Float16Tensor</span><span class="p">(</span><span class="n">Float32Tensor</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="k">if</span> <span class="n">skip_weight_conversion</span> <span class="k">else</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="n">_DTYPE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
        <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span>
            <span class="n">act_mat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_DTYPE</span><span class="p">),</span>
            <span class="n">w_qtensor</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_DTYPE</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">orig_dtype</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">skip_weight_conversion</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQFloat32LinearWeight</span><span class="p">(</span><span class="n">Float32Tensor</span><span class="p">,</span> <span class="n">AQMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version for float32 precision weight</span>

<span class="sd">    (also converts input activation and bias to float32, and restores the original precision after</span>
<span class="sd">    linear)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQFloat32LinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQBFloat16LinearWeight</span><span class="p">(</span><span class="n">BFloat16Tensor</span><span class="p">,</span> <span class="n">AQMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version for bfloat16 precision weight</span>

<span class="sd">    (also converts input activation and bias to bfloat16, and restores the original precision after</span>
<span class="sd">    linear)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQBFloat16LinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQFloat16LinearWeight</span><span class="p">(</span><span class="n">Float16Tensor</span><span class="p">,</span> <span class="n">AQMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version for float16 precision weight</span>

<span class="sd">    (also converts input activation and bias to float16, and restores the original precision after</span>
<span class="sd">    linear)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQFloat16LinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQFloat8WeightOnlyQuantizedLinearWeight</span><span class="p">(</span><span class="n">AffineQuantizedTensor</span><span class="p">,</span> <span class="n">AQMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Float8WeightOnlyQuantizedLinearWeight for target_dtype=torch.float8_e4m3fn</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="n">w_qtensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(),</span> <span class="n">bias</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AQFloat8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_hp_to_floatx</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">,</span> <span class="n">_layout</span><span class="o">=</span><span class="n">Float8Layout</span><span class="p">()</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQFloat8PerRowScalingDynamicallyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQMixin</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Float8DynamicallyQuantizedLinearWeight using per row scaling</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_granularity</span> <span class="o">=</span> <span class="n">PerRow</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="c1"># avoid circular dep</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_floatx</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">_input_activation_quant_func_fp8</span>

        <span class="c1"># weight settings</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>
        <span class="n">input_target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="c1"># TODO: test serializable</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
        <span class="n">input_quant_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">activation_granularity</span><span class="p">,</span>
            <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">input_target_dtype</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_weight_block_size</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span>
            <span class="n">AQFloat8PerRowScalingDynamicallyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span>
        <span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">,</span> <span class="n">input_quant_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>


<span class="k">class</span><span class="w"> </span><span class="nc">AQFloat8PerTensorScalingDynamicallyQuantizedLinearWeight</span><span class="p">(</span>
    <span class="n">AQMixin</span><span class="p">,</span> <span class="n">LinearActivationQuantizedTensor</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AutoQuantizable version of Float8DynamicallyQuantizedLinearWeight using per tensor scaling</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_granularity</span> <span class="o">=</span> <span class="n">PerTensor</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="c1"># avoid circular dep</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_floatx</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">_input_activation_quant_func_fp8</span>

        <span class="c1"># weight settings</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_block_size</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Only works for 2D tensors&quot;</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>
        <span class="n">input_target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="n">Float8Layout</span><span class="p">(</span><span class="n">mm_config</span><span class="o">=</span><span class="n">Float8MMConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="c1"># TODO: test serializable</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">_input_activation_quant_func_fp8</span>
        <span class="n">input_quant_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;activation_granularity&quot;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">activation_granularity</span><span class="p">,</span>
            <span class="s2">&quot;activation_dtype&quot;</span><span class="p">:</span> <span class="n">input_target_dtype</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="n">get_weight_block_size</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">to_affine_quantized_floatx</span><span class="p">(</span>
            <span class="n">input_float</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
            <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
            <span class="n">scale_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span>
            <span class="n">AQFloat8PerTensorScalingDynamicallyQuantizedLinearWeight</span><span class="p">,</span> <span class="bp">cls</span>
        <span class="p">)</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">,</span> <span class="n">input_quant_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight</span>


<span class="c1"># here we don&#39;t include int4 quantization in since int8 tends to be a better apples to apples comparison</span>
<span class="n">DEFAULT_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AQDefaultLinearWeight</span><span class="p">,</span>
    <span class="n">AQInt8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">AQInt8WeightOnlyQuantizedLinearWeight2</span><span class="p">,</span>
    <span class="c1"># AQInt8WeightOnlyQuantizedLinearWeight3,</span>
    <span class="c1"># TODO this gets picked in places where it makes perf worse, why?</span>
    <span class="n">AQInt8DynamicallyQuantizedLinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">DEFAULT_INT4_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AQDefaultLinearWeight</span><span class="p">,</span>
    <span class="n">AQInt8DynamicallyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">AQInt4G64WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">GEMLITE_INT4_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AQDefaultLinearWeight</span><span class="p">,</span>
    <span class="n">AQInt8DynamicallyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">AQGemliteInt4G64WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">DEFAULT_FLOAT_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AQFloat32LinearWeight</span><span class="p">,</span>
    <span class="n">AQBFloat16LinearWeight</span><span class="p">,</span>
    <span class="n">AQFloat16LinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">OTHER_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AQDefaultLinearWeight</span><span class="p">,</span>
    <span class="n">AQFloat8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">AQFloat8PerRowScalingDynamicallyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="n">AQFloat8PerTensorScalingDynamicallyQuantizedLinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">DEFAULT_SPARSE_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AQDefaultLinearWeight</span><span class="p">,</span>
    <span class="c1"># TODO: investigate why there are some problems when adding sparse kernels for sam2</span>
    <span class="n">AQInt4G128WeightOnlyQuantizedMarlinSparseLinearWeight</span><span class="p">,</span>
    <span class="c1"># some errors when calling cusparse kernels when running on sam2</span>
    <span class="n">AQInt8DynamicallyQuantizedSemiSparseLinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">ALL_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">DEFAULT_AUTOQUANT_CLASS_LIST</span>
    <span class="o">+</span> <span class="n">DEFAULT_INT4_AUTOQUANT_CLASS_LIST</span>
    <span class="o">+</span> <span class="n">DEFAULT_FLOAT_AUTOQUANT_CLASS_LIST</span>
<span class="p">)</span>

<span class="c1"># add gemlite options</span>
<span class="n">ALL_AUTOQUANT_CLASS_LIST</span> <span class="o">+=</span> <span class="p">[</span>
    <span class="n">AQGemliteInt4G64WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">if</span> <span class="n">is_sm_at_least_89</span><span class="p">():</span>
    <span class="n">ALL_AUTOQUANT_CLASS_LIST</span> <span class="o">+=</span> <span class="p">[</span>
        <span class="n">AQFloat8WeightOnlyQuantizedLinearWeight</span><span class="p">,</span>
        <span class="n">AQFloat8PerTensorScalingDynamicallyQuantizedLinearWeight</span><span class="p">,</span>
    <span class="p">]</span>

<span class="k">if</span> <span class="n">is_sm_at_least_90</span><span class="p">():</span>
    <span class="n">ALL_AUTOQUANT_CLASS_LIST</span> <span class="o">+=</span> <span class="p">[</span><span class="n">AQFloat8PerRowScalingDynamicallyQuantizedLinearWeight</span><span class="p">]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">is_sm_at_least_89</span><span class="p">():</span>
    <span class="n">ALL_AUTOQUANT_CLASS_LIST</span> <span class="o">+=</span> <span class="n">DEFAULT_SPARSE_AUTOQUANT_CLASS_LIST</span>

<span class="c1"># deduplicate</span>
<span class="n">ALL_AUTOQUANT_CLASS_LIST</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">ALL_AUTOQUANT_CLASS_LIST</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_change_linears_to_autoquantizable</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts all linear weight tensors to the</span>
<span class="sd">    AutoQuantizableLinearWeight tensor subclass. Expectation is that this is followed</span>
<span class="sd">    by running the model and then calling _change_autoquantizable_to_quantized</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">_is_linear</span>

    <span class="n">filter_fn</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;filter_fn&quot;</span><span class="p">,</span> <span class="n">_is_linear</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
        <span class="s2">&quot;error_on_unseen&quot;</span><span class="p">,</span> <span class="kc">True</span>
    <span class="p">)</span>  <span class="c1"># same kwargs used for this and to_quantized</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;qtensor_class_list&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;qtensor_class_list&quot;</span><span class="p">,</span> <span class="n">DEFAULT_AUTOQUANT_CLASS_LIST</span>
    <span class="p">)</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;min_sqnr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;min_sqnr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">_get_subclass_inserter</span><span class="p">,</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">_get_subclass_inserter</span><span class="p">(</span><span class="n">AutoQuantizableLinearWeight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
        <span class="n">filter_fn</span> <span class="k">if</span> <span class="n">filter_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_is_linear</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_change_autoquantizable_to_quantized</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">supress_autoquant_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts AutoQuantizableLinearWeight tensor subclasses</span>
<span class="sd">    to various quantized/non-quantized tensor subclasses depending</span>
<span class="sd">    on benchmark results. Expectation is that these modules are</span>
<span class="sd">    torch.compiled afterwards.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hold_automatic_dynamic_shapes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">automatic_dynamic_shapes</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">automatic_dynamic_shapes</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">supress_autoquant_errors</span><span class="p">:</span>
        <span class="n">hold_supress_errors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">suppress_errors</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">suppress_errors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">(</span><span class="n">inductor</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">CRITICAL</span><span class="p">,</span> <span class="n">dynamo</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">CRITICAL</span><span class="p">)</span>
    <span class="n">filter_fn</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
        <span class="s2">&quot;filter_fn&quot;</span><span class="p">,</span>
        <span class="k">lambda</span> <span class="n">mod</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">AutoQuantizableLinearWeight</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">error_on_unseen</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;error_on_unseen&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">_get_subclass_inserter</span><span class="p">,</span>
        <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">_get_subclass_inserter</span><span class="p">(</span>
            <span class="n">AutoQuantizableLinearWeight</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="s2">&quot;to_quantized&quot;</span><span class="p">,</span>
            <span class="n">error_on_unseen</span><span class="o">=</span><span class="n">error_on_unseen</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">filter_fn</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># undo dynamic shape change</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">automatic_dynamic_shapes</span> <span class="o">=</span> <span class="n">hold_automatic_dynamic_shapes</span>

    <span class="c1"># undo error supression</span>
    <span class="k">if</span> <span class="n">supress_autoquant_errors</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">suppress_errors</span> <span class="o">=</span> <span class="n">hold_supress_errors</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


<span class="c1"># TODO: example_input seems weird to include in the API</span>
<span class="c1"># TODO: Document all the modes</span>
<span class="c1"># TODO: Mode being a list is weird, should be a string or some object</span>
<div class="viewcode-block" id="autoquant"><a class="viewcode-back" href="../../../generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">autoquant</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">example_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">qtensor_class_list</span><span class="o">=</span><span class="n">DEFAULT_AUTOQUANT_CLASS_LIST</span><span class="p">,</span>
    <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;interpolate&quot;</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">],</span>
    <span class="n">manual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">set_inductor_config</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">supress_autoquant_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">min_sqnr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">aq_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Autoquantization is a process which identifies the fastest way to quantize each layer of a model over some set of potential</span>
<span class="sd">    qtensor subclasses.</span>

<span class="sd">    Autoquantization happens in three steps:</span>

<span class="sd">    1-Prepare Model: the model is searched for Linear layers whose weights are exchanged for AutoQuantizableLinearWeight.</span>
<span class="sd">    2-Shape Calibration: the user runs the model on one or more inputs, the details of the activation shape/dtype seen by</span>
<span class="sd">        the AutoQuantizableLinearWeight are recorded so we know what shapes/dtypes to use in order to optimize the quantized op in step 3</span>
<span class="sd">    3-Finalize Autoquantization: for each AutoQuantizableLinearWeight, benchmarks are run for each shape/dtype on each member of the qtensor_class_list.</span>
<span class="sd">        the fastest option is picked, resulting in a highly performant model</span>

<span class="sd">    This autoquant function performs step 1. Steps 2 and 3 can be completed by simply running the model.</span>
<span class="sd">    If `example_input` is provided, this function also runs the model (which completes steps 2 and 3).</span>
<span class="sd">    This autoquant api can handle models which have already had torch.compile applied to them, in which case, once the model is run and quantized,</span>
<span class="sd">    the torch.compile process normally proceeds as well.</span>

<span class="sd">    To optimize over a combination of input shapes/dtypes, the user can set manual=True, run the model with all desired shapes/dtypes, then</span>
<span class="sd">    call model.finalize_autoquant to finalize the quantization once the desired set of inputs have been logged.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model to be autoquantized.</span>
<span class="sd">        example_input (Any, optional): An example input for the model. If provided, the function performs a forward pass</span>
<span class="sd">                                       on this input (which fully autoquantizes the model unless manual=True). Defaults to None.</span>
<span class="sd">        qtensor_class_list (list, optional): A list of tensor classes to be used for quantization. Defaults to DEFAULT_AUTOQUANT_CLASS_LIST.</span>
<span class="sd">        filter_fn (callable, optional): A filter function to apply to the model parameters. Defaults to None.</span>
<span class="sd">        mode (list, optional): A list containing mode settings for quantization. The first element is the mode type (e.g., &quot;interpolate&quot;),</span>
<span class="sd">                               and the second element is the mode value (e.g., 0.85). Defaults to [&quot;interpolate&quot;, .85].</span>
<span class="sd">        manual (bool, optional): Whether to stop shape calibration and do autoquant after a single run (default, False) or to wait for</span>
<span class="sd">                                the user to call model.finalize_autoquant (True) so inputs with several shapes/dtypes can be logged.</span>
<span class="sd">        set_inductor_config (bool, optional): Whether to automatically use recommended inductor config settings (defaults to True)</span>
<span class="sd">        supress_autoquant_errors (bool, optional): Whether to suppress errors during autoquantization. (defaults to True)</span>
<span class="sd">        min_sqnr (float, optional): minimum acceptable signal to quantization noise ration (https://en.wikipedia.org/wiki/Signal-to-quantization-noise_ratio) for output of quantized layer v.s. non-quantized layer, this is used to filter</span>
<span class="sd">        out quantization methods that causes too large numerical impact, user can start with a resaonable</span>
<span class="sd">        number like 40 and adjust depending on the result</span>
<span class="sd">        **aq_kwargs: Additional keyword arguments for the autoquantization process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The autoquantized and wrapped model. If `example_input` is provided, the function performs a forward pass</span>
<span class="sd">                         on the input and returns the result of the forward pass.</span>

<span class="sd">    Example usage:</span>
<span class="sd">        torchao.autoquant(torch.compile(model))</span>
<span class="sd">        model(*example_input)</span>

<span class="sd">        # multiple input shapes</span>
<span class="sd">        torchao.autoquant(model, manual=True)</span>
<span class="sd">        model(*example_input1)</span>
<span class="sd">        model(*example_input2)</span>
<span class="sd">        model.finalize_autoquant()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.autoquant&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">set_inductor_config</span><span class="p">:</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">recommended_inductor_config_setter</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">qtensor_class_list</span> <span class="ow">is</span> <span class="n">OTHER_AUTOQUANT_CLASS_LIST</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span>
            <span class="mi">8</span><span class="p">,</span>
            <span class="mi">9</span><span class="p">,</span>
        <span class="p">),</span> <span class="s2">&quot;float8 requires CUDA arch &gt;= 8.9&quot;</span>

    <span class="c1"># perform initial swap from linear weights</span>
    <span class="c1"># to AutoQuantizableLinearWeight</span>
    <span class="n">_change_linears_to_autoquantizable</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">filter_fn</span><span class="o">=</span><span class="n">filter_fn</span><span class="p">,</span>
        <span class="n">qtensor_class_list</span><span class="o">=</span><span class="n">qtensor_class_list</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">min_sqnr</span><span class="o">=</span><span class="n">min_sqnr</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aq_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># access actual model of torch.compile wrapper if needed</span>
    <span class="n">is_compiled</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">eval_frame</span><span class="o">.</span><span class="n">OptimizedModule</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_compiled</span><span class="p">:</span>
        <span class="n">real_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_orig_mod</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">if</span> <span class="n">manual</span><span class="p">:</span>
        <span class="c1"># we don&#39;t want model.forward to trigger</span>
        <span class="c1"># torch.compilation</span>
        <span class="k">if</span> <span class="n">is_compiled</span><span class="p">:</span>
            <span class="n">real_model</span><span class="o">.</span><span class="n">old_forward</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span>
            <span class="n">model</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">real_model</span><span class="o">.</span><span class="n">forward</span>

    <span class="c1"># we want to automatically do autoquant after a single model run</span>
    <span class="c1"># and have it occur before torch.compilation if applicable</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># the hook we will use to intercept the model forward and perform</span>
        <span class="c1"># autoquantization</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">autoquant_prehook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
            <span class="n">real_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">finalize_autoquant</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>

        <span class="c1"># the autoquant_prehook intercepts the forward call, performs logging then</span>
        <span class="c1"># does autoquantization. if model is a torch.compile wrapper, it then</span>
        <span class="c1"># does the tracing/compile since the prehook is naturally followed by the normal.</span>
        <span class="c1"># model run.</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">autoquant_prehook</span><span class="p">,</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># note the torch.compile wrapper (eval_frame) moves the assignment of any assigned</span>
    <span class="c1"># attributes to the inner model that didn&#39;t exist before, so we have to call delattr on the inner model</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">finalize_autoquant</span><span class="p">():</span>
        <span class="n">_change_autoquantizable_to_quantized</span><span class="p">(</span>
            <span class="n">real_model</span><span class="p">,</span>
            <span class="n">supress_autoquant_errors</span><span class="p">,</span>
            <span class="o">**</span><span class="n">aq_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">real_model</span><span class="p">,</span> <span class="s2">&quot;old_forward&quot;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">real_model</span><span class="o">.</span><span class="n">old_forward</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="n">real_model</span><span class="p">,</span> <span class="s2">&quot;old_forward&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">real_model</span><span class="p">,</span> <span class="s2">&quot;finalize_autoquant&quot;</span><span class="p">):</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="n">real_model</span><span class="p">,</span> <span class="s2">&quot;finalize_autoquant&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">manual</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

    <span class="n">real_model</span><span class="o">.</span><span class="n">finalize_autoquant</span> <span class="o">=</span> <span class="n">finalize_autoquant</span>

    <span class="c1"># if example input was provided, check it and run it</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_input</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">example_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">example_input</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_input</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">example_input</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span></div>


<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">(</span><span class="n">ALL_AUTOQUANT_CLASS_LIST</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">_to_float16</span><span class="p">,</span>
        <span class="n">_to_bfloat16</span><span class="p">,</span>
        <span class="n">_identity</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
         <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
         <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
         <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>