
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.quantization.qat.linear &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=b417fedc" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-binder.css?v=ca3c1c84" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-dataframe.css?v=e5fbc548" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=f533b996" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/quantization/qat/linear';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/pytorch/ao/gh-pages/torchao-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/quantization/qat/linear.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>


<link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main (0.17.0+gite915c07 )');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->


<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">

<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>

  
  <div class="navbar-header-items__mobile-logo">
    







  
  
  
  


<a class="navbar-brand logo" href="../../../../index.html">
  
    
    <img src="../../../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
  </div>

  
  
  <div class=" navbar-header-items__start">
    
      
      
        <div class="navbar-item">
          







  
  
  
  


<a class="navbar-brand logo" href="../../../../index.html">
  
    
    <img src="../../../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
        </div>
      
    
      
      
        
        <div class="navbar-item desktop-only-version">
          
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
        </div>
      
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
      
        <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        




  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    <div class="sidebar-header-items__start">
      <div class="navbar-item">
        
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
      </div>
    </div>
    

    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
        
          <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
        
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



<div id="rtd-footer-container"></div>
      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.quan...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              

<div id="searchbox"></div>
<div id="pytorch-article">
  <!-- Hidden breadcrumb schema for SEO only -->
  <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <link itemprop="item" href="../../../index.html">
      <meta itemprop="name" content="Module code">
      <meta itemprop="position" content="1">
    </div>
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <meta itemprop="name" content="torchao.quantization.qat.linear">
      <meta itemprop="position" content="2">
    </div>
  </div>

  
  

  
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.quantization.qat.linear</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>

<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.granularity</span><span class="w"> </span><span class="kn">import</span> <span class="n">PerGroup</span><span class="p">,</span> <span class="n">PerRow</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.linear_quant_modules</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Int8DynActInt4WeightLinear</span><span class="p">,</span>
    <span class="n">WeightOnlyInt4Linear</span><span class="p">,</span>
    <span class="n">_check_linear_int4_k</span><span class="p">,</span>
    <span class="n">_replace_linear_8da4w</span><span class="p">,</span>
    <span class="n">_replace_linear_int4</span><span class="p">,</span>
    <span class="n">groupwise_affine_quantize_tensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TorchAODType</span><span class="p">,</span>
    <span class="n">ZeroPointDomain</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.unified</span><span class="w"> </span><span class="kn">import</span> <span class="n">TwoStepQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_group_qparams_symmetric</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.fake_quantize_config</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">FakeQuantizeConfigBase</span><span class="p">,</span>
    <span class="n">Float8FakeQuantizeConfig</span><span class="p">,</span>
    <span class="n">IntxFakeQuantizeConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.fake_quantizer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">FakeQuantizerBase</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_qmin_qmax</span><span class="p">,</span>
<span class="p">)</span>


<div class="viewcode-block" id="FakeQuantizedLinear">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.FakeQuantizedLinear.html#torchao.quantization.qat.FakeQuantizedLinear">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">FakeQuantizedLinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General linear layer with fake quantized weights and activations.</span>

<span class="sd">    Specific target dtypes, granularity, schemes etc. are specified</span>
<span class="sd">    through separate configs for weights and activations.</span>

<span class="sd">    Example usage::</span>

<span class="sd">        activation_config = IntxFakeQuantizeConfig(</span>
<span class="sd">            dtype=torch.int8,</span>
<span class="sd">            granularity=&quot;per_token&quot;,</span>
<span class="sd">            is_symmetric=False,</span>
<span class="sd">        )</span>
<span class="sd">        weight_config = IntxFakeQuantizeConfig(</span>
<span class="sd">            dtype=torch.int4,</span>
<span class="sd">            group_size=8,</span>
<span class="sd">            is_symmetric=True,</span>
<span class="sd">        )</span>
<span class="sd">        fq_linear = FakeQuantizedLinear(</span>
<span class="sd">            16, 32, False, activation_config, weight_config,</span>
<span class="sd">        )</span>
<span class="sd">        fq_linear(torch.randn(16))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">activation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_features</span><span class="p">,</span>
            <span class="n">out_features</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torchao.quantization.qat.FakeQuantizedLinear&quot;</span><span class="p">)</span>
        <span class="c1"># initialize activation fake quantizer</span>
        <span class="k">if</span> <span class="n">activation_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_fake_quantizer</span> <span class="o">=</span> <span class="n">FakeQuantizerBase</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span>
                <span class="n">activation_config</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation_fake_quantizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># initialize weight fake quantizer</span>
        <span class="k">if</span> <span class="n">weight_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_config</span><span class="p">,</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">weight_config</span><span class="o">.</span><span class="n">granularity</span><span class="p">,</span> <span class="n">PerGroup</span>
            <span class="p">):</span>
                <span class="n">group_size</span> <span class="o">=</span> <span class="n">weight_config</span><span class="o">.</span><span class="n">group_size</span>
                <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">in_features</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;in_features (</span><span class="si">%s</span><span class="s2">) </span><span class="si">%%</span><span class="s2"> group_size (</span><span class="si">%s</span><span class="s2">) must be == 0&quot;</span>
                        <span class="o">%</span> <span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_fake_quantizer</span> <span class="o">=</span> <span class="n">FakeQuantizerBase</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">weight_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_fake_quantizer</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="FakeQuantizedLinear.forward">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.FakeQuantizedLinear.html#torchao.quantization.qat.FakeQuantizedLinear.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fake_quantizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fake_quantizer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_fake_quantizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_fake_quantizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">to_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">new_linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># In distributed training, the model may be instantiated</span>
        <span class="c1"># on the meta device, in which case there is no need to</span>
        <span class="c1"># copy the weights, and doing so will result in an error</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
            <span class="n">new_linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">new_linear</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_linear</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span>
        <span class="n">activation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">new_linear</span> <span class="o">=</span> <span class="n">FakeQuantizedLinear</span><span class="p">(</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">activation_config</span><span class="o">=</span><span class="n">activation_config</span><span class="p">,</span>
            <span class="n">weight_config</span><span class="o">=</span><span class="n">weight_config</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># In distributed training, the model may be instantiated</span>
        <span class="c1"># on the meta device, in which case there is no need to</span>
        <span class="c1"># copy the weights, and doing so will result in an error</span>
        <span class="k">if</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight</span>
            <span class="n">new_linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">new_linear</span></div>



<div class="viewcode-block" id="enable_linear_fake_quant">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.linear.enable_linear_fake_quant.html#torchao.quantization.qat.enable_linear_fake_quant">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">enable_linear_fake_quant</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to enable fake quantization in `FakeQuantizedLinear`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">FakeQuantizedLinear</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_fake_quantizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">activation_fake_quantizer</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="k">if</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight_fake_quantizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mod</span><span class="o">.</span><span class="n">weight_fake_quantizer</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span></div>



<div class="viewcode-block" id="disable_linear_fake_quant">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.linear.disable_linear_fake_quant.html#torchao.quantization.qat.disable_linear_fake_quant">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">disable_linear_fake_quant</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to disable fake quantization in `FakeQuantizedLinear`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">enable_linear_fake_quant</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>



<span class="c1"># ===========================</span>
<span class="c1"># | QAT quantizer interface |</span>
<span class="c1"># ===========================</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_LegacyQATQuantizer</span><span class="p">(</span><span class="n">TwoStepQuantizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for sharing common methods across legacy QAT quantizers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_activation_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># ===========================================</span>
<span class="c1"># | int8 dynamic activations + int4 weights |</span>
<span class="c1"># ===========================================</span>


<div class="viewcode-block" id="Int8DynActInt4WeightQATQuantizer">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.html#torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynActInt4WeightQATQuantizer</span><span class="p">(</span><span class="n">_LegacyQATQuantizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizer for performing QAT on a model, where linear layers have int8</span>
<span class="sd">    dynamic per token fake quantized activations and int4 fake quantized</span>
<span class="sd">    grouped per channel weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">padding_allowed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">scales_precision</span>
        <span class="c1"># TODO: generalize this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_scales_precision</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prepare</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">_replace_linear_8da4w</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">,</span>
            <span class="n">Int8DynActInt4WeightQATLinear</span><span class="p">,</span>
            <span class="n">copy_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">convert</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_convert_qat_linear_8da4w</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_convert_qat_linear_8da4w</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replace all `Int8DynActInt4WeightQATLinear` with `Int8DynActInt4WeightLinear`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">Int8DynActInt4WeightQATLinear</span><span class="p">):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">weight_fake_quantizer</span><span class="o">.</span><span class="n">config</span>
                <span class="n">quantized_linear</span> <span class="o">=</span> <span class="n">Int8DynActInt4WeightLinear</span><span class="p">(</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">groupsize</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
                    <span class="n">precision</span><span class="o">=</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">scales_precision</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">scale_precision</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">quantized_linear</span><span class="p">)</span>

                <span class="c1"># Load weights and qparams into quantized linear</span>
                <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
                <span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span> <span class="o">=</span> <span class="n">_get_qmin_qmax</span><span class="p">(</span><span class="n">n_bit</span><span class="p">)</span>
                <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">zp</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_group_qparams_symmetric</span><span class="p">(</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                    <span class="n">n_bit</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
                    <span class="n">precision</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">scale_precision</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">zp</span> <span class="o">=</span> <span class="n">zp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">zero_point_precision</span><span class="p">)</span>
                <span class="kn">from</span><span class="w"> </span><span class="nn">torchao._executorch_ops</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
                    <span class="n">_quantized_decomposed_quantize_per_channel_group_wrapper</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">q_weight</span> <span class="o">=</span> <span class="n">_quantized_decomposed_quantize_per_channel_group_wrapper</span><span class="p">(</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                    <span class="n">s</span><span class="p">,</span>
                    <span class="n">zp</span><span class="p">,</span>
                    <span class="n">qmin</span><span class="p">,</span>
                    <span class="n">qmax</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">quantized_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">q_weight</span>
                <span class="n">quantized_linear</span><span class="o">.</span><span class="n">scales</span> <span class="o">=</span> <span class="n">s</span>
                <span class="n">quantized_linear</span><span class="o">.</span><span class="n">zeros</span> <span class="o">=</span> <span class="n">zp</span>
                <span class="k">if</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">quantized_linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_convert_qat_linear_8da4w</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_activation_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_get_8da4w_activation_config</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_scales_precision</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_get_8da4w_weight_config</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">)</span></div>



<div class="viewcode-block" id="Int8DynActInt4WeightQATLinear">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.linear.Int8DynActInt4WeightQATLinear.html#torchao.quantization.qat.Int8DynActInt4WeightQATLinear">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Int8DynActInt4WeightQATLinear</span><span class="p">(</span><span class="n">FakeQuantizedLinear</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This module implements a linear layer with int8 dynamic per token fake</span>
<span class="sd">    quantized activations with int4 fake quantized grouped per channel weights.</span>

<span class="sd">    args:</span>
<span class="sd">        groupsize: the number of elements in each quantized group for weights</span>
<span class="sd">        precision: precision of weights</span>
<span class="sd">        scales_precision: precision of per group scales and zero points</span>

<span class="sd">    Note: we hardcode activation scales to use torch.fp32, but allow users to specify the weight scales (defaults to torch.fp32).</span>
<span class="sd">    To get an exact numerical match with Int8DynamicActivationInt4WeightConfig, users must use the same dtype for both the weights</span>
<span class="sd">    and the scales. Here scales_precision refers specifically to the weight scales only, not the activation scales.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Use torch.float32 to match torchao.quantization.quant_api._int8_asymm_per_token_quant,</span>
        <span class="c1"># which is used in PTQ routines</span>
        <span class="c1"># TODO: generalize this</span>
        <span class="n">activation_config</span> <span class="o">=</span> <span class="n">_get_8da4w_activation_config</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">weight_config</span> <span class="o">=</span> <span class="n">_get_8da4w_weight_config</span><span class="p">(</span><span class="n">groupsize</span><span class="p">,</span> <span class="n">scales_precision</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_features</span><span class="p">,</span>
            <span class="n">out_features</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">activation_config</span><span class="p">,</span>
            <span class="n">weight_config</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">enable_fake_quant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fake_quantizer</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_fake_quantizer</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">disable_fake_quant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_fake_quant</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span></div>



<span class="c1"># TODO: remove these in favor of enable_linear_fake_quant</span>
<span class="k">def</span><span class="w"> </span><span class="nf">enable_8da4w_fake_quant</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (deprecated) Enable fake quantization for `Int8DynActInt4WeightQATLinear`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">Int8DynActInt4WeightQATLinear</span><span class="p">):</span>
        <span class="n">mod</span><span class="o">.</span><span class="n">enable_fake_quant</span><span class="p">()</span>


<span class="c1"># TODO: remove in favor of disable_linear_fake_quant</span>
<span class="k">def</span><span class="w"> </span><span class="nf">disable_8da4w_fake_quant</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (deprecated) Disable fake quantization for `Int8DynActInt4WeightQATLinear`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">Int8DynActInt4WeightQATLinear</span><span class="p">):</span>
        <span class="n">mod</span><span class="o">.</span><span class="n">disable_fake_quant</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_8da4w_activation_config</span><span class="p">(</span>
    <span class="n">qparams_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the activation `IntxFakeQuantizeConfig` for `Int8DynActInt4WeightQATQuantizer`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: generalize this</span>
    <span class="k">assert</span> <span class="n">qparams_precision</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">granularity</span><span class="o">=</span><span class="s2">&quot;per_token&quot;</span><span class="p">,</span>
        <span class="n">is_symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">scale_precision</span><span class="o">=</span><span class="n">qparams_precision</span><span class="p">,</span>
        <span class="n">zero_point_precision</span><span class="o">=</span><span class="n">qparams_precision</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">qparams_precision</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_8da4w_weight_config</span><span class="p">(</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">qparams_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the weight `IntxFakeQuantizeConfig` for `Int8DynActInt4WeightQATQuantizer`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">TorchAODType</span><span class="o">.</span><span class="n">INT4</span><span class="p">,</span>
        <span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span>
        <span class="n">is_symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">scale_precision</span><span class="o">=</span><span class="n">qparams_precision</span><span class="p">,</span>
        <span class="n">zero_point_precision</span><span class="o">=</span><span class="n">qparams_precision</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># ====================</span>
<span class="c1"># | int4 weight-only |</span>
<span class="c1"># ====================</span>


<div class="viewcode-block" id="Int4WeightOnlyQATQuantizer">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.html#torchao.quantization.qat.Int4WeightOnlyQATQuantizer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4WeightOnlyQATQuantizer</span><span class="p">(</span><span class="n">_LegacyQATQuantizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizer for performing QAT on a model, where linear layers have</span>
<span class="sd">    int4 fake quantized grouped per channel weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">inner_k_tiles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.qat.Int4WeightOnlyQATQuantizer&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">inner_k_tiles</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">groupsize</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">inner_k_tiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span> <span class="o">=</span> <span class="n">scales_precision</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prepare</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">_replace_linear_int4</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">,</span>
            <span class="n">padding_allowed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">scales_precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">,</span>
            <span class="n">linear_class</span><span class="o">=</span><span class="n">Int4WeightOnlyQATLinear</span><span class="p">,</span>
            <span class="n">copy_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">convert</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_convert_qat_linear_4w</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_convert_qat_linear_4w</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replace all `Int4WeightOnlyQATLinear` with `WeightOnlyInt4Linear`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">Int4WeightOnlyQATLinear</span><span class="p">):</span>
                <span class="n">in_features</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">in_features</span>
                <span class="n">out_features</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">out_features</span>
                <span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">inner_k_tiles</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">weight_fake_quantizer</span><span class="o">.</span><span class="n">config</span>
                <span class="n">quantized_linear</span> <span class="o">=</span> <span class="n">WeightOnlyInt4Linear</span><span class="p">(</span>
                    <span class="n">in_features</span><span class="p">,</span>
                    <span class="n">out_features</span><span class="p">,</span>
                    <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">groupsize</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
                    <span class="n">inner_k_tiles</span><span class="o">=</span><span class="n">inner_k_tiles</span><span class="p">,</span>
                    <span class="n">precision</span><span class="o">=</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">scales_precision</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">scale_precision</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">quantized_linear</span><span class="p">)</span>

                <span class="c1"># Load weights and qparams into quantized linear</span>
                <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
                <span class="p">(</span><span class="n">q_weight</span><span class="p">,</span> <span class="n">scales_and_zeros</span><span class="p">)</span> <span class="o">=</span> <span class="n">groupwise_affine_quantize_tensor</span><span class="p">(</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                    <span class="n">n_bit</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">is_device</span><span class="p">(</span><span class="n">q_weight</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                    <span class="n">q_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_convert_weight_to_int4pack_for_cpu</span><span class="p">(</span>
                        <span class="n">q_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                        <span class="n">child</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">q_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_convert_weight_to_int4pack</span><span class="p">(</span>
                        <span class="n">q_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                        <span class="n">child</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="n">quantized_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">q_weight</span>
                <span class="n">quantized_linear</span><span class="o">.</span><span class="n">scales_and_zeros</span> <span class="o">=</span> <span class="n">scales_and_zeros</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_convert_qat_linear_4w</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_get_4w_weight_config</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">)</span></div>



<div class="viewcode-block" id="Int4WeightOnlyQATLinear">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.linear.Int4WeightOnlyQATLinear.html#torchao.quantization.qat.Int4WeightOnlyQATLinear">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Int4WeightOnlyQATLinear</span><span class="p">(</span><span class="n">FakeQuantizedLinear</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This module implements a linear layer with int4 fake quantized grouped</span>
<span class="sd">    per channel weights, with forward numerics matching `WeightOnlyInt4Linear`,</span>
<span class="sd">    which uses the efficient int4 tinygemm kernel.</span>

<span class="sd">    args:</span>
<span class="sd">        groupsize: the number of elements in each quantized group for weights</span>
<span class="sd">        precision: precision of weights</span>
<span class="sd">        scales_precision: precision of per group scales and zero points</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">inner_k_tiles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">scales_precision</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s2">&quot;only bf16 is supported for scales&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="n">inner_k_tiles</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Padding for QAT 4w is not supported yet&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">inner_k_tiles</span>
        <span class="n">weight_config</span> <span class="o">=</span> <span class="n">_get_4w_weight_config</span><span class="p">(</span><span class="n">groupsize</span><span class="p">,</span> <span class="n">scales_precision</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_features</span><span class="p">,</span>
            <span class="n">out_features</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">activation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">weight_config</span><span class="o">=</span><span class="n">weight_config</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">enable_fake_quant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fake_quantizer</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_fake_quantizer</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">disable_fake_quant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_fake_quant</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span></div>



<span class="c1"># TODO: remove these in favor of enable_linear_fake_quant</span>
<span class="k">def</span><span class="w"> </span><span class="nf">enable_4w_fake_quant</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (deprecated) Enable fake quantization for `Int4WeightOnlyQATLinear`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">Int4WeightOnlyQATLinear</span><span class="p">):</span>
        <span class="n">mod</span><span class="o">.</span><span class="n">enable_fake_quant</span><span class="p">()</span>


<span class="c1"># TODO: remove these in favor of disable_linear_fake_quant</span>
<span class="k">def</span><span class="w"> </span><span class="nf">disable_4w_fake_quant</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (deprecated) Disable fake quantization for `Int4WeightOnlyQATLinear`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">Int4WeightOnlyQATLinear</span><span class="p">):</span>
        <span class="n">mod</span><span class="o">.</span><span class="n">disable_fake_quant</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_4w_weight_config</span><span class="p">(</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">qparams_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the weight `IntxFakeQuantizeConfig` for `Int4WeightOnlyQATQuantizer`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint4</span><span class="p">,</span>
        <span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span>
        <span class="n">is_symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">scale_precision</span><span class="o">=</span><span class="n">qparams_precision</span><span class="p">,</span>
        <span class="n">zero_point_precision</span><span class="o">=</span><span class="n">qparams_precision</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="o">=</span><span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># =============================================</span>
<span class="c1"># | float8 rowwise activations + int4 weights |</span>
<span class="c1"># =============================================</span>


<div class="viewcode-block" id="Float8ActInt4WeightQATQuantizer">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.Float8ActInt4WeightQATQuantizer.html#torchao.quantization.qat.Float8ActInt4WeightQATQuantizer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8ActInt4WeightQATQuantizer</span><span class="p">(</span><span class="n">_LegacyQATQuantizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QAT quantizer for applying dynamic rowwise float8 activation + int4</span>
<span class="sd">    per group/channel symmetric weight fake quantization to linear layers</span>
<span class="sd">    in the model. Currently only supports rowwise granularity for float8</span>
<span class="sd">    activations.</span>

<span class="sd">    args:</span>
<span class="sd">        group_size (Optional[int]): the number of elements in each quantized</span>
<span class="sd">            group for weights, defaults to 64. Use None for per channel.</span>
<span class="sd">        scale_precision: precision of weight scales, defaults to torch.bfloat16.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">group_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">scale_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span>
            <span class="s2">&quot;torchao.quantization.qat.Float8ActInt4WeightQATQuantizer&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weight_granularity</span> <span class="o">=</span> <span class="s2">&quot;per_group&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight_granularity</span> <span class="o">=</span> <span class="s2">&quot;per_channel&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation_config</span> <span class="o">=</span> <span class="n">Float8FakeQuantizeConfig</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
            <span class="n">granularity</span><span class="o">=</span><span class="n">PerRow</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight_config</span> <span class="o">=</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">,</span>
            <span class="n">granularity</span><span class="o">=</span><span class="n">weight_granularity</span><span class="p">,</span>
            <span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span>
            <span class="n">is_symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">scale_precision</span><span class="o">=</span><span class="n">scale_precision</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Float8ActInt4WeightQATQuantizer.prepare">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.quantization.qat.Float8ActInt4WeightQATQuantizer.html#torchao.quantization.qat.Float8ActInt4WeightQATQuantizer.prepare">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">prepare</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Swap all `nn.Linear` with `FakeQuantizedLinear` with float8</span>
<span class="sd">        fake quantizer for activations and int4 fake quantizer for weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">new_linear</span> <span class="o">=</span> <span class="n">FakeQuantizedLinear</span><span class="o">.</span><span class="n">from_linear</span><span class="p">(</span>
                    <span class="n">child</span><span class="p">,</span>
                    <span class="n">activation_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_activation_config</span><span class="p">,</span>
                    <span class="n">weight_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight_config</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_linear</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span></div>


    <span class="c1"># TODO: add convert path</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">convert</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_activation_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Float8 FakeQuantizeConfig does not exist yet&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_weight_fake_quantize_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FakeQuantizeConfigBase</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_config</span></div>

</pre></div>

                </article>
              
</div>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.quantization.qat.linear",
       "headline": "torchao.quantization.qat.linear",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/_modules/torchao/quantization/qat/linear.html",
       "articleBody": "Source code for torchao.quantization.qat.linear # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # This source code is licensed under the license found in the # LICENSE file in the root directory of this source tree. from typing import Any, Optional import torch import torch.nn.functional as F from torchao.dtypes.utils import is_device from torchao.quantization.granularity import PerGroup, PerRow from torchao.quantization.linear_quant_modules import ( Int8DynActInt4WeightLinear, WeightOnlyInt4Linear, _check_linear_int4_k, _replace_linear_8da4w, _replace_linear_int4, groupwise_affine_quantize_tensor, ) from torchao.quantization.quant_primitives import ( TorchAODType, ZeroPointDomain, ) from torchao.quantization.unified import TwoStepQuantizer from torchao.quantization.utils import get_group_qparams_symmetric from .fake_quantize_config import ( FakeQuantizeConfigBase, Float8FakeQuantizeConfig, IntxFakeQuantizeConfig, ) from .fake_quantizer import ( FakeQuantizerBase, ) from .utils import ( _get_qmin_qmax, ) [docs] class FakeQuantizedLinear(torch.nn.Linear): \"\"\" General linear layer with fake quantized weights and activations. Specific target dtypes, granularity, schemes etc. are specified through separate configs for weights and activations. Example usage:: activation_config = IntxFakeQuantizeConfig( dtype=torch.int8, granularity=\"per_token\", is_symmetric=False, ) weight_config = IntxFakeQuantizeConfig( dtype=torch.int4, group_size=8, is_symmetric=True, ) fq_linear = FakeQuantizedLinear( 16, 32, False, activation_config, weight_config, ) fq_linear(torch.randn(16)) \"\"\" def __init__( self, in_features: int, out_features: int, bias: bool = False, activation_config: Optional[FakeQuantizeConfigBase] = None, weight_config: Optional[FakeQuantizeConfigBase] = None, *args, **kwargs, ) -\u003e None: super().__init__( in_features, out_features, bias, *args, **kwargs, ) torch._C._log_api_usage_once(\"torchao.quantization.qat.FakeQuantizedLinear\") # initialize activation fake quantizer if activation_config is not None: self.activation_fake_quantizer = FakeQuantizerBase.from_config( activation_config ) else: self.activation_fake_quantizer = None # initialize weight fake quantizer if weight_config is not None: if isinstance(weight_config, IntxFakeQuantizeConfig) and isinstance( weight_config.granularity, PerGroup ): group_size = weight_config.group_size if group_size is not None and in_features % group_size != 0: raise ValueError( \"in_features (%s) %% group_size (%s) must be == 0\" % (in_features, group_size) ) self.weight_fake_quantizer = FakeQuantizerBase.from_config(weight_config) else: self.weight_fake_quantizer = None [docs] def forward(self, x: torch.Tensor) -\u003e torch.Tensor: if self.activation_fake_quantizer is not None: x = self.activation_fake_quantizer(x) if self.weight_fake_quantizer is not None: w = self.weight_fake_quantizer(self.weight) else: w = self.weight return F.linear(x, w, self.bias) def to_linear(self) -\u003e torch.nn.Linear: new_linear = torch.nn.Linear( self.in_features, self.out_features, self.bias is not None, device=self.weight.device, dtype=self.weight.dtype, ) # In distributed training, the model may be instantiated # on the meta device, in which case there is no need to # copy the weights, and doing so will result in an error if self.weight.device != torch.device(\"meta\"): new_linear.weight = self.weight new_linear.bias = self.bias return new_linear @classmethod def from_linear( cls, mod: torch.nn.Linear, activation_config: Optional[FakeQuantizeConfigBase] = None, weight_config: Optional[FakeQuantizeConfigBase] = None, ): new_linear = FakeQuantizedLinear( mod.in_features, mod.out_features, mod.bias is not None, activation_config=activation_config, weight_config=weight_config, device=mod.weight.device, dtype=mod.weight.dtype, ) # In distributed training, the model may be instantiated # on the meta device, in which case there is no need to # copy the weights, and doing so will result in an error if mod.weight.device != torch.device(\"meta\"): new_linear.weight = mod.weight new_linear.bias = mod.bias return new_linear [docs] def enable_linear_fake_quant( mod: torch.nn.Module, enabled: bool = True, ): \"\"\" Helper function to enable fake quantization in `FakeQuantizedLinear`. \"\"\" if isinstance(mod, FakeQuantizedLinear): if mod.activation_fake_quantizer is not None: mod.activation_fake_quantizer.enabled = enabled if mod.weight_fake_quantizer is not None: mod.weight_fake_quantizer.enabled = enabled [docs] def disable_linear_fake_quant(mod: torch.nn.Module): \"\"\" Helper function to disable fake quantization in `FakeQuantizedLinear`. \"\"\" enable_linear_fake_quant(mod, enabled=False) # =========================== # | QAT quantizer interface | # =========================== class _LegacyQATQuantizer(TwoStepQuantizer): \"\"\" Base class for sharing common methods across legacy QAT quantizers. \"\"\" def get_activation_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: return None def get_weight_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: return None # =========================================== # | int8 dynamic activations + int4 weights | # =========================================== [docs] class Int8DynActInt4WeightQATQuantizer(_LegacyQATQuantizer): \"\"\" Quantizer for performing QAT on a model, where linear layers have int8 dynamic per token fake quantized activations and int4 fake quantized grouped per channel weights. \"\"\" def __init__( self, groupsize: int = 256, padding_allowed: bool = False, precision: torch.dtype = torch.float32, scales_precision: torch.dtype = torch.float32, ) -\u003e None: super().__init__() torch._C._log_api_usage_once( \"torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer\" ) self.groupsize: int = groupsize self.padding_allowed: bool = padding_allowed self.precision: torch.dtype = precision self.scales_precision: torch.dtype = scales_precision # TODO: generalize this self.activation_scales_precision = torch.float32 def prepare( self, model: torch.nn.Module, *args: Any, **kwargs: Any ) -\u003e torch.nn.Module: _replace_linear_8da4w( model, self.groupsize, self.padding_allowed, self.precision, self.scales_precision, Int8DynActInt4WeightQATLinear, copy_weights=True, ) return model def convert( self, model: torch.nn.Module, *args: Any, **kwargs: Any ) -\u003e torch.nn.Module: self._convert_qat_linear_8da4w(model) return model def _convert_qat_linear_8da4w(self, module: torch.nn.Module): \"\"\" Replace all `Int8DynActInt4WeightQATLinear` with `Int8DynActInt4WeightLinear`. \"\"\" for name, child in module.named_children(): if isinstance(child, Int8DynActInt4WeightQATLinear): config = child.weight_fake_quantizer.config quantized_linear = Int8DynActInt4WeightLinear( child.in_features, child.out_features, child.bias is not None, groupsize=config.group_size, precision=child.weight.dtype, scales_precision=config.scale_precision, ) setattr(module, name, quantized_linear) # Load weights and qparams into quantized linear n_bit = 4 (qmin, qmax) = _get_qmin_qmax(n_bit) (s, zp) = get_group_qparams_symmetric( child.weight, n_bit, config.group_size, precision=config.scale_precision, ) zp = zp.to(config.zero_point_precision) from torchao._executorch_ops import ( _quantized_decomposed_quantize_per_channel_group_wrapper, ) q_weight = _quantized_decomposed_quantize_per_channel_group_wrapper( child.weight, s, zp, qmin, qmax, torch.int8, config.group_size, ) quantized_linear.weight = q_weight quantized_linear.scales = s quantized_linear.zeros = zp if child.bias is not None: quantized_linear.bias = child.bias else: self._convert_qat_linear_8da4w(child) def get_activation_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: return _get_8da4w_activation_config(self.activation_scales_precision) def get_weight_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: return _get_8da4w_weight_config(self.groupsize, self.scales_precision) [docs] class Int8DynActInt4WeightQATLinear(FakeQuantizedLinear): \"\"\" This module implements a linear layer with int8 dynamic per token fake quantized activations with int4 fake quantized grouped per channel weights. args: groupsize: the number of elements in each quantized group for weights precision: precision of weights scales_precision: precision of per group scales and zero points Note: we hardcode activation scales to use torch.fp32, but allow users to specify the weight scales (defaults to torch.fp32). To get an exact numerical match with Int8DynamicActivationInt4WeightConfig, users must use the same dtype for both the weights and the scales. Here scales_precision refers specifically to the weight scales only, not the activation scales. \"\"\" def __init__( self, in_features: int, out_features: int, bias: bool = False, device: torch.device = None, groupsize: int = 256, precision: torch.dtype = torch.float32, scales_precision: torch.dtype = torch.float32, ) -\u003e None: # Use torch.float32 to match torchao.quantization.quant_api._int8_asymm_per_token_quant, # which is used in PTQ routines # TODO: generalize this activation_config = _get_8da4w_activation_config(torch.float32) weight_config = _get_8da4w_weight_config(groupsize, scales_precision) super().__init__( in_features, out_features, bias, activation_config, weight_config, device=device, dtype=precision, ) def enable_fake_quant(self, enabled: bool = True): self.activation_fake_quantizer.enabled = enabled self.weight_fake_quantizer.enabled = enabled def disable_fake_quant(self): self.enable_fake_quant(False) # TODO: remove these in favor of enable_linear_fake_quant def enable_8da4w_fake_quant(mod: torch.nn.Module): \"\"\" (deprecated) Enable fake quantization for `Int8DynActInt4WeightQATLinear`. \"\"\" if isinstance(mod, Int8DynActInt4WeightQATLinear): mod.enable_fake_quant() # TODO: remove in favor of disable_linear_fake_quant def disable_8da4w_fake_quant(mod: torch.nn.Module): \"\"\" (deprecated) Disable fake quantization for `Int8DynActInt4WeightQATLinear`. \"\"\" if isinstance(mod, Int8DynActInt4WeightQATLinear): mod.disable_fake_quant() def _get_8da4w_activation_config( qparams_precision: torch.dtype, ) -\u003e IntxFakeQuantizeConfig: \"\"\" Return the activation `IntxFakeQuantizeConfig` for `Int8DynActInt4WeightQATQuantizer`. \"\"\" # TODO: generalize this assert qparams_precision == torch.float32 return IntxFakeQuantizeConfig( dtype=torch.int8, granularity=\"per_token\", is_symmetric=False, is_dynamic=True, scale_precision=qparams_precision, zero_point_precision=qparams_precision, eps=torch.finfo(qparams_precision).eps, ) def _get_8da4w_weight_config( group_size: int, qparams_precision: torch.dtype, ) -\u003e IntxFakeQuantizeConfig: \"\"\" Return the weight `IntxFakeQuantizeConfig` for `Int8DynActInt4WeightQATQuantizer`. \"\"\" return IntxFakeQuantizeConfig( dtype=TorchAODType.INT4, group_size=group_size, is_symmetric=True, is_dynamic=True, scale_precision=qparams_precision, zero_point_precision=qparams_precision, ) # ==================== # | int4 weight-only | # ==================== [docs] class Int4WeightOnlyQATQuantizer(_LegacyQATQuantizer): \"\"\" Quantizer for performing QAT on a model, where linear layers have int4 fake quantized grouped per channel weights. \"\"\" def __init__( self, groupsize: int = 256, inner_k_tiles: Optional[int] = 8, precision: torch.dtype = torch.bfloat16, scales_precision: torch.dtype = torch.bfloat16, ) -\u003e None: super().__init__() torch._C._log_api_usage_once( \"torchao.quantization.qat.Int4WeightOnlyQATQuantizer\" ) assert inner_k_tiles in [2, 4, 8] assert groupsize in [32, 64, 128, 256] self.inner_k_tiles = inner_k_tiles self.groupsize = groupsize self.precision = precision self.scales_precision = scales_precision def prepare( self, model: torch.nn.Module, *args: Any, **kwargs: Any ) -\u003e torch.nn.Module: _replace_linear_int4( model, self.groupsize, self.inner_k_tiles, padding_allowed=True, precision=self.precision, scales_precision=self.scales_precision, linear_class=Int4WeightOnlyQATLinear, copy_weights=True, ) return model def convert( self, model: torch.nn.Module, *args: Any, **kwargs: Any ) -\u003e torch.nn.Module: self._convert_qat_linear_4w(model) return model def _convert_qat_linear_4w(self, module: torch.nn.Module): \"\"\" Replace all `Int4WeightOnlyQATLinear` with `WeightOnlyInt4Linear`. \"\"\" for name, child in module.named_children(): if isinstance(child, Int4WeightOnlyQATLinear): in_features = child.in_features out_features = child.out_features inner_k_tiles = child.inner_k_tiles config = child.weight_fake_quantizer.config quantized_linear = WeightOnlyInt4Linear( in_features, out_features, bias=False, groupsize=config.group_size, inner_k_tiles=inner_k_tiles, precision=child.weight.dtype, scales_precision=config.scale_precision, device=next(child.parameters()).device, ) setattr(module, name, quantized_linear) # Load weights and qparams into quantized linear n_bit = 4 (q_weight, scales_and_zeros) = groupwise_affine_quantize_tensor( child.weight, n_bit, config.group_size, ) if is_device(q_weight.device.type, \"cpu\"): q_weight = torch.ops.aten._convert_weight_to_int4pack_for_cpu( q_weight.to(child.weight.device), child.inner_k_tiles, ) else: q_weight = torch.ops.aten._convert_weight_to_int4pack( q_weight.to(child.weight.device), child.inner_k_tiles, ) quantized_linear.weight = q_weight quantized_linear.scales_and_zeros = scales_and_zeros else: self._convert_qat_linear_4w(child) def get_weight_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: return _get_4w_weight_config(self.groupsize, self.scales_precision) [docs] class Int4WeightOnlyQATLinear(FakeQuantizedLinear): \"\"\" This module implements a linear layer with int4 fake quantized grouped per channel weights, with forward numerics matching `WeightOnlyInt4Linear`, which uses the efficient int4 tinygemm kernel. args: groupsize: the number of elements in each quantized group for weights precision: precision of weights scales_precision: precision of per group scales and zero points \"\"\" def __init__( self, in_features: int, out_features: int, bias: bool = False, device: torch.device = None, groupsize: int = 256, inner_k_tiles: int = 8, precision: torch.dtype = torch.bfloat16, scales_precision: torch.dtype = torch.bfloat16, ) -\u003e None: assert scales_precision == torch.bfloat16, \"only bf16 is supported for scales\" if not _check_linear_int4_k(in_features, groupsize, inner_k_tiles): raise ValueError(\"Padding for QAT 4w is not supported yet\") self.inner_k_tiles = inner_k_tiles weight_config = _get_4w_weight_config(groupsize, scales_precision) super().__init__( in_features, out_features, bias, activation_config=None, weight_config=weight_config, device=device, dtype=precision, ) def enable_fake_quant(self, enabled: bool = True): self.activation_fake_quantizer.enabled = enabled self.weight_fake_quantizer.enabled = enabled def disable_fake_quant(self): self.enable_fake_quant(False) # TODO: remove these in favor of enable_linear_fake_quant def enable_4w_fake_quant(mod: torch.nn.Module): \"\"\" (deprecated) Enable fake quantization for `Int4WeightOnlyQATLinear`. \"\"\" if isinstance(mod, Int4WeightOnlyQATLinear): mod.enable_fake_quant() # TODO: remove these in favor of disable_linear_fake_quant def disable_4w_fake_quant(mod: torch.nn.Module): \"\"\" (deprecated) Disable fake quantization for `Int4WeightOnlyQATLinear`. \"\"\" if isinstance(mod, Int4WeightOnlyQATLinear): mod.disable_fake_quant() def _get_4w_weight_config( group_size: int, qparams_precision: torch.dtype, ) -\u003e IntxFakeQuantizeConfig: \"\"\" Return the weight `IntxFakeQuantizeConfig` for `Int4WeightOnlyQATQuantizer`. \"\"\" return IntxFakeQuantizeConfig( dtype=torch.uint4, group_size=group_size, is_symmetric=False, is_dynamic=True, scale_precision=qparams_precision, zero_point_precision=qparams_precision, zero_point_domain=ZeroPointDomain.FLOAT, ) # ============================================= # | float8 rowwise activations + int4 weights | # ============================================= [docs] class Float8ActInt4WeightQATQuantizer(_LegacyQATQuantizer): \"\"\" QAT quantizer for applying dynamic rowwise float8 activation + int4 per group/channel symmetric weight fake quantization to linear layers in the model. Currently only supports rowwise granularity for float8 activations. args: group_size (Optional[int]): the number of elements in each quantized group for weights, defaults to 64. Use None for per channel. scale_precision: precision of weight scales, defaults to torch.bfloat16. \"\"\" def __init__( self, group_size: Optional[int] = 64, scale_precision: torch.dtype = torch.bfloat16, ): torch._C._log_api_usage_once( \"torchao.quantization.qat.Float8ActInt4WeightQATQuantizer\" ) if group_size is not None: weight_granularity = \"per_group\" else: weight_granularity = \"per_channel\" self._activation_config = Float8FakeQuantizeConfig( dtype=torch.float8_e4m3fn, granularity=PerRow(), ) self._weight_config = IntxFakeQuantizeConfig( dtype=torch.int4, granularity=weight_granularity, group_size=group_size, is_symmetric=True, is_dynamic=True, scale_precision=scale_precision, ) [docs] def prepare( self, model: torch.nn.Module, *args: Any, **kwargs: Any ) -\u003e torch.nn.Module: \"\"\" Swap all `nn.Linear` with `FakeQuantizedLinear` with float8 fake quantizer for activations and int4 fake quantizer for weights. \"\"\" for name, child in model.named_children(): if isinstance(child, torch.nn.Linear): new_linear = FakeQuantizedLinear.from_linear( child, activation_config=self._activation_config, weight_config=self._weight_config, ) setattr(model, name, new_linear) else: self.prepare(child) return model # TODO: add convert path def convert( self, model: torch.nn.Module, *args: Any, **kwargs: Any ) -\u003e torch.nn.Module: raise NotImplementedError def get_activation_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: raise NotImplementedError(\"Float8 FakeQuantizeConfig does not exist yet\") def get_weight_fake_quantize_config(self) -\u003e Optional[FakeQuantizeConfigBase]: return self.weight_config",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/quantization/qat/linear.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>