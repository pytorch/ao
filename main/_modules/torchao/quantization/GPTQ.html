


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.quantization.GPTQ &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='tba'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_intro.html"><code class="docutils literal notranslate"><span class="pre">torchao</span></code> API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.quantization.GPTQ</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.quantization.GPTQ</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>


<span class="c1"># This source code is licensed under the license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">torch.fx</span> <span class="k">as</span> <span class="nn">fx</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_lm_eval_available</span><span class="p">,</span>
    <span class="n">_MultiInput</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchao.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">find_multiple</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchao.utils</span> <span class="kn">import</span> <span class="n">TORCH_VERSION_AT_LEAST_2_3</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">.unified</span> <span class="kn">import</span> <span class="n">Quantizer</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_groupwise_affine_qparams</span><span class="p">,</span>
    <span class="n">groupwise_affine_quantize_tensor_from_qparams</span><span class="p">,</span>
    <span class="n">groupwise_affine_dequantize_tensor_from_qparams</span><span class="p">,</span>
    <span class="n">pack_tinygemm_scales_and_zeros</span><span class="p">,</span>
    <span class="n">groupwise_affine_quantize_tensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="kn">from</span> <span class="nn">.quant_primitives</span> <span class="kn">import</span> <span class="n">MappingType</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">_lm_eval_available</span><span class="p">:</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;lm_eval is not installed, GPTQ may not be usable&quot;</span><span class="p">)</span>

<span class="n">add_ons</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_3</span><span class="p">:</span>
    <span class="n">add_ons</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;Int8DynActInt4WeightQuantizer&quot;</span><span class="p">,</span> <span class="s2">&quot;Int8DynActInt4WeightGPTQQuantizer&quot;</span><span class="p">]</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Int4WeightOnlyGPTQQuantizer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Int4WeightOnlyQuantizer&quot;</span><span class="p">,</span>
<span class="p">]</span> <span class="o">+</span> <span class="n">add_ons</span>


<span class="k">class</span> <span class="nc">GenericGPTQRunner</span><span class="p">(</span><span class="n">fx</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a generic GPTQ runner that takes an existing model and applies GPTQ.</span>
<span class="sd">    It uses torch._dynamo.export to obtain a graph of the model and then hooks</span>
<span class="sd">    into function calls and when it detects a linear, it applies GPTQ to the weight</span>
<span class="sd">    given the calibration of inputs passed in at initialization. It puts the results</span>
<span class="sd">    into the state_dict so that the quantized model weights/qparams can be loaded</span>
<span class="sd">    directly into the model.</span>

<span class="sd">    intended to be used in concert with a GPTQQuantizer class to define the quantization mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">_MultiInput</span><span class="p">,</span>
        <span class="n">blocksize</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">percdamp</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">id_to_name</span> <span class="o">=</span> <span class="p">{</span>
            <span class="nb">id</span><span class="p">(</span><span class="n">value</span><span class="p">):</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="c1"># trace model for one input</span>
        <span class="n">one_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">multi</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">for</span> <span class="n">multi</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>  <span class="c1"># pyre-ignore[16]</span>
        <span class="c1"># needed for GPTQ on the torchao llama model</span>
        <span class="kn">import</span> <span class="nn">torchao</span>
        <span class="n">torchao</span><span class="o">.</span><span class="n">_models</span><span class="o">.</span><span class="n">llama</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_index_put_for_kv_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">exported_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">aten_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tracing_mode</span><span class="o">=</span><span class="s2">&quot;fake&quot;</span>
        <span class="p">)(</span><span class="o">*</span><span class="n">one_input</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">exported_model</span><span class="o">.</span><span class="n">graph_module</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">new_state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">blocksize</span> <span class="o">=</span> <span class="n">blocksize</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">percdamp</span> <span class="o">=</span> <span class="n">percdamp</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gptq_done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">configure_quantization_mode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">get_qparams_func</span><span class="p">,</span>
        <span class="n">quantize_func</span><span class="p">,</span>
        <span class="n">dequantize_func</span><span class="p">,</span>
        <span class="n">combine_qparams_list_func</span><span class="p">,</span>
        <span class="n">make_names_and_values_dict_func</span><span class="p">,</span>
        <span class="n">skip_layer_func</span><span class="p">,</span>
        <span class="n">act_fake_quant_func</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># these functions need to already be curried with all inputs other than weight, qparams</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">get_qparams_func</span>  <span class="c1"># accepts [2d weight tensor], outputs qparams.</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span> <span class="o">=</span> <span class="n">quantize_func</span>  <span class="c1"># accepts [2d weight tensor], [qparams], outputs a 2d quantized tensor of desired dtype</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span> <span class="o">=</span> <span class="n">dequantize_func</span>
        <span class="c1"># accepts [quantized] tensor and [qparams], outputs a 2d dequantized tensor of type float,</span>
        <span class="c1"># assumes this output .to(w_orig_dtype) is ~eventual desired dequant behavior</span>

        <span class="c1">#  `combine_qparams_list_func`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">combine_qparams_list_func</span> <span class="o">=</span> <span class="n">combine_qparams_list_func</span>
        <span class="c1"># accepts [`list` of qparams] from quantizing one group at a time,</span>
        <span class="c1"># outputs a qparams object that could be passed into quant/dequantize_func</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span> <span class="o">=</span> <span class="n">skip_layer_func</span>  <span class="c1"># accepts [weight tensor], outputs a bool on whether or not to apply gptq to this layer</span>

        <span class="c1">#  `make_names_and_values_dict_func`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">make_names_and_values_dict_func</span> <span class="o">=</span> <span class="n">make_names_and_values_dict_func</span>  <span class="c1"># accepts [2d quantized tensor], [qparams], returns a dict of names, values to put in state_dict</span>
        <span class="c1"># note any final packing for storage should happen here</span>

        <span class="c1"># `act_fake_quant_func`</span>
        <span class="k">if</span> <span class="n">act_fake_quant_func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span> <span class="o">=</span> <span class="n">act_fake_quant_func</span> <span class="c1"># accepts [activation tensor], returns a fake-quantized activation tensor</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;need to configure quantization mode before running&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gptq_done</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_quantized_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gptq_done</span>
        <span class="p">),</span> <span class="s2">&quot;need to run GPTQRunner before you can get_quantized_state_dict&quot;</span>
        <span class="n">quantized_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_state_dict</span>
        <span class="c1"># Don&#39;t want to store/load the kv_cache so remove it from the state_dict</span>
        <span class="n">del_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param_fqn</span> <span class="ow">in</span> <span class="n">quantized_state_dict</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;kv_cache&quot;</span> <span class="ow">in</span> <span class="n">param_fqn</span><span class="p">:</span>
                <span class="n">del_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_fqn</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param_fqn</span> <span class="ow">in</span> <span class="n">del_list</span><span class="p">:</span>
            <span class="n">quantized_state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">param_fqn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">quantized_state_dict</span>

    <span class="k">def</span> <span class="nf">call_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">already_quantized</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>  <span class="c1"># noqa: C901</span>

        <span class="k">def</span> <span class="nf">tensors_to_cuda</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">new_args</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
                <span class="n">new_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">new_args</span>

        <span class="c1"># flatten args and kwargs together</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
        <span class="c1"># move all single tensors to cuda, will move _MultiInputs to cuda one at a time</span>
        <span class="n">flat_args</span> <span class="o">=</span> <span class="n">tensors_to_cuda</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="n">has_multi_input</span> <span class="o">=</span> <span class="n">_MultiInput</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">has_multi_input</span><span class="p">:</span>
            <span class="c1"># Just some trickery to convert</span>
            <span class="c1"># [_MultiInput[a, a, a], _MultiInput(b, b, b)] =&gt; [a, b], [a, b], [a, b]</span>
            <span class="n">multi_input_count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_MultiInput</span><span class="p">)</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">transposed_args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span>
                    <span class="o">*</span><span class="p">[</span>
                        <span class="p">(</span>
                            <span class="n">x</span><span class="o">.</span><span class="n">values</span>
                            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_MultiInput</span><span class="p">)</span>
                            <span class="k">else</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">multi_input_count</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transposed_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">flat_args</span><span class="p">]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># check whether we apply GPTQ to this module</span>
        <span class="n">quantize_linear</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">target</span> <span class="o">==</span> <span class="n">aten</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>  <span class="c1"># if its a linear</span>
            <span class="ow">and</span> <span class="nb">id</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_to_name</span>  <span class="c1"># and if we know the layer name</span>
            <span class="c1"># and we haven&#39;t already quantized this layer</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">already_quantized</span>
            <span class="c1"># and if the skip_layer_func doesn&#39;t say we should skip</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="p">)</span>  <span class="c1"># then we will quantize this linear layer/weight</span>

        <span class="k">if</span> <span class="n">quantize_linear</span><span class="p">:</span>  <span class="c1"># instantiate variables for GPTQ</span>
            <span class="n">H</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total_batches</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">transposed_args</span><span class="p">:</span>
            <span class="n">inp</span> <span class="o">=</span> <span class="n">tensors_to_cuda</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
            <span class="n">cur_args</span><span class="p">,</span> <span class="n">cur_kwargs</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">spec</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="n">quantize_linear</span>
            <span class="p">):</span>  <span class="c1"># calculate H instead of output (will run the linear eventually with updated weight)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">cur_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">H</span> <span class="o">*=</span> <span class="n">total_batches</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_batches</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
                <span class="n">total_batches</span> <span class="o">+=</span> <span class="n">n</span>
                <span class="n">x</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span> <span class="o">/</span> <span class="n">total_batches</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">H</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># weight has already been quantized but still need to apply</span>
                <span class="c1"># activation quant for final calculation</span>
                <span class="k">if</span> <span class="n">already_quantized</span><span class="p">:</span>
                    <span class="n">cur_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span><span class="p">(</span><span class="n">cur_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">*</span><span class="n">cur_args</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

                <span class="c1"># get output if its not a linear</span>
                <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">cur_args</span><span class="p">,</span> <span class="n">cur_kwargs</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">quantize_linear</span><span class="p">:</span>
            <span class="n">mod_fqn</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">id_to_name</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

            <span class="n">W</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="n">Q</span><span class="p">,</span> <span class="n">DQ</span><span class="p">,</span> <span class="n">qparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">faster_quant</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">mod_fqn</span><span class="p">)</span>

            <span class="c1">#  `make_names_and_values_dict_func`.</span>
            <span class="n">names_and_values_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_names_and_values_dict_func</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">qparams</span><span class="p">)</span>

            <span class="c1"># delete old weight</span>
            <span class="k">if</span> <span class="n">mod_fqn</span> <span class="o">+</span> <span class="s2">&quot;.weight&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_state_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">mod_fqn</span> <span class="o">+</span> <span class="s2">&quot;.weight&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_state_dict</span><span class="p">[</span><span class="n">mod_fqn</span> <span class="o">+</span> <span class="s2">&quot;.bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">names_and_values_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">new_state_dict</span><span class="p">[</span><span class="n">mod_fqn</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

            <span class="c1"># run linear with new weight to get corrected output</span>
            <span class="n">new_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
                <span class="n">target</span><span class="p">,</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">DQ</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">already_quantized</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
                <span class="n">old_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
                    <span class="n">target</span><span class="p">,</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">already_quantized</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

                <span class="k">def</span> <span class="nf">SQNR</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                    <span class="c1"># TODO: Use of deprecated function torch.norm</span>
                    <span class="k">return</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
                    <span class="p">)</span>

                <span class="c1">#  `dequantize_func`.</span>
                <span class="n">DQ_after</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">qparams</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;SQNR for QDQ (this should be inf)&quot;</span><span class="p">,</span> <span class="n">SQNR</span><span class="p">(</span><span class="n">DQ</span><span class="p">,</span> <span class="n">DQ_after</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># matches</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;SQNR for weight (can be low)&quot;</span><span class="p">,</span> <span class="n">SQNR</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">DQ</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
                <span class="p">)</span>  <span class="c1"># fine to not match</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;SQNR for output with GPTQ (hopefully 35+)&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">SQNR</span><span class="p">(</span><span class="n">old</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">new</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                            <span class="k">for</span> <span class="p">(</span><span class="n">old</span><span class="p">,</span> <span class="n">new</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_out</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">new_out</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
                        <span class="p">]</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                <span class="p">)</span>

                <span class="c1">#  `get_qparams_func`.</span>
                <span class="n">qparams2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

                <span class="n">Q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">qparams2</span><span class="p">)</span>
                <span class="n">DQ2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span><span class="p">(</span><span class="n">Q2</span><span class="p">,</span> <span class="n">qparams2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">old_q_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
                    <span class="n">target</span><span class="p">,</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">2</span><span class="p">],</span> <span class="n">DQ2</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">already_quantized</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;SQNR for output without GPTQ (should be less than above)&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">SQNR</span><span class="p">(</span><span class="n">old</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">old_q</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                            <span class="k">for</span> <span class="p">(</span><span class="n">old</span><span class="p">,</span> <span class="n">old_q</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_out</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">old_q_out</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
                        <span class="p">]</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_out</span>

        <span class="k">return</span> <span class="n">_MultiInput</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">has_multi_input</span> <span class="k">else</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">faster_quant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
        <span class="n">percdamp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">percdamp</span>
        <span class="n">blocksize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocksize</span>
        <span class="n">groupsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span>
        <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span>

        <span class="k">if</span> <span class="n">groupsize</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>

            <span class="n">cur_qparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">dead</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">H</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">H</span><span class="p">[</span><span class="n">dead</span><span class="p">,</span> <span class="n">dead</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">W</span><span class="p">[:,</span> <span class="n">dead</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">Losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">DQ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

        <span class="n">damp</span> <span class="o">=</span> <span class="n">percdamp</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">H</span><span class="p">))</span>
        <span class="n">diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">H</span><span class="p">[</span><span class="n">diag</span><span class="p">,</span> <span class="n">diag</span><span class="p">]</span> <span class="o">+=</span> <span class="n">damp</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky_inverse</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">Hinv</span> <span class="o">=</span> <span class="n">H</span>

        <span class="n">all_qparams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">blocksize</span><span class="p">):</span>
            <span class="n">i2</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i1</span> <span class="o">+</span> <span class="n">blocksize</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">i2</span> <span class="o">-</span> <span class="n">i1</span>
            <span class="n">W1</span> <span class="o">=</span> <span class="n">W</span><span class="p">[:,</span> <span class="n">i1</span><span class="p">:</span><span class="n">i2</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">DQ1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
            <span class="n">Err1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
            <span class="n">Losses1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
            <span class="n">Hinv1</span> <span class="o">=</span> <span class="n">Hinv</span><span class="p">[</span><span class="n">i1</span><span class="p">:</span><span class="n">i2</span><span class="p">,</span> <span class="n">i1</span><span class="p">:</span><span class="n">i2</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">):</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">W1</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">d</span> <span class="o">=</span> <span class="n">Hinv1</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">groupsize</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i1</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="n">groupsize</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># start of new group</span>
                    <span class="n">cur_qparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span><span class="p">(</span>
                        <span class="n">W</span><span class="p">[:,</span> <span class="p">(</span><span class="n">i1</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">i1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="n">groupsize</span><span class="p">)]</span>
                    <span class="p">)</span>
                    <span class="n">all_qparams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_qparams</span><span class="p">)</span>

                <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">cur_qparams</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

                <span class="c1">#  `dequantize_func`.</span>

                <span class="n">dq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">cur_qparams</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

                <span class="n">DQ1</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dq</span>
                <span class="n">Losses1</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">-</span> <span class="n">dq</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">d</span><span class="o">**</span><span class="mi">2</span>

                <span class="n">err1</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">-</span> <span class="n">dq</span><span class="p">)</span> <span class="o">/</span> <span class="n">d</span>
                <span class="n">W1</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:]</span> <span class="o">-=</span> <span class="p">(</span>
                    <span class="n">err1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">Hinv1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Hinv1</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="n">Err1</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">err1</span>

            <span class="n">DQ</span><span class="p">[:,</span> <span class="n">i1</span><span class="p">:</span><span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="n">DQ1</span>
            <span class="n">Losses</span><span class="p">[:,</span> <span class="n">i1</span><span class="p">:</span><span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="n">Losses1</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="n">W</span><span class="p">[:,</span> <span class="n">i2</span><span class="p">:]</span> <span class="o">-=</span> <span class="n">Err1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">Hinv</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Hinv</span><span class="p">[</span><span class="n">i1</span><span class="p">:</span><span class="n">i2</span><span class="p">,</span> <span class="n">i2</span><span class="p">:])</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">all_qparams</span> <span class="o">==</span> <span class="p">[]:</span>

            <span class="n">all_qparams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_qparams</span><span class="p">)</span>

        <span class="c1"># convert a list of qparams objects into a single one. enerally by</span>
        <span class="c1"># concatenating a bunch of n,1 scale/zeros tensors into a n,num_groups tensor</span>

        <span class="c1">#  `combine_qparams_list_func`.</span>
        <span class="n">all_qparams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">combine_qparams_list_func</span><span class="p">(</span><span class="n">all_qparams</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span><span class="p">(</span><span class="n">DQ</span><span class="p">,</span> <span class="n">all_qparams</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">DQ</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">orig_dtype</span><span class="p">),</span> <span class="n">all_qparams</span>


<span class="k">class</span> <span class="nc">GPTQQuantizer</span><span class="p">(</span><span class="n">Quantizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements a GPTQ Quantizer that can be used to apply GPTQ to a model in concert with the GenericGPTQRunner class.</span>
<span class="sd">    Unlike the base Quantizer class, the user does not need to implement the create_quantized_state_dict, instead they have to reimplement</span>
<span class="sd">    __init__ such that it defines the functions for the quantization mode. User is expected to reimplement convert_for_runtime.</span>

<span class="sd">    The following functions (which must be defined in __init__) are used to define the quantization mode for both GPTQ and</span>
<span class="sd">    create_quantized_state_dict. Here is a description of each function.</span>

<span class="sd">    get_qparams_func:</span>
<span class="sd">        A function that calculates the quantization qparams for an input tensor.</span>
<span class="sd">        Args:</span>
<span class="sd">            weight: A 2d weight tensor with non-integer dtype.</span>
<span class="sd">        Returns:</span>
<span class="sd">            qparams: it can have any format but will need to be handled by the other defined functions below.</span>

<span class="sd">    quantize_func:</span>
<span class="sd">        A function that applies quantization to an input tensor. It should be noted</span>
<span class="sd">        that this function needs to be able to handle quantizing the entire weight tensor, a single group,</span>
<span class="sd">        or a single column.</span>
<span class="sd">        Args:</span>
<span class="sd">            weight: A 2d weight tensor with non-integer dtype.</span>
<span class="sd">            qparams: the output from get_qparams_func</span>
<span class="sd">        Returns:</span>
<span class="sd">            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)</span>


<span class="sd">    dequantize_func:</span>
<span class="sd">        A function that dequantizes an input quantized weight tensor. It should be noted</span>
<span class="sd">        that this function needs to be able to handle dequantizing the entire weight tensor, a single group,</span>
<span class="sd">        or a single column.</span>
<span class="sd">        Args:</span>
<span class="sd">            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)</span>
<span class="sd">            qparams: the output from get_qparams_func</span>
<span class="sd">        Returns:</span>
<span class="sd">            weight: A 2d weight tensor with non-integer dtype.</span>

<span class="sd">    act_fake_quant_func (optional):</span>
<span class="sd">            A function that (dynamically) quantizes activation to input</span>
<span class="sd">            Args:</span>
<span class="sd">                input: input Tensor in f32/bf16/f16</span>
<span class="sd">            Returns:</span>
<span class="sd">                output: dynamically quantized and dequantized Tensor (with the same dtype as input)</span>

<span class="sd">    combine_qparams_list_func:</span>
<span class="sd">        A function that combines several qparams into one qparam.</span>
<span class="sd">        Args:</span>
<span class="sd">            qparams_list: a list of qparams objects, each obtained by calling get_qparams_func</span>
<span class="sd">            on a single group from a weight tensor</span>
<span class="sd">        Returns:</span>
<span class="sd">            qparams: an object of the same format as the qparams above.</span>

<span class="sd">    skip_layer_func:</span>
<span class="sd">        A function that determines which linear layers should be skipped during GPTQ</span>
<span class="sd">        Args:</span>
<span class="sd">            weight: A 2d weight tensor with non-integer dtype.</span>
<span class="sd">        Returns:</span>
<span class="sd">            skip: boolean indicating whether layer should be skipped</span>

<span class="sd">    make_names_and_values_dict_func:</span>
<span class="sd">        A function that prepares the qparams and quantized_weight and creates a dictionary indicating how they</span>
<span class="sd">        should be inserted into the state_dict. Generally any packing of the weight and qparams should be done here.</span>
<span class="sd">        Args:</span>
<span class="sd">            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)</span>
<span class="sd">            qparams: the output from get_qparams_func</span>
<span class="sd">        Returns:</span>
<span class="sd">            names_and_values_dict: a dictionary mapping the name of the parameters of the quantized module to the</span>
<span class="sd">            corresponding quantized weights and qparams.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">combine_qparams_list_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1">#  `make_names_and_values_dict_func`.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_names_and_values_dict_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_create_quantized_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">blocksize</span><span class="p">,</span>
        <span class="n">percdamp</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">,</span>
        <span class="c1">#  `typing.Dict[&lt;key type&gt;, &lt;value type&gt;]` to avoid runtime subscripting errors.</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tracing model for GPTQ&quot;</span><span class="p">)</span>
        <span class="n">GPTQ_runner</span> <span class="o">=</span> <span class="n">GenericGPTQRunner</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">blocksize</span><span class="p">,</span>
            <span class="n">percdamp</span><span class="p">,</span>
            <span class="n">groupsize</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">configure_quantization_mode</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">combine_qparams_list_func</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_names_and_values_dict_func</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;act_fake_quant_func&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># pyre-ignore[16]</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Applying GPTQ to weights&quot;</span><span class="p">)</span>
        <span class="n">GPTQ_runner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">GPTQ_runner</span><span class="o">.</span><span class="n">get_quantized_state_dict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_convert_for_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;nn.Module&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;_convert_for_runtime not implemented&quot;</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_MultiInput</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="k">def</span> <span class="nf">_check_linear_int4_k</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">groupsize</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">k_divisible_by_groupsize</span> <span class="o">=</span> <span class="n">k</span> <span class="o">%</span> <span class="n">groupsize</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">inner_k_tiles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">k_divisible_by_16_times_inner_k_tiles</span> <span class="o">=</span> <span class="n">k</span> <span class="o">%</span> <span class="p">(</span><span class="n">inner_k_tiles</span> <span class="o">*</span> <span class="mi">16</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">k_divisible_by_groupsize</span> <span class="ow">and</span> <span class="n">k_divisible_by_16_times_inner_k_tiles</span>
    <span class="k">return</span> <span class="n">k_divisible_by_groupsize</span>

<span class="k">def</span> <span class="nf">linear_forward_int4</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weight_int4pack</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scales_and_zeros</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">origin_x_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">origin_x_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_weight_int4pack_mm</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">precision</span><span class="p">),</span>
        <span class="n">weight_int4pack</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">,</span>
        <span class="n">scales_and_zeros</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">scales_precision</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">origin_x_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">out_features</span><span class="p">,)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">class</span> <span class="nc">WeightOnlyInt4Linear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;in_features&#39;</span><span class="p">,</span> <span class="s1">&#39;out_features&#39;</span><span class="p">]</span>
    <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># TODO: remove dtype field, not used</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">inner_k_tiles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="n">inner_k_tiles</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">find_multiple</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">origin_in_features</span> <span class="o">=</span> <span class="n">in_features</span>
            <span class="n">in_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">,</span> <span class="s2">&quot;require bias=False&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">inner_k_tiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span> <span class="o">=</span> <span class="n">scales_precision</span>

        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please specify &#39;precision&#39; instead of &#39;dtype&#39;&quot;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">out_features</span> <span class="o">%</span> <span class="mi">8</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;require out_features % 8 == 0&quot;</span>
        <span class="k">assert</span> <span class="n">in_features</span> <span class="o">%</span> <span class="p">(</span><span class="n">inner_k_tiles</span> <span class="o">*</span> <span class="mi">16</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;require in_features % (innerKTiles * 16) == 0&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">out_features</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="n">in_features</span> <span class="o">//</span> <span class="p">(</span><span class="n">inner_k_tiles</span> <span class="o">*</span> <span class="mi">16</span><span class="p">),</span> <span class="mi">32</span><span class="p">,</span> <span class="n">inner_k_tiles</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;scales_and_zeros&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">in_features</span> <span class="o">//</span> <span class="n">groupsize</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">origin_in_features</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">linear_forward_int4</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scales_and_zeros</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_replace_linear_int4</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">inner_k_tiles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">skip_layer_func</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">linear_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeightOnlyInt4Linear</span><span class="p">,</span>
    <span class="n">copy_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="c1"># TODO: support linear bias</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">skip_layer_func</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">skip_layer_func</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="n">inner_k_tiles</span><span class="p">)</span> <span class="ow">or</span> <span class="n">padding_allowed</span><span class="p">:</span>
                <span class="n">new_linear</span> <span class="o">=</span> <span class="n">linear_class</span><span class="p">(</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
                    <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="n">groupsize</span><span class="o">=</span><span class="n">groupsize</span><span class="p">,</span>
                    <span class="n">inner_k_tiles</span><span class="o">=</span><span class="n">inner_k_tiles</span><span class="p">,</span>
                    <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
                    <span class="n">scales_precision</span><span class="o">=</span><span class="n">scales_precision</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># TODO: merge with 8da4w?</span>
                <span class="c1"># In distributed training, the model may be instantiated</span>
                <span class="c1"># on the meta device, in which case there is no need to</span>
                <span class="c1"># copy the weights, and doing so will result in an error</span>
                <span class="k">if</span> <span class="n">copy_weights</span> <span class="ow">and</span> <span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
                    <span class="n">new_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">weight</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_linear</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_replace_linear_int4</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span>
                <span class="n">groupsize</span><span class="p">,</span>
                <span class="n">inner_k_tiles</span><span class="p">,</span>
                <span class="n">padding_allowed</span><span class="p">,</span>
                <span class="n">skip_layer_func</span><span class="p">,</span>
                <span class="n">precision</span><span class="p">,</span>
                <span class="n">scales_precision</span><span class="p">,</span>
                <span class="n">linear_class</span><span class="p">,</span>
                <span class="n">copy_weights</span><span class="p">,</span>
            <span class="p">)</span>


<span class="k">def</span> <span class="nf">replace_linear_int4</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="n">inner_k_tiles</span><span class="p">,</span> <span class="n">padding_allowed</span><span class="p">,</span> <span class="n">skip_layer_func</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">_replace_linear_int4</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">,</span>
        <span class="n">inner_k_tiles</span><span class="p">,</span>
        <span class="n">padding_allowed</span><span class="p">,</span>
        <span class="n">skip_layer_func</span><span class="p">,</span>
        <span class="n">linear_class</span><span class="o">=</span><span class="n">WeightOnlyInt4Linear</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="Int4WeightOnlyQuantizer"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int4WeightOnlyQuantizer.html#torchao.quantization.Int4WeightOnlyQuantizer">[docs]</a><span class="k">class</span> <span class="nc">Int4WeightOnlyQuantizer</span><span class="p">(</span><span class="n">Quantizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">inner_k_tiles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">inner_k_tiles</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">groupsize</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">inner_k_tiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">padding_allowed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="c1"># precision and dtype are being used interchangeably here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">precision</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_create_quantized_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">cur_state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span>
                <span class="n">out_features</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">out_features</span>
                <span class="n">in_features</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">in_features</span>
                <span class="c1"># assert out_features % 8 == 0, &quot;require out_features % 8 == 0&quot;</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;linear: </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">, in=</span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2">, out=</span><span class="si">{</span><span class="n">out_features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">in_features</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;require in_features:</span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2"> % self.groupsize:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="si">}</span><span class="s2"> == 0&quot;</span>

                <span class="n">weight</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_linear_int4_k</span><span class="p">(</span>
                    <span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">:</span>
                        <span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">find_multiple</span>
                        <span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
                        <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;warning: </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> is padded to satisfy in_features % 1024 == 0&quot;</span><span class="p">)</span>
                        <span class="n">padded_in_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
                        <span class="n">weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_in_features</span> <span class="o">-</span> <span class="n">in_features</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;warning: </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, &quot;</span> <span class="o">+</span>
                                <span class="s2">&quot;and that groupsize and inner_k_tiles*16 evenly divide into it&quot;</span><span class="p">)</span>
                        <span class="k">continue</span>
                <span class="p">(</span>
                    <span class="n">w_int4x8</span><span class="p">,</span>
                    <span class="n">scales_and_zeros</span>
                <span class="p">)</span> <span class="o">=</span> <span class="n">groupwise_affine_quantize_tensor</span><span class="p">(</span>
                    <span class="n">weight</span><span class="p">,</span>
                    <span class="mi">4</span><span class="p">,</span>  <span class="c1"># n_bit</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span> <span class="c1"># dtype for scales_and_zeros</span>
                <span class="p">)</span>
                <span class="c1"># TODO: just get the device from mod.weight.device?</span>
                <span class="n">weight_int4pack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_convert_weight_to_int4pack</span><span class="p">(</span><span class="n">w_int4x8</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">)</span>
                <span class="n">cur_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_int4pack</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">cur_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.scales_and_zeros&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scales_and_zeros</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cur_state_dict</span>

    <span class="k">def</span> <span class="nf">_convert_for_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">_replace_linear_int4</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">,</span>
            <span class="n">skip_layer_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">scales_precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_quantized_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_for_runtime</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="c1"># TODO: make it strict</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span></div>


<div class="viewcode-block" id="Int4WeightOnlyGPTQQuantizer"><a class="viewcode-back" href="../../../generated/torchao.quantization.Int4WeightOnlyGPTQQuantizer.html#torchao.quantization.Int4WeightOnlyGPTQQuantizer">[docs]</a><span class="k">class</span> <span class="nc">Int4WeightOnlyGPTQQuantizer</span><span class="p">(</span><span class="n">GPTQQuantizer</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">blocksize</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">percdamp</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="n">groupsize</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">padding_allowed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocksize</span> <span class="o">=</span> <span class="n">blocksize</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">percdamp</span> <span class="o">=</span> <span class="n">percdamp</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">=</span> <span class="n">groupsize</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">inner_k_tiles</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span> <span class="o">=</span> <span class="n">padding_allowed</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">get_groupwise_affine_qparams</span><span class="p">(</span>
                <span class="n">w</span><span class="p">,</span> <span class="n">n_bit</span><span class="p">,</span> <span class="n">groupsize</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">qparams</span><span class="p">:</span> <span class="n">groupwise_affine_quantize_tensor_from_qparams</span><span class="p">(</span>
                <span class="n">w</span><span class="p">,</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_bit</span><span class="p">,</span> <span class="n">groupsize</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">,</span> <span class="n">qparams</span><span class="p">:</span> <span class="n">groupwise_affine_dequantize_tensor_from_qparams</span><span class="p">(</span>
                <span class="n">q</span><span class="p">,</span>
                <span class="n">qparams</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">qparams</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">n_bit</span><span class="p">,</span>
                <span class="n">groupsize</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">combine_qparams_list_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">qparams_list</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">qparams_list</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="c1"># skip unless padding_allowed=True or its correctly sized</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">linear_weight</span><span class="p">:</span> <span class="ow">not</span> <span class="p">(</span>
                <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">linear_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">groupsize</span><span class="p">)</span> <span class="ow">or</span> <span class="n">padding_allowed</span>
            <span class="p">)</span>

            <span class="c1"># we need to do the padding here, both for q and the qparams if necessary</span>

            <span class="c1"># TODO: this is the gpt-fast version, merge with the main version later</span>
            <span class="k">def</span> <span class="nf">make_names_and_values_dict_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">qparams</span><span class="p">):</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">2</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">):</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">k</span>
                <span class="c1"># how much we need to pad the weight</span>
                <span class="n">delta_k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">new_k</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">final_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_convert_weight_to_int4pack</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">delta_k</span><span class="p">)),</span> <span class="n">inner_k_tiles</span><span class="p">)</span>
                <span class="n">scales</span> <span class="o">=</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">zeros</span> <span class="o">=</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">scales_and_zeros</span> <span class="o">=</span> <span class="n">pack_tinygemm_scales_and_zeros</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
                <span class="c1"># how many new groups we need for padded weight</span>
                <span class="n">delta_groups</span> <span class="o">=</span> <span class="n">new_k</span> <span class="o">//</span> <span class="n">groupsize</span> <span class="o">-</span> <span class="n">scales_and_zeros</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">final_s_and_z</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">scales_and_zeros</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span> <span class="n">delta_groups</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">final_q</span><span class="p">,</span> <span class="s2">&quot;scales_and_zeros&quot;</span><span class="p">:</span> <span class="n">final_s_and_z</span><span class="p">}</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">make_names_and_values_dict_func</span> <span class="o">=</span> <span class="n">make_names_and_values_dict_func</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_convert_for_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
            <span class="n">replace_linear_int4</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">,</span>
                <span class="n">skip_layer_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span>

        <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_MultiInput</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_quantized_state_dict</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">blocksize</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">percdamp</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_for_runtime</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span></div>


<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_group_qparams_symmetric</span><span class="p">,</span>
    <span class="n">group_quantize_tensor_symmetric</span><span class="p">,</span>
    <span class="n">per_token_dynamic_quant</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">linear_forward_8da4w</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">weight_int8</span><span class="p">,</span>
    <span class="n">scales</span><span class="p">,</span>
    <span class="n">zeros</span><span class="p">,</span>
    <span class="n">out_features</span><span class="p">,</span>
    <span class="n">groupsize</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">per_token_dynamic_quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># TODO: verify and remove following reshape code</span>
    <span class="c1"># origin_x_size = x.size()</span>
    <span class="c1"># x = x.reshape(-1, origin_x_size[-1])</span>

    <span class="c1"># TODO: better API</span>
    <span class="c1"># weight_int8 = torch.ops.quantized_decomposed.unpack_int4_to_int8(weight_int4packed)</span>
    <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n_bit</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n_bit</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="kn">from</span> <span class="nn">torchao._executorch_ops</span> <span class="kn">import</span> <span class="n">_quantized_decomposed_dequantize_per_channel_group_wrapper</span>
    <span class="n">w_dq</span> <span class="o">=</span> <span class="n">_quantized_decomposed_dequantize_per_channel_group_wrapper</span><span class="p">(</span>
        <span class="n">weight_int8</span><span class="p">,</span>
        <span class="n">scales</span><span class="p">,</span>
        <span class="n">zeros</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># x = x.to(torch.float16)</span>
    <span class="c1"># w_dq = w_dq.to(torch.float16)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_dq</span><span class="p">)</span>

    <span class="c1"># new_shape = origin_x_size[:-1] + (out_features,)</span>
    <span class="c1"># c = c.reshape(new_shape)</span>

    <span class="k">return</span> <span class="n">c</span>

<span class="k">class</span> <span class="nc">Int8DynActInt4WeightLinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;in_features&quot;</span><span class="p">,</span> <span class="s2">&quot;out_features&quot;</span><span class="p">]</span>

    <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This module implements a dynamic quantized linear layer with int4 weight.</span>
<span class="sd">    Weights are per channel groupwise quantized. Parameters of importance</span>
<span class="sd">    groupsize: the number of elements in each quantized group</span>
<span class="sd">    precision: precision of input and output. e.g. torch.float32 means input</span>
<span class="sd">    activation is float32 and output is float32.</span>
<span class="sd">    scales_precision: precision of per group scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="c1"># TODO: remove this field, not used</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># always pad if needed since it becomes a noop at runtime if not needed</span>
        <span class="c1"># self.origin_in_features = in_features</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">in_features</span> <span class="o">%</span> <span class="n">groupsize</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;require in_features:</span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2"> % groupsize:</span><span class="si">{</span><span class="n">groupsize</span><span class="si">}</span><span class="s2"> == 0&quot;</span>
        <span class="c1"># in_features = _calc_padded_size_linear_int4(</span>
        <span class="c1">#    in_features, groupsize</span>
        <span class="c1"># )</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">,</span> <span class="s2">&quot;require bias=False&quot;</span>
        <span class="c1"># TODO: align groupsize naming</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="c1"># Precision of the activation which also indicates</span>
        <span class="c1"># output precision of the dynamically quantized linear layer</span>
        <span class="c1"># that his module represents.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span>

        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please specify &#39;precision&#39; instead of &#39;dtype&#39;&quot;</span><span class="p">)</span>

        <span class="c1"># currently storing unpacked int8 weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;scales&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span> <span class="o">//</span> <span class="n">groupsize</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">scales_precision</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span> <span class="o">//</span> <span class="n">groupsize</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">scales_precision</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">)</span>
        <span class="c1"># padding is removed for perf</span>
        <span class="c1"># input = F.pad(input, pad=(0, self.in_features - self.origin_in_features))</span>
        <span class="k">return</span> <span class="n">linear_forward_8da4w</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">_replace_linear_8da4w</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">linear_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
    <span class="n">copy_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    
    <span class="c1">#import the util function here to avoid circular dependency</span>
    <span class="kn">from</span> <span class="nn">torchao.quantization.quant_api</span> <span class="kn">import</span> <span class="n">_replace_with_custom_fn_if_matches_filter</span>

    <span class="k">def</span> <span class="nf">filter_fn</span><span class="p">(</span><span class="n">child</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">cur_fqn</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># TODO: support linear bias</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">)</span> <span class="ow">or</span> <span class="n">padding_allowed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">replacement_fn</span><span class="p">(</span><span class="n">child</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">new_linear</span> <span class="o">=</span> <span class="n">linear_class</span><span class="p">(</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
                    <span class="n">child</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
                    <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="n">groupsize</span><span class="o">=</span><span class="n">groupsize</span><span class="p">,</span>
                    <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
                    <span class="n">scales_precision</span><span class="o">=</span><span class="n">scales_precision</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># In distributed training, the model may be instantiated</span>
        <span class="c1"># on the meta device, in which case there is no need to</span>
        <span class="c1"># copy the weights, and doing so will result in an error</span>
        <span class="k">if</span> <span class="n">copy_weights</span> <span class="ow">and</span> <span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">new_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">child</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">return</span> <span class="n">new_linear</span>

    <span class="n">_replace_with_custom_fn_if_matches_filter</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">replacement_fn</span><span class="p">,</span> <span class="n">filter_fn</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">replace_linear_8da4w</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">_replace_linear_8da4w</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">,</span>
        <span class="n">padding_allowed</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">,</span>
        <span class="n">Int8DynActInt4WeightLinear</span><span class="p">,</span>
    <span class="p">)</span>

<span class="k">class</span> <span class="nc">Int8DynActInt4WeightQuantizer</span><span class="p">(</span><span class="n">Quantizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
        <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">padding_allowed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">scales_precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span> <span class="o">=</span> <span class="n">mapping_type</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_create_quantized_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">cur_state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span>
                <span class="n">out_features</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">out_features</span>
                <span class="n">in_features</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">in_features</span>
                <span class="c1"># assert out_features % 8 == 0, &quot;require out_features % 8 == 0&quot;</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;linear: </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">, in=</span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2">, out=</span><span class="si">{</span><span class="n">out_features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">in_features</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;require in_features:</span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2"> % self.groupsize:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="si">}</span><span class="s2"> == 0&quot;</span>

                <span class="n">weight</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">:</span>
                        <span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">find_multiple</span>
                        <span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
                        <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;warning: </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> is padded to satisfy in_features % 1024 == 0&quot;</span><span class="p">)</span>
                        <span class="n">padded_in_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
                        <span class="n">weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_in_features</span> <span class="o">-</span> <span class="n">in_features</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;warning: </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, &quot;</span> <span class="o">+</span>
                              <span class="s2">&quot;and that groupsize and inner_k_tiles*16 evenly divide into it&quot;</span><span class="p">)</span>
                        <span class="k">continue</span>
                <span class="p">(</span>
                    <span class="n">weight_int8</span><span class="p">,</span>
                    <span class="n">scales</span><span class="p">,</span>
                    <span class="n">zeros</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="n">group_quantize_tensor_symmetric</span><span class="p">(</span>
                    <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">),</span>
                    <span class="mi">4</span><span class="p">,</span>  <span class="c1"># n_bit</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scales_precision</span><span class="p">,</span>
                    <span class="n">mapping_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mapping_type</span>
                <span class="p">)</span>
                <span class="n">cur_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_int8</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">cur_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.scales&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scales</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">cur_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">.zeros&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">zeros</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="c1"># TODO: support bias?</span>

        <span class="k">return</span> <span class="n">cur_state_dict</span>

    <span class="k">def</span> <span class="nf">_convert_for_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">replace_linear_8da4w</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="c1"># TODO: this should be self.scales_precision?</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_quantized_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_for_runtime</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="c1"># TODO: make it strict</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">class</span> <span class="nc">Int8DynActInt4WeightGPTQQuantizer</span><span class="p">(</span><span class="n">GPTQQuantizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">blocksize</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">percdamp</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span>
        <span class="n">groupsize</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">inner_k_tiles</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">padding_allowed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">precision</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocksize</span> <span class="o">=</span> <span class="n">blocksize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">percdamp</span> <span class="o">=</span> <span class="n">percdamp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span> <span class="o">=</span> <span class="n">groupsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">inner_k_tiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span> <span class="o">=</span> <span class="n">padding_allowed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">act_fake_quant_func</span> <span class="o">=</span> <span class="n">per_token_dynamic_quant</span>
        <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_qparams_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">get_group_qparams_symmetric</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">n_bit</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>
        <span class="n">quant_min</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n_bit</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n_bit</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="kn">from</span> <span class="nn">torchao._executorch_ops</span> <span class="kn">import</span> <span class="n">_quantized_decomposed_quantize_per_channel_group_wrapper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantize_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">qparams</span><span class="p">:</span> <span class="n">_quantized_decomposed_quantize_per_channel_group_wrapper</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">groupsize</span>
        <span class="p">)</span>

        <span class="kn">from</span> <span class="nn">torchao._executorch_ops</span> <span class="kn">import</span> <span class="n">_quantized_decomposed_dequantize_per_channel_group_wrapper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">,</span> <span class="n">qparams</span><span class="p">:</span> <span class="n">_quantized_decomposed_dequantize_per_channel_group_wrapper</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">qparams</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">qparams</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
            <span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">combine_qparams_list_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">qparams_list</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">qparams_list</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="c1"># skip unless padding_allowed=True or its correctly sized</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">skip_layer_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">linear_weight</span><span class="p">:</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">_check_linear_int4_k</span><span class="p">(</span><span class="n">linear_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">groupsize</span><span class="p">)</span> <span class="ow">or</span> <span class="n">padding_allowed</span>
        <span class="p">)</span>

        <span class="c1"># we need to do the padding here, both for q and the qparams if necessary</span>
        <span class="k">def</span> <span class="nf">make_names_and_values_dict_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">qparams</span><span class="p">):</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">new_k</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">groupsize</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">groupsize</span><span class="p">)</span>
            <span class="c1"># how much we need to pad the weight</span>
            <span class="n">delta_k</span> <span class="o">=</span> <span class="n">new_k</span> <span class="o">-</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">final_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">delta_k</span><span class="p">))</span>
            <span class="n">scales</span> <span class="o">=</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">)</span>
            <span class="n">zeros</span> <span class="o">=</span> <span class="n">qparams</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">final_q</span><span class="p">,</span> <span class="s2">&quot;scales&quot;</span><span class="p">:</span> <span class="n">scales</span><span class="p">,</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span> <span class="n">zeros</span><span class="p">}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">make_names_and_values_dict_func</span> <span class="o">=</span> <span class="n">make_names_and_values_dict_func</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_convert_for_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="n">replace_linear_8da4w</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_allowed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="c1"># TODO: this should be self.scales_precision?</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_MultiInput</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_quantized_state_dict</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocksize</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">percdamp</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groupsize</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_for_runtime</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch-labs/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>