
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.float8.config &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=b417fedc" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=ca3c1c84" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=e5fbc548" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=f533b996" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/float8/config';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/pytorch/ao/gh-pages/torchao-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/float8/config.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>


<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main (0.17.0+git83b0e7c )');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->


<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">

<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>

  
  <div class="navbar-header-items__mobile-logo">
    







  
  
  
  


<a class="navbar-brand logo" href="../../../index.html">
  
    
    <img src="../../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
  </div>

  
  
  <div class=" navbar-header-items__start">
    
      
      
        <div class="navbar-item">
          







  
  
  
  


<a class="navbar-brand logo" href="../../../index.html">
  
    
    <img src="../../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
        </div>
      
    
      
      
        
        <div class="navbar-item desktop-only-version">
          
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
        </div>
      
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
      
        <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        




  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    <div class="sidebar-header-items__start">
      <div class="navbar-item">
        
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
      </div>
    </div>
    

    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
        
          <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
        
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



<div id="rtd-footer-container"></div>
      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.floa...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              

<div id="searchbox"></div>
<div id="pytorch-article">
  <!-- Hidden breadcrumb schema for SEO only -->
  <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <link itemprop="item" href="../../index.html">
      <meta itemprop="name" content="Module code">
      <meta itemprop="position" content="1">
    </div>
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <meta itemprop="name" content="torchao.float8.config">
      <meta itemprop="position" content="2">
    </div>
  </div>

  
  

  
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.float8.config</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">enum</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_MI300</span>

<span class="n">logger</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>


<div class="viewcode-block" id="ScalingType">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.ScalingType.html#torchao.float8.ScalingType">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ScalingType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines the type of scaling to use for casting to float8.</span>

<span class="sd">    Values:</span>

<span class="sd">    * ``DYNAMIC``: Compute scaling factor dynamically based on the tensor&#39;s values.</span>
<span class="sd">    * ``DISABLED``: Skip scaling for this tensor, leave it in its original precision.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">DYNAMIC</span> <span class="o">=</span> <span class="s2">&quot;dynamic&quot;</span>
    <span class="c1"># ScalingType.DISABLED means &quot;skip scaling for this tensor, leave it in</span>
    <span class="c1"># its original precision.</span>
    <span class="n">DISABLED</span> <span class="o">=</span> <span class="s2">&quot;disabled&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">short_str</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;dyn&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span>
            <span class="k">return</span> <span class="s2">&quot;dis&quot;</span></div>



<div class="viewcode-block" id="ScalingGranularity">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.ScalingGranularity.html#torchao.float8.ScalingGranularity">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ScalingGranularity</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines the granularity of scaling strategies for casting to float8.</span>

<span class="sd">    Values:</span>

<span class="sd">    * ``TENSORWISE``: A single scaling factor for the entire tensor.</span>
<span class="sd">    * ``AXISWISE``: Scaling factors computed along one axis of the tensor (rowwise scaling).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># A single scaling factor for the entire tensor</span>
    <span class="n">TENSORWISE</span> <span class="o">=</span> <span class="s2">&quot;tensorwise&quot;</span>
    <span class="c1"># Scaling factors computed along one axis of the tensor, reducing it to</span>
    <span class="c1"># size 1.</span>
    <span class="n">AXISWISE</span> <span class="o">=</span> <span class="s2">&quot;axiswise&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">short_str</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;ten&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span>
            <span class="k">return</span> <span class="s2">&quot;axs&quot;</span></div>



<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8TypeConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for selecting the preferred float8 type pair, either e4m3fn/e5m2 or e4m3fnuz/e5m2fnuz.</span>

<span class="sd">    Currently, ROCm supports 1. fnuz variants in MI300. 2. OCP F8 variants in MI350/Navi4.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># The preferred e4m3 type.</span>
    <span class="n">e4m3_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>

    <span class="c1"># The preferred e5m2 type.</span>
    <span class="n">e5m2_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">is_MI300</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">e4m3_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">e5m2_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span>


<span class="c1"># User defined type for using the individual F8 type based on config</span>
<span class="n">type_config</span> <span class="o">=</span> <span class="n">Float8TypeConfig</span><span class="p">()</span>
<span class="n">e4m3_dtype</span> <span class="o">=</span> <span class="n">type_config</span><span class="o">.</span><span class="n">e4m3_dtype</span>
<span class="n">e5m2_dtype</span> <span class="o">=</span> <span class="n">type_config</span><span class="o">.</span><span class="n">e5m2_dtype</span>


<div class="viewcode-block" id="CastConfig">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.CastConfig.html#torchao.float8.CastConfig">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CastConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for casting a single tensor to float8.</span>

<span class="sd">    Args:</span>
<span class="sd">        scaling_type: The type of scaling to use. See :class:`ScalingType`.</span>
<span class="sd">            Default: ``ScalingType.DYNAMIC``</span>
<span class="sd">        scaling_granularity: The granularity of scaling. See :class:`ScalingGranularity`.</span>
<span class="sd">            Default: ``ScalingGranularity.TENSORWISE``</span>
<span class="sd">        target_dtype: The target float8 dtype (e.g., ``torch.float8_e4m3fn``).</span>
<span class="sd">            Default: ``None`` (will be set based on the recipe)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scaling_type</span><span class="p">:</span> <span class="n">ScalingType</span> <span class="o">=</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DYNAMIC</span>
    <span class="n">scaling_granularity</span><span class="p">:</span> <span class="n">ScalingGranularity</span> <span class="o">=</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">short_str</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="p">{</span><span class="n">e4m3_dtype</span><span class="p">:</span> <span class="s2">&quot;e4m3&quot;</span><span class="p">,</span> <span class="n">e5m2_dtype</span><span class="p">:</span> <span class="s2">&quot;e5m2&quot;</span><span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">]</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_type</span><span class="o">.</span><span class="n">short_str</span><span class="p">()</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_granularity</span><span class="o">.</span><span class="n">short_str</span><span class="p">()</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_granularity</span> <span class="ow">is</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_type</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;only dynamic scaling type is supported for axiswise scaling granularity&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span><span class="o">.</span><span class="n">itemsize</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">),</span> <span class="s2">&quot;must specify a 8-bit floating-point dtype&quot;</span></div>



<div class="viewcode-block" id="Float8GemmConfig">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.Float8GemmConfig.html#torchao.float8.Float8GemmConfig">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8GemmConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for a float8 gemm.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_fast_accum: If True, use fast accumulation in lower precision.</span>
<span class="sd">            This can improve performance but may reduce numerical accuracy.</span>
<span class="sd">            Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># If True, fast accumulation in lower precision is used.</span>
    <span class="c1"># Note: this flag is currently a no-op if emulation is turned on.</span>
    <span class="n">use_fast_accum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></div>



<span class="c1"># Pre-made recipes for common configurations</span>
<div class="viewcode-block" id="Float8LinearRecipeName">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.Float8LinearRecipeName.html#torchao.float8.Float8LinearRecipeName">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8LinearRecipeName</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pre-made recipes for common float8 training configurations.</span>

<span class="sd">    Values:</span>

<span class="sd">    * ``TENSORWISE``: Default, dynamic per-tensor scaling with the cuBLAS tensorwise kernel.</span>
<span class="sd">      Fastest option.</span>
<span class="sd">    * ``ROWWISE``: Dynamic rowwise scaling with the CUTLASS rowwise kernel.</span>
<span class="sd">      Uses e4m3 for activations, weights, gradients. Scales are rounded (floor) to</span>
<span class="sd">      the nearest power of two for increased accuracy.</span>
<span class="sd">    * ``ROWWISE_WITH_GW_HP``: A modification on rowwise scaling with increased accuracy</span>
<span class="sd">      for grad_weight by keeping grad_weight computation in high precision. Most accurate option.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Default, dynamic per-tensor scaling with the cuBLAS tensorwise kernel</span>
    <span class="n">TENSORWISE</span> <span class="o">=</span> <span class="s2">&quot;tensorwise&quot;</span>

    <span class="c1"># dynamic rowwise scaling with the CUTLASS rowwise kernel</span>
    <span class="c1"># * e4m3 for activations, weights, gradients</span>
    <span class="c1"># * scales rounded (floor) to the nearest power of two for increased accuracy</span>
    <span class="n">ROWWISE</span> <span class="o">=</span> <span class="s2">&quot;rowwise&quot;</span>

    <span class="c1"># lw&#39;s recipe for a modification on rowwise scaling:</span>
    <span class="c1">#</span>
    <span class="c1">#   output_hp = input_fp8_rowwise_dim0 @ weight_t_rowwise_dim1</span>
    <span class="c1">#   grad_input_hp = grad_output_fp8_rowwise_dim0 @ weight_fp8_tensorwise</span>
    <span class="c1">#   grad_weight_hp = input_t_hp @ grad_output_hp</span>
    <span class="c1">#</span>
    <span class="c1"># key characteristics:</span>
    <span class="c1">#   * increased accuracy for grad_weight</span>
    <span class="c1">#   * `input`, `weight` and `grad_output` now only need to be scaled</span>
    <span class="c1">#     rowwise across a single dim compared to vanilla rowwise,</span>
    <span class="c1">#     which is more amenable to fast kernels</span>
    <span class="c1">#   * the e4m3 dtype is used across the board, including for gradients</span>
    <span class="n">ROWWISE_WITH_GW_HP</span> <span class="o">=</span> <span class="s2">&quot;rowwise_with_gw_hp&quot;</span></div>



<div class="viewcode-block" id="Float8LinearConfig">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.Float8LinearConfig.html#torchao.float8.Float8LinearConfig">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8LinearConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for converting a `torch.nn.Linear` module to float8</span>
<span class="sd">    for training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">#</span>
    <span class="c1"># Per-tensor configuration for casting of `input`, `weight`, `grad_output`</span>
    <span class="c1"># for the operands of gemms calculating `output`, `grad_weight`, and `grad_input`.</span>
    <span class="c1">#</span>
    <span class="c1"># Note:</span>
    <span class="c1"># 1. if `cast_config_input_for_grad_weight` is None, then</span>
    <span class="c1">#    `cast_config_input` is used for scaling `input` for both gemms that</span>
    <span class="c1">#    use `input.</span>
    <span class="c1"># 2. if `cast_config_input_for_grad_weight` is specified, then</span>
    <span class="c1">#    a. `cast_config_input` is used for scaling `input` for the gemm that calculates</span>
    <span class="c1">#       `output`</span>
    <span class="c1">#    b. `cast_config_input_for_grad_weight` is used for scaling `input` for</span>
    <span class="c1">#       the gemm that calculates `grad_weight`</span>
    <span class="c1"># 3. the same behavior holds for `cast_config_weight` and `cast_config_grad_output`.</span>
    <span class="c1">#</span>
    <span class="c1"># `input`</span>
    <span class="n">cast_config_input</span><span class="p">:</span> <span class="n">CastConfig</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">()</span>
    <span class="n">cast_config_input_for_grad_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CastConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># `weight`</span>
    <span class="n">cast_config_weight</span><span class="p">:</span> <span class="n">CastConfig</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">()</span>
    <span class="n">cast_config_weight_for_grad_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CastConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># `grad_output`</span>
    <span class="n">cast_config_grad_output</span><span class="p">:</span> <span class="n">CastConfig</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">()</span>
    <span class="n">cast_config_grad_output_for_grad_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CastConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1">#</span>
    <span class="c1"># Per-gemm configuration for gemms calculating `output`, `grad_input` and</span>
    <span class="c1"># `grad_weight`</span>
    <span class="c1">#</span>
    <span class="n">gemm_config_output</span><span class="p">:</span> <span class="n">Float8GemmConfig</span> <span class="o">=</span> <span class="n">Float8GemmConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">gemm_config_grad_input</span><span class="p">:</span> <span class="n">Float8GemmConfig</span> <span class="o">=</span> <span class="n">Float8GemmConfig</span><span class="p">()</span>
    <span class="n">gemm_config_grad_weight</span><span class="p">:</span> <span class="n">Float8GemmConfig</span> <span class="o">=</span> <span class="n">Float8GemmConfig</span><span class="p">()</span>

    <span class="c1">#</span>
    <span class="c1"># Per-linear configuration</span>
    <span class="c1">#</span>

    <span class="c1"># If True, then uses a tensor subclass for the float8 linear module&#39;s weight that</span>
    <span class="c1"># implements pre/post-all-gather methods to do float8 all-gather with FSDP2.</span>
    <span class="n">enable_fsdp_float8_all_gather</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If True, then prior to performing the fp8 scaled mamtmul we will pad the</span>
    <span class="c1"># inner dimension of a (dim 1) and b (dim 2) with 0s. This is needed for matmuls</span>
    <span class="c1"># _scaled_mm since it has the strong constraint that for M,N,K  N, K must be a multiple of 16.</span>
    <span class="c1"># This can cause a memory spike however so we keep this off by default.</span>
    <span class="n">pad_inner_dim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If True, emulation is used instead of hardware accelerated gemm</span>
    <span class="n">emulate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># This flag is deprecated and currently has no effect. It will be removed</span>
    <span class="c1"># in a future release. Please see https://github.com/pytorch/ao/issues/2251</span>
    <span class="c1"># for more context.</span>
    <span class="n">force_recompute_fp8_weight_in_bwd</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If this option is enabled, the scaling factor used for float8 quantization</span>
    <span class="c1"># will be rounded down to the nearest power of 2. This has been shown to help</span>
    <span class="c1"># reduce quantization error by avoiding rounding errors when multiplying/dividing</span>
    <span class="c1"># by the scaling factor, as well as ensuring large values are quantized to the</span>
    <span class="c1"># same value in the forward pass as the backward passes.</span>
    <span class="n">round_scales_to_power_of_2</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Populate the additional cast overrides, if the user did not specify them</span>
        <span class="c1"># Note: this hacks around the frozen-ness of this dataclass</span>
        <span class="c1"># by using `object.__setattr__`.  This is fine, as what we really need</span>
        <span class="c1"># is for this object to be frozen after `__post_init__` for torch.compile</span>
        <span class="c1"># to work.</span>
        <span class="c1"># Source of hack: https://stackoverflow.com/a/65959419/</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input_for_grad_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;cast_config_input_for_grad_weight&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight_for_grad_input</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;cast_config_weight_for_grad_input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output_for_grad_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="s2">&quot;cast_config_grad_output_for_grad_weight&quot;</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># float8 all-gather only supports tensorwise, in the future may support blockwise</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span><span class="o">.</span><span class="n">scaling_granularity</span> <span class="o">!=</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_fsdp_float8_all_gather</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;enable_fsdp_float8_all_gather only supports tensorwise scaling granularity, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span><span class="o">.</span><span class="n">scaling_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># save some characters in the compatibility checks below</span>
        <span class="n">cc_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input</span>
        <span class="n">cc_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span>
        <span class="n">cc_go</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output</span>
        <span class="n">cc_i_gw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input_for_grad_weight</span>
        <span class="n">cc_w_gi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight_for_grad_input</span>
        <span class="n">cc_go_gw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output_for_grad_weight</span>
        <span class="c1"># for now, we only have gemm kernels where both operands are either both</span>
        <span class="c1"># in high precision, or both in float8. In the future, this may be relaxed.</span>
        <span class="c1"># TODO(future): make the float8 check more precise with the specific dtypes.</span>
        <span class="k">for</span> <span class="n">cc1</span><span class="p">,</span> <span class="n">cc2</span><span class="p">,</span> <span class="n">gemm_name</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">cc_i</span><span class="p">,</span> <span class="n">cc_w</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_go</span><span class="p">,</span> <span class="n">cc_w_gi</span><span class="p">,</span> <span class="s2">&quot;grad_input&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_i_gw</span><span class="p">,</span> <span class="n">cc_go_gw</span><span class="p">,</span> <span class="s2">&quot;grad_weight&quot;</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="n">is_disabled_1</span> <span class="o">=</span> <span class="n">cc1</span><span class="o">.</span><span class="n">scaling_type</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span>
            <span class="n">is_disabled_2</span> <span class="o">=</span> <span class="n">cc1</span><span class="o">.</span><span class="n">scaling_type</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span>
            <span class="k">assert</span> <span class="n">is_disabled_1</span> <span class="o">==</span> <span class="n">is_disabled_2</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;incompatible operand precision for </span><span class="si">{</span><span class="n">gemm_name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">cc1</span><span class="p">,</span> <span class="n">cc2</span><span class="p">,</span> <span class="n">operand_name</span><span class="p">,</span> <span class="n">default_dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">cc_i</span><span class="p">,</span> <span class="n">cc_i_gw</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">e4m3_dtype</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_w</span><span class="p">,</span> <span class="n">cc_w_gi</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">e4m3_dtype</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_go</span><span class="p">,</span> <span class="n">cc_go_gw</span><span class="p">,</span> <span class="s2">&quot;grad_output&quot;</span><span class="p">,</span> <span class="n">e5m2_dtype</span><span class="p">),</span>
        <span class="p">]:</span>
            <span class="c1"># Override the dataclass being frozen</span>
            <span class="k">if</span> <span class="n">cc1</span><span class="o">.</span><span class="n">target_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">cc1</span><span class="p">,</span> <span class="s2">&quot;target_dtype&quot;</span><span class="p">,</span> <span class="n">default_dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cc2</span><span class="o">.</span><span class="n">target_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">cc2</span><span class="p">,</span> <span class="s2">&quot;target_dtype&quot;</span><span class="p">,</span> <span class="n">default_dtype</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">cc1</span><span class="o">.</span><span class="n">target_dtype</span> <span class="o">==</span> <span class="n">cc2</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">operand_name</span><span class="si">}</span><span class="s2"> must be cast to the same dtype in both matmuls it&#39;s used in&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_recompute_fp8_weight_in_bwd</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;`config.force_recompute_fp8_weight_in_bwd` is deprecated and will be removed in a future release. Please see https://github.com/pytorch/ao/issues/2251 for more details.&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="Float8LinearConfig.from_recipe_name">
<a class="viewcode-back" href="../../../api_reference/generated/torchao.float8.Float8LinearConfig.html#torchao.float8.Float8LinearConfig.from_recipe_name">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_recipe_name</span><span class="p">(</span>
        <span class="n">recipe_name</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Float8LinearRecipeName</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Float8LinearConfig&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Input: `Float8LinearRecipeName` value, or a string representing a `Float8LinearRecipeName` value</span>
<span class="sd">        Output: a `Float8LinearConfig` configured to implement the specified recipe</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">recipe_name</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="n">valid_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">Float8LinearRecipeName</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">recipe_name</span> <span class="ow">in</span> <span class="n">valid_names</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;recipe_name </span><span class="si">{</span><span class="n">recipe_name</span><span class="si">}</span><span class="s2"> not in valid names </span><span class="si">{</span><span class="n">valid_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">recipe_name</span> <span class="o">=</span> <span class="n">Float8LinearRecipeName</span><span class="p">(</span><span class="n">recipe_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">recipe_name</span> <span class="ow">is</span> <span class="n">Float8LinearRecipeName</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Float8LinearConfig</span><span class="p">()</span>

        <span class="k">elif</span> <span class="n">recipe_name</span> <span class="ow">is</span> <span class="n">Float8LinearRecipeName</span><span class="o">.</span><span class="n">ROWWISE</span><span class="p">:</span>
            <span class="n">cc_i</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>
            <span class="n">cc_w</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>
            <span class="n">cc_go</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">Float8LinearConfig</span><span class="p">(</span>
                <span class="n">cast_config_input</span><span class="o">=</span><span class="n">cc_i</span><span class="p">,</span>
                <span class="n">cast_config_weight</span><span class="o">=</span><span class="n">cc_w</span><span class="p">,</span>
                <span class="n">cast_config_grad_output</span><span class="o">=</span><span class="n">cc_go</span><span class="p">,</span>
                <span class="c1"># enable power of 2 scaling factors by default for row-wise scaling</span>
                <span class="n">round_scales_to_power_of_2</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">recipe_name</span> <span class="ow">is</span> <span class="n">Float8LinearRecipeName</span><span class="o">.</span><span class="n">ROWWISE_WITH_GW_HP</span><span class="p">:</span>
            <span class="c1"># output_hp = input_fp8_axiswise_dim0 @ weight_t_axiswise_dim1</span>
            <span class="n">cc_i</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">)</span>
            <span class="n">cc_w</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">)</span>

            <span class="c1"># grad_input_hp = grad_output_fp8_axiswise_dim0 @ weight_fp8_tensorwise</span>
            <span class="n">cc_go</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>
            <span class="n">cc_w_gi</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">)</span>

            <span class="c1"># grad_weight_hp = input_t_hp @ grad_output_hp</span>
            <span class="n">cc_i_gw</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_type</span><span class="o">=</span><span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">)</span>
            <span class="n">cc_go_gw</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_type</span><span class="o">=</span><span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">Float8LinearConfig</span><span class="p">(</span>
                <span class="n">cast_config_input</span><span class="o">=</span><span class="n">cc_i</span><span class="p">,</span>
                <span class="n">cast_config_weight</span><span class="o">=</span><span class="n">cc_w</span><span class="p">,</span>
                <span class="n">cast_config_grad_output</span><span class="o">=</span><span class="n">cc_go</span><span class="p">,</span>
                <span class="n">cast_config_input_for_grad_weight</span><span class="o">=</span><span class="n">cc_i_gw</span><span class="p">,</span>
                <span class="n">cast_config_weight_for_grad_input</span><span class="o">=</span><span class="n">cc_w_gi</span><span class="p">,</span>
                <span class="n">cast_config_grad_output_for_grad_weight</span><span class="o">=</span><span class="n">cc_go_gw</span><span class="p">,</span>
                <span class="n">round_scales_to_power_of_2</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;unknown recipe_name </span><span class="si">{</span><span class="n">recipe_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>
</div>

</pre></div>

                </article>
              
</div>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.float8.config",
       "headline": "torchao.float8.config",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/_modules/torchao/float8/config.html",
       "articleBody": "Source code for torchao.float8.config # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD 3-Clause license found in the # LICENSE file in the root directory of this source tree. import enum import logging from dataclasses import dataclass from typing import Optional, Union import torch from torchao.utils import is_MI300 logger: logging.Logger = logging.getLogger() [docs] class ScalingType(enum.Enum): \"\"\" Defines the type of scaling to use for casting to float8. Values: * ``DYNAMIC``: Compute scaling factor dynamically based on the tensor\u0027s values. * ``DISABLED``: Skip scaling for this tensor, leave it in its original precision. \"\"\" DYNAMIC = \"dynamic\" # ScalingType.DISABLED means \"skip scaling for this tensor, leave it in # its original precision. DISABLED = \"disabled\" def short_str(self): if self is ScalingType.DYNAMIC: return \"dyn\" else: assert self is ScalingType.DISABLED return \"dis\" [docs] class ScalingGranularity(enum.Enum): \"\"\" Defines the granularity of scaling strategies for casting to float8. Values: * ``TENSORWISE``: A single scaling factor for the entire tensor. * ``AXISWISE``: Scaling factors computed along one axis of the tensor (rowwise scaling). \"\"\" # A single scaling factor for the entire tensor TENSORWISE = \"tensorwise\" # Scaling factors computed along one axis of the tensor, reducing it to # size 1. AXISWISE = \"axiswise\" def short_str(self): if self is ScalingGranularity.TENSORWISE: return \"ten\" else: assert self is ScalingGranularity.AXISWISE return \"axs\" @dataclass class Float8TypeConfig: \"\"\" Configuration for selecting the preferred float8 type pair, either e4m3fn/e5m2 or e4m3fnuz/e5m2fnuz. Currently, ROCm supports 1. fnuz variants in MI300. 2. OCP F8 variants in MI350/Navi4. \"\"\" # The preferred e4m3 type. e4m3_dtype = torch.float8_e4m3fn # The preferred e5m2 type. e5m2_dtype = torch.float8_e5m2 def __post_init__(self): if torch.version.hip and torch.cuda.is_available() and is_MI300(): self.e4m3_dtype = torch.float8_e4m3fnuz self.e5m2_dtype = torch.float8_e5m2fnuz # User defined type for using the individual F8 type based on config type_config = Float8TypeConfig() e4m3_dtype = type_config.e4m3_dtype e5m2_dtype = type_config.e5m2_dtype [docs] @dataclass(frozen=True) class CastConfig: \"\"\" Configuration for casting a single tensor to float8. Args: scaling_type: The type of scaling to use. See :class:`ScalingType`. Default: ``ScalingType.DYNAMIC`` scaling_granularity: The granularity of scaling. See :class:`ScalingGranularity`. Default: ``ScalingGranularity.TENSORWISE`` target_dtype: The target float8 dtype (e.g., ``torch.float8_e4m3fn``). Default: ``None`` (will be set based on the recipe) \"\"\" scaling_type: ScalingType = ScalingType.DYNAMIC scaling_granularity: ScalingGranularity = ScalingGranularity.TENSORWISE target_dtype: Optional[torch.dtype] = None def short_str(self): dtype = {e4m3_dtype: \"e4m3\", e5m2_dtype: \"e5m2\"}[self.target_dtype] return f\"{self.scaling_type.short_str()}_{self.scaling_granularity.short_str()}_{dtype}\" def __post_init__(self): if self.scaling_granularity is ScalingGranularity.AXISWISE: assert self.scaling_type is ScalingType.DYNAMIC, ( \"only dynamic scaling type is supported for axiswise scaling granularity\" ) assert self.target_dtype is None or ( self.target_dtype.is_floating_point and self.target_dtype.itemsize == 1 ), \"must specify a 8-bit floating-point dtype\" [docs] @dataclass(frozen=True) class Float8GemmConfig: \"\"\" Configuration for a float8 gemm. Args: use_fast_accum: If True, use fast accumulation in lower precision. This can improve performance but may reduce numerical accuracy. Default: ``False`` \"\"\" # If True, fast accumulation in lower precision is used. # Note: this flag is currently a no-op if emulation is turned on. use_fast_accum: bool = False # Pre-made recipes for common configurations [docs] class Float8LinearRecipeName(enum.Enum): \"\"\" Pre-made recipes for common float8 training configurations. Values: * ``TENSORWISE``: Default, dynamic per-tensor scaling with the cuBLAS tensorwise kernel. Fastest option. * ``ROWWISE``: Dynamic rowwise scaling with the CUTLASS rowwise kernel. Uses e4m3 for activations, weights, gradients. Scales are rounded (floor) to the nearest power of two for increased accuracy. * ``ROWWISE_WITH_GW_HP``: A modification on rowwise scaling with increased accuracy for grad_weight by keeping grad_weight computation in high precision. Most accurate option. \"\"\" # Default, dynamic per-tensor scaling with the cuBLAS tensorwise kernel TENSORWISE = \"tensorwise\" # dynamic rowwise scaling with the CUTLASS rowwise kernel # * e4m3 for activations, weights, gradients # * scales rounded (floor) to the nearest power of two for increased accuracy ROWWISE = \"rowwise\" # lw\u0027s recipe for a modification on rowwise scaling: # # output_hp = input_fp8_rowwise_dim0 @ weight_t_rowwise_dim1 # grad_input_hp = grad_output_fp8_rowwise_dim0 @ weight_fp8_tensorwise # grad_weight_hp = input_t_hp @ grad_output_hp # # key characteristics: # * increased accuracy for grad_weight # * `input`, `weight` and `grad_output` now only need to be scaled # rowwise across a single dim compared to vanilla rowwise, # which is more amenable to fast kernels # * the e4m3 dtype is used across the board, including for gradients ROWWISE_WITH_GW_HP = \"rowwise_with_gw_hp\" [docs] @dataclass(frozen=True) class Float8LinearConfig: \"\"\" Configuration for converting a `torch.nn.Linear` module to float8 for training. \"\"\" # # Per-tensor configuration for casting of `input`, `weight`, `grad_output` # for the operands of gemms calculating `output`, `grad_weight`, and `grad_input`. # # Note: # 1. if `cast_config_input_for_grad_weight` is None, then # `cast_config_input` is used for scaling `input` for both gemms that # use `input. # 2. if `cast_config_input_for_grad_weight` is specified, then # a. `cast_config_input` is used for scaling `input` for the gemm that calculates # `output` # b. `cast_config_input_for_grad_weight` is used for scaling `input` for # the gemm that calculates `grad_weight` # 3. the same behavior holds for `cast_config_weight` and `cast_config_grad_output`. # # `input` cast_config_input: CastConfig = CastConfig() cast_config_input_for_grad_weight: Optional[CastConfig] = None # `weight` cast_config_weight: CastConfig = CastConfig() cast_config_weight_for_grad_input: Optional[CastConfig] = None # `grad_output` cast_config_grad_output: CastConfig = CastConfig() cast_config_grad_output_for_grad_weight: Optional[CastConfig] = None # # Per-gemm configuration for gemms calculating `output`, `grad_input` and # `grad_weight` # gemm_config_output: Float8GemmConfig = Float8GemmConfig(use_fast_accum=True) gemm_config_grad_input: Float8GemmConfig = Float8GemmConfig() gemm_config_grad_weight: Float8GemmConfig = Float8GemmConfig() # # Per-linear configuration # # If True, then uses a tensor subclass for the float8 linear module\u0027s weight that # implements pre/post-all-gather methods to do float8 all-gather with FSDP2. enable_fsdp_float8_all_gather: bool = False # If True, then prior to performing the fp8 scaled mamtmul we will pad the # inner dimension of a (dim 1) and b (dim 2) with 0s. This is needed for matmuls # _scaled_mm since it has the strong constraint that for M,N,K N, K must be a multiple of 16. # This can cause a memory spike however so we keep this off by default. pad_inner_dim: bool = False # If True, emulation is used instead of hardware accelerated gemm emulate: bool = False # This flag is deprecated and currently has no effect. It will be removed # in a future release. Please see https://github.com/pytorch/ao/issues/2251 # for more context. force_recompute_fp8_weight_in_bwd: bool = False # If this option is enabled, the scaling factor used for float8 quantization # will be rounded down to the nearest power of 2. This has been shown to help # reduce quantization error by avoiding rounding errors when multiplying/dividing # by the scaling factor, as well as ensuring large values are quantized to the # same value in the forward pass as the backward passes. round_scales_to_power_of_2: bool = False def __post_init__(self): # Populate the additional cast overrides, if the user did not specify them # Note: this hacks around the frozen-ness of this dataclass # by using `object.__setattr__`. This is fine, as what we really need # is for this object to be frozen after `__post_init__` for torch.compile # to work. # Source of hack: https://stackoverflow.com/a/65959419/ if self.cast_config_input_for_grad_weight is None: object.__setattr__( self, \"cast_config_input_for_grad_weight\", self.cast_config_input ) if self.cast_config_weight_for_grad_input is None: object.__setattr__( self, \"cast_config_weight_for_grad_input\", self.cast_config_weight ) if self.cast_config_grad_output_for_grad_weight is None: object.__setattr__( self, \"cast_config_grad_output_for_grad_weight\", self.cast_config_grad_output, ) # float8 all-gather only supports tensorwise, in the future may support blockwise if self.cast_config_weight.scaling_granularity != ScalingGranularity.TENSORWISE: assert not self.enable_fsdp_float8_all_gather, ( f\"enable_fsdp_float8_all_gather only supports tensorwise scaling granularity, got {self.cast_config_weight.scaling_granularity}\" ) # save some characters in the compatibility checks below cc_i = self.cast_config_input cc_w = self.cast_config_weight cc_go = self.cast_config_grad_output cc_i_gw = self.cast_config_input_for_grad_weight cc_w_gi = self.cast_config_weight_for_grad_input cc_go_gw = self.cast_config_grad_output_for_grad_weight # for now, we only have gemm kernels where both operands are either both # in high precision, or both in float8. In the future, this may be relaxed. # TODO(future): make the float8 check more precise with the specific dtypes. for cc1, cc2, gemm_name in ( (cc_i, cc_w, \"output\"), (cc_go, cc_w_gi, \"grad_input\"), (cc_i_gw, cc_go_gw, \"grad_weight\"), ): is_disabled_1 = cc1.scaling_type is ScalingType.DISABLED is_disabled_2 = cc1.scaling_type is ScalingType.DISABLED assert is_disabled_1 == is_disabled_2, ( f\"incompatible operand precision for {gemm_name}\" ) for cc1, cc2, operand_name, default_dtype in [ (cc_i, cc_i_gw, \"input\", e4m3_dtype), (cc_w, cc_w_gi, \"weight\", e4m3_dtype), (cc_go, cc_go_gw, \"grad_output\", e5m2_dtype), ]: # Override the dataclass being frozen if cc1.target_dtype is None: object.__setattr__(cc1, \"target_dtype\", default_dtype) if cc2.target_dtype is None: object.__setattr__(cc2, \"target_dtype\", default_dtype) assert cc1.target_dtype == cc2.target_dtype, ( f\"{operand_name} must be cast to the same dtype in both matmuls it\u0027s used in\" ) if self.force_recompute_fp8_weight_in_bwd: logger.warning( \"`config.force_recompute_fp8_weight_in_bwd` is deprecated and will be removed in a future release. Please see https://github.com/pytorch/ao/issues/2251 for more details.\" ) [docs] @staticmethod def from_recipe_name( recipe_name: Union[Float8LinearRecipeName, str], ) -\u003e \"Float8LinearConfig\": \"\"\" Input: `Float8LinearRecipeName` value, or a string representing a `Float8LinearRecipeName` value Output: a `Float8LinearConfig` configured to implement the specified recipe \"\"\" if type(recipe_name) == str: valid_names = [n.value for n in Float8LinearRecipeName] assert recipe_name in valid_names, ( f\"recipe_name {recipe_name} not in valid names {valid_names}\" ) recipe_name = Float8LinearRecipeName(recipe_name) if recipe_name is Float8LinearRecipeName.TENSORWISE: return Float8LinearConfig() elif recipe_name is Float8LinearRecipeName.ROWWISE: cc_i = CastConfig( scaling_granularity=ScalingGranularity.AXISWISE, target_dtype=e4m3_dtype ) cc_w = CastConfig( scaling_granularity=ScalingGranularity.AXISWISE, target_dtype=e4m3_dtype ) cc_go = CastConfig( scaling_granularity=ScalingGranularity.AXISWISE, target_dtype=e4m3_dtype ) return Float8LinearConfig( cast_config_input=cc_i, cast_config_weight=cc_w, cast_config_grad_output=cc_go, # enable power of 2 scaling factors by default for row-wise scaling round_scales_to_power_of_2=True, ) elif recipe_name is Float8LinearRecipeName.ROWWISE_WITH_GW_HP: # output_hp = input_fp8_axiswise_dim0 @ weight_t_axiswise_dim1 cc_i = CastConfig(scaling_granularity=ScalingGranularity.AXISWISE) cc_w = CastConfig(scaling_granularity=ScalingGranularity.AXISWISE) # grad_input_hp = grad_output_fp8_axiswise_dim0 @ weight_fp8_tensorwise cc_go = CastConfig( scaling_granularity=ScalingGranularity.AXISWISE, target_dtype=e4m3_dtype ) cc_w_gi = CastConfig(scaling_granularity=ScalingGranularity.TENSORWISE) # grad_weight_hp = input_t_hp @ grad_output_hp cc_i_gw = CastConfig(scaling_type=ScalingType.DISABLED) cc_go_gw = CastConfig( scaling_type=ScalingType.DISABLED, target_dtype=e4m3_dtype ) return Float8LinearConfig( cast_config_input=cc_i, cast_config_weight=cc_w, cast_config_grad_output=cc_go, cast_config_input_for_grad_weight=cc_i_gw, cast_config_weight_for_grad_input=cc_w_gi, cast_config_grad_output_for_grad_weight=cc_go_gw, round_scales_to_power_of_2=True, ) else: raise AssertionError(f\"unknown recipe_name {recipe_name}\")",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/float8/config.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>