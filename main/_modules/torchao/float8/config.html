


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.float8.config &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributor_guide.html">Contributor Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_float8.html">torchao.float8</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pretraining.html">Pretraining with float8</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.float8.config</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.float8.config</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">enum</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_MI300</span>

<span class="n">logger</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>


<div class="viewcode-block" id="ScalingType"><a class="viewcode-back" href="../../../generated/torchao.float8.ScalingType.html#torchao.float8.ScalingType">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ScalingType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">DYNAMIC</span> <span class="o">=</span> <span class="s2">&quot;dynamic&quot;</span>
    <span class="c1"># ScalingType.DISABLED means &quot;skip scaling for this tensor, leave it in</span>
    <span class="c1"># its original precision.</span>
    <span class="n">DISABLED</span> <span class="o">=</span> <span class="s2">&quot;disabled&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">short_str</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;dyn&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span>
            <span class="k">return</span> <span class="s2">&quot;dis&quot;</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">ScalingGranularity</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines the granularity of scaling strategies for casting to float8</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># A single scaling factor for the entire tensor</span>
    <span class="n">TENSORWISE</span> <span class="o">=</span> <span class="s2">&quot;tensorwise&quot;</span>
    <span class="c1"># Scaling factors computed along one axis of the tensor, reducing it to</span>
    <span class="c1"># size 1.</span>
    <span class="n">AXISWISE</span> <span class="o">=</span> <span class="s2">&quot;axiswise&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">short_str</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;ten&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span> <span class="ow">is</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span>
            <span class="k">return</span> <span class="s2">&quot;axs&quot;</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8TypeConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for selecting the preferred float8 type pair, either e4m3fn/e5m2 or e4m3fnuz/e5m2fnuz.</span>

<span class="sd">    Currently, ROCm supports 1. fnuz variants in MI300. 2. OCP F8 variants in MI350/Navi4.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># The preferred e4m3 type.</span>
    <span class="n">e4m3_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>

    <span class="c1"># The preferred e5m2 type.</span>
    <span class="n">e5m2_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">is_MI300</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">e4m3_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">e5m2_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span>


<span class="c1"># User defined type for using the individual F8 type based on config</span>
<span class="n">type_config</span> <span class="o">=</span> <span class="n">Float8TypeConfig</span><span class="p">()</span>
<span class="n">e4m3_dtype</span> <span class="o">=</span> <span class="n">type_config</span><span class="o">.</span><span class="n">e4m3_dtype</span>
<span class="n">e5m2_dtype</span> <span class="o">=</span> <span class="n">type_config</span><span class="o">.</span><span class="n">e5m2_dtype</span>


<div class="viewcode-block" id="CastConfig"><a class="viewcode-back" href="../../../generated/torchao.float8.CastConfig.html#torchao.float8.CastConfig">[docs]</a><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CastConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for maybe casting a single tensor to float8</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scaling_type</span><span class="p">:</span> <span class="n">ScalingType</span> <span class="o">=</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DYNAMIC</span>
    <span class="n">scaling_granularity</span><span class="p">:</span> <span class="n">ScalingGranularity</span> <span class="o">=</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span>
    <span class="n">target_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">short_str</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="p">{</span><span class="n">e4m3_dtype</span><span class="p">:</span> <span class="s2">&quot;e4m3&quot;</span><span class="p">,</span> <span class="n">e5m2_dtype</span><span class="p">:</span> <span class="s2">&quot;e5m2&quot;</span><span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">]</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_type</span><span class="o">.</span><span class="n">short_str</span><span class="p">()</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scaling_granularity</span><span class="o">.</span><span class="n">short_str</span><span class="p">()</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_granularity</span> <span class="ow">is</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_type</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;only dynamic scaling type is supported for axiswise scaling granularity&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_dtype</span><span class="o">.</span><span class="n">itemsize</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">),</span> <span class="s2">&quot;must specify a 8-bit floating-point dtype&quot;</span></div>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8GemmConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for a float8 gemm.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># If True, fast accumulation in lower precision is used.</span>
    <span class="c1"># Note: this flag is currently a no-op if emulation is turned on.</span>
    <span class="n">use_fast_accum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="c1"># Pre-made recipes for common configurations</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8LinearRecipeName</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="c1"># Default, dynamic per-tensor scaling with the cuBLAS tensorwise kernel</span>
    <span class="n">TENSORWISE</span> <span class="o">=</span> <span class="s2">&quot;tensorwise&quot;</span>

    <span class="c1"># dynamic rowwise scaling with the CUTLASS rowwise kernel</span>
    <span class="c1"># * e4m3 for activations, weights, gradients</span>
    <span class="c1"># * scales rounded (floor) to the nearest power of two for increased accuracy</span>
    <span class="n">ROWWISE</span> <span class="o">=</span> <span class="s2">&quot;rowwise&quot;</span>

    <span class="c1"># lw&#39;s recipe for a modification on rowwise scaling:</span>
    <span class="c1">#</span>
    <span class="c1">#   output_hp = input_fp8_rowwise_dim0 @ weight_t_rowwise_dim1</span>
    <span class="c1">#   grad_input_hp = grad_output_fp8_rowwise_dim0 @ weight_fp8_tensorwise</span>
    <span class="c1">#   grad_weight_hp = input_t_hp @ grad_output_hp</span>
    <span class="c1">#</span>
    <span class="c1"># key characteristics:</span>
    <span class="c1">#   * increased accuracy for grad_weight</span>
    <span class="c1">#   * `input`, `weight` and `grad_output` now only need to be scaled</span>
    <span class="c1">#     rowwise across a single dim compared to vanilla rowwise,</span>
    <span class="c1">#     which is more amenable to fast kernels</span>
    <span class="c1">#   * the e4m3 dtype is used across the board, including for gradients</span>
    <span class="n">ROWWISE_WITH_GW_HP</span> <span class="o">=</span> <span class="s2">&quot;rowwise_with_gw_hp&quot;</span>


<div class="viewcode-block" id="Float8LinearConfig"><a class="viewcode-back" href="../../../generated/torchao.float8.Float8LinearConfig.html#torchao.float8.Float8LinearConfig">[docs]</a><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Float8LinearConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for converting a `torch.nn.Linear` module to float8</span>
<span class="sd">    for training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">#</span>
    <span class="c1"># Per-tensor configuration for casting of `input`, `weight`, `grad_output`</span>
    <span class="c1"># for the operands of gemms calculating `output`, `grad_weight`, and `grad_input`.</span>
    <span class="c1">#</span>
    <span class="c1"># Note:</span>
    <span class="c1"># 1. if `cast_config_input_for_grad_weight` is None, then</span>
    <span class="c1">#    `cast_config_input` is used for scaling `input` for both gemms that</span>
    <span class="c1">#    use `input.</span>
    <span class="c1"># 2. if `cast_config_input_for_grad_weight` is specified, then</span>
    <span class="c1">#    a. `cast_config_input` is used for scaling `input` for the gemm that calculates</span>
    <span class="c1">#       `output`</span>
    <span class="c1">#    b. `cast_config_input_for_grad_weight` is used for scaling `input` for</span>
    <span class="c1">#       the gemm that calculates `grad_weight`</span>
    <span class="c1"># 3. the same behavior holds for `cast_config_weight` and `cast_config_grad_output`.</span>
    <span class="c1">#</span>
    <span class="c1"># `input`</span>
    <span class="n">cast_config_input</span><span class="p">:</span> <span class="n">CastConfig</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">()</span>
    <span class="n">cast_config_input_for_grad_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CastConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># `weight`</span>
    <span class="n">cast_config_weight</span><span class="p">:</span> <span class="n">CastConfig</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">()</span>
    <span class="n">cast_config_weight_for_grad_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CastConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># `grad_output`</span>
    <span class="n">cast_config_grad_output</span><span class="p">:</span> <span class="n">CastConfig</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">()</span>
    <span class="n">cast_config_grad_output_for_grad_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CastConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1">#</span>
    <span class="c1"># Per-gemm configuration for gemms calculating `output`, `grad_input` and</span>
    <span class="c1"># `grad_weight`</span>
    <span class="c1">#</span>
    <span class="n">gemm_config_output</span><span class="p">:</span> <span class="n">Float8GemmConfig</span> <span class="o">=</span> <span class="n">Float8GemmConfig</span><span class="p">(</span><span class="n">use_fast_accum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">gemm_config_grad_input</span><span class="p">:</span> <span class="n">Float8GemmConfig</span> <span class="o">=</span> <span class="n">Float8GemmConfig</span><span class="p">()</span>
    <span class="n">gemm_config_grad_weight</span><span class="p">:</span> <span class="n">Float8GemmConfig</span> <span class="o">=</span> <span class="n">Float8GemmConfig</span><span class="p">()</span>

    <span class="c1">#</span>
    <span class="c1"># Per-linear configuration</span>
    <span class="c1">#</span>

    <span class="c1"># If True, then uses a tensor subclass for the float8 linear module&#39;s weight that</span>
    <span class="c1"># implements pre/post-all-gather methods to do float8 all-gather with FSDP2.</span>
    <span class="n">enable_fsdp_float8_all_gather</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If True, then prior to performing the fp8 scaled mamtmul we will pad the</span>
    <span class="c1"># inner dimension of a (dim 1) and b (dim 2) with 0s. This is needed for matmuls</span>
    <span class="c1"># _scaled_mm since it has the strong constraint that for M,N,K  N, K must be a multiple of 16.</span>
    <span class="c1"># This can cause a memory spike however so we keep this off by default.</span>
    <span class="n">pad_inner_dim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If True, emulation is used instead of hardware accelerated gemm</span>
    <span class="n">emulate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If the option is enabled, fp8_weight will always be re-computed in backward.</span>
    <span class="c1"># It&#39;s recommended to enable this flag when using FSDP.</span>
    <span class="c1"># Otherwise, the entire fp8_weight, instead of the sharded weight may be saved.</span>
    <span class="c1"># If using outer activation checkpointing context or SAC, you may disable this option</span>
    <span class="c1"># and handle the recomputation of fp8 weight in your customized AC context.</span>
    <span class="c1">#</span>
    <span class="c1"># Details:</span>
    <span class="c1"># When using float8 training with FSDP, the original weight is sharded; fp8_weight (in forward) and fp8_weight_transpose (in backward) are used by the model.</span>
    <span class="c1"># However, when partitioning the forward_backward graph, torch.compile may decide to</span>
    <span class="c1"># save the fp8_weight_transpose for backward, which is an un-sahrded weight and costs a high memory utilization.</span>
    <span class="c1"># The longer-term solution is to let compile decide how to partition the graph with optimal computation and memory savings.</span>
    <span class="c1"># For now, we use the checkpointing api to force the recomputation of fp8 weight in backward.</span>
    <span class="c1"># TODO(future PR): either enable by default or have a warning and set up the</span>
    <span class="c1"># tests so that the warning does not spam the CI stdout.</span>
    <span class="n">force_recompute_fp8_weight_in_bwd</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># If this option is enabled, the scaling factor used for float8 quantization</span>
    <span class="c1"># will be rounded down to the nearest power of 2. This has been shown to help</span>
    <span class="c1"># reduce quantization error by avoiding rounding errors when multiplying/dividing</span>
    <span class="c1"># by the scaling factor, as well as ensuring large values are quantized to the</span>
    <span class="c1"># same value in the forward pass as the backward passes.</span>
    <span class="n">round_scales_to_power_of_2</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Populate the additional cast overrides, if the user did not specify them</span>
        <span class="c1"># Note: this hacks around the frozen-ness of this dataclass</span>
        <span class="c1"># by using `object.__setattr__`.  This is fine, as what we really need</span>
        <span class="c1"># is for this object to be frozen after `__post_init__` for torch.compile</span>
        <span class="c1"># to work.</span>
        <span class="c1"># Source of hack: https://stackoverflow.com/a/65959419/</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input_for_grad_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;cast_config_input_for_grad_weight&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight_for_grad_input</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;cast_config_weight_for_grad_input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output_for_grad_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="s2">&quot;cast_config_grad_output_for_grad_weight&quot;</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># float8 all-gather only supports tensorwise, in the future may support blockwise</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span><span class="o">.</span><span class="n">scaling_granularity</span> <span class="o">!=</span> <span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_fsdp_float8_all_gather</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;enable_fsdp_float8_all_gather only supports tensorwise scaling granularity, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span><span class="o">.</span><span class="n">scaling_granularity</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># save some characters in the compatibility checks below</span>
        <span class="n">cc_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input</span>
        <span class="n">cc_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight</span>
        <span class="n">cc_go</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output</span>
        <span class="n">cc_i_gw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_input_for_grad_weight</span>
        <span class="n">cc_w_gi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_weight_for_grad_input</span>
        <span class="n">cc_go_gw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast_config_grad_output_for_grad_weight</span>
        <span class="c1"># for now, we only have gemm kernels where both operands are either both</span>
        <span class="c1"># in high precision, or both in float8. In the future, this may be relaxed.</span>
        <span class="c1"># TODO(future): make the float8 check more precise with the specific dtypes.</span>
        <span class="k">for</span> <span class="n">cc1</span><span class="p">,</span> <span class="n">cc2</span><span class="p">,</span> <span class="n">gemm_name</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">cc_i</span><span class="p">,</span> <span class="n">cc_w</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_go</span><span class="p">,</span> <span class="n">cc_w_gi</span><span class="p">,</span> <span class="s2">&quot;grad_input&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_i_gw</span><span class="p">,</span> <span class="n">cc_go_gw</span><span class="p">,</span> <span class="s2">&quot;grad_weight&quot;</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="n">is_disabled_1</span> <span class="o">=</span> <span class="n">cc1</span><span class="o">.</span><span class="n">scaling_type</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span>
            <span class="n">is_disabled_2</span> <span class="o">=</span> <span class="n">cc1</span><span class="o">.</span><span class="n">scaling_type</span> <span class="ow">is</span> <span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span>
            <span class="k">assert</span> <span class="n">is_disabled_1</span> <span class="o">==</span> <span class="n">is_disabled_2</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;incompatible operand precision for </span><span class="si">{</span><span class="n">gemm_name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">cc1</span><span class="p">,</span> <span class="n">cc2</span><span class="p">,</span> <span class="n">operand_name</span><span class="p">,</span> <span class="n">default_dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">cc_i</span><span class="p">,</span> <span class="n">cc_i_gw</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">e4m3_dtype</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_w</span><span class="p">,</span> <span class="n">cc_w_gi</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">e4m3_dtype</span><span class="p">),</span>
            <span class="p">(</span><span class="n">cc_go</span><span class="p">,</span> <span class="n">cc_go_gw</span><span class="p">,</span> <span class="s2">&quot;grad_output&quot;</span><span class="p">,</span> <span class="n">e5m2_dtype</span><span class="p">),</span>
        <span class="p">]:</span>
            <span class="c1"># Override the dataclass being frozen</span>
            <span class="k">if</span> <span class="n">cc1</span><span class="o">.</span><span class="n">target_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">cc1</span><span class="p">,</span> <span class="s2">&quot;target_dtype&quot;</span><span class="p">,</span> <span class="n">default_dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cc2</span><span class="o">.</span><span class="n">target_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">cc2</span><span class="p">,</span> <span class="s2">&quot;target_dtype&quot;</span><span class="p">,</span> <span class="n">default_dtype</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">cc1</span><span class="o">.</span><span class="n">target_dtype</span> <span class="o">==</span> <span class="n">cc2</span><span class="o">.</span><span class="n">target_dtype</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">operand_name</span><span class="si">}</span><span class="s2"> must be cast to the same dtype in both matmuls it&#39;s used in&quot;</span>
            <span class="p">)</span>

        <span class="c1"># See the comments around `force_recompute_fp8_weight_in_bwd` for more details of this warning.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_fsdp_float8_all_gather</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">force_recompute_fp8_weight_in_bwd</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;When using FSDP, it&#39;s recommended to enable config.force_recompute_fp8_weight_in_bwd.&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="Float8LinearConfig.from_recipe_name"><a class="viewcode-back" href="../../../generated/torchao.float8.Float8LinearConfig.html#torchao.float8.Float8LinearConfig.from_recipe_name">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_recipe_name</span><span class="p">(</span>
        <span class="n">recipe_name</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Float8LinearRecipeName</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Float8LinearConfig&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Input: `Float8LinearRecipeName` value, or a string representing a `Float8LinearRecipeName` value</span>
<span class="sd">        Output: a `Float8LinearConfig` configured to implement the specified recipe</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">recipe_name</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="n">valid_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">Float8LinearRecipeName</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">recipe_name</span> <span class="ow">in</span> <span class="n">valid_names</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;recipe_name </span><span class="si">{</span><span class="n">recipe_name</span><span class="si">}</span><span class="s2"> not in valid names </span><span class="si">{</span><span class="n">valid_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">recipe_name</span> <span class="o">=</span> <span class="n">Float8LinearRecipeName</span><span class="p">(</span><span class="n">recipe_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">recipe_name</span> <span class="ow">is</span> <span class="n">Float8LinearRecipeName</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Float8LinearConfig</span><span class="p">()</span>

        <span class="k">elif</span> <span class="n">recipe_name</span> <span class="ow">is</span> <span class="n">Float8LinearRecipeName</span><span class="o">.</span><span class="n">ROWWISE</span><span class="p">:</span>
            <span class="n">cc_i</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>
            <span class="n">cc_w</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>
            <span class="n">cc_go</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">Float8LinearConfig</span><span class="p">(</span>
                <span class="n">cast_config_input</span><span class="o">=</span><span class="n">cc_i</span><span class="p">,</span>
                <span class="n">cast_config_weight</span><span class="o">=</span><span class="n">cc_w</span><span class="p">,</span>
                <span class="n">cast_config_grad_output</span><span class="o">=</span><span class="n">cc_go</span><span class="p">,</span>
                <span class="c1"># enable power of 2 scaling factors by default for row-wise scaling</span>
                <span class="n">round_scales_to_power_of_2</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">recipe_name</span> <span class="ow">is</span> <span class="n">Float8LinearRecipeName</span><span class="o">.</span><span class="n">ROWWISE_WITH_GW_HP</span><span class="p">:</span>
            <span class="c1"># output_hp = input_fp8_axiswise_dim0 @ weight_t_axiswise_dim1</span>
            <span class="n">cc_i</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">)</span>
            <span class="n">cc_w</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">)</span>

            <span class="c1"># grad_input_hp = grad_output_fp8_axiswise_dim0 @ weight_fp8_tensorwise</span>
            <span class="n">cc_go</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">AXISWISE</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>
            <span class="n">cc_w_gi</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_granularity</span><span class="o">=</span><span class="n">ScalingGranularity</span><span class="o">.</span><span class="n">TENSORWISE</span><span class="p">)</span>

            <span class="c1"># grad_weight_hp = input_t_hp @ grad_output_hp</span>
            <span class="n">cc_i_gw</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span><span class="n">scaling_type</span><span class="o">=</span><span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">)</span>
            <span class="n">cc_go_gw</span> <span class="o">=</span> <span class="n">CastConfig</span><span class="p">(</span>
                <span class="n">scaling_type</span><span class="o">=</span><span class="n">ScalingType</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">,</span> <span class="n">target_dtype</span><span class="o">=</span><span class="n">e4m3_dtype</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">Float8LinearConfig</span><span class="p">(</span>
                <span class="n">cast_config_input</span><span class="o">=</span><span class="n">cc_i</span><span class="p">,</span>
                <span class="n">cast_config_weight</span><span class="o">=</span><span class="n">cc_w</span><span class="p">,</span>
                <span class="n">cast_config_grad_output</span><span class="o">=</span><span class="n">cc_go</span><span class="p">,</span>
                <span class="n">cast_config_input_for_grad_weight</span><span class="o">=</span><span class="n">cc_i_gw</span><span class="p">,</span>
                <span class="n">cast_config_weight_for_grad_input</span><span class="o">=</span><span class="n">cc_w_gi</span><span class="p">,</span>
                <span class="n">cast_config_grad_output_for_grad_weight</span><span class="o">=</span><span class="n">cc_go_gw</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;unknown recipe_name </span><span class="si">{</span><span class="n">recipe_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>