


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.utils &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/ao/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quantization_overview.html">Quantization Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributor_guide.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparsity.html">Sparsity Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarking_api_guide.html">Benchmarking API Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarking_user_guide.html">Benchmarking User Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_dtypes.html">torchao.dtypes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_qat.html">torchao.quantization.qat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_float8.html">torchao.float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_utils.html">torchao.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_ref_utils.html#torchao-quantization-quantize-common">torchao.quantization.quantize_.common</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Eager Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pretraining.html">(Part 1) Pre-training with float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../finetuning.html">(Part 2) Fine-tuning with QAT, QLoRA, and float8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serving.html">(Part 3) Serving on vLLM, SGLang, ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torchao_vllm_integration.html">Integration with VLLM: Architecture and Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../static_quantization.html">Static Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../subclass_basic.html">Writing Your Own Quantized Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../subclass_advanced.html">Writing Your Own Quantized Tensor (advanced)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PT2E Quantization Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials_source/pt2e_quant_ptq.html">PyTorch 2 Export Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials_source/pt2e_quant_qat.html">PyTorch 2 Export Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials_source/pt2e_quant_x86_inductor.html">PyTorch 2 Export Quantization with X86 Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials_source/pt2e_quant_xpu_inductor.html">PyTorch 2 Export Quantization with Intel GPU Backend through Inductor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials_source/pt2e_quant_openvino_inductor.html">PyTorch 2 Export Quantization for OpenVINO torch.compile Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials_source/pt2e_quantizer.html">How to Write a <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code> for PyTorch 2 Export Quantization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
      <li>torchao.utils</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">importlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">importlib.metadata</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">gcd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.parametrize</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">parametrize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils._python_dispatch</span><span class="w"> </span><span class="kn">import</span> <span class="n">return_and_correct_aliasing</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;benchmark_model&quot;</span><span class="p">,</span>
    <span class="s2">&quot;profiler_runner&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_available_devices&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_compute_capability&quot;</span><span class="p">,</span>
    <span class="s2">&quot;benchmark_torch_function_in_microseconds&quot;</span><span class="p">,</span>
    <span class="s2">&quot;find_multiple&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_register_custom_op&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_model_size_in_bytes&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unwrap_tensor_subclass&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TorchAOBaseTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_MI300&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_sm_at_least_89&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_sm_at_least_90&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_package_at_least&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DummyModule&quot;</span><span class="p">,</span>
    <span class="c1"># Deprecated</span>
    <span class="s2">&quot;TORCH_VERSION_AT_LEAST_2_2&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AT_LEAST_2_3&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AT_LEAST_2_4&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AT_LEAST_2_5&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AT_LEAST_2_6&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AT_LEAST_2_7&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AFTER_2_2&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AFTER_2_3&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AFTER_2_4&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TORCH_VERSION_AFTER_2_5&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="c1"># Referenced from: https://github.com/pytorch/pytorch/blob/9105d54c6b37099575c0059ef274c86c4dc80c57/torch/ao/quantization/utils.py#L711</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_assert_and_get_unique_device</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the unique device for a module, or None if no device is found.</span>
<span class="sd">    Throws an error if multiple devices are detected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span> <span class="o">|</span> <span class="p">{</span>
        <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;prepare only works with cpu or single-device CUDA modules, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;but got devices </span><span class="si">{</span><span class="n">devices</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">device</span>


<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_runs</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark model runs with `args` and `kwargs` both are optional&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Expecting `model` to be torch.nn.Module if device_type is not provided&quot;</span>
        <span class="p">)</span>
        <span class="n">device_type</span> <span class="o">=</span> <span class="n">_assert_and_get_unique_device</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">type</span>

    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">start_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">start_event</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>

    <span class="k">elif</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">start_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">start_event</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>

    <span class="k">elif</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">average_time_per_run</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>
        <span class="k">return</span> <span class="n">average_time_per_run</span>


<span class="k">def</span><span class="w"> </span><span class="nf">profiler_runner</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
        <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_available_devices</span><span class="p">():</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;xpu&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">devices</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_compute_capability</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">capability</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">capability</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">capability</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">0.0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">compute_max_diff</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">output_ref</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">output_ref</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output_ref</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.benchmark</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">benchmark</span>  <span class="c1"># this avoids importing numpy when torchao module is loaded</span>

    <span class="c1"># Manual warmup</span>
    <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s2">&quot;f(*args, **kwargs)&quot;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="n">args</span><span class="p">,</span> <span class="s2">&quot;kwargs&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">f</span><span class="p">},</span>  <span class="c1"># noqa: E501</span>
    <span class="p">)</span>
    <span class="n">measurement</span> <span class="o">=</span> <span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">measurement</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mf">1e6</span>


<span class="k">def</span><span class="w"> </span><span class="nf">find_multiple</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">//</span> <span class="n">gcd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>  <span class="c1"># type: ignore[9]</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">n</span>
    <span class="k">return</span> <span class="n">n</span> <span class="o">+</span> <span class="n">k</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_custom_op</span><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">inductor_decomposed</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This decorator is used to preserve some high level operators for torch.export.export</span>
<span class="sd">    while still allow them to be decomposed for inductor path</span>

<span class="sd">    requirement: make sure `fn.__name__[1:]` is the operator name you want to register</span>

<span class="sd">    NOTE: This should be applied at the top, after all other decorators have been applied</span>
<span class="sd">    NOTE: We haven&#39;t tested the case when `fn` accepts tensor subclass instance as input,</span>
<span class="sd">    e.g. uint4 tensor subclass instance, and we&#39;ll probably need to figure out what would make</span>
<span class="sd">    sense for downstream system (like executorch) to accept as well</span>

<span class="sd">    Example:</span>
<span class="sd">        lib = torch.library.Library(&quot;my_namespace&#39;, &quot;FRAGMENT&quot;)</span>

<span class="sd">        register_custom_op = _register_custom_op(lib)</span>

<span class="sd">        @register_custom_op</span>
<span class="sd">        def _the_op_that_needs_to_be_preserved(...)</span>
<span class="sd">            ...</span>

<span class="sd">        # after this, `_the_op_that_needs_to_be_preserved` will be preserved as</span>
<span class="sd">        # torch.ops.my_namespace.the_op_that_needs_to_be_preserved operator after</span>
<span class="sd">        # torch.export.export</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch._inductor.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_decomposition</span>

    <span class="n">dispatch_key</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;CompositeImplicitAutograd&quot;</span>
        <span class="k">if</span> <span class="n">inductor_decomposed</span>
        <span class="k">else</span> <span class="s2">&quot;CompositeExplicitAutograd&quot;</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._library.infer_schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">infer_schema</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">c</span> <span class="ow">in</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s2">&quot;.&lt;&gt;&quot;</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expecting op to be defined in normal functions, not lambda or local: </span><span class="si">{</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">op_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">op_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;_&quot;</span><span class="p">:</span>
            <span class="n">op_name</span> <span class="o">=</span> <span class="n">op_name</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">op_name</span> <span class="o">+</span> <span class="n">infer_schema</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">{})</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span>

        <span class="n">lib_namespace</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">ns</span>
        <span class="n">op</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="n">lib_namespace</span><span class="p">),</span> <span class="n">op_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inductor_decomposed</span><span class="p">:</span>
            <span class="n">register_decomposition</span><span class="p">([</span><span class="n">op</span><span class="p">])(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_meta_op</span><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="s2">&quot;Meta&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_model_size_in_bytes</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ignore_embeddings</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the model size in bytes. The option to ignore embeddings</span>
<span class="sd">    is useful for models with disproportionately large embeddings compared</span>
<span class="sd">    to other model parameters that get quantized/sparsified.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">flat_size</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;__tensor_flatten__&quot;</span><span class="p">):</span>
            <span class="n">size</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># 0th element is a list of attributes that</span>
            <span class="c1"># hold tensors</span>
            <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">tensor</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">sub_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                <span class="n">size</span> <span class="o">+=</span> <span class="n">flat_size</span><span class="p">(</span><span class="n">sub_tensor</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>

    <span class="n">model_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ignore_embeddings</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="n">child</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">child</span><span class="o">.</span><span class="n">buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">model_size</span> <span class="o">+=</span> <span class="n">flat_size</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">model_size</span> <span class="o">+=</span> <span class="n">get_model_size_in_bytes</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">ignore_embeddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_size</span>


<span class="k">class</span><span class="w"> </span><span class="nc">UnwrapTensorSubclass</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
        <span class="n">todo</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tp</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">inner_tensors</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rebuild_stack</span><span class="p">):</span>
            <span class="n">nb_tensor</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">)</span>
            <span class="n">inner_tensors</span> <span class="o">=</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="n">b</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">,</span> <span class="n">todo</span><span class="p">[</span><span class="o">-</span><span class="n">nb_tensor</span><span class="p">:])}</span>
            <span class="n">todo</span> <span class="o">=</span> <span class="n">todo</span><span class="p">[</span><span class="n">nb_tensor</span><span class="p">:]</span>
            <span class="n">rebuilt</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">__tensor_unflatten__</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rebuilt</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">todo</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">todo</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">right_inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
        <span class="n">rebuild_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">plain_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">todo</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
        <span class="k">while</span> <span class="n">todo</span><span class="p">:</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="n">todo</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="n">inner_tensors</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
            <span class="n">rebuild_stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">),</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">inner_tensors</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">inner_tensors</span><span class="p">:</span>
                <span class="n">val</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
                    <span class="n">plain_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rebuild_stack</span> <span class="o">=</span> <span class="n">rebuild_stack</span>

        <span class="k">return</span> <span class="n">plain_tensors</span>


<span class="k">def</span><span class="w"> </span><span class="nf">unwrap_tensor_subclass</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Unwraps (nested) tensor subclass in the model to plain tensors</span>
<span class="sd">    This is a workaround to make a model with tensor subclass to work with `torch.export.export`</span>
<span class="sd">    and `torch.aot_compile`, we hope this can be integrated into compile stack soon</span>
<span class="sd">    tracking issue: https://github.com/pytorch/ao/issues/345</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="c1"># make sure child.weight is a tensor subclass</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">TorchAOBaseTensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">parametrize</span><span class="o">.</span><span class="n">is_parametrized</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">UnwrapTensorSubclass</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="n">unwrap_tensor_subclass</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_float8_type</span><span class="p">(</span><span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">fp8_types</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">fp8_types</span>


<span class="k">def</span><span class="w"> </span><span class="nf">parse_version</span><span class="p">(</span><span class="n">version_string</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parse version string representing pre-release with -1</span>

<span class="sd">    Examples: &quot;2.5.0.dev20240708+cu121&quot; -&gt; [2, 5, -1], &quot;2.5.0&quot; -&gt; [2, 5, 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check for pre-release indicators</span>
    <span class="n">is_prerelease</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(git|dev)&quot;</span><span class="p">,</span> <span class="n">version_string</span><span class="p">))</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(\d+)\.(\d+)\.(\d+)&quot;</span><span class="p">,</span> <span class="n">version_string</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">,</span> <span class="n">patch</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">is_prerelease</span><span class="p">:</span>
            <span class="n">patch</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">,</span> <span class="n">patch</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid version string format: </span><span class="si">{</span><span class="n">version_string</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_fbcode</span><span class="p">():</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="p">,</span> <span class="s2">&quot;git_version&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">torch_version_at_least</span><span class="p">(</span><span class="n">min_version</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_fbcode</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># Parser for local identifiers</span>
    <span class="k">return</span> <span class="n">parse_version</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">parse_version</span><span class="p">(</span><span class="n">min_version</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="n">version_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper for existing TORCH_VERSION_AT_LEAST* variables that will log</span>
<span class="sd">    a deprecation warning if the variable is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">version_str_var_name</span> <span class="o">=</span> <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">version_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">deprecation_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;TORCH_VERSION_AT_LEAST_</span><span class="si">{</span><span class="n">version_str_var_name</span><span class="si">}</span><span class="s2"> is deprecated and will be removed in torchao 0.14.0&quot;</span>
    <span class="k">return</span> <span class="n">_BoolDeprecationWrapper</span><span class="p">(</span>
        <span class="n">torch_version_at_least</span><span class="p">(</span><span class="n">version_str</span><span class="p">),</span>
        <span class="n">deprecation_msg</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_deprecated_torch_version_after</span><span class="p">(</span><span class="n">version_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper for existing TORCH_VERSION_AFTER* variables that will log</span>
<span class="sd">    a deprecation warning if the variable is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bool_value</span> <span class="o">=</span> <span class="n">is_fbcode</span><span class="p">()</span> <span class="ow">or</span> <span class="n">version</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version_str</span>
    <span class="n">version_str_var_name</span> <span class="o">=</span> <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">version_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">deprecation_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;TORCH_VERSION_AFTER_</span><span class="si">{</span><span class="n">version_str_var_name</span><span class="si">}</span><span class="s2"> is deprecated and will be removed in torchao 0.14.0&quot;</span>
    <span class="k">return</span> <span class="n">_BoolDeprecationWrapper</span><span class="p">(</span><span class="n">bool_value</span><span class="p">,</span> <span class="n">deprecation_msg</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_BoolDeprecationWrapper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A deprecation wrapper that logs a warning when the given bool value is accessed.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bool_value</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bool_value</span> <span class="o">=</span> <span class="n">bool_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msg</span> <span class="o">=</span> <span class="n">msg</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bool_value</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>


<span class="c1"># Deprecated, use `torch_version_at_least` directly instead</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_8</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.8.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_7</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.7.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_6</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.6.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_5</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.5.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_4</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.4.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_3</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.3.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AT_LEAST_2_2</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.2.0&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AFTER_2_5</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_after</span><span class="p">(</span><span class="s2">&quot;2.5.0.dev&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AFTER_2_4</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_after</span><span class="p">(</span><span class="s2">&quot;2.4.0.dev&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AFTER_2_3</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_after</span><span class="p">(</span><span class="s2">&quot;2.3.0.dev&quot;</span><span class="p">)</span>
<span class="n">TORCH_VERSION_AFTER_2_2</span> <span class="o">=</span> <span class="n">_deprecated_torch_version_after</span><span class="p">(</span><span class="s2">&quot;2.2.0.dev&quot;</span><span class="p">)</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Helper function for implementing aten op or torch function dispatch</span>
<span class="sd">and dispatching to these implementations.</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_implements</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">aten_ops_or_torch_fns</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this decorator to implement a function for an aten ops in __torch_dispatch__</span>
<span class="sd">    (if user passed in a list of ops)</span>
<span class="sd">    or torch function in __torch_function__ (if user passed in a single object)</span>

<span class="sd">    class MyTensor(torch.Tensor):</span>
<span class="sd">        ...</span>
<span class="sd">        implements = classmethod(_implements)</span>

<span class="sd">    implements = MyTensor.implements</span>

<span class="sd">    @implements(torch.nn.functional.linear):</span>
<span class="sd">    def _(func, types, args, kwargs):</span>
<span class="sd">        ...</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_OR_TORCH_FN_TABLE&quot;</span><span class="p">):</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">:</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aten_ops_or_torch_fns</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">aten_ops_or_torch_fns</span> <span class="o">=</span> <span class="p">[</span><span class="n">aten_ops_or_torch_fns</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">aten_ops_or_torch_fns</span><span class="p">:</span>

            <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

            <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapper</span>
        <span class="k">return</span> <span class="n">func</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_implements_common_tensor_ops</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">implements</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">implements</span>
    <span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

    <span class="nd">@implements</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">contiguous</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

    <span class="nd">@implements</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">alias</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">contiguous</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="n">args</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="p">,</span>
            <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_same_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">TorchAOBaseTensor</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="n">TorchAOBaseTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">_tensor_shape_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">for</span> <span class="n">t_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span>
        <span class="p">)</span>
        <span class="n">_optional_tensor_shape_match</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
            <span class="c1"># either both are None or both are not Tensors and the shape match</span>
            <span class="n">_optional_tensor_shape_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
                <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">t_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span>
            <span class="p">)</span>

        <span class="n">_attr_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">a_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
        <span class="p">)</span>

        <span class="n">_optional_attr_match</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
            <span class="n">_optional_attr_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">a_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span>
            <span class="ow">and</span> <span class="n">_tensor_shape_match</span>
            <span class="ow">and</span> <span class="n">_optional_tensor_shape_match</span>
            <span class="ow">and</span> <span class="n">_attr_match</span>
            <span class="ow">and</span> <span class="n">_optional_attr_match</span>
        <span class="p">)</span>

    <span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">copy_</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">_same_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
            <span class="n">self_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">self_tensors</span><span class="p">:</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">))</span>
            <span class="k">return</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Not supported args for copy_ due to metadata mismatch: </span><span class="si">{</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_to_kwargs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
            <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensors</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">maybe_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_data_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">optional_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">optional_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># change device</span>
            <span class="n">tensor_attributes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span> <span class="k">if</span> <span class="n">attr_name</span> <span class="o">!=</span> <span class="s2">&quot;device&quot;</span> <span class="k">else</span> <span class="n">device</span>
                <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensor_attributes</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">optional_tensor_attributes</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span> <span class="k">if</span> <span class="n">attr_name</span> <span class="o">!=</span> <span class="s2">&quot;device&quot;</span> <span class="k">else</span> <span class="n">device</span>
                    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">]</span>

            <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                <span class="o">*</span><span class="n">tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">tensor_attributes</span><span class="p">,</span>
                <span class="o">*</span><span class="n">optional_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">optional_tensor_attributes</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses must implement `aten._to_copy.default` or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it&quot;</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_torchao_base_tensor__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
    <span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_set_obj_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">optional_tensor_data_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">,</span> <span class="p">[]):</span>
        <span class="k">if</span> <span class="n">optional_tensor_data_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_data_name</span>
        <span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_data_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">optional_tensor_attribute_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">,</span> <span class="p">[]</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">optional_tensor_attribute_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_attribute_name</span>
        <span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_attribute_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dispatch__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this util function for a common `__torch_function__` implementation</span>
<span class="sd">    that dispatches to ops/functions registered with `_implements`</span>

<span class="sd">    class MyTensor(torch.Tensor):</span>
<span class="sd">        ...</span>
<span class="sd">        __torch_function__ = classmethod(_dispatch__torch_function__)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_OR_TORCH_FN_TABLE&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span>
        <span class="ow">and</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">func</span><span class="p">](</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dispatch__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this util function for a common `__torch_dispatch__` implementation</span>
<span class="sd">    that dispatches to ops/functions registered with `_implements`</span>

<span class="sd">    class MyTensor(torch.Tensor):</span>
<span class="sd">        ...</span>
<span class="sd">        __torch_dispatch__ = classmethod(_dispatch__torch_dispatch__)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_OR_TORCH_FN_TABLE&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span>
        <span class="ow">and</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">func</span><span class="p">](</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">arg_types</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">kwarg_types</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> dispatch: attempting to run unimplemented operator/function: </span><span class="si">{</span><span class="n">func</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">types</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">arg_types</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">kwarg_types</span><span class="si">=}</span><span class="s2">&quot;</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_layout</span><span class="p">(</span><span class="n">tensor_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">layout_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function for layout registrations, this is used to implement</span>
<span class="sd">    register_layout decorator for each tensor subclass, see aqt.py for example usage</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_class: Tensor subclass type</span>
<span class="sd">        layout_class: the class type of subclass of `Layout`, e.g. `PlainLayout`</span>

<span class="sd">    Returns:</span>
<span class="sd">        a decorator that registers the tensor impl constructor in the table</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># tensor_class._LAYOUT_CONSTRUCTOR_TABLE is a map from layout_class like TensorCoreTiledLayout</span>
    <span class="c1"># to tensor_impl class constructor like TensorCoreTiledAQTTensorImpl.from_plain that can construct a tensor_impl</span>
    <span class="c1"># from plain data like (quantized, unpacked) `data`, `scale`, `zero_point`</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_class</span><span class="p">,</span> <span class="s2">&quot;_LAYOUT_CONSTRUCTOR_TABLE&quot;</span><span class="p">):</span>
        <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">tensor_impl_class</span><span class="p">):</span>
        <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span><span class="p">[</span><span class="n">layout_class</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tensor_impl_class</span><span class="o">.</span><span class="n">from_plain</span>
        <span class="p">)</span>
        <span class="c1"># Allow serialization to work for models uses this tensor impl subclass</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">layout_class</span><span class="p">,</span> <span class="n">tensor_impl_class</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">tensor_impl_class</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_tensor_impl_constructor</span><span class="p">(</span>
    <span class="n">tensor_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">layout_class</span><span class="p">:</span> <span class="n">Callable</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get TensorImpl class constructor (TensorImplClass.from_plain) for `tensor_class` based on `layout_class`</span>
<span class="sd">    `layout_class` means the class type of subclass of `Layout`, e.g. `PlainLayout`</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_class: Tensor subclass type</span>
<span class="sd">        layout_class: the class type of subclass of `Layout`, e.g. `PlainLayout`</span>

<span class="sd">    Returns:</span>
<span class="sd">        tensor impl subclass constructor for the layout_class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_class</span><span class="p">,</span> <span class="s2">&quot;_LAYOUT_CONSTRUCTOR_TABLE&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;no registered tensor_impl class constructor for: </span><span class="si">{</span><span class="n">tensor_class</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">layout_class</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;layout_name: </span><span class="si">{</span><span class="n">layout_class</span><span class="si">}</span><span class="s2"> is not supported yet for </span><span class="si">{</span><span class="n">tensor_class</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span><span class="p">[</span><span class="n">layout_class</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_to_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># `torch._C._nn._parse_to` can&#39;t handle `layout` argument</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">layout</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;layout&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span>
    <span class="c1"># ignoring `non_blocking` and `memory_format` args since these are not</span>
    <span class="c1"># very useful for most of the tensor subclasses</span>
    <span class="c1"># if in the future there are use cases that need these, we&#39;d recommend</span>
    <span class="c1"># to override `_get_to_kwargs` and return these args</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">device</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span>
        <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">kwargs</span>


<div class="viewcode-block" id="TorchAOBaseTensor"><a class="viewcode-back" href="../../generated/torchao.utils.TorchAOBaseTensor.html#torchao.utils.TorchAOBaseTensor">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TorchAOBaseTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A util tensor subclass that provides commonly used functions</span>
<span class="sd">       new tensor subclass can inherit it to get all the utility functions</span>

<span class="sd">       class MyTensor(TorchAOBaseTensor):</span>
<span class="sd">           pass</span>

<span class="sd">    This includes:</span>
<span class="sd">       `_get_to_kwargs` that can get the kwargs for `to`</span>
<span class="sd">            class MyTensor(TorchAOBaseTensor):</span>
<span class="sd">                def to(self, *args, **kwargs):</span>
<span class="sd">                    kwargs = _get_to_kwargs(*args, **kwargs)</span>
<span class="sd">                    ...</span>
<span class="sd">        `implements`:</span>
<span class="sd">            implements = MyTensor.implements</span>

<span class="sd">            @implements(torch.nn.functional.linear):</span>
<span class="sd">            def _(func, types, args, kwargs):</span>
<span class="sd">                ...</span>

<span class="sd">        `register_layout`:</span>
<span class="sd">            register_layout = MyTensor.register_layout</span>

<span class="sd">            @register_layout(PlainLayout)</span>
<span class="sd">            class PlainAQTTensorImpl(...):</span>
<span class="sd">                ...</span>

<span class="sd">         `get_tensor_impl_constructor`:</span>
<span class="sd">            get_tensor_impl_constructor = MyTensor.get_tensor_impl_constructor</span>
<span class="sd">            # in constructor of MyTensor:</span>
<span class="sd">            tensor_impl_ctr = get_tensor_impl_constructor(type(_layout))</span>
<span class="sd">            tensor_impl = tensor_impl_ctr(data, scale, zero_point, _layout)</span>

<span class="sd">    class variables to define to simplify implmentation of tensor subclasses:</span>
<span class="sd">       `tensor_data_names` (List[str]): list of names of all requires tensor_data, order should match</span>
<span class="sd">          the `__init__` list of tensor subclass</span>
<span class="sd">       `tensor_attribute_names` (List[str]): list of names of non-Tensor attributes,</span>
<span class="sd">            order should match the `__init__` list of tensor subclass, following all the `tensor_data_names` arguments</span>
<span class="sd">       `optional_tensor_data_names` (List[str]): it&#39;s optional to define this field to have the additional boilerplate functions been implemented for you, but this will be need if there are some optional Tensor data attributes, when defined, this will be a list of names of Tensors that can be optional</span>
<span class="sd">       `optional_tensor_attribute_names` (List[str]): it&#39;s optional to define this field to have the additional boilerplate functions been implemented for you, but this will be need if there are some optional non-Tensor attributes, when defined, this will be a list of names of attributes that can be optional</span>
<span class="sd">       Note: Argument order in __init__ and __new__ should match exaclty with tensor_data_names + tensor_attribute_names + optional_tensor_data_names (if present) + optional_tensor_attribute_names (if present)</span>


<span class="sd">    If `tensor_data_names` (torch.Tensor data attribute names) and `tensor_attribute_names` (non-torch.Tensor attribute names) are defined, there are some additional</span>
<span class="sd">    functions that will be added, this includes:</span>
<span class="sd">    `__tensor_flatten__`: flattens a subclassed tensor instance, returns a tuple, first element is tensor data names for valid tensor data,</span>
<span class="sd">        second element is a dict from attribute_name to non-Tensor attributes</span>
<span class="sd">    `__tensor_unflatten__`: takes a tensor_data_dict (a map from tensor name to Tensor), and list of non-tensor attributes, returns a new instance of the subclassed tensor</span>
<span class="sd">    `_apply_fn_to_data`: takes a function (Tensor -&gt; Tensor),  applies function to all tensor data and</span>
<span class="sd">        recreate a new subclassed Tensor with the transformed tensor data</span>
<span class="sd">    `__repr__`: the string representation of the subclassed tensor instance</span>
<span class="sd">    `_same_metadata`: returns whether the metadata is the same between two instances of cls</span>
<span class="sd">    `__setstate__`: when loading a serialized tensor subclass checkpoints, it sets the new</span>
<span class="sd">    optional tensor and tensor attribute that is saved in the old checkpoint to None,</span>
<span class="sd">    to maintain BC of old checkpoints when we add new optional tensor data or attributes to</span>
<span class="sd">    the tensor subclass</span>
<span class="sd">    torch ops: torch.Tensor.contiguous</span>
<span class="sd">    aten ops: aten.detach.default, aten.clone.default, aten.alias,default, aten.contiguous.default, aten.copy_.default, aten._to_copy.default (enables t.to)</span>

<span class="sd">    Example:</span>
<span class="sd">        class MyTensor(torch.Tensor):</span>
<span class="sd">            tensor_data_names = [&quot;a&quot;, &quot;b&quot;]</span>
<span class="sd">            tensor_attribute_names = [&quot;c&quot;, &quot;d&quot;]</span>
<span class="sd">            optional_tensor_data_names = [&quot;e&quot;, &quot;f&quot;]</span>
<span class="sd">            optional_tensor_attribute_names = [&quot;g&quot;, &quot;h&quot;]</span>


<span class="sd">            def __new__(</span>
<span class="sd">                cls,</span>
<span class="sd">                a: Tensor,</span>
<span class="sd">                b: Tensor,</span>
<span class="sd">                c: int,</span>
<span class="sd">                d: str,</span>
<span class="sd">                e: Optional[Tensor] = None,</span>
<span class="sd">                f: Optional[Tensor] = None,</span>
<span class="sd">                g: Optional[int] = None,</span>
<span class="sd">                h: Optional[int] = None,</span>
<span class="sd">            ):</span>
<span class="sd">                pass</span>

<span class="sd">            def __init__(</span>
<span class="sd">                self,</span>
<span class="sd">                a: Tensor,</span>
<span class="sd">                b: Tensor,</span>
<span class="sd">                c: int,</span>
<span class="sd">                d: str</span>
<span class="sd">                e: Optional[Tensor] = None,</span>
<span class="sd">                f: Optional[Tensor] = None,</span>
<span class="sd">                g: Optional[int] = None,</span>
<span class="sd">                h: Optional[int] = None,</span>
<span class="sd">            ):</span>
<span class="sd">                pass</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_OR_TORCH_FN_TABLE&quot;</span><span class="p">):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">cls</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># define the common ops and __set_state__ for BC</span>
        <span class="c1"># if the tensor_data_names and tensor_attribute_names are defined</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span><span class="p">):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_implements_common_tensor_ops</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">__setstate__</span> <span class="o">=</span> <span class="n">_torchao_base_tensor__setstate__</span>

        <span class="c1"># inherit the torch function and dispatch implementations from direct parent classes</span>
        <span class="c1"># e.g. for `class C(B, A)`, C.__bases__ == (B, A)</span>
        <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__bases__</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">parent</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">:</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_OR_TORCH_FN_TABLE</span><span class="p">[</span><span class="n">parent</span><span class="p">]</span>
                <span class="p">)</span>

    <span class="n">implements</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_implements</span><span class="p">)</span>
    <span class="n">_implements_common_tensor_ops</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_implements_common_tensor_ops</span><span class="p">)</span>
    <span class="n">__torch_dispatch__</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_dispatch__torch_dispatch__</span><span class="p">)</span>
    <span class="n">__torch_function__</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_dispatch__torch_function__</span><span class="p">)</span>
    <span class="n">register_layout</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_register_layout</span><span class="p">)</span>
    <span class="n">get_tensor_impl_constructor</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_get_tensor_impl_constructor</span><span class="p">)</span>
    <span class="n">_get_to_kwargs</span> <span class="o">=</span> <span class="n">_get_to_kwargs</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">tensor_data_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">maybe_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_data_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">tensor_data_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_data_name</span><span class="p">)</span>

            <span class="n">attr_dict</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">attr_dict</span> <span class="o">=</span> <span class="n">attr_dict</span> <span class="o">|</span> <span class="p">{</span>
                    <span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">}</span>

            <span class="k">return</span> <span class="n">tensor_data_names</span><span class="p">,</span> <span class="n">attr_dict</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses should implement __tensor_flatten__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class before using it&quot;</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span><span class="p">):</span>
            <span class="n">required_tensors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">tensor_data_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">tensor_data_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensor_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="n">optional_tensor_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">tensor_data_name</span><span class="p">:</span> <span class="n">tensor_data_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tensor_data_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">optional_tensor_data_names</span>
                <span class="p">}</span>

            <span class="n">required_attributes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">tensor_attributes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">]</span>
            <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">name</span><span class="p">:</span> <span class="n">tensor_attributes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">}</span>

            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
                <span class="o">*</span><span class="n">required_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">required_attributes</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_tensor_dict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_attribute_dict</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses should implement __tensor_unflatten__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class before using it&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">required_tensors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">fn</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensor_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">maybe_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_data_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">optional_tensor_dict</span><span class="p">[</span><span class="n">tensor_data_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">optional_tensor_dict</span><span class="p">[</span><span class="n">tensor_data_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">required_attributes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">]</span>
            <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">attr_name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">}</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                <span class="o">*</span><span class="n">required_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">required_attributes</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_tensor_dict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_attribute_dict</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses should implement _apply_fn_to_data or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">repr_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="c1"># required tensor data</span>
            <span class="n">repr_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="n">repr_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_data_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_data_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="c1"># required attributes</span>
            <span class="k">for</span> <span class="n">tensor_attribute_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span><span class="p">:</span>
                <span class="n">repr_str</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_attribute_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_attribute_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1"># optional tensor data</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">repr_str</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_data_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_data_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

            <span class="c1"># optional tensor attributes</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_attribute_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span><span class="p">:</span>
                    <span class="n">repr_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_attribute_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_attribute_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">repr_str</span><span class="si">}</span><span class="s2">)&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses must implement __repr__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_layout&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">fill_defaults</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">defaults_tail</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __torch_dispatch__ doesn&#39;t guarantee the number of arguments you are</span>
<span class="sd">    passed (e.g., defaulted arguments are not passed); but usually it is</span>
<span class="sd">    convenient to pad out the arguments list with defaults.  This function</span>
<span class="sd">    helps you do that.</span>
<span class="sd">    Args:</span>
<span class="sd">        args: the list of positional arguments passed to __torch_dispatch__</span>
<span class="sd">        n: the number of arguments you are expecting to get</span>
<span class="sd">        defaults_tail: default values for the arguments, starting from the</span>
<span class="sd">            end of the list</span>
<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; fill_defaults([1, 2, 3], 5, [3, 4, 5])</span>
<span class="sd">        [1, 2, 3, 4, 5]</span>
<span class="sd">        &gt;&gt;&gt; fill_defaults([1, 2, 3], 5, [None, None, None])</span>
<span class="sd">        [1, 2, 3, None, None]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">defaults_tail</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;not enough defaults to fill arguments&quot;</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">defaults_tail</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">defaults_tail</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">r</span>


<span class="c1"># Supported AMD GPU Models and their LLVM gfx Codes:</span>
<span class="c1">#</span>
<span class="c1"># | AMD GPU Model | LLVM gfx Code          |</span>
<span class="c1"># |---------------|------------------------|</span>
<span class="c1"># | Navi4         | gfx1200, gfx1201       |</span>
<span class="c1"># | MI300X        | gfx940, gfx941, gfx942 |</span>
<span class="c1"># | MI350         | gfx950                 |</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_ROCM</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_MI300</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_ROCM</span><span class="p">():</span>
        <span class="n">mxArchName</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gfx940&quot;</span><span class="p">,</span> <span class="s2">&quot;gfx941&quot;</span><span class="p">,</span> <span class="s2">&quot;gfx942&quot;</span><span class="p">]</span>
        <span class="n">archName</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">gcnArchName</span>
        <span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">mxArchName</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">archName</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_MI350</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_ROCM</span><span class="p">():</span>
        <span class="n">archName</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">gcnArchName</span>
        <span class="k">if</span> <span class="s2">&quot;gfx950&quot;</span> <span class="ow">in</span> <span class="n">archName</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_Navi4</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_ROCM</span><span class="p">():</span>
        <span class="n">archName</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">gcnArchName</span>
        <span class="k">if</span> <span class="s2">&quot;gfx1200&quot;</span> <span class="ow">or</span> <span class="s2">&quot;gfx1201&quot;</span> <span class="ow">in</span> <span class="n">archName</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_version</span><span class="p">(</span><span class="n">major</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">minor</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if the CUDA version is exactly major.minor&quot;&quot;&quot;</span>
    <span class="n">is_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_cuda</span> <span class="k">else</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_at_least_89</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_at_least_90</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>


<span class="c1"># TODO(future PR): rename to 8_9, 9_0, 10_0 instead of 89, 10, 100</span>
<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_at_least_100</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_cpu_version</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;2.6.0&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
    <span class="k">return</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="ow">and</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_xpu_version</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;2.8.0&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
    <span class="k">return</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span> <span class="ow">and</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">ceil_div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">b</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_package_at_least</span><span class="p">(</span><span class="n">package_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">min_version</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">package_exists</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="n">package_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">package_exists</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="n">version</span><span class="p">(</span><span class="n">package_name</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">min_version</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_fbgemm_genai_gpu_available</span><span class="p">():</span>
    <span class="c1"># TODO: use is_package_at_least(&quot;fbgemm_gpu&quot;, &quot;1.2.0&quot;) when</span>
    <span class="c1"># https://github.com/pytorch/FBGEMM/issues/4198 is fixed</span>
    <span class="k">if</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;fbgemm_gpu&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">fbgemm_gpu.experimental.gen_ai</span>  <span class="c1"># noqa: F401</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_fbcode</span><span class="p">()</span> <span class="ow">and</span> <span class="n">fbgemm_gpu</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&lt;</span> <span class="s2">&quot;1.2.0&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="kc">True</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DummyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is used because the TorchAO quantization functions tend to operate on modules so to apply the transform to a tensor, we can load a</span>
<span class="sd">    DummyModule with the target tensor and then apply the transformation to the module and then extract the transformed tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
         <script src="../../_static/design-tabs.js"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
         <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
         <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
         <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
         <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p> Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>