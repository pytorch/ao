
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.utils &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=b417fedc" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=ca3c1c84" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=e5fbc548" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=f533b996" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/utils';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/pytorch/ao/gh-pages/torchao-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/utils.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>


<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main (0.17.0+git1a9a884 )');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->


<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">

<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>

  
  <div class="navbar-header-items__mobile-logo">
    







  
  
  
  


<a class="navbar-brand logo" href="../../index.html">
  
    
    <img src="../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
  </div>

  
  
  <div class=" navbar-header-items__start">
    
      
      
        <div class="navbar-item">
          







  
  
  
  


<a class="navbar-brand logo" href="../../index.html">
  
    
    <img src="../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
        </div>
      
    
      
      
        
        <div class="navbar-item desktop-only-version">
          
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
        </div>
      
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
      
        <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        




  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    <div class="sidebar-header-items__start">
      <div class="navbar-item">
        
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
      </div>
    </div>
    

    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
        
          <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
        
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



<div id="rtd-footer-container"></div>
      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.utils</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              

<div id="searchbox"></div>
<div id="pytorch-article">
  <!-- Hidden breadcrumb schema for SEO only -->
  <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <link itemprop="item" href="../index.html">
      <meta itemprop="name" content="Module code">
      <meta itemprop="position" content="1">
    </div>
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <meta itemprop="name" content="torchao.utils">
      <meta itemprop="position" content="2">
    </div>
  </div>

  
  

  
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">importlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">importlib.metadata</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">gcd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.parametrize</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">parametrize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils._python_dispatch</span><span class="w"> </span><span class="kn">import</span> <span class="n">return_and_correct_aliasing</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;benchmark_model&quot;</span><span class="p">,</span>
    <span class="s2">&quot;profiler_runner&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_available_devices&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_compute_capability&quot;</span><span class="p">,</span>
    <span class="s2">&quot;benchmark_torch_function_in_microseconds&quot;</span><span class="p">,</span>
    <span class="s2">&quot;find_multiple&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_register_custom_op&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_model_size_in_bytes&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unwrap_tensor_subclass&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TorchAOBaseTensor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_cuda_version_at_least&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_MI300&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_sm_at_least_89&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_sm_at_least_90&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_sm_at_least_100&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_package_at_least&quot;</span><span class="p">,</span>
    <span class="s2">&quot;DummyModule&quot;</span><span class="p">,</span>
    <span class="s2">&quot;register_as_pytree_constant&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">register_as_pytree_constant</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decorator to register a class as a pytree constant for dynamo non-strict trace mode.&quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_pytree</span><span class="o">.</span><span class="n">register_constant</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">cls</span>


<span class="c1"># Referenced from: https://github.com/pytorch/pytorch/blob/9105d54c6b37099575c0059ef274c86c4dc80c57/torch/ao/quantization/utils.py#L711</span>
<span class="nd">@functools</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_assert_and_get_unique_device</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the unique device for a module, or None if no device is found.</span>
<span class="sd">    Throws an error if multiple devices are detected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span> <span class="o">|</span> <span class="p">{</span>
        <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;prepare only works with cpu or single-device CUDA modules, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;but got devices </span><span class="si">{</span><span class="n">devices</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">device</span>


<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_runs</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark model runs with `args` and `kwargs` both are optional&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;Expecting `model` to be torch.nn.Module if device_type is not provided&quot;</span>
        <span class="p">)</span>
        <span class="n">device_type</span> <span class="o">=</span> <span class="n">_assert_and_get_unique_device</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">type</span>

    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">start_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">start_event</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>

    <span class="k">elif</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">start_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">start_event</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>

    <span class="k">elif</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">average_time_per_run</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>
        <span class="k">return</span> <span class="n">average_time_per_run</span>

    <span class="k">elif</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">start_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>

        <span class="c1"># benchmark</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;timed region&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">end_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">start_event</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span>


<span class="k">def</span><span class="w"> </span><span class="nf">profiler_runner</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
        <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_available_devices</span><span class="p">():</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;xpu&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">devices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">devices</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_current_accelerator_device</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">current_accelerator</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_compute_capability</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">capability</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">capability</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">capability</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">0.0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">compute_max_diff</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">output_ref</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">output_ref</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output_ref</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.benchmark</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">benchmark</span>  <span class="c1"># this avoids importing numpy when torchao module is loaded</span>

    <span class="c1"># Manual warmup</span>
    <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s2">&quot;f(*args, **kwargs)&quot;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="n">args</span><span class="p">,</span> <span class="s2">&quot;kwargs&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">f</span><span class="p">},</span>  <span class="c1"># noqa: E501</span>
    <span class="p">)</span>
    <span class="n">measurement</span> <span class="o">=</span> <span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">measurement</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mf">1e6</span>


<span class="k">def</span><span class="w"> </span><span class="nf">find_multiple</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">//</span> <span class="n">gcd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>  <span class="c1"># type: ignore[9]</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">n</span>
    <span class="k">return</span> <span class="n">n</span> <span class="o">+</span> <span class="n">k</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_custom_op</span><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">inductor_decomposed</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This decorator is used to preserve some high level operators for torch.export.export</span>
<span class="sd">    while still allow them to be decomposed for inductor path</span>

<span class="sd">    requirement: make sure `fn.__name__[1:]` is the operator name you want to register</span>

<span class="sd">    NOTE: This should be applied at the top, after all other decorators have been applied</span>
<span class="sd">    NOTE: We haven&#39;t tested the case when `fn` accepts tensor subclass instance as input,</span>
<span class="sd">    e.g. uint4 tensor subclass instance, and we&#39;ll probably need to figure out what would make</span>
<span class="sd">    sense for downstream system (like executorch) to accept as well</span>

<span class="sd">    Example:</span>
<span class="sd">        lib = torch.library.Library(&quot;my_namespace&#39;, &quot;FRAGMENT&quot;)</span>

<span class="sd">        register_custom_op = _register_custom_op(lib)</span>

<span class="sd">        @register_custom_op</span>
<span class="sd">        def _the_op_that_needs_to_be_preserved(...)</span>
<span class="sd">            ...</span>

<span class="sd">        # after this, `_the_op_that_needs_to_be_preserved` will be preserved as</span>
<span class="sd">        # torch.ops.my_namespace.the_op_that_needs_to_be_preserved operator after</span>
<span class="sd">        # torch.export.export</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch._inductor.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_decomposition</span>

    <span class="n">dispatch_key</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;CompositeImplicitAutograd&quot;</span>
        <span class="k">if</span> <span class="n">inductor_decomposed</span>
        <span class="k">else</span> <span class="s2">&quot;CompositeExplicitAutograd&quot;</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torch._library.infer_schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">infer_schema</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">c</span> <span class="ow">in</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s2">&quot;.&lt;&gt;&quot;</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Expecting op to be defined in normal functions, not lambda or local: </span><span class="si">{</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">op_name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="n">op_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;_&quot;</span><span class="p">:</span>
            <span class="n">op_name</span> <span class="o">=</span> <span class="n">op_name</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">op_name</span> <span class="o">+</span> <span class="n">infer_schema</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">{})</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span>

        <span class="n">lib_namespace</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">ns</span>
        <span class="n">op</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="n">lib_namespace</span><span class="p">),</span> <span class="n">op_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inductor_decomposed</span><span class="p">:</span>
            <span class="n">register_decomposition</span><span class="p">([</span><span class="n">op</span><span class="p">])(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_meta_op</span><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="s2">&quot;Meta&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">op</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_model_size_in_bytes</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ignore_embeddings</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the model size in bytes. The option to ignore embeddings</span>
<span class="sd">    is useful for models with disproportionately large embeddings compared</span>
<span class="sd">    to other model parameters that get quantized/sparsified.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">flat_size</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;__tensor_flatten__&quot;</span><span class="p">):</span>
            <span class="n">size</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># 0th element is a list of attributes that</span>
            <span class="c1"># hold tensors</span>
            <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">tensor</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">sub_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                <span class="n">size</span> <span class="o">+=</span> <span class="n">flat_size</span><span class="p">(</span><span class="n">sub_tensor</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>

    <span class="n">model_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ignore_embeddings</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="n">child</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">child</span><span class="o">.</span><span class="n">buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">model_size</span> <span class="o">+=</span> <span class="n">flat_size</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">model_size</span> <span class="o">+=</span> <span class="n">get_model_size_in_bytes</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">ignore_embeddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_size</span>


<span class="k">class</span><span class="w"> </span><span class="nc">UnwrapTensorSubclass</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
        <span class="n">todo</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tp</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">inner_tensors</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rebuild_stack</span><span class="p">):</span>
            <span class="n">nb_tensor</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">)</span>
            <span class="n">inner_tensors</span> <span class="o">=</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="n">b</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">,</span> <span class="n">todo</span><span class="p">[</span><span class="o">-</span><span class="n">nb_tensor</span><span class="p">:])}</span>
            <span class="n">todo</span> <span class="o">=</span> <span class="n">todo</span><span class="p">[</span><span class="n">nb_tensor</span><span class="p">:]</span>
            <span class="n">rebuilt</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">__tensor_unflatten__</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rebuilt</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">todo</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">todo</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">right_inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
        <span class="n">rebuild_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">plain_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">todo</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
        <span class="k">while</span> <span class="n">todo</span><span class="p">:</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="n">todo</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="n">inner_tensors</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
            <span class="n">rebuild_stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">),</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">inner_tensors</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">inner_tensors</span><span class="p">:</span>
                <span class="n">val</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
                    <span class="n">plain_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rebuild_stack</span> <span class="o">=</span> <span class="n">rebuild_stack</span>

        <span class="k">return</span> <span class="n">plain_tensors</span>


<span class="k">def</span><span class="w"> </span><span class="nf">unwrap_tensor_subclass</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Unwraps (nested) tensor subclass in the model to plain tensors</span>
<span class="sd">    This is a workaround to make a model with tensor subclass to work with `torch.export.export`</span>
<span class="sd">    and `torch.aot_compile`, we hope this can be integrated into compile stack soon</span>
<span class="sd">    tracking issue: https://github.com/pytorch/ao/issues/345</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="c1"># make sure child.weight is a tensor subclass</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">TorchAOBaseTensor</span><span class="p">)</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">parametrize</span><span class="o">.</span><span class="n">is_parametrized</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">UnwrapTensorSubclass</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="n">unwrap_tensor_subclass</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_float8_type</span><span class="p">(</span><span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">fp8_types</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">fp8_types</span>


<span class="k">def</span><span class="w"> </span><span class="nf">parse_version</span><span class="p">(</span><span class="n">version_string</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parse version string representing pre-release with -1</span>

<span class="sd">    Examples: &quot;2.5.0.dev20240708+cu121&quot; -&gt; [2, 5, -1], &quot;2.5.0&quot; -&gt; [2, 5, 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check for pre-release indicators</span>
    <span class="n">is_prerelease</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(git|dev)&quot;</span><span class="p">,</span> <span class="n">version_string</span><span class="p">))</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(\d+)\.(\d+)\.(\d+)&quot;</span><span class="p">,</span> <span class="n">version_string</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">,</span> <span class="n">patch</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">is_prerelease</span><span class="p">:</span>
            <span class="n">patch</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">,</span> <span class="n">patch</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid version string format: </span><span class="si">{</span><span class="n">version_string</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_fbcode</span><span class="p">():</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="p">,</span> <span class="s2">&quot;git_version&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">torch_version_at_least</span><span class="p">(</span><span class="n">min_version</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_fbcode</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># Parser for local identifiers</span>
    <span class="k">return</span> <span class="n">parse_version</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">parse_version</span><span class="p">(</span><span class="n">min_version</span><span class="p">)</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Helper function for implementing aten op or torch function dispatch</span>
<span class="sd">and dispatching to these implementations.</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_implements</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">aten_ops</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decorator for implementing aten ops like `torch.ops.aten.linear.default` for</span>
<span class="sd">    tensor subclass, the implemented functions are called in ``__torch_dispatch__`` callback</span>
<span class="sd">    for ``torch.Tensor`` subclasses</span>

<span class="sd">    Examples::</span>

<span class="sd">        implements = MyTensor.implements</span>

<span class="sd">        @implements(torch.ops.aten.linear.default):</span>
<span class="sd">        def _(func, types, args, kwargs):</span>
<span class="sd">            ...</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_TABLE&quot;</span><span class="p">):</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">:</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aten_ops</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">aten_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">aten_ops</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">aten_ops</span><span class="p">:</span>

            <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">_func</span><span class="o">=</span><span class="n">func</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">_func</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

            <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapper</span>
        <span class="k">return</span> <span class="n">func</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_implements_torch_function</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">torch_fns</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decorator for implementing torch functions / ops</span>
<span class="sd">    like ``torch.nn.functional.linear``, ``torch.Tensor.t`` for the tensor subclass</span>
<span class="sd">    the implemented functions are called in ``__torch_function__`` callback</span>
<span class="sd">    for ``torch.Tensor`` subclasses</span>

<span class="sd">    Examples::</span>

<span class="sd">        implements_torch_function = MyTensor.implements_torch_function</span>

<span class="sd">        @implements_torch_function(torch.nn.functional.linear):</span>
<span class="sd">        def _(func, types, args, kwargs):</span>
<span class="sd">            ...</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_TORCH_FN_TABLE&quot;</span><span class="p">):</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">:</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">torch_fns</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">torch_fns</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch_fns</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">torch_fns</span><span class="p">:</span>

            <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">_func</span><span class="o">=</span><span class="n">func</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">_func</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

            <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">fn</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapper</span>
        <span class="k">return</span> <span class="n">func</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_implements_common_tensor_ops</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">implements</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">implements</span>
    <span class="n">implements_torch_function</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">implements_torch_function</span>
    <span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

    <span class="nd">@implements</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="n">dtype_layout</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># only support kwargs for now</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># only support dtype, layout, and device for now</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">]</span>
        <span class="c1"># only support same dtype and layout</span>
        <span class="c1"># different dtype and layout has undefined behavior</span>
        <span class="k">if</span> <span class="s2">&quot;dtype&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">if</span> <span class="s2">&quot;layout&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span>
        <span class="c1"># if device is the same, treat this like a no-op</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">new_tensor</span><span class="p">)</span>

    <span class="c1"># This is called during _apply() to see if we can shallow</span>
    <span class="c1"># copy the content of one tensor into another. For now,</span>
    <span class="c1"># we only allow shallow copy if both tensors are of the</span>
    <span class="c1"># same type and have the same shape.</span>
    <span class="nd">@implements_torch_function</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_has_compatible_shallow_copy_type</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">and</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>

    <span class="nd">@implements_torch_function</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">contiguous</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

    <span class="nd">@implements</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">alias</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">aten</span><span class="o">.</span><span class="n">contiguous</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="n">args</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="p">,</span>
            <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_same_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">TorchAOBaseTensor</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="n">TorchAOBaseTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">_tensor_shape_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">for</span> <span class="n">t_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span>
        <span class="p">)</span>
        <span class="n">_optional_tensor_shape_match</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
            <span class="c1"># either both are None or both are not Tensors and the shape match</span>
            <span class="n">_optional_tensor_shape_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
                    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">t_name</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">t_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span>
            <span class="p">)</span>

        <span class="n">_attr_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">a_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
        <span class="p">)</span>

        <span class="n">_optional_attr_match</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
            <span class="n">_optional_attr_match</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">a_name</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">a_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span>
            <span class="ow">and</span> <span class="n">_tensor_shape_match</span>
            <span class="ow">and</span> <span class="n">_optional_tensor_shape_match</span>
            <span class="ow">and</span> <span class="n">_attr_match</span>
            <span class="ow">and</span> <span class="n">_optional_attr_match</span>
        <span class="p">)</span>

    <span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">copy_</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">_same_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
            <span class="n">self_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">self_tensors</span><span class="p">:</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">))</span>
            <span class="k">return</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Not supported args for copy_ due to metadata mismatch: </span><span class="si">{</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_to_kwargs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
            <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensors</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">maybe_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_data_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">optional_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">optional_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># change device</span>
            <span class="n">tensor_attributes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span> <span class="k">if</span> <span class="n">attr_name</span> <span class="o">!=</span> <span class="s2">&quot;device&quot;</span> <span class="k">else</span> <span class="n">device</span>
                <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensor_attributes</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">optional_tensor_attributes</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span> <span class="k">if</span> <span class="n">attr_name</span> <span class="o">!=</span> <span class="s2">&quot;device&quot;</span> <span class="k">else</span> <span class="n">device</span>
                    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">]</span>

            <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                <span class="o">*</span><span class="n">tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">tensor_attributes</span><span class="p">,</span>
                <span class="o">*</span><span class="n">optional_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">optional_tensor_attributes</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses must implement `aten._to_copy.default` or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it&quot;</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_torchao_base_tensor__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
    <span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_set_obj_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">optional_tensor_data_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">,</span> <span class="p">[]):</span>
        <span class="k">if</span> <span class="n">optional_tensor_data_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_data_name</span>
        <span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_data_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">optional_tensor_attribute_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">,</span> <span class="p">[]</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">optional_tensor_attribute_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_attribute_name</span>
        <span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optional_tensor_attribute_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dispatch__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this util function for a common `__torch_function__` implementation</span>
<span class="sd">    that dispatches to ops/functions registered with `_implements`</span>

<span class="sd">    class MyTensor(torch.Tensor):</span>
<span class="sd">        ...</span>
<span class="sd">        __torch_function__ = classmethod(_dispatch__torch_function__)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_TORCH_FN_TABLE&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span>
        <span class="ow">and</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">func</span><span class="p">](</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_dispatch__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this util function for a common `__torch_dispatch__` implementation</span>
<span class="sd">    that dispatches to ops/functions registered with `_implements`</span>

<span class="sd">    class MyTensor(torch.Tensor):</span>
<span class="sd">        ...</span>
<span class="sd">        __torch_dispatch__ = classmethod(_dispatch__torch_dispatch__)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_TABLE&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span>
        <span class="ow">and</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">][</span><span class="n">func</span><span class="p">](</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">arg_types</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">kwarg_types</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> dispatch: attempting to run unimplemented operator/function: </span><span class="si">{</span><span class="n">func</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">types</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">arg_types</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">kwarg_types</span><span class="si">=}</span><span class="s2">&quot;</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_register_layout</span><span class="p">(</span><span class="n">tensor_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">layout_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. deprecated:: 0.15.1</span>
<span class="sd">       This method is deprecated as of version 0.15.1 since it&#39;s</span>
<span class="sd">       part of the older tensor subclass development stack,</span>
<span class="sd">       for information about new dev stack, please check</span>
<span class="sd">       https://docs.pytorch.org/ao/main/quantization_overview.html</span>
<span class="sd">       and https://docs.pytorch.org/ao/main/contributor_guide.html</span>

<span class="sd">    Helper function for layout registrations, this is used to implement</span>
<span class="sd">    register_layout decorator for each tensor subclass, see aqt.py for example usage</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_class: Tensor subclass type</span>
<span class="sd">        layout_class: the class type of subclass of `Layout`, e.g. `PlainLayout`</span>

<span class="sd">    Returns:</span>
<span class="sd">        a decorator that registers the tensor impl constructor in the table</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># tensor_class._LAYOUT_CONSTRUCTOR_TABLE is a map from layout_class like TensorCoreTiledLayout</span>
    <span class="c1"># to tensor_impl class constructor like TensorCoreTiledAQTTensorImpl.from_plain that can construct a tensor_impl</span>
    <span class="c1"># from plain data like (quantized, unpacked) `data`, `scale`, `zero_point`</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_class</span><span class="p">,</span> <span class="s2">&quot;_LAYOUT_CONSTRUCTOR_TABLE&quot;</span><span class="p">):</span>
        <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">tensor_impl_class</span><span class="p">):</span>
        <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span><span class="p">[</span><span class="n">layout_class</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tensor_impl_class</span><span class="o">.</span><span class="n">from_plain</span>
        <span class="p">)</span>
        <span class="c1"># Allow serialization to work for models uses this tensor impl subclass</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">layout_class</span><span class="p">,</span> <span class="n">tensor_impl_class</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">tensor_impl_class</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_tensor_impl_constructor</span><span class="p">(</span>
    <span class="n">tensor_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">layout_class</span><span class="p">:</span> <span class="n">Callable</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. deprecated:: 0.15.1</span>
<span class="sd">       This method is deprecated as of version 0.15.1 since it&#39;s</span>
<span class="sd">       part of the older tensor subclass development stack,</span>
<span class="sd">       for information about new dev stack, please check</span>
<span class="sd">       https://docs.pytorch.org/ao/main/quantization_overview.html</span>
<span class="sd">       and https://docs.pytorch.org/ao/main/contributor_guide.html</span>

<span class="sd">    Get TensorImpl class constructor (TensorImplClass.from_plain) for `tensor_class` based on `layout_class`</span>
<span class="sd">    `layout_class` means the class type of subclass of `Layout`, e.g. `PlainLayout`</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_class: Tensor subclass type</span>
<span class="sd">        layout_class: the class type of subclass of `Layout`, e.g. `PlainLayout`</span>

<span class="sd">    Returns:</span>
<span class="sd">        tensor impl subclass constructor for the layout_class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor_class</span><span class="p">,</span> <span class="s2">&quot;_LAYOUT_CONSTRUCTOR_TABLE&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;no registered tensor_impl class constructor for: </span><span class="si">{</span><span class="n">tensor_class</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">layout_class</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;layout_name: </span><span class="si">{</span><span class="n">layout_class</span><span class="si">}</span><span class="s2"> is not supported yet for </span><span class="si">{</span><span class="n">tensor_class</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_class</span><span class="o">.</span><span class="n">_LAYOUT_CONSTRUCTOR_TABLE</span><span class="p">[</span><span class="n">layout_class</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_to_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to get the device and dtype keyword args for `aten._to_copy.default` op</span>
<span class="sd">    only device and dtype are kept</span>

<span class="sd">    Returns: {&quot;device&quot;: device, &quot;dtype&quot;: dtype}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># `torch._C._nn._parse_to` can&#39;t handle `layout` argument</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">arg</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">layout</span><span class="p">))</span>
    <span class="k">if</span> <span class="s2">&quot;layout&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span>
    <span class="c1"># ignoring `non_blocking` and `memory_format` args since these are not</span>
    <span class="c1"># very useful for most of the tensor subclasses</span>
    <span class="c1"># if in the future there are use cases that need these, we&#39;d recommend</span>
    <span class="c1"># to override `_get_to_kwargs` and return these args</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">device</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span>
        <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">kwargs</span>


<div class="viewcode-block" id="TorchAOBaseTensor">
<a class="viewcode-back" href="../../api_reference/generated/torchao.utils.TorchAOBaseTensor.html#torchao.utils.TorchAOBaseTensor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">TorchAOBaseTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A util tensor subclass that provides commonly used functions</span>
<span class="sd">    new tensor subclass can inherit to get all the utility functions</span>

<span class="sd">    Attributes (defined by subclass of ``TorchAOBaseTensor``):</span>

<span class="sd">    * :attr:`tensor_data_names` (List[str]): list of names of all required tensor_data, order should match</span>
<span class="sd">        the `__init__` list of tensor subclass (optional to define to use ``TorchAOBaseTensor``,</span>
<span class="sd">        required for getting the util functions been defined (see Note section))</span>

<span class="sd">    * :attr:`tensor_attribute_names` (List[str]): list of names of non-``Tensor`` attributes,</span>
<span class="sd">        order should match the ``__init__`` list of tensor subclass, following all the ``tensor_data_names`` arguments (optional to define to use ``TorchAOBaseTensor``, required for getting the</span>
<span class="sd">        util functions been defined (see Note section))</span>

<span class="sd">    * :attr:`optional_tensor_data_names` (List[str]): it&#39;s not required to get the</span>
<span class="sd">        additional util functions been implemented, but this will be need if there are</span>
<span class="sd">        some optional ``Tensor`` data attributes, when defined, this will be a list of names of ``Tensor``s that</span>
<span class="sd">        can be optional</span>

<span class="sd">    * :attr:`optional_tensor_attribute_names` (List[str]): it&#39;s not required to get the</span>
<span class="sd">        additional util functions been implemented, but this will be need if there are</span>
<span class="sd">        some optional non-``Tensor`` attributes, when defined, this will be a list of names of attributes</span>
<span class="sd">        that can be optional</span>

<span class="sd">    Note:</span>
<span class="sd">        **Argument Order**</span>

<span class="sd">        Argument order in ``__init__`` and ``__new__`` of subclass of ``TorchAOBaseTensor`` should match exaclty with ``tensor_data_names`` + ``tensor_attribute_names`` + ``optional_tensor_data_names`` (if present) + ``optional_tensor_attribute_names`` (if present)</span>


<span class="sd">    Note:</span>
<span class="sd">        **How to Get Predefined Util Functions**</span>

<span class="sd">        If ``tensor_data_names`` (torch.Tensor data attribute names) and ``tensor_attribute_names`` (non-torch.Tensor attribute names) are defined, there are some additional util</span>
<span class="sd">        functions that will be added, this includes:</span>

<span class="sd">        ``__tensor_flatten__``: flattens a subclassed tensor instance, returns a ``tuple``, first element is tensor data names for valid tensor data, second element is a dict from attribute_name to non-``Tensor`` attributes</span>

<span class="sd">        ``__tensor_unflatten__``: takes a ``tensor_data_dict`` (a map from tensor name to ``Tensor``), and list of non-``Tensor`` attributes, returns a new instance of the subclassed tensor</span>

<span class="sd">        ``_apply_fn_to_data``: takes a function (``Tensor -&gt; Tensor``),  applies function to all tensor data and recreate a new subclassed Tensor with the transformed tensor data</span>

<span class="sd">        ``__repr__``: the string representation of the subclassed tensor instance</span>

<span class="sd">        ``_same_metadata``: returns whether the metadata is the same between two instances of cls</span>

<span class="sd">        ``__setstate__``: when loading a serialized tensor subclass checkpoints, it sets the new optional tensor and tensor attribute that is saved in the old checkpoint to None, to maintain BC of old checkpoints when we add new optional tensor data or attributes to the tensor subclass</span>

<span class="sd">        torch function supported (``__torch_function__``): ``torch.Tensor.contiguous``</span>

<span class="sd">        aten ops supported (``__torch_dispatch__``): ``aten.detach.default``, ``aten.clone.default``, ``aten.alias,default``, ``aten.contiguous.default``, ``aten.copy_.default``, ``aten._to_copy.default`` (enables ``t.to``)</span>

<span class="sd">    Note:</span>
<span class="sd">        **Subclassing and Op Inheritance**</span>

<span class="sd">        Subclasses of ``TorchAOBaseTensor`` automatically inherit aten op (``__torch_dispatch__``)</span>
<span class="sd">        and torch function (``__torch_function__``) implementations from their parent classes.</span>
<span class="sd">        Each subclass gets its own independent dispatch tables, so registering a new op on a child</span>
<span class="sd">        does not affect the parent, and vice versa.</span>

<span class="sd">        A child class can override an inherited op by registering its own implementation with</span>
<span class="sd">        ``@ChildClass.implements(...)``. If no override is provided, the parent&#39;s implementation</span>
<span class="sd">        is used automatically.</span>

<span class="sd">        For multiple inheritance (e.g., ``class C(B, A)``), ops are inherited from all parents</span>
<span class="sd">        following Python&#39;s MRO (Method Resolution Order), with later bases taking priority.</span>

<span class="sd">        Example::</span>

<span class="sd">            class Parent(TorchAOBaseTensor):</span>
<span class="sd">                tensor_data_names = [&quot;qdata&quot;]</span>
<span class="sd">                tensor_attribute_names = [&quot;attr&quot;]</span>

<span class="sd">                def __new__(cls, qdata, attr):</span>
<span class="sd">                    r = torch.Tensor._make_wrapper_subclass(cls, qdata.shape)</span>
<span class="sd">                    r.qdata = qdata</span>
<span class="sd">                    r.attr = attr</span>
<span class="sd">                    return r</span>

<span class="sd">                def __init__(self, qdata, attr):</span>
<span class="sd">                    pass</span>

<span class="sd">            @Parent.implements([torch.ops.aten.cat.default])</span>
<span class="sd">            def parent_cat(func, types, args, kwargs):</span>
<span class="sd">                # parent implementation</span>
<span class="sd">                ...</span>

<span class="sd">            # Child inherits Parent&#39;s aten.cat implementation automatically</span>
<span class="sd">            class Child(Parent):</span>
<span class="sd">                tensor_data_names = [&quot;qdata&quot;]</span>
<span class="sd">                tensor_attribute_names = [&quot;attr&quot;]</span>

<span class="sd">            # Optionally override an inherited op:</span>
<span class="sd">            @Child.implements([torch.ops.aten.cat.default])</span>
<span class="sd">            def child_cat(func, types, args, kwargs):</span>
<span class="sd">                # child-specific implementation</span>
<span class="sd">                ...</span>

<span class="sd">    Note:</span>
<span class="sd">        **Safetensors Support**</span>

<span class="sd">        ``TorchAOBaseTensor`` subclasses can be serialized to and loaded from the</span>
<span class="sd">        `safetensors &lt;https://huggingface.co/docs/safetensors&gt;`_ format. Since safetensors</span>
<span class="sd">        only stores plain ``torch.Tensor`` objects, the serialization layer (in</span>
<span class="sd">        ``torchao.prototype.safetensors``) decomposes each tensor subclass into its</span>
<span class="sd">        constituent plain tensors (from ``tensor_data_names``/``optional_tensor_data_names``)</span>
<span class="sd">        plus JSON metadata (from ``tensor_attribute_names``/``optional_tensor_attribute_names``),</span>
<span class="sd">        and reconstructs the subclass on load.</span>

<span class="sd">        To add safetensors support for a new ``TorchAOBaseTensor`` subclass:</span>

<span class="sd">        1. Define ``tensor_data_names``, ``tensor_attribute_names`` (and optionally</span>
<span class="sd">           ``optional_tensor_data_names``, ``optional_tensor_attribute_names``) on the subclass,</span>
<span class="sd">           which is already required for the other utility functions above.</span>

<span class="sd">        2. Register the subclass in ``torchao/prototype/safetensors/safetensors_utils.py``:</span>

<span class="sd">           - Add the class name string to ``ALLOWED_TENSORS_SUBCLASSES``.</span>
<span class="sd">           - Add a ``&quot;ClassName&quot;: ClassName`` entry to ``ALLOWED_CLASSES``.</span>
<span class="sd">           - If the subclass has non-Tensor attributes with custom types (dataclasses, enums,</span>
<span class="sd">             named tuples), add those types to ``ALLOWED_CLASSES`` as well.</span>

<span class="sd">        Once registered, Hugging Face Transformers users can use ``save_pretrained`` and</span>
<span class="sd">        ``push_to_hub`` with the default ``safe_serialization=True`` option. See</span>
<span class="sd">        https://docs.pytorch.org/ao/main/eager_tutorials/torchao_hf_integration.html#saving-the-model</span>
<span class="sd">        for a full end-to-end example.</span>

<span class="sd">    Examples::</span>

<span class="sd">        class MyTensor(TorchAOBaseTensor):</span>
<span class="sd">            tensor_data_names = [&quot;a&quot;, &quot;b&quot;]</span>
<span class="sd">            tensor_attribute_names = [&quot;c&quot;, &quot;d&quot;]</span>
<span class="sd">            optional_tensor_data_names = [&quot;e&quot;, &quot;f&quot;]</span>
<span class="sd">            optional_tensor_attribute_names = [&quot;g&quot;, &quot;h&quot;]</span>


<span class="sd">            def __new__(</span>
<span class="sd">                cls,</span>
<span class="sd">                a: Tensor,</span>
<span class="sd">                b: Tensor,</span>
<span class="sd">                c: int,</span>
<span class="sd">                d: str,</span>
<span class="sd">                e: Optional[Tensor] = None,</span>
<span class="sd">                f: Optional[Tensor] = None,</span>
<span class="sd">                g: Optional[int] = None,</span>
<span class="sd">                h: Optional[int] = None,</span>
<span class="sd">            ):</span>
<span class="sd">                pass</span>

<span class="sd">            def __init__(</span>
<span class="sd">                self,</span>
<span class="sd">                a: Tensor,</span>
<span class="sd">                b: Tensor,</span>
<span class="sd">                c: int,</span>
<span class="sd">                d: str,</span>
<span class="sd">                e: Optional[Tensor] = None,</span>
<span class="sd">                f: Optional[Tensor] = None,</span>
<span class="sd">                g: Optional[int] = None,</span>
<span class="sd">                h: Optional[int] = None,</span>
<span class="sd">            ):</span>
<span class="sd">                pass</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;_ATEN_OP_TABLE&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;_TORCH_FN_TABLE&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">cls</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="bp">cls</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># define the common ops and __set_state__ for BC</span>
        <span class="c1"># if the tensor_data_names and tensor_attribute_names are defined</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span><span class="p">):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_implements_common_tensor_ops</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">__setstate__</span> <span class="o">=</span> <span class="n">_torchao_base_tensor__setstate__</span>

        <span class="c1"># inherit the torch function and dispatch implementations from direct parent classes</span>
        <span class="c1"># e.g. for `class C(B, A)`, C.__bases__ == (B, A)</span>
        <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__bases__</span><span class="p">:</span>
            <span class="n">parent_aten_table</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="s2">&quot;_ATEN_OP_TABLE&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parent_aten_table</span> <span class="ow">and</span> <span class="n">parent</span> <span class="ow">in</span> <span class="n">parent_aten_table</span><span class="p">:</span>
                <span class="c1"># shallow-copy parent&#39;s per-class op mapping into child&#39;s per-class mapping</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">_ATEN_OP_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">parent_aten_table</span><span class="p">[</span><span class="n">parent</span><span class="p">])</span>

            <span class="n">parent_torch_table</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="s2">&quot;_TORCH_FN_TABLE&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parent_torch_table</span> <span class="ow">and</span> <span class="n">parent</span> <span class="ow">in</span> <span class="n">parent_torch_table</span><span class="p">:</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">_TORCH_FN_TABLE</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">parent_torch_table</span><span class="p">[</span><span class="n">parent</span><span class="p">])</span>

    <span class="n">implements</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_implements</span><span class="p">)</span>
    <span class="n">implements_torch_function</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_implements_torch_function</span><span class="p">)</span>
    <span class="n">_implements_common_tensor_ops</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_implements_common_tensor_ops</span><span class="p">)</span>
    <span class="n">__torch_dispatch__</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_dispatch__torch_dispatch__</span><span class="p">)</span>
    <span class="n">__torch_function__</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_dispatch__torch_function__</span><span class="p">)</span>
    <span class="n">_get_to_kwargs</span> <span class="o">=</span> <span class="n">_get_to_kwargs</span>

    <span class="c1"># deprecated, will be removed later</span>
    <span class="n">register_layout</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_register_layout</span><span class="p">)</span>
    <span class="n">get_tensor_impl_constructor</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_get_tensor_impl_constructor</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">tensor_data_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">maybe_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_data_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">tensor_data_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_data_name</span><span class="p">)</span>

            <span class="n">attr_dict</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">attr_dict</span> <span class="o">=</span> <span class="n">attr_dict</span> <span class="o">|</span> <span class="p">{</span>
                    <span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">}</span>

            <span class="k">return</span> <span class="n">tensor_data_names</span><span class="p">,</span> <span class="n">attr_dict</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses should implement __tensor_flatten__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class before using it&quot;</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span><span class="p">):</span>
            <span class="n">required_tensors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">tensor_data_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">tensor_data_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensor_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="n">optional_tensor_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">tensor_data_name</span><span class="p">:</span> <span class="n">tensor_data_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tensor_data_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">optional_tensor_data_names</span>
                <span class="p">}</span>

            <span class="n">required_attributes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">tensor_attributes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">]</span>
            <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">name</span><span class="p">:</span> <span class="n">tensor_attributes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">}</span>

            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
                <span class="o">*</span><span class="n">required_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">required_attributes</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_tensor_dict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_attribute_dict</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses should implement __tensor_unflatten__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class before using it&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">required_tensors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">fn</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span>
            <span class="p">]</span>
            <span class="n">optional_tensor_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">maybe_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_data_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">maybe_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">optional_tensor_dict</span><span class="p">[</span><span class="n">tensor_data_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">optional_tensor_dict</span><span class="p">[</span><span class="n">tensor_data_name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">required_attributes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span>
            <span class="p">]</span>
            <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="n">optional_attribute_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">attr_name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span>
                <span class="p">}</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                <span class="o">*</span><span class="n">required_tensors</span><span class="p">,</span>
                <span class="o">*</span><span class="n">required_attributes</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_tensor_dict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optional_attribute_dict</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses should implement _apply_fn_to_data or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_data_names&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tensor_attribute_names&quot;</span>
        <span class="p">):</span>
            <span class="n">repr_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="c1"># required tensor data</span>
            <span class="n">repr_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_data_names</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="n">repr_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_data_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_data_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="c1"># required attributes</span>
            <span class="k">for</span> <span class="n">tensor_attribute_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_attribute_names</span><span class="p">:</span>
                <span class="n">repr_str</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_attribute_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_attribute_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1"># optional tensor data</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_data_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_data_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_data_names</span><span class="p">:</span>
                    <span class="n">repr_str</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_data_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_data_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

            <span class="c1"># optional tensor attributes</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;optional_tensor_attribute_names&quot;</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">tensor_attribute_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optional_tensor_attribute_names</span><span class="p">:</span>
                    <span class="n">repr_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;, </span><span class="si">{</span><span class="n">tensor_attribute_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_attribute_name</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">repr_str</span><span class="si">}</span><span class="s2">)&quot;</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Subclasses must implement __repr__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="TorchAOBaseTensor.get_layout">
<a class="viewcode-back" href="../../api_reference/generated/torchao.utils.TorchAOBaseTensor.html#torchao.utils.TorchAOBaseTensor.get_layout">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: 0.15.1</span>
<span class="sd">           This method is deprecated as of version 0.15.1 since it&#39;s</span>
<span class="sd">           part of the older tensor subclass development stack,</span>
<span class="sd">           for information about new dev stack, please check</span>
<span class="sd">           https://docs.pytorch.org/ao/main/quantization_overview.html</span>
<span class="sd">           and https://docs.pytorch.org/ao/main/contributor_guide.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_layout&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span></div>
</div>



<span class="k">def</span><span class="w"> </span><span class="nf">fill_defaults</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">defaults_tail</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __torch_dispatch__ doesn&#39;t guarantee the number of arguments you are</span>
<span class="sd">    passed (e.g., defaulted arguments are not passed); but usually it is</span>
<span class="sd">    convenient to pad out the arguments list with defaults.  This function</span>
<span class="sd">    helps you do that.</span>
<span class="sd">    Args:</span>
<span class="sd">        args: the list of positional arguments passed to __torch_dispatch__</span>
<span class="sd">        n: the number of arguments you are expecting to get</span>
<span class="sd">        defaults_tail: default values for the arguments, starting from the</span>
<span class="sd">            end of the list</span>
<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; fill_defaults([1, 2, 3], 5, [3, 4, 5])</span>
<span class="sd">        [1, 2, 3, 4, 5]</span>
<span class="sd">        &gt;&gt;&gt; fill_defaults([1, 2, 3], 5, [None, None, None])</span>
<span class="sd">        [1, 2, 3, None, None]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">defaults_tail</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;not enough defaults to fill arguments&quot;</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">defaults_tail</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">defaults_tail</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">r</span>


<span class="c1"># Supported AMD GPU Models and their LLVM gfx Codes:</span>
<span class="c1">#</span>
<span class="c1"># | AMD GPU Model | LLVM gfx Code          |</span>
<span class="c1"># |---------------|------------------------|</span>
<span class="c1"># | Navi4         | gfx1200, gfx1201       |</span>
<span class="c1"># | MI300X        | gfx940, gfx941, gfx942 |</span>
<span class="c1"># | MI350         | gfx950                 |</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_ROCM</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_MI300</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_ROCM</span><span class="p">():</span>
        <span class="n">mxArchName</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gfx940&quot;</span><span class="p">,</span> <span class="s2">&quot;gfx941&quot;</span><span class="p">,</span> <span class="s2">&quot;gfx942&quot;</span><span class="p">]</span>
        <span class="n">archName</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">gcnArchName</span>
        <span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">mxArchName</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">archName</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_MI350</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_ROCM</span><span class="p">():</span>
        <span class="n">archName</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">gcnArchName</span>
        <span class="k">if</span> <span class="s2">&quot;gfx950&quot;</span> <span class="ow">in</span> <span class="n">archName</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_Navi4</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_ROCM</span><span class="p">():</span>
        <span class="n">archName</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">gcnArchName</span>
        <span class="k">if</span> <span class="s2">&quot;gfx1200&quot;</span> <span class="ow">or</span> <span class="s2">&quot;gfx1201&quot;</span> <span class="ow">in</span> <span class="n">archName</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_version</span><span class="p">(</span><span class="n">major</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">minor</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if the CUDA version is exactly major.minor&quot;&quot;&quot;</span>
    <span class="n">is_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_cuda</span> <span class="k">else</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_at_least_89</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_at_least_90</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>


<span class="c1"># TODO(future PR): rename to 8_9, 9_0, 10_0 instead of 89, 10, 100</span>
<span class="k">def</span><span class="w"> </span><span class="nf">is_sm_at_least_100</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_cuda_version_at_least</span><span class="p">(</span><span class="n">major</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">minor</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="n">cuda_version</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span>
    <span class="k">if</span> <span class="n">cuda_version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="n">cuda_major</span><span class="p">,</span> <span class="n">cuda_minor</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">cuda_version</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">cuda_major</span><span class="p">,</span> <span class="n">cuda_minor</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">major</span><span class="p">,</span> <span class="n">minor</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_cpu_version</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;2.6.0&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
    <span class="k">return</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="ow">and</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_xpu_version</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;2.8.0&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
    <span class="k">return</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span> <span class="ow">and</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">ceil_div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">b</span>


<span class="k">def</span><span class="w"> </span><span class="nf">round_up</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">ceil_div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_package_at_least</span><span class="p">(</span><span class="n">package_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">min_version</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">package_exists</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="n">package_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">package_exists</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="n">version</span><span class="p">(</span><span class="n">package_name</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">min_version</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_mslk_available</span><span class="p">():</span>
    <span class="n">has_mslk</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;mslk&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">is_fbcode</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_mslk</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">mslk</span>  <span class="c1"># noqa: F401</span>

    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_is_flashinfer_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="c1"># flashinfer-python</span>
        <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;flashinfer&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="c1"># apache-tvm-ffi</span>
        <span class="ow">and</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;tvm_ffi&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="c1"># nvidia-ml-py</span>
        <span class="ow">and</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;pynvml&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="ow">or</span> <span class="n">is_fbcode</span><span class="p">()</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DummyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is used because the TorchAO quantization functions tend to operate on modules so to apply the transform to a tensor, we can load a</span>
<span class="sd">    DummyModule with the target tensor and then apply the transformation to the module and then extract the transformed tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
</pre></div>

                </article>
              
</div>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.utils",
       "headline": "torchao.utils",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/_modules/torchao/utils.html",
       "articleBody": "Source code for torchao.utils # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD 3-Clause license found in the # LICENSE file in the root directory of this source tree. import functools import importlib import itertools import re import time from functools import reduce from importlib.metadata import version from math import gcd from typing import Any, Callable, Optional import torch import torch.nn.utils.parametrize as parametrize from torch.utils._python_dispatch import return_and_correct_aliasing __all__ = [ \"benchmark_model\", \"profiler_runner\", \"get_available_devices\", \"get_compute_capability\", \"benchmark_torch_function_in_microseconds\", \"find_multiple\", \"_register_custom_op\", \"get_model_size_in_bytes\", \"unwrap_tensor_subclass\", \"TorchAOBaseTensor\", \"is_cuda_version_at_least\", \"is_MI300\", \"is_sm_at_least_89\", \"is_sm_at_least_90\", \"is_sm_at_least_100\", \"is_package_at_least\", \"DummyModule\", \"register_as_pytree_constant\", ] def register_as_pytree_constant(cls): \"\"\"Decorator to register a class as a pytree constant for dynamo non-strict trace mode.\"\"\" torch.utils._pytree.register_constant(cls) return cls # Referenced from: https://github.com/pytorch/pytorch/blob/9105d54c6b37099575c0059ef274c86c4dc80c57/torch/ao/quantization/utils.py#L711 @functools.cache def _assert_and_get_unique_device(module: torch.nn.Module) -\u003e Any: \"\"\" Returns the unique device for a module, or None if no device is found. Throws an error if multiple devices are detected. \"\"\" devices = {p.device for p in module.parameters()} | { p.device for p in module.buffers() } assert len(devices) \u003c= 1, ( \"prepare only works with cpu or single-device CUDA modules, \" f\"but got devices {devices}\" ) device = next(iter(devices)) if len(devices) \u003e 0 else None return device def benchmark_model(model, num_runs, args=(), kwargs=None, device_type=None): \"\"\"Benchmark model runs with `args` and `kwargs` both are optional\"\"\" if kwargs is None: kwargs = {} if device_type is None: assert isinstance(model, torch.nn.Module), ( \"Expecting `model` to be torch.nn.Module if device_type is not provided\" ) device_type = _assert_and_get_unique_device(model).type if device_type == \"cuda\": torch.cuda.synchronize() start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) start_event.record() # benchmark for _ in range(num_runs): with torch.autograd.profiler.record_function(\"timed region\"): model(*args, **kwargs) end_event.record() torch.cuda.synchronize() return start_event.elapsed_time(end_event) / num_runs elif device_type == \"mps\": torch.mps.synchronize() start_event = torch.mps.event.Event(enable_timing=True) end_event = torch.mps.event.Event(enable_timing=True) start_event.record() # benchmark for _ in range(num_runs): with torch.autograd.profiler.record_function(\"timed region\"): model(*args, **kwargs) end_event.record() torch.mps.synchronize() return start_event.elapsed_time(end_event) / num_runs elif device_type == \"cpu\": torch.cpu.synchronize() start_time = time.time() # benchmark for _ in range(num_runs): with torch.autograd.profiler.record_function(\"timed region\"): model(*args, **kwargs) end_time = time.time() torch.cpu.synchronize() average_time_per_run = (end_time - start_time) / num_runs return average_time_per_run elif device_type == \"xpu\": torch.xpu.synchronize() start_event = torch.xpu.Event(enable_timing=True) end_event = torch.xpu.Event(enable_timing=True) start_event.record() # benchmark for _ in range(num_runs): with torch.autograd.profiler.record_function(\"timed region\"): model(*args, **kwargs) end_event.record() torch.xpu.synchronize() return start_event.elapsed_time(end_event) / num_runs def profiler_runner(path, fn, *args, **kwargs): with torch.profiler.profile( activities=[ torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA, ], record_shapes=True, ) as prof: result = fn(*args, **kwargs) prof.export_chrome_trace(path) return result def get_available_devices(): devices = [\"cpu\"] if torch.cuda.is_available(): devices.append(\"cuda\") elif torch.xpu.is_available(): devices.append(\"xpu\") if torch.mps.is_available(): devices.append(\"mps\") return devices def get_current_accelerator_device(): assert torch.accelerator.is_available() return torch.accelerator.current_accelerator() def get_compute_capability(): if torch.cuda.is_available(): capability = torch.cuda.get_device_capability() return float(f\"{capability[0]}.{capability[1]}\") return 0.0 def compute_max_diff(output: torch.Tensor, output_ref: torch.Tensor) -\u003e torch.Tensor: return torch.mean(torch.abs(output - output_ref)) / torch.mean( torch.abs(output_ref) ) def benchmark_torch_function_in_microseconds(f, *args, **kwargs): import torch.utils.benchmark as benchmark # this avoids importing numpy when torchao module is loaded # Manual warmup f(*args, **kwargs) f(*args, **kwargs) t0 = benchmark.Timer( stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}, # noqa: E501 ) measurement = t0.blocked_autorange() return measurement.mean * 1e6 def find_multiple(n: int, *args: int) -\u003e int: k: int = reduce(lambda x, y: x * y // gcd(x, y), args + (1,)) # type: ignore[9] if n % k == 0: return n return n + k - (n % k) def _register_custom_op(lib, inductor_decomposed=True): \"\"\"This decorator is used to preserve some high level operators for torch.export.export while still allow them to be decomposed for inductor path requirement: make sure `fn.__name__[1:]` is the operator name you want to register NOTE: This should be applied at the top, after all other decorators have been applied NOTE: We haven\u0027t tested the case when `fn` accepts tensor subclass instance as input, e.g. uint4 tensor subclass instance, and we\u0027ll probably need to figure out what would make sense for downstream system (like executorch) to accept as well Example: lib = torch.library.Library(\"my_namespace\u0027, \"FRAGMENT\") register_custom_op = _register_custom_op(lib) @register_custom_op def _the_op_that_needs_to_be_preserved(...) ... # after this, `_the_op_that_needs_to_be_preserved` will be preserved as # torch.ops.my_namespace.the_op_that_needs_to_be_preserved operator after # torch.export.export \"\"\" from torch._inductor.decomposition import register_decomposition dispatch_key = ( \"CompositeImplicitAutograd\" if inductor_decomposed else \"CompositeExplicitAutograd\" ) def decorator(fn): from torch._library.infer_schema import infer_schema assert not any(c in fn.__name__ for c in \".\u003c\u003e\"), ( f\"Expecting op to be defined in normal functions, not lambda or local: {fn.__name__}\" ) op_name = fn.__name__ if op_name[0] == \"_\": op_name = op_name[1:] schema = op_name + infer_schema(fn, mutates_args={}) lib.define(schema) lib.impl(op_name, fn, dispatch_key) lib_namespace = lib.ns op = getattr(getattr(torch.ops, lib_namespace), op_name) if inductor_decomposed: register_decomposition([op])(fn) return op return decorator def _register_meta_op(lib, op_name): def decorator(fn): op = lib.impl(op_name, fn, \"Meta\") return op return decorator def get_model_size_in_bytes(model, ignore_embeddings=False): \"\"\" Returns the model size in bytes. The option to ignore embeddings is useful for models with disproportionately large embeddings compared to other model parameters that get quantized/sparsified. \"\"\" def flat_size(tensor): if hasattr(tensor, \"__tensor_flatten__\"): size = 0 # 0th element is a list of attributes that # hold tensors for attr_name in tensor.__tensor_flatten__()[0]: sub_tensor = getattr(tensor, attr_name) size += flat_size(sub_tensor) return size else: return tensor.numel() * tensor.element_size() model_size = 0 for name, child in model.named_children(): if not (isinstance(child, torch.nn.Embedding) and ignore_embeddings): for p in itertools.chain( child.parameters(recurse=False), child.buffers(recurse=False) ): model_size += flat_size(p) model_size += get_model_size_in_bytes(child, ignore_embeddings) return model_size class UnwrapTensorSubclass(torch.nn.Module): def forward(self, *tensors): todo = list(tensors) for tp, meta, inner_tensors in reversed(self.rebuild_stack): nb_tensor = len(inner_tensors) inner_tensors = {a: b for a, b in zip(inner_tensors, todo[-nb_tensor:])} todo = todo[nb_tensor:] rebuilt = tp.__tensor_unflatten__(inner_tensors, meta, None, None) todo.append(rebuilt) assert len(todo) == 1 return todo[0] def right_inverse(self, tensor): assert type(tensor) is not torch.Tensor rebuild_stack = [] plain_tensors = [] todo = [tensor] while todo: obj = todo.pop() inner_tensors, metadata = obj.__tensor_flatten__() rebuild_stack.append((type(obj), metadata, inner_tensors)) for attr_name in inner_tensors: val = getattr(obj, attr_name) if type(val) is torch.Tensor: plain_tensors.append(val) else: assert isinstance(val, torch.Tensor) todo.append(val) self.rebuild_stack = rebuild_stack return plain_tensors def unwrap_tensor_subclass(model, filter_fn=None): \"\"\"Unwraps (nested) tensor subclass in the model to plain tensors This is a workaround to make a model with tensor subclass to work with `torch.export.export` and `torch.aot_compile`, we hope this can be integrated into compile stack soon tracking issue: https://github.com/pytorch/ao/issues/345 \"\"\" for name, child in model.named_children(): # make sure child.weight is a tensor subclass if ( ( isinstance(child, torch.nn.Linear) or isinstance(child, torch.nn.Embedding) ) and hasattr(child, \"weight\") and type(child.weight) is not torch.Tensor and type(child.weight) is not torch.nn.Parameter and isinstance(child.weight, torch.Tensor) and issubclass(type(child.weight), torch.Tensor) and isinstance(child.weight, TorchAOBaseTensor) and not parametrize.is_parametrized(child) ): parametrize.register_parametrization( child, \"weight\", UnwrapTensorSubclass() ) unwrap_tensor_subclass(child) return model def _is_float8_type(dtype: torch.dtype) -\u003e bool: fp8_types = { torch.float8_e4m3fn, torch.float8_e4m3fnuz, torch.float8_e5m2, torch.float8_e5m2fnuz, } return dtype in fp8_types def parse_version(version_string): \"\"\" Parse version string representing pre-release with -1 Examples: \"2.5.0.dev20240708+cu121\" -\u003e [2, 5, -1], \"2.5.0\" -\u003e [2, 5, 0] \"\"\" # Check for pre-release indicators is_prerelease = bool(re.search(r\"(git|dev)\", version_string)) match = re.match(r\"(\\d+)\\.(\\d+)\\.(\\d+)\", version_string) if match: major, minor, patch = map(int, match.groups()) if is_prerelease: patch = -1 return [major, minor, patch] else: raise ValueError(f\"Invalid version string format: {version_string}\") def is_fbcode(): return not hasattr(torch.version, \"git_version\") def torch_version_at_least(min_version): if is_fbcode(): return True # Parser for local identifiers return parse_version(torch.__version__) \u003e= parse_version(min_version) \"\"\" Helper function for implementing aten op or torch function dispatch and dispatching to these implementations. \"\"\" def _implements(cls, aten_ops): \"\"\"Decorator for implementing aten ops like `torch.ops.aten.linear.default` for tensor subclass, the implemented functions are called in ``__torch_dispatch__`` callback for ``torch.Tensor`` subclasses Examples:: implements = MyTensor.implements @implements(torch.ops.aten.linear.default): def _(func, types, args, kwargs): ... \"\"\" if not hasattr(cls, \"_ATEN_OP_TABLE\"): cls._ATEN_OP_TABLE = {} if cls not in cls._ATEN_OP_TABLE: cls._ATEN_OP_TABLE[cls] = {} if not isinstance(aten_ops, (list, tuple)): aten_ops = [aten_ops] def decorator(func): for op in aten_ops: @functools.wraps(func) def wrapper(f, types, args, kwargs, _func=func): return _func(f, types, args, kwargs) cls._ATEN_OP_TABLE[cls][op] = wrapper return func return decorator def _implements_torch_function(cls, torch_fns): \"\"\"Decorator for implementing torch functions / ops like ``torch.nn.functional.linear``, ``torch.Tensor.t`` for the tensor subclass the implemented functions are called in ``__torch_function__`` callback for ``torch.Tensor`` subclasses Examples:: implements_torch_function = MyTensor.implements_torch_function @implements_torch_function(torch.nn.functional.linear): def _(func, types, args, kwargs): ... \"\"\" if not hasattr(cls, \"_TORCH_FN_TABLE\"): cls._TORCH_FN_TABLE = {} if cls not in cls._TORCH_FN_TABLE: cls._TORCH_FN_TABLE[cls] = {} if not isinstance(torch_fns, (list, tuple)): torch_fns = [torch_fns] def decorator(func): for fn in torch_fns: @functools.wraps(func) def wrapper(f, types, args, kwargs, _func=func): return _func(f, types, args, kwargs) cls._TORCH_FN_TABLE[cls][fn] = wrapper return func return decorator def _implements_common_tensor_ops(cls): implements = cls.implements implements_torch_function = cls.implements_torch_function aten = torch.ops.aten @implements(torch.ops.aten.to.dtype_layout) def _(func, types, args, kwargs): # only support kwargs for now assert len(args) == 1 self = args[0] # only support dtype, layout, and device for now for k in kwargs.keys(): assert k in [\"dtype\", \"layout\", \"device\"] # only support same dtype and layout # different dtype and layout has undefined behavior if \"dtype\" in kwargs: assert kwargs[\"dtype\"] == self.dtype if \"layout\" in kwargs: assert kwargs[\"layout\"] == self.layout # if device is the same, treat this like a no-op device = kwargs.get(\"device\") if device == self.device: return self new_tensor = args[0]._apply_fn_to_data(lambda x: func(x, device=device)) return return_and_correct_aliasing(func, args, kwargs, new_tensor) # This is called during _apply() to see if we can shallow # copy the content of one tensor into another. For now, # we only allow shallow copy if both tensors are of the # same type and have the same shape. @implements_torch_function(torch._has_compatible_shallow_copy_type) def _(func, types, args, kwargs): assert len(args) == 2 return type(args[0]) == type(args[1]) and args[0].shape == args[1].shape @implements_torch_function( [ torch.Tensor.contiguous, ] ) def _(func, types, args, kwargs): return args[0]._apply_fn_to_data(lambda x: func(x, *args[1:], **kwargs)) @implements( [ aten.detach.default, aten.clone.default, aten.alias.default, aten.contiguous.default, ] ) def _(func, types, args, kwargs): return return_and_correct_aliasing( func, args, kwargs, args[0]._apply_fn_to_data(lambda x: func(x, *args[1:], **kwargs)), ) def _same_metadata(self: TorchAOBaseTensor, src: TorchAOBaseTensor) -\u003e bool: _tensor_shape_match = all( getattr(self, t_name).shape == getattr(src, t_name).shape for t_name in self.tensor_data_names ) _optional_tensor_shape_match = True if hasattr(self, \"optional_tensor_data_names\"): # either both are None or both are not Tensors and the shape match _optional_tensor_shape_match = all( ( getattr(self, t_name).shape == getattr(src, t_name).shape if getattr(self, t_name) is not None else getattr(src, t_name) is None ) for t_name in self.optional_tensor_data_names ) _attr_match = all( getattr(self, a_name) == getattr(src, a_name) for a_name in self.tensor_attribute_names ) _optional_attr_match = True if hasattr(self, \"optional_tensor_attribute_names\"): _optional_attr_match = all( getattr(self, a_name) == getattr(src, a_name) for a_name in self.optional_tensor_attribute_names ) return ( type(self) == type(src) and self.shape == src.shape and _tensor_shape_match and _optional_tensor_shape_match and _attr_match and _optional_attr_match ) @implements(aten.copy_.default) def _(func, types, args, kwargs): self = args[0] src = args[1] if _same_metadata(self, src): self_tensors = self.__tensor_flatten__()[0] for tensor_name in self_tensors: getattr(self, tensor_name).copy_(getattr(src, tensor_name)) return raise ValueError( f\"Not supported args for copy_ due to metadata mismatch: {args[0], args[1]}\" ) @implements(aten._to_copy.default) def _(func, types, args, kwargs): self = args[0] if hasattr(self, \"tensor_data_names\") and hasattr( self, \"tensor_attribute_names\" ): kwargs = self._get_to_kwargs(*args[1:], **kwargs) device = kwargs.pop(\"device\") tensors = [ getattr(self, name).to(device) for name in self.tensor_data_names ] optional_tensors = [] if hasattr(self, \"optional_tensor_data_names\"): for tensor_data_name in self.optional_tensor_data_names: maybe_tensor = getattr(self, tensor_data_name) if maybe_tensor is not None: optional_tensors.append(maybe_tensor.to(device)) else: optional_tensors.append(None) # change device tensor_attributes = [ getattr(self, attr_name) if attr_name != \"device\" else device for attr_name in self.tensor_attribute_names ] optional_tensor_attributes = [] if hasattr(self, \"optional_tensor_attribute_names\"): optional_tensor_attributes = [ getattr(self, attr_name) if attr_name != \"device\" else device for attr_name in self.optional_tensor_attribute_names ] t = self.__class__( *tensors, *tensor_attributes, *optional_tensors, *optional_tensor_attributes, ) return return_and_correct_aliasing(func, args, kwargs, t) raise NotImplementedError( \"Subclasses must implement `aten._to_copy.default` or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it\" ) def _torchao_base_tensor__setstate__(self, state): assert hasattr(self, \"tensor_data_names\") and hasattr( self, \"tensor_attribute_names\" ) torch._utils._set_obj_state(self, state) for optional_tensor_data_name in getattr(self, \"optional_tensor_data_names\", []): if optional_tensor_data_name not in self.__dict__ and not hasattr( self, optional_tensor_data_name ): setattr(self, optional_tensor_data_name, None) for optional_tensor_attribute_name in getattr( self, \"optional_tensor_attribute_names\", [] ): if optional_tensor_attribute_name not in self.__dict__ and not hasattr( self, optional_tensor_attribute_name ): setattr(self, optional_tensor_attribute_name, None) def _dispatch__torch_function__(cls, func, types, args=(), kwargs=None): \"\"\"Use this util function for a common `__torch_function__` implementation that dispatches to ops/functions registered with `_implements` class MyTensor(torch.Tensor): ... __torch_function__ = classmethod(_dispatch__torch_function__) \"\"\" kwargs = {} if kwargs is None else kwargs if ( hasattr(cls, \"_TORCH_FN_TABLE\") and cls in cls._TORCH_FN_TABLE and func in cls._TORCH_FN_TABLE[cls] ): return cls._TORCH_FN_TABLE[cls][func](func, types, args, kwargs) with torch._C.DisableTorchFunctionSubclass(): return func(*args, **kwargs) def _dispatch__torch_dispatch__(cls, func, types, args, kwargs): \"\"\"Use this util function for a common `__torch_dispatch__` implementation that dispatches to ops/functions registered with `_implements` class MyTensor(torch.Tensor): ... __torch_dispatch__ = classmethod(_dispatch__torch_dispatch__) \"\"\" if ( hasattr(cls, \"_ATEN_OP_TABLE\") and cls in cls._ATEN_OP_TABLE and func in cls._ATEN_OP_TABLE[cls] ): return cls._ATEN_OP_TABLE[cls][func](func, types, args, kwargs) arg_types = tuple(type(arg) for arg in args) kwarg_types = {k: type(arg) for k, arg in kwargs.items()} raise NotImplementedError( f\"{cls.__name__} dispatch: attempting to run unimplemented operator/function: {func=}, {types=}, {arg_types=}, {kwarg_types=}\" ) def _register_layout(tensor_class: Callable, layout_class: Callable): \"\"\" .. deprecated:: 0.15.1 This method is deprecated as of version 0.15.1 since it\u0027s part of the older tensor subclass development stack, for information about new dev stack, please check https://docs.pytorch.org/ao/main/quantization_overview.html and https://docs.pytorch.org/ao/main/contributor_guide.html Helper function for layout registrations, this is used to implement register_layout decorator for each tensor subclass, see aqt.py for example usage Args: tensor_class: Tensor subclass type layout_class: the class type of subclass of `Layout`, e.g. `PlainLayout` Returns: a decorator that registers the tensor impl constructor in the table \"\"\" # tensor_class._LAYOUT_CONSTRUCTOR_TABLE is a map from layout_class like TensorCoreTiledLayout # to tensor_impl class constructor like TensorCoreTiledAQTTensorImpl.from_plain that can construct a tensor_impl # from plain data like (quantized, unpacked) `data`, `scale`, `zero_point` if not hasattr(tensor_class, \"_LAYOUT_CONSTRUCTOR_TABLE\"): tensor_class._LAYOUT_CONSTRUCTOR_TABLE = {} def decorator(tensor_impl_class): tensor_class._LAYOUT_CONSTRUCTOR_TABLE[layout_class] = ( tensor_impl_class.from_plain ) # Allow serialization to work for models uses this tensor impl subclass torch.serialization.add_safe_globals([layout_class, tensor_impl_class]) return tensor_impl_class return decorator def _get_tensor_impl_constructor( tensor_class: Callable, layout_class: Callable ) -\u003e Callable: \"\"\" .. deprecated:: 0.15.1 This method is deprecated as of version 0.15.1 since it\u0027s part of the older tensor subclass development stack, for information about new dev stack, please check https://docs.pytorch.org/ao/main/quantization_overview.html and https://docs.pytorch.org/ao/main/contributor_guide.html Get TensorImpl class constructor (TensorImplClass.from_plain) for `tensor_class` based on `layout_class` `layout_class` means the class type of subclass of `Layout`, e.g. `PlainLayout` Args: tensor_class: Tensor subclass type layout_class: the class type of subclass of `Layout`, e.g. `PlainLayout` Returns: tensor impl subclass constructor for the layout_class \"\"\" if not hasattr(tensor_class, \"_LAYOUT_CONSTRUCTOR_TABLE\"): raise ValueError( f\"no registered tensor_impl class constructor for: {tensor_class}\" ) if layout_class not in tensor_class._LAYOUT_CONSTRUCTOR_TABLE: raise ValueError( f\"layout_name: {layout_class} is not supported yet for {tensor_class}\" ) return tensor_class._LAYOUT_CONSTRUCTOR_TABLE[layout_class] def _get_to_kwargs(self, *args, **kwargs): \"\"\"Helper function to get the device and dtype keyword args for `aten._to_copy.default` op only device and dtype are kept Returns: {\"device\": device, \"dtype\": dtype} \"\"\" # `torch._C._nn._parse_to` can\u0027t handle `layout` argument args = tuple(arg for arg in args if not isinstance(arg, torch.layout)) if \"layout\" in kwargs: kwargs.pop(\"layout\") # ignoring `non_blocking` and `memory_format` args since these are not # very useful for most of the tensor subclasses # if in the future there are use cases that need these, we\u0027d recommend # to override `_get_to_kwargs` and return these args device, dtype, _, _ = torch._C._nn._parse_to(*args, **kwargs) device = self.device if device is None else device dtype = self.dtype if dtype is None else dtype kwargs = { \"device\": device, \"dtype\": dtype, } return kwargs [docs] class TorchAOBaseTensor(torch.Tensor): r\"\"\"A util tensor subclass that provides commonly used functions new tensor subclass can inherit to get all the utility functions Attributes (defined by subclass of ``TorchAOBaseTensor``): * :attr:`tensor_data_names` (List[str]): list of names of all required tensor_data, order should match the `__init__` list of tensor subclass (optional to define to use ``TorchAOBaseTensor``, required for getting the util functions been defined (see Note section)) * :attr:`tensor_attribute_names` (List[str]): list of names of non-``Tensor`` attributes, order should match the ``__init__`` list of tensor subclass, following all the ``tensor_data_names`` arguments (optional to define to use ``TorchAOBaseTensor``, required for getting the util functions been defined (see Note section)) * :attr:`optional_tensor_data_names` (List[str]): it\u0027s not required to get the additional util functions been implemented, but this will be need if there are some optional ``Tensor`` data attributes, when defined, this will be a list of names of ``Tensor``s that can be optional * :attr:`optional_tensor_attribute_names` (List[str]): it\u0027s not required to get the additional util functions been implemented, but this will be need if there are some optional non-``Tensor`` attributes, when defined, this will be a list of names of attributes that can be optional Note: **Argument Order** Argument order in ``__init__`` and ``__new__`` of subclass of ``TorchAOBaseTensor`` should match exaclty with ``tensor_data_names`` + ``tensor_attribute_names`` + ``optional_tensor_data_names`` (if present) + ``optional_tensor_attribute_names`` (if present) Note: **How to Get Predefined Util Functions** If ``tensor_data_names`` (torch.Tensor data attribute names) and ``tensor_attribute_names`` (non-torch.Tensor attribute names) are defined, there are some additional util functions that will be added, this includes: ``__tensor_flatten__``: flattens a subclassed tensor instance, returns a ``tuple``, first element is tensor data names for valid tensor data, second element is a dict from attribute_name to non-``Tensor`` attributes ``__tensor_unflatten__``: takes a ``tensor_data_dict`` (a map from tensor name to ``Tensor``), and list of non-``Tensor`` attributes, returns a new instance of the subclassed tensor ``_apply_fn_to_data``: takes a function (``Tensor -\u003e Tensor``), applies function to all tensor data and recreate a new subclassed Tensor with the transformed tensor data ``__repr__``: the string representation of the subclassed tensor instance ``_same_metadata``: returns whether the metadata is the same between two instances of cls ``__setstate__``: when loading a serialized tensor subclass checkpoints, it sets the new optional tensor and tensor attribute that is saved in the old checkpoint to None, to maintain BC of old checkpoints when we add new optional tensor data or attributes to the tensor subclass torch function supported (``__torch_function__``): ``torch.Tensor.contiguous`` aten ops supported (``__torch_dispatch__``): ``aten.detach.default``, ``aten.clone.default``, ``aten.alias,default``, ``aten.contiguous.default``, ``aten.copy_.default``, ``aten._to_copy.default`` (enables ``t.to``) Note: **Subclassing and Op Inheritance** Subclasses of ``TorchAOBaseTensor`` automatically inherit aten op (``__torch_dispatch__``) and torch function (``__torch_function__``) implementations from their parent classes. Each subclass gets its own independent dispatch tables, so registering a new op on a child does not affect the parent, and vice versa. A child class can override an inherited op by registering its own implementation with ``@ChildClass.implements(...)``. If no override is provided, the parent\u0027s implementation is used automatically. For multiple inheritance (e.g., ``class C(B, A)``), ops are inherited from all parents following Python\u0027s MRO (Method Resolution Order), with later bases taking priority. Example:: class Parent(TorchAOBaseTensor): tensor_data_names = [\"qdata\"] tensor_attribute_names = [\"attr\"] def __new__(cls, qdata, attr): r = torch.Tensor._make_wrapper_subclass(cls, qdata.shape) r.qdata = qdata r.attr = attr return r def __init__(self, qdata, attr): pass @Parent.implements([torch.ops.aten.cat.default]) def parent_cat(func, types, args, kwargs): # parent implementation ... # Child inherits Parent\u0027s aten.cat implementation automatically class Child(Parent): tensor_data_names = [\"qdata\"] tensor_attribute_names = [\"attr\"] # Optionally override an inherited op: @Child.implements([torch.ops.aten.cat.default]) def child_cat(func, types, args, kwargs): # child-specific implementation ... Note: **Safetensors Support** ``TorchAOBaseTensor`` subclasses can be serialized to and loaded from the `safetensors \u003chttps://huggingface.co/docs/safetensors\u003e`_ format. Since safetensors only stores plain ``torch.Tensor`` objects, the serialization layer (in ``torchao.prototype.safetensors``) decomposes each tensor subclass into its constituent plain tensors (from ``tensor_data_names``/``optional_tensor_data_names``) plus JSON metadata (from ``tensor_attribute_names``/``optional_tensor_attribute_names``), and reconstructs the subclass on load. To add safetensors support for a new ``TorchAOBaseTensor`` subclass: 1. Define ``tensor_data_names``, ``tensor_attribute_names`` (and optionally ``optional_tensor_data_names``, ``optional_tensor_attribute_names``) on the subclass, which is already required for the other utility functions above. 2. Register the subclass in ``torchao/prototype/safetensors/safetensors_utils.py``: - Add the class name string to ``ALLOWED_TENSORS_SUBCLASSES``. - Add a ``\"ClassName\": ClassName`` entry to ``ALLOWED_CLASSES``. - If the subclass has non-Tensor attributes with custom types (dataclasses, enums, named tuples), add those types to ``ALLOWED_CLASSES`` as well. Once registered, Hugging Face Transformers users can use ``save_pretrained`` and ``push_to_hub`` with the default ``safe_serialization=True`` option. See https://docs.pytorch.org/ao/main/eager_tutorials/torchao_hf_integration.html#saving-the-model for a full end-to-end example. Examples:: class MyTensor(TorchAOBaseTensor): tensor_data_names = [\"a\", \"b\"] tensor_attribute_names = [\"c\", \"d\"] optional_tensor_data_names = [\"e\", \"f\"] optional_tensor_attribute_names = [\"g\", \"h\"] def __new__( cls, a: Tensor, b: Tensor, c: int, d: str, e: Optional[Tensor] = None, f: Optional[Tensor] = None, g: Optional[int] = None, h: Optional[int] = None, ): pass def __init__( self, a: Tensor, b: Tensor, c: int, d: str, e: Optional[Tensor] = None, f: Optional[Tensor] = None, g: Optional[int] = None, h: Optional[int] = None, ): pass \"\"\" @classmethod def __init_subclass__(cls, **kwargs): if \"_ATEN_OP_TABLE\" not in cls.__dict__: cls._ATEN_OP_TABLE = {} if \"_TORCH_FN_TABLE\" not in cls.__dict__: cls._TORCH_FN_TABLE = {} if cls not in cls._ATEN_OP_TABLE: cls._ATEN_OP_TABLE[cls] = {} if cls not in cls._TORCH_FN_TABLE: cls._TORCH_FN_TABLE[cls] = {} # define the common ops and __set_state__ for BC # if the tensor_data_names and tensor_attribute_names are defined if hasattr(cls, \"tensor_data_names\") and hasattr(cls, \"tensor_attribute_names\"): cls._implements_common_tensor_ops() cls.__setstate__ = _torchao_base_tensor__setstate__ # inherit the torch function and dispatch implementations from direct parent classes # e.g. for `class C(B, A)`, C.__bases__ == (B, A) for parent in cls.__bases__: parent_aten_table = getattr(parent, \"_ATEN_OP_TABLE\", None) if parent_aten_table and parent in parent_aten_table: # shallow-copy parent\u0027s per-class op mapping into child\u0027s per-class mapping cls._ATEN_OP_TABLE[cls].update(parent_aten_table[parent]) parent_torch_table = getattr(parent, \"_TORCH_FN_TABLE\", None) if parent_torch_table and parent in parent_torch_table: cls._TORCH_FN_TABLE[cls].update(parent_torch_table[parent]) implements = classmethod(_implements) implements_torch_function = classmethod(_implements_torch_function) _implements_common_tensor_ops = classmethod(_implements_common_tensor_ops) __torch_dispatch__ = classmethod(_dispatch__torch_dispatch__) __torch_function__ = classmethod(_dispatch__torch_function__) _get_to_kwargs = _get_to_kwargs # deprecated, will be removed later register_layout = classmethod(_register_layout) get_tensor_impl_constructor = classmethod(_get_tensor_impl_constructor) def __init__(self, *args, **kwargs): torch._C._log_api_usage_once(str(type(self))) def __tensor_flatten__(self): if hasattr(self, \"tensor_data_names\") and hasattr( self, \"tensor_attribute_names\" ): tensor_data_names = self.tensor_data_names.copy() if hasattr(self, \"optional_tensor_data_names\"): for tensor_data_name in self.optional_tensor_data_names: maybe_tensor = getattr(self, tensor_data_name) if maybe_tensor is not None: tensor_data_names.append(tensor_data_name) attr_dict = { attr: getattr(self, attr) for attr in self.tensor_attribute_names } if hasattr(self, \"optional_tensor_attribute_names\"): attr_dict = attr_dict | { attr: getattr(self, attr) for attr in self.optional_tensor_attribute_names } return tensor_data_names, attr_dict raise NotImplementedError( \"Subclasses should implement __tensor_flatten__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class before using it\" ) @classmethod def __tensor_unflatten__( cls, tensor_data_dict, tensor_attributes, outer_size, outer_stride ): if hasattr(cls, \"tensor_data_names\") and hasattr(cls, \"tensor_attribute_names\"): required_tensors = [ tensor_data_dict[name] for name in cls.tensor_data_names ] optional_tensor_dict = {} if hasattr(cls, \"optional_tensor_data_names\"): optional_tensor_dict = { tensor_data_name: tensor_data_dict.get(tensor_data_name, None) for tensor_data_name in cls.optional_tensor_data_names } required_attributes = [ tensor_attributes[name] for name in cls.tensor_attribute_names ] optional_attribute_dict = {} if hasattr(cls, \"optional_tensor_attribute_names\"): optional_attribute_dict = { name: tensor_attributes[name] for name in cls.optional_tensor_attribute_names } return cls( *required_tensors, *required_attributes, **optional_tensor_dict, **optional_attribute_dict, ) raise NotImplementedError( \"Subclasses should implement __tensor_unflatten__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class before using it\" ) def _apply_fn_to_data(self, fn): if hasattr(self, \"tensor_data_names\") and hasattr( self, \"tensor_attribute_names\" ): required_tensors = [ fn(getattr(self, attr)) for attr in self.tensor_data_names ] optional_tensor_dict = {} if hasattr(self, \"optional_tensor_data_names\"): for tensor_data_name in self.optional_tensor_data_names: maybe_tensor = getattr(self, tensor_data_name) if maybe_tensor is not None: optional_tensor_dict[tensor_data_name] = fn(maybe_tensor) else: optional_tensor_dict[tensor_data_name] = None required_attributes = [ getattr(self, attr) for attr in self.tensor_attribute_names ] optional_attribute_dict = {} if hasattr(self, \"optional_tensor_attribute_names\"): optional_attribute_dict = { attr_name: getattr(self, attr_name) for attr_name in self.optional_tensor_attribute_names } return self.__class__( *required_tensors, *required_attributes, **optional_tensor_dict, **optional_attribute_dict, ) raise NotImplementedError( \"Subclasses should implement _apply_fn_to_data or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it\" ) def __repr__(self): if hasattr(self, \"tensor_data_names\") and hasattr( self, \"tensor_attribute_names\" ): repr_str = \"\" # required tensor data repr_str += f\"{self.tensor_data_names[0]}={getattr(self, self.tensor_data_names[0])}\" for tensor_data_name in self.tensor_data_names[1:]: repr_str += f\", {tensor_data_name}={getattr(self, tensor_data_name)}\" # required attributes for tensor_attribute_name in self.tensor_attribute_names: repr_str += ( f\", {tensor_attribute_name}={getattr(self, tensor_attribute_name)}\" ) # optional tensor data if hasattr(self, \"optional_tensor_data_names\"): for tensor_data_name in self.optional_tensor_data_names: repr_str += ( f\", {tensor_data_name}={getattr(self, tensor_data_name)}\" ) # optional tensor attributes if hasattr(self, \"optional_tensor_attribute_names\"): for tensor_attribute_name in self.optional_tensor_attribute_names: repr_str += f\", {tensor_attribute_name}={getattr(self, tensor_attribute_name)}\" return f\"{self.__class__.__name__}({repr_str})\" raise NotImplementedError( \"Subclasses must implement __repr__ or specify `tensor_data_names` and `tensor_attribute_names` for tensor class or tensor instance before using it\" ) [docs] def get_layout(self): \"\"\" .. deprecated:: 0.15.1 This method is deprecated as of version 0.15.1 since it\u0027s part of the older tensor subclass development stack, for information about new dev stack, please check https://docs.pytorch.org/ao/main/quantization_overview.html and https://docs.pytorch.org/ao/main/contributor_guide.html \"\"\" if not hasattr(self, \"_layout\"): return None return self._layout def fill_defaults(args, n, defaults_tail): \"\"\" __torch_dispatch__ doesn\u0027t guarantee the number of arguments you are passed (e.g., defaulted arguments are not passed); but usually it is convenient to pad out the arguments list with defaults. This function helps you do that. Args: args: the list of positional arguments passed to __torch_dispatch__ n: the number of arguments you are expecting to get defaults_tail: default values for the arguments, starting from the end of the list Example: \u003e\u003e\u003e fill_defaults([1, 2, 3], 5, [3, 4, 5]) [1, 2, 3, 4, 5] \u003e\u003e\u003e fill_defaults([1, 2, 3], 5, [None, None, None]) [1, 2, 3, None, None]] \"\"\" if n - len(defaults_tail) \u003e len(args): raise RuntimeError(\"not enough defaults to fill arguments\") r = list(args) for i in range(len(args), n): r.append(defaults_tail[i - n + len(defaults_tail)]) return r # Supported AMD GPU Models and their LLVM gfx Codes: # # | AMD GPU Model | LLVM gfx Code | # |---------------|------------------------| # | Navi4 | gfx1200, gfx1201 | # | MI300X | gfx940, gfx941, gfx942 | # | MI350 | gfx950 | def is_ROCM(): return torch.cuda.is_available() and torch.version.hip def is_MI300(): if is_ROCM(): mxArchName = [\"gfx940\", \"gfx941\", \"gfx942\"] archName = torch.cuda.get_device_properties(0).gcnArchName for arch in mxArchName: if arch in archName: return True return False def is_MI350(): if is_ROCM(): archName = torch.cuda.get_device_properties(0).gcnArchName if \"gfx950\" in archName: return True return False def is_Navi4(): if is_ROCM(): archName = torch.cuda.get_device_properties(0).gcnArchName if \"gfx1200\" or \"gfx1201\" in archName: return True return False def is_sm_version(major: int, minor: int) -\u003e bool: \"\"\"Check if the CUDA version is exactly major.minor\"\"\" is_cuda = torch.cuda.is_available() and torch.version.cuda return torch.cuda.get_device_capability() == (major, minor) if is_cuda else False def is_sm_at_least_89(): return ( torch.cuda.is_available() and torch.version.cuda and torch.cuda.get_device_capability() \u003e= (8, 9) ) def is_sm_at_least_90(): return ( torch.cuda.is_available() and torch.version.cuda and torch.cuda.get_device_capability() \u003e= (9, 0) ) # TODO(future PR): rename to 8_9, 9_0, 10_0 instead of 89, 10, 100 def is_sm_at_least_100(): return ( torch.cuda.is_available() and torch.version.cuda and torch.cuda.get_device_capability() \u003e= (10, 0) ) def is_cuda_version_at_least(major: int, minor: int) -\u003e bool: if not torch.cuda.is_available(): return False cuda_version = torch.version.cuda if cuda_version is None: return False cuda_major, cuda_minor = map(int, cuda_version.split(\".\")[:2]) return (cuda_major, cuda_minor) \u003e= (major, minor) def check_cpu_version(device, version=\"2.6.0\"): if isinstance(device, torch.device): device = device.type return device == \"cpu\" and torch_version_at_least(version) def check_xpu_version(device, version=\"2.8.0\"): if isinstance(device, torch.device): device = device.type return device == \"xpu\" and torch_version_at_least(version) def ceil_div(a, b): return (a + b - 1) // b def round_up(a: int, b: int) -\u003e int: return ceil_div(a, b) * b def is_package_at_least(package_name: str, min_version: str): package_exists = importlib.util.find_spec(package_name) is not None if not package_exists: return False return version(package_name) \u003e= min_version def _is_mslk_available(): has_mslk = importlib.util.find_spec(\"mslk\") is not None or is_fbcode() if not has_mslk: return False import mslk # noqa: F401 return True def _is_flashinfer_available(): return ( # flashinfer-python importlib.util.find_spec(\"flashinfer\") is not None # apache-tvm-ffi and importlib.util.find_spec(\"tvm_ffi\") is not None # nvidia-ml-py and importlib.util.find_spec(\"pynvml\") is not None ) or is_fbcode() class DummyModule(torch.nn.Module): \"\"\"This is used because the TorchAO quantization functions tend to operate on modules so to apply the transform to a tensor, we can load a DummyModule with the target tensor and then apply the transformation to the module and then extract the transformed tensor. \"\"\" def __init__(self, weight: torch.Tensor, bias: Optional[torch.Tensor] = None): super().__init__() self.weight = weight self.bias = bias",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/utils.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>