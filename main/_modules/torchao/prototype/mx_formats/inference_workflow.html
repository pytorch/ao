
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.prototype.mx_formats.inference_workflow &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=b417fedc" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-binder.css?v=ca3c1c84" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-dataframe.css?v=e5fbc548" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=f533b996" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/prototype/mx_formats/inference_workflow';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/pytorch/ao/gh-pages/torchao-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/prototype/mx_formats/inference_workflow.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>


<link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main (0.17.0+gitcd062f2 )');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->


<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">

<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>

  
  <div class="navbar-header-items__mobile-logo">
    







  
  
  
  


<a class="navbar-brand logo" href="../../../../index.html">
  
    
    <img src="../../../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
  </div>

  
  
  <div class=" navbar-header-items__start">
    
      
      
        <div class="navbar-item">
          







  
  
  
  


<a class="navbar-brand logo" href="../../../../index.html">
  
    
    <img src="../../../../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../../../../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
        </div>
      
    
      
      
        
        <div class="navbar-item desktop-only-version">
          
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
        </div>
      
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
      
        <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        




  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    <div class="sidebar-header-items__start">
      <div class="navbar-item">
        
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
      </div>
    </div>
    

    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../workflows/index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/qat.html">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../workflows/inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../../../../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../../../../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
        
          <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
        
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



<div id="rtd-footer-container"></div>
      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.prot...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              

<div id="searchbox"></div>
<div id="pytorch-article">
  <!-- Hidden breadcrumb schema for SEO only -->
  <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <link itemprop="item" href="../../../index.html">
      <meta itemprop="name" content="Module code">
      <meta itemprop="position" content="1">
    </div>
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <meta itemprop="name" content="torchao.prototype.mx_formats.inference_workflow">
      <meta itemprop="position" content="2">
    </div>
  </div>

  
  

  
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.prototype.mx_formats.inference_workflow</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.core.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">AOBaseConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.prototype.mx_formats.config</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_validate_elem_dtype</span><span class="p">,</span>
    <span class="n">_validate_kernel_preference</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.prototype.mx_formats.mx_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">MXTensor</span><span class="p">,</span>
    <span class="n">QuantizeTensorToMXKwargs</span><span class="p">,</span>
    <span class="n">ScaleCalculationMode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.prototype.mx_formats.nvfp4_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">NVFP4Tensor</span><span class="p">,</span>
    <span class="n">QuantizeTensorToNVFP4Kwargs</span><span class="p">,</span>
    <span class="n">per_tensor_amax_to_scale</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">_quantization_type</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.common.kernel_preference</span><span class="w"> </span><span class="kn">import</span> <span class="n">KernelPreference</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quantize_.common.quantization_step</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationStep</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.transform_module</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">register_quantize_module_handler</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_sm_at_least_100</span><span class="p">,</span>
    <span class="n">torch_version_at_least</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">NVFP4ObservedLinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A linear module with an observer for NVFP4 static quantization.</span>

<span class="sd">    During calibration, this module tracks the per-tensor absolute maximum (amax)</span>
<span class="sd">    of the input activations. After calibration, the amax is converted to a</span>
<span class="sd">    per_tensor_scale using per_tensor_amax_to_scale() during the convert step.</span>

<span class="sd">    The block-level dynamic quantization remains unchanged - only the global</span>
<span class="sd">    per_tensor_scale is determined statically.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">input_amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">amax</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">input_amax</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">float_linear</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;NVFP4ObservedLinear&quot;</span><span class="p">:</span>
        <span class="n">observed_linear</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">float_linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
            <span class="n">float_linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">float_linear</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">float_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">float_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">observed_linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">float_linear</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">observed_linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">float_linear</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">observed_linear</span>


<div class="viewcode-block" id="MXDynamicActivationMXWeightConfig">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.prototype.mx_formats.MXDynamicActivationMXWeightConfig.html#torchao.prototype.mx_formats.MXDynamicActivationMXWeightConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MXDynamicActivationMXWeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MX Format Inference Quantization</span>

<span class="sd">    This module provides support for running inference with float8 quantization using MX formats.</span>

<span class="sd">    Requirements:</span>
<span class="sd">    - NVIDIA SM100+ hardware (Blackwell or newer) is required for execution</span>
<span class="sd">    - PyTorch 2.5+ for proper serialization support</span>

<span class="sd">    Example (mxfp8):</span>

<span class="sd">    .. literalinclude:: ../../examples/inference/mxfp8_dynamic_activation_mxfp8_weight.py</span>
<span class="sd">       :language: python</span>

<span class="sd">    Example (mxfp4):</span>

<span class="sd">    .. literalinclude:: ../../examples/inference/mxfp4_dynamic_activation_mxfp4_weight.py</span>
<span class="sd">       :language: python</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="c1"># Dtypes for Input and Weights, supports Fp8 and Fp4 formats</span>
    <span class="n">activation_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>
    <span class="n">weight_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>

    <span class="c1"># Which kernel to run for mm</span>
    <span class="n">kernel_preference</span><span class="p">:</span> <span class="n">KernelPreference</span> <span class="o">=</span> <span class="n">KernelPreference</span><span class="o">.</span><span class="n">AUTO</span>

    <span class="c1"># How to calculate the block scales</span>
    <span class="n">scaling_mode</span><span class="p">:</span> <span class="n">ScaleCalculationMode</span> <span class="o">=</span> <span class="n">ScaleCalculationMode</span><span class="o">.</span><span class="n">RCEIL</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;For now - we only support matching input/weight dtypes.&quot;</span>
        <span class="p">)</span>
        <span class="n">_validate_elem_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_dtype</span><span class="p">)</span>
        <span class="n">_validate_elem_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span><span class="p">)</span>
        <span class="n">_validate_kernel_preference</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_preference</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dtype</span>
        <span class="p">)</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_linear_extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, weight=</span><span class="si">{</span><span class="n">_quantization_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">MXDynamicActivationMXWeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_mx_inference_linear_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">MXDynamicActivationMXWeightConfig</span>
<span class="p">):</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Only supporting bf16 out dtype for now, got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">act_quant_kwargs</span> <span class="o">=</span> <span class="n">QuantizeTensorToMXKwargs</span><span class="p">(</span>
        <span class="n">elem_dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">activation_dtype</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">kernel_preference</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">kernel_preference</span><span class="p">,</span>
        <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">scaling_mode</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">scaling_mode</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Convert weight to MX Tensor</span>
    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">MXTensor</span><span class="o">.</span><span class="n">to_mx</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">config</span><span class="o">.</span><span class="n">weight_dtype</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">kernel_preference</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">kernel_preference</span><span class="p">,</span>
        <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">act_quant_kwargs</span><span class="p">,</span>
        <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">scaling_mode</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">scaling_mode</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<div class="viewcode-block" id="NVFP4DynamicActivationNVFP4WeightConfig">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.prototype.mx_formats.NVFP4DynamicActivationNVFP4WeightConfig.html#torchao.prototype.mx_formats.NVFP4DynamicActivationNVFP4WeightConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NVFP4DynamicActivationNVFP4WeightConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NVIDIA FP4 (NVFP4) Inference Quantization Configuration</span>

<span class="sd">    This is a specialized configuration for NVIDIA&#39;s FP4 format.</span>
<span class="sd">    NVFP4 uses &quot;double quantization&quot; with two scale levels:</span>
<span class="sd">    - A global per_tensor_scale (float32)</span>
<span class="sd">    - Per-block scales (float8_e4m3fn, block_size=16), always dynamically calculated</span>

<span class="sd">    The activation per_tensor_scale can be determined in two ways:</span>

<span class="sd">    1. Dynamic per_tensor_scale (default, step=None, use_dynamic_per_tensor_scale=True):</span>
<span class="sd">        - Both weight and activation per_tensor_scale are computed at runtime</span>
<span class="sd">          from the tensor amax</span>

<span class="sd">    2. Static per_tensor_scale via observer flow (step=&quot;prepare&quot;/&quot;convert&quot;):</span>
<span class="sd">        - Weight per_tensor_scale is computed from weight amax at convert time</span>
<span class="sd">        - Activation per_tensor_scale is determined statically during calibration:</span>
<span class="sd">          step=&quot;prepare&quot; inserts observers, then after running calibration data,</span>
<span class="sd">          step=&quot;convert&quot; extracts the observed amax and bakes the activation</span>
<span class="sd">          per_tensor_scale into the quantized weight tensor</span>
<span class="sd">        - At inference, the static activation per_tensor_scale is read from the</span>
<span class="sd">          weight tensor instead of being computed dynamically</span>
<span class="sd">        - Note: activation per-block scales are still computed dynamically at</span>
<span class="sd">          inference time</span>

<span class="sd">    Note: When step is specified, use_dynamic_per_tensor_scale is automatically</span>
<span class="sd">    set to False.</span>

<span class="sd">    Configuration parameters:</span>
<span class="sd">    - use_triton_kernel: bool, whether to use fused triton kernel for activation scaling (default: True)</span>
<span class="sd">    - use_dynamic_per_tensor_scale: bool, whether to dynamically compute per tensor scale (default: True)</span>
<span class="sd">    - step: Optional[QuantizationStep], the quantization step for observer-based flow</span>
<span class="sd">    - Data: float4_e2m1fn_x2</span>
<span class="sd">    - Scales: float8_e4m3fn</span>
<span class="sd">    - Block size: 16 along the reduction dim</span>

<span class="sd">    Note: Triton kernel only works with DYNAMIC mode and has constraints that input dimensions</span>
<span class="sd">    must satisfy M % 128 == 0 and K % 64 == 0. Will automatically fallback when constraints aren&#39;t met.</span>

<span class="sd">    Example:</span>

<span class="sd">    .. literalinclude:: ../../examples/inference/nvfp4_dynamic_activation_nvfp4_weight.py</span>
<span class="sd">       :language: python</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">use_triton_kernel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">use_dynamic_per_tensor_scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;QuantizationStep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="n">QuantizationStep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
        <span class="c1"># Validate PyTorch version</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.8.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;NVFP4DynamicActivationNVFP4WeightConfig requires PyTorch 2.8 or later&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Static quantization implies use_dynamic_per_tensor_scale=False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_dynamic_per_tensor_scale</span> <span class="o">=</span> <span class="kc">False</span></div>



<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">NVFP4DynamicActivationNVFP4WeightConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_nvfp4_inference_linear_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">NVFP4DynamicActivationNVFP4WeightConfig</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantization handler for NVFP4DynamicActivationNVFP4WeightConfig</span>

<span class="sd">    Behavior depends on the step:</span>
<span class="sd">    - PREPARE: Insert NVFP4ObservedLinear to collect activation statistics</span>
<span class="sd">    - CONVERT: Extract amax from observer, compute static per_tensor_scale, quantize</span>
<span class="sd">    - None (default): Original dynamic quantization behavior</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;NVFP4 only supports weight shape with last 2 dims divisible by 16, got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">step</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">step</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">QuantizationStep</span><span class="o">.</span><span class="n">PREPARE</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">==</span> <span class="s2">&quot;prepare&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">NVFP4ObservedLinear</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">step</span> <span class="o">==</span> <span class="n">QuantizationStep</span><span class="o">.</span><span class="n">CONVERT</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">==</span> <span class="s2">&quot;convert&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">NVFP4ObservedLinear</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">module</span>

        <span class="c1"># Compute activation per_tensor_scale from observed amax</span>
        <span class="n">act_per_tensor_scale</span> <span class="o">=</span> <span class="n">per_tensor_amax_to_scale</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">amax</span><span class="p">)</span>

        <span class="c1"># Weight quantization (same as dynamic path)</span>

        <span class="n">tensor_amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span>
        <span class="n">weight_per_tensor_scale</span> <span class="o">=</span> <span class="n">per_tensor_amax_to_scale</span><span class="p">(</span><span class="n">tensor_amax</span><span class="p">)</span>

        <span class="n">act_quant_kwargs</span> <span class="o">=</span> <span class="n">QuantizeTensorToNVFP4Kwargs</span><span class="p">(</span>
            <span class="n">use_dynamic_per_tensor_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_triton_kernel</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_triton_kernel</span><span class="p">,</span>
            <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">NVFP4Tensor</span><span class="o">.</span><span class="n">to_nvfp4</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">per_tensor_scale</span><span class="o">=</span><span class="n">weight_per_tensor_scale</span><span class="p">,</span>
            <span class="n">act_per_tensor_scale</span><span class="o">=</span><span class="n">act_per_tensor_scale</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
            <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_triton_kernel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Always use traditional construction for weights</span>
            <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">act_quant_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">quantized_weight</span><span class="o">.</span><span class="n">use_triton_kernel</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_triton_kernel</span>

        <span class="c1"># Create new Linear (not observed) with quantized weight</span>
        <span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">module</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">linear</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">linear</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">linear</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">linear</span>

    <span class="k">elif</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Dynamic quantization</span>
        <span class="k">assert</span> <span class="n">is_sm_at_least_100</span><span class="p">(),</span> <span class="p">(</span>
            <span class="s2">&quot;NVFP4 DYNAMIC mode is only supported on sm100+ machines&quot;</span>
        <span class="p">)</span>

        <span class="n">per_tensor_scale</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_per_tensor_scale</span><span class="p">:</span>
            <span class="n">tensor_amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span>
            <span class="n">per_tensor_scale</span> <span class="o">=</span> <span class="n">per_tensor_amax_to_scale</span><span class="p">(</span><span class="n">tensor_amax</span><span class="p">)</span>

        <span class="n">act_quant_kwargs</span> <span class="o">=</span> <span class="n">QuantizeTensorToNVFP4Kwargs</span><span class="p">(</span>
            <span class="n">use_dynamic_per_tensor_scale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_per_tensor_scale</span><span class="p">,</span>
            <span class="n">use_triton_kernel</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_triton_kernel</span><span class="p">,</span>
            <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">NVFP4Tensor</span><span class="o">.</span><span class="n">to_nvfp4</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">per_tensor_scale</span><span class="o">=</span><span class="n">per_tensor_scale</span><span class="p">,</span>
            <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_triton_kernel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Always use traditional construction for weights</span>
            <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="n">act_quant_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">quantized_weight</span><span class="o">.</span><span class="n">use_triton_kernel</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">use_triton_kernel</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">module</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Unexpected step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">. Expected one of </span><span class="si">{</span><span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">value</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">QuantizationStep</span><span class="p">]</span><span class="si">}</span><span class="s2"> or None.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="NVFP4WeightOnlyConfig">
<a class="viewcode-back" href="../../../../api_reference/generated/torchao.prototype.mx_formats.NVFP4WeightOnlyConfig.html#torchao.prototype.mx_formats.NVFP4WeightOnlyConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NVFP4WeightOnlyConfig</span><span class="p">(</span><span class="n">AOBaseConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NVIDIA FP4 (NVFP4) Weight-Only Quantization Configuration</span>

<span class="sd">    This configuration applies NVFP4 quantization to weights only, keeping activations</span>
<span class="sd">    in their original precision.</span>

<span class="sd">    Example:</span>

<span class="sd">    .. literalinclude:: ../../examples/inference/nvfp4_weight_only.py</span>
<span class="sd">       :language: python</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">use_dynamic_per_tensor_scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Validate PyTorch version</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.8.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;NVFP4DynamicActivationNVFP4WeightConfig requires PyTorch 2.8 or later&quot;</span>
            <span class="p">)</span></div>



<span class="nd">@register_quantize_module_handler</span><span class="p">(</span><span class="n">NVFP4WeightOnlyConfig</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_nvfp4_weight_only_linear_transform</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">NVFP4WeightOnlyConfig</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantization handler for NVFP4WeightOnlyConfig&quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>

    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;NVFP4 only supports weight shape with last 2 dims divisible by 16, got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">per_tensor_scale</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_per_tensor_scale</span><span class="p">:</span>
        <span class="n">tensor_amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span>
        <span class="n">per_tensor_scale</span> <span class="o">=</span> <span class="n">per_tensor_amax_to_scale</span><span class="p">(</span><span class="n">tensor_amax</span><span class="p">)</span>

    <span class="n">quantized_weight</span> <span class="o">=</span> <span class="n">NVFP4Tensor</span><span class="o">.</span><span class="n">to_nvfp4</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">per_tensor_scale</span><span class="o">=</span><span class="n">per_tensor_scale</span><span class="p">,</span>
        <span class="n">is_swizzled_scales</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">act_quant_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Set triton preference after construction</span>
    <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">quantized_weight</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">extra_repr</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">_linear_extra_repr</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">MXTensor</span><span class="p">,</span>
        <span class="n">NVFP4Tensor</span><span class="p">,</span>
        <span class="n">QuantizeTensorToMXKwargs</span><span class="p">,</span>
        <span class="n">QuantizeTensorToNVFP4Kwargs</span><span class="p">,</span>
        <span class="n">ScaleCalculationMode</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_auto_filter_for_nfp4</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">fqn</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generic Filter fn for NVFP4 that is best practice for most models.&quot;&quot;&quot;</span>
    <span class="c1"># Define any FQNs you want to exclude directly in the function</span>
    <span class="n">filter_fqns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;embedder&quot;</span><span class="p">,</span> <span class="s2">&quot;embed&quot;</span><span class="p">,</span> <span class="s2">&quot;embedding&quot;</span><span class="p">,</span> <span class="s2">&quot;time_text_embed&quot;</span><span class="p">]</span>

    <span class="c1"># Only support Linear modules</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># If the fqn matches any filtered fqn, then we should not convert this module</span>
    <span class="n">is_filtered_fqn</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">filter_fqn</span> <span class="ow">in</span> <span class="n">fqn</span> <span class="k">for</span> <span class="n">filter_fqn</span> <span class="ow">in</span> <span class="n">filter_fqns</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_filtered_fqn</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># All dims must be divisible by 16 due to float8 hardware requirements.</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dims_multiples_of_16</span> <span class="o">=</span> <span class="n">K</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">N</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dims_multiples_of_16</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="mi">64</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;skiping small linear layer&quot;</span><span class="p">)</span>
        <span class="c1"># TODO cublas doesn&#39;t like this one</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># Dims below these thresholds may result in worse performance</span>
    <span class="k">if</span> <span class="n">K</span> <span class="o">&lt;=</span> <span class="mi">1024</span> <span class="ow">and</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="mi">1024</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;skiping small linear layer&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>
</pre></div>

                </article>
              
</div>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.prototype.mx_formats.inference_workflow",
       "headline": "torchao.prototype.mx_formats.inference_workflow",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/_modules/torchao/prototype/mx_formats/inference_workflow.html",
       "articleBody": "Source code for torchao.prototype.mx_formats.inference_workflow # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD 3-Clause license found in the # LICENSE file in the root directory of this source tree. import types from dataclasses import dataclass from typing import Optional import torch import torch.nn.functional as F from torch import Tensor from torchao.core.config import AOBaseConfig from torchao.prototype.mx_formats.config import ( _validate_elem_dtype, _validate_kernel_preference, ) from torchao.prototype.mx_formats.mx_tensor import ( MXTensor, QuantizeTensorToMXKwargs, ScaleCalculationMode, ) from torchao.prototype.mx_formats.nvfp4_tensor import ( NVFP4Tensor, QuantizeTensorToNVFP4Kwargs, per_tensor_amax_to_scale, ) from torchao.quantization.quant_api import _quantization_type from torchao.quantization.quantize_.common.kernel_preference import KernelPreference from torchao.quantization.quantize_.common.quantization_step import QuantizationStep from torchao.quantization.transform_module import ( register_quantize_module_handler, ) from torchao.utils import ( is_sm_at_least_100, torch_version_at_least, ) class NVFP4ObservedLinear(torch.nn.Linear): \"\"\"A linear module with an observer for NVFP4 static quantization. During calibration, this module tracks the per-tensor absolute maximum (amax) of the input activations. After calibration, the amax is converted to a per_tensor_scale using per_tensor_amax_to_scale() during the convert step. The block-level dynamic quantization remains unchanged - only the global per_tensor_scale is determined statically. \"\"\" def __init__( self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None, ): super().__init__(in_features, out_features, bias, device, dtype) self.amax = torch.tensor(0.0, device=device) def forward(self, input: Tensor) -\u003e Tensor: with torch.no_grad(): input_amax = torch.max(torch.abs(input)) self.amax = torch.max(self.amax.to(input.device), input_amax) output = F.linear(input, self.weight, self.bias) return output @classmethod def from_float(cls, float_linear: torch.nn.Linear) -\u003e \"NVFP4ObservedLinear\": observed_linear = cls( float_linear.in_features, float_linear.out_features, bias=float_linear.bias is not None, device=float_linear.weight.device, dtype=float_linear.weight.dtype, ) observed_linear.weight = float_linear.weight observed_linear.bias = float_linear.bias return observed_linear [docs] @dataclass class MXDynamicActivationMXWeightConfig(AOBaseConfig): \"\"\" MX Format Inference Quantization This module provides support for running inference with float8 quantization using MX formats. Requirements: - NVIDIA SM100+ hardware (Blackwell or newer) is required for execution - PyTorch 2.5+ for proper serialization support Example (mxfp8): .. literalinclude:: ../../examples/inference/mxfp8_dynamic_activation_mxfp8_weight.py :language: python Example (mxfp4): .. literalinclude:: ../../examples/inference/mxfp4_dynamic_activation_mxfp4_weight.py :language: python \"\"\" block_size: int = 32 # Dtypes for Input and Weights, supports Fp8 and Fp4 formats activation_dtype: torch.dtype = torch.float8_e4m3fn weight_dtype: torch.dtype = torch.float8_e4m3fn # Which kernel to run for mm kernel_preference: KernelPreference = KernelPreference.AUTO # How to calculate the block scales scaling_mode: ScaleCalculationMode = ScaleCalculationMode.RCEIL def __post_init__(self): assert self.activation_dtype == self.weight_dtype, ( \"For now - we only support matching input/weight dtypes.\" ) _validate_elem_dtype(self.activation_dtype) _validate_elem_dtype(self.weight_dtype) _validate_kernel_preference( self.kernel_preference, self.block_size, self.weight_dtype ) def _linear_extra_repr(self): return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight={_quantization_type(self.weight)}\" @register_quantize_module_handler(MXDynamicActivationMXWeightConfig) def _mx_inference_linear_transform( module: torch.nn.Module, config: MXDynamicActivationMXWeightConfig ): weight = module.weight assert weight.dtype == torch.bfloat16, ( f\"Only supporting bf16 out dtype for now, got {weight.dtype}\" ) act_quant_kwargs = QuantizeTensorToMXKwargs( elem_dtype=config.activation_dtype, block_size=config.block_size, kernel_preference=config.kernel_preference, is_swizzled_scales=True, scaling_mode=config.scaling_mode, ) # Convert weight to MX Tensor quantized_weight = MXTensor.to_mx( weight, config.weight_dtype, block_size=config.block_size, kernel_preference=config.kernel_preference, act_quant_kwargs=act_quant_kwargs, is_swizzled_scales=True, scaling_mode=config.scaling_mode, ) module.weight = torch.nn.Parameter(quantized_weight, requires_grad=False) module.extra_repr = types.MethodType(_linear_extra_repr, module) return module [docs] @dataclass class NVFP4DynamicActivationNVFP4WeightConfig(AOBaseConfig): \"\"\" NVIDIA FP4 (NVFP4) Inference Quantization Configuration This is a specialized configuration for NVIDIA\u0027s FP4 format. NVFP4 uses \"double quantization\" with two scale levels: - A global per_tensor_scale (float32) - Per-block scales (float8_e4m3fn, block_size=16), always dynamically calculated The activation per_tensor_scale can be determined in two ways: 1. Dynamic per_tensor_scale (default, step=None, use_dynamic_per_tensor_scale=True): - Both weight and activation per_tensor_scale are computed at runtime from the tensor amax 2. Static per_tensor_scale via observer flow (step=\"prepare\"/\"convert\"): - Weight per_tensor_scale is computed from weight amax at convert time - Activation per_tensor_scale is determined statically during calibration: step=\"prepare\" inserts observers, then after running calibration data, step=\"convert\" extracts the observed amax and bakes the activation per_tensor_scale into the quantized weight tensor - At inference, the static activation per_tensor_scale is read from the weight tensor instead of being computed dynamically - Note: activation per-block scales are still computed dynamically at inference time Note: When step is specified, use_dynamic_per_tensor_scale is automatically set to False. Configuration parameters: - use_triton_kernel: bool, whether to use fused triton kernel for activation scaling (default: True) - use_dynamic_per_tensor_scale: bool, whether to dynamically compute per tensor scale (default: True) - step: Optional[QuantizationStep], the quantization step for observer-based flow - Data: float4_e2m1fn_x2 - Scales: float8_e4m3fn - Block size: 16 along the reduction dim Note: Triton kernel only works with DYNAMIC mode and has constraints that input dimensions must satisfy M % 128 == 0 and K % 64 == 0. Will automatically fallback when constraints aren\u0027t met. Example: .. literalinclude:: ../../examples/inference/nvfp4_dynamic_activation_nvfp4_weight.py :language: python \"\"\" use_triton_kernel: bool = True use_dynamic_per_tensor_scale: bool = True step: Optional[\"QuantizationStep\"] = None def __post_init__(self): if isinstance(self.step, str): self.step = QuantizationStep(self.step) # Validate PyTorch version if not torch_version_at_least(\"2.8.0\"): raise RuntimeError( \"NVFP4DynamicActivationNVFP4WeightConfig requires PyTorch 2.8 or later\" ) if self.step is not None: # Static quantization implies use_dynamic_per_tensor_scale=False self.use_dynamic_per_tensor_scale = False @register_quantize_module_handler(NVFP4DynamicActivationNVFP4WeightConfig) def _nvfp4_inference_linear_transform( module: torch.nn.Linear, config: NVFP4DynamicActivationNVFP4WeightConfig ): \"\"\"Quantization handler for NVFP4DynamicActivationNVFP4WeightConfig Behavior depends on the step: - PREPARE: Insert NVFP4ObservedLinear to collect activation statistics - CONVERT: Extract amax from observer, compute static per_tensor_scale, quantize - None (default): Original dynamic quantization behavior \"\"\" weight = module.weight if weight.shape[-2] % 16 != 0 or weight.shape[-1] % 16 != 0: raise RuntimeError( f\"NVFP4 only supports weight shape with last 2 dims divisible by 16, got {weight.shape}\" ) step = config.step if step == QuantizationStep.PREPARE or step == \"prepare\": return NVFP4ObservedLinear.from_float(module) elif step == QuantizationStep.CONVERT or step == \"convert\": if not isinstance(module, NVFP4ObservedLinear): return module # Compute activation per_tensor_scale from observed amax act_per_tensor_scale = per_tensor_amax_to_scale(module.amax) # Weight quantization (same as dynamic path) tensor_amax = torch.max(torch.abs(weight)) weight_per_tensor_scale = per_tensor_amax_to_scale(tensor_amax) act_quant_kwargs = QuantizeTensorToNVFP4Kwargs( use_dynamic_per_tensor_scale=False, use_triton_kernel=config.use_triton_kernel, is_swizzled_scales=True, ) quantized_weight = NVFP4Tensor.to_nvfp4( weight, per_tensor_scale=weight_per_tensor_scale, act_per_tensor_scale=act_per_tensor_scale.detach(), is_swizzled_scales=True, use_triton_kernel=False, # Always use traditional construction for weights act_quant_kwargs=act_quant_kwargs, ) quantized_weight.use_triton_kernel = config.use_triton_kernel # Create new Linear (not observed) with quantized weight linear = torch.nn.Linear( module.in_features, module.out_features, bias=module.bias is not None, device=module.weight.device, dtype=module.weight.dtype, ) linear.weight = torch.nn.Parameter(quantized_weight, requires_grad=False) linear.bias = module.bias linear.extra_repr = types.MethodType(_linear_extra_repr, linear) return linear elif step is None: # Dynamic quantization assert is_sm_at_least_100(), ( \"NVFP4 DYNAMIC mode is only supported on sm100+ machines\" ) per_tensor_scale = None if config.use_dynamic_per_tensor_scale: tensor_amax = torch.max(torch.abs(weight)) per_tensor_scale = per_tensor_amax_to_scale(tensor_amax) act_quant_kwargs = QuantizeTensorToNVFP4Kwargs( use_dynamic_per_tensor_scale=config.use_dynamic_per_tensor_scale, use_triton_kernel=config.use_triton_kernel, is_swizzled_scales=True, ) quantized_weight = NVFP4Tensor.to_nvfp4( weight, per_tensor_scale=per_tensor_scale, is_swizzled_scales=True, use_triton_kernel=False, # Always use traditional construction for weights act_quant_kwargs=act_quant_kwargs, ) quantized_weight.use_triton_kernel = config.use_triton_kernel module.weight = torch.nn.Parameter(quantized_weight, requires_grad=False) module.extra_repr = types.MethodType(_linear_extra_repr, module) return module else: raise ValueError( f\"Unexpected step: {step}. Expected one of {[s.value for s in QuantizationStep]} or None.\" ) [docs] @dataclass class NVFP4WeightOnlyConfig(AOBaseConfig): \"\"\" NVIDIA FP4 (NVFP4) Weight-Only Quantization Configuration This configuration applies NVFP4 quantization to weights only, keeping activations in their original precision. Example: .. literalinclude:: ../../examples/inference/nvfp4_weight_only.py :language: python \"\"\" use_dynamic_per_tensor_scale: bool = True def __post_init__(self): # Validate PyTorch version if not torch_version_at_least(\"2.8.0\"): raise RuntimeError( \"NVFP4DynamicActivationNVFP4WeightConfig requires PyTorch 2.8 or later\" ) @register_quantize_module_handler(NVFP4WeightOnlyConfig) def _nvfp4_weight_only_linear_transform( module: torch.nn.Linear, config: NVFP4WeightOnlyConfig ): \"\"\"Quantization handler for NVFP4WeightOnlyConfig\"\"\" weight = module.weight if weight.shape[-2] % 16 != 0 or weight.shape[-1] % 16 != 0: raise RuntimeError( f\"NVFP4 only supports weight shape with last 2 dims divisible by 16, got {weight.shape}\" ) per_tensor_scale = None if config.use_dynamic_per_tensor_scale: tensor_amax = torch.max(torch.abs(weight)) per_tensor_scale = per_tensor_amax_to_scale(tensor_amax) quantized_weight = NVFP4Tensor.to_nvfp4( weight, per_tensor_scale=per_tensor_scale, is_swizzled_scales=True, act_quant_kwargs=None, ) # Set triton preference after construction module.weight = torch.nn.Parameter(quantized_weight, requires_grad=False) module.extra_repr = types.MethodType(_linear_extra_repr, module) return module torch.serialization.add_safe_globals( [ MXTensor, NVFP4Tensor, QuantizeTensorToMXKwargs, QuantizeTensorToNVFP4Kwargs, ScaleCalculationMode, ] ) import torch.nn as nn def _auto_filter_for_nfp4(mod: nn.Module, fqn: str) -\u003e bool: \"\"\"Generic Filter fn for NVFP4 that is best practice for most models.\"\"\" # Define any FQNs you want to exclude directly in the function filter_fqns = [\"embedder\", \"embed\", \"embedding\", \"time_text_embed\"] # Only support Linear modules if not isinstance(mod, nn.Linear): return False # If the fqn matches any filtered fqn, then we should not convert this module is_filtered_fqn = any(filter_fqn in fqn for filter_fqn in filter_fqns) if is_filtered_fqn: return False # All dims must be divisible by 16 due to float8 hardware requirements. N, K = mod.weight.shape dims_multiples_of_16 = K % 16 == 0 and N % 16 == 0 if not dims_multiples_of_16: return False if N \u003c= 64: print(\"skiping small linear layer\") # TODO cublas doesn\u0027t like this one return False # Dims below these thresholds may result in worse performance if K \u003c= 1024 and N \u003c= 1024: print(\"skiping small linear layer\") return False return True",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/prototype/mx_formats/inference_workflow.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>