

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchao.dtypes.nf4tensor &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/design-tabs.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchao/dtypes/nf4tensor';</script>
    <link rel="canonical" href="https://pytorch.org/ao/_modules/torchao/dtypes/nf4tensor.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quick_start.html">
    Quick Start Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quantization_overview.html">
    Quantization Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../contributor_guide.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../sparsity.html">
    Sparsity Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../benchmarking_api_guide.html">
    Benchmarking API Guide
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../benchmarking_user_guide.html">
    Benchmarking User Guide
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_dtypes.html">
    torchao.dtypes
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_quantization.html">
    torchao.quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_qat.html">
    torchao.quantization.qat
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_sparsity.html">
    torchao.sparsity
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_float8.html">
    torchao.float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../api_ref_utils.html">
    torchao.utils
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../pretraining.html">
    (Part 1) Pre-training with float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../finetuning.html">
    (Part 2) Fine-tuning with QAT, QLoRA, and float8
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../serving.html">
    (Part 3) Serving on vLLM, SGLang, ExecuTorch
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../torchao_vllm_integration.html">
    Integration with VLLM: Architecture and Usage Guide
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../torchao_hf_integration.html">
    Hugging Face Integration
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../serialization.html">
    Serialization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../static_quantization.html">
    Static Quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../subclass_basic.html">
    Writing Your Own Quantized Tensor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../subclass_advanced.html">
    Writing Your Own Quantized Tensor (advanced)
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_ptq.html">
    PyTorch 2 Export Post Training Quantization
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_qat.html">
    PyTorch 2 Export Quantization-Aware Training (QAT)
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">
    PyTorch 2 Export Quantization with X86 Backend through Inductor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">
    PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">
    PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../tutorials_source/pt2e_quantizer.html">
    How to Write a Quantizer for PyTorch 2 Export Quantization
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quick_start.html">
    Quick Start Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quantization_overview.html">
    Quantization Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../contributor_guide.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../sparsity.html">
    Sparsity Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../benchmarking_api_guide.html">
    Benchmarking API Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../benchmarking_user_guide.html">
    Benchmarking User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_dtypes.html">
    torchao.dtypes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_quantization.html">
    torchao.quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_qat.html">
    torchao.quantization.qat
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_sparsity.html">
    torchao.sparsity
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_float8.html">
    torchao.float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api_ref_utils.html">
    torchao.utils
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../pretraining.html">
    (Part 1) Pre-training with float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../finetuning.html">
    (Part 2) Fine-tuning with QAT, QLoRA, and float8
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../serving.html">
    (Part 3) Serving on vLLM, SGLang, ExecuTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../torchao_vllm_integration.html">
    Integration with VLLM: Architecture and Usage Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../torchao_hf_integration.html">
    Hugging Face Integration
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../serialization.html">
    Serialization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../static_quantization.html">
    Static Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../subclass_basic.html">
    Writing Your Own Quantized Tensor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../subclass_advanced.html">
    Writing Your Own Quantized Tensor (advanced)
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_ptq.html">
    PyTorch 2 Export Post Training Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_qat.html">
    PyTorch 2 Export Quantization-Aware Training (QAT)
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_x86_inductor.html">
    PyTorch 2 Export Quantization with X86 Backend through Inductor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_xpu_inductor.html">
    PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quant_openvino_inductor.html">
    PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials_source/pt2e_quantizer.html">
    How to Write a Quantizer for PyTorch 2 Export Quantization
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchao.dtyp...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="torchao.dtypes.nf4tensor">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchao.dtypes.nf4tensor</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD 3-Clause license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">replace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">auto</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._prims_common</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_contiguous_strides_for</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.device_mesh</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeviceMesh</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">torch_version_at_least</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="n">c10d_functional</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">c10d_functional</span>


<span class="k">def</span><span class="w"> </span><span class="nf">nf4_all_gather_into_tensor</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Expected valid input&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Expected 3 input args&quot;</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">updated_attrs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">])),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="n">updatedNF4Tensor</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">updatedNF4Tensor</span>


<span class="k">def</span><span class="w"> </span><span class="nf">scatter_nf4tensor</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Expected valid input&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Expected 1 output tensor&quot;</span>
    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">input_tensors</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">new_attr</span><span class="p">,</span> <span class="n">update_work</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">input_attrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">input_tensors</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="p">(</span>
                    <span class="s2">&quot;Input tensor size must match output tensor size, tensors are not evenly divided.&quot;</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
                    <span class="n">input_attrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span>
            <span class="n">input_attrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_attrs</span><span class="p">]</span>
        <span class="n">new_attr</span><span class="p">,</span> <span class="n">update_work</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)],</span> <span class="n">input_attrs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">:]</span>
        <span class="p">)</span>
    <span class="c1"># there are 3 works, return one of them, same as the tensor to fit the required output format</span>
    <span class="k">return</span> <span class="n">new_attr</span><span class="p">,</span> <span class="n">update_work</span>


<span class="n">NF4_OPS_TABLE</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">_c10d_functional</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">nf4_all_gather_into_tensor</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">c10d</span><span class="o">.</span><span class="n">scatter_</span><span class="o">.</span><span class="n">default</span><span class="p">:</span> <span class="n">scatter_nf4tensor</span><span class="p">,</span>
<span class="p">}</span>


<span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;quantized_scalers&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantization_factor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantized_data&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Note: Quantize in Chunks</span>
<span class="c1"># During quantization to NF4, one of the steps to convert from the original float number</span>
<span class="c1"># to the index of the nearest value in the NF4 format. This can cause a large memory spike</span>
<span class="c1"># Due to intermediates of the quantization process. Instead we process the original</span>
<span class="c1"># tensor in chunks. This is a tradeoff between memory and speed. This number seems to</span>
<span class="c1"># strike a good balance between memory and speed</span>
<span class="n">CHUNK_SIZE</span> <span class="o">=</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>


<span class="k">def</span><span class="w"> </span><span class="nf">same_metadata</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="s2">&quot;NF4Tensor&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="s2">&quot;NF4Tensor&quot;</span><span class="p">):</span>
    <span class="n">both_nf4</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">NF4Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">NF4Tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">both_nf4</span>
        <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">block_size</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">block_size</span>
        <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">scaler_block_size</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">scaler_block_size</span>
        <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">n_blocks</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">implements</span><span class="p">(</span><span class="n">aten_ops</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use this decorator to implement a function for an aten op in __torch_dispatch__&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">aten_ops</span><span class="p">:</span>
            <span class="n">NF4_OPS_TABLE</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span>
        <span class="k">return</span> <span class="n">func</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">:</span> <span class="s2">&quot;NF4Tensor&quot;</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tensor_meta</span> <span class="o">=</span> <span class="n">SubclassTensorArgs</span><span class="p">(</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;storage_offset&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">tensor_meta</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;block_size&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_blocks&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">n_blocks</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scaler_block_size&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">scaler_block_size</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;quantized_scalers&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">quantized_scalers</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;quantization_factor&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">quantization_factor</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scaler_mean&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">scaler_mean</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;quantized_data&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">quantized_data</span><span class="p">),</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">nf4</span><span class="p">),</span>
    <span class="p">)</span>


<span class="c1"># __torch_dispatch__ utils: apply aten op to inner tensors</span>
<span class="k">def</span><span class="w"> </span><span class="nf">apply_to_inner_tensors</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">:</span> <span class="s2">&quot;NF4Tensor&quot;</span><span class="p">,</span> <span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">attr_to_tensor</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">attr_to_tensor</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">aten_op</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attr_to_tensor</span>


<span class="c1"># __torch_function__ utils: call tensor ops from inner tensors</span>
<span class="k">def</span><span class="w"> </span><span class="nf">call_from_inner_tensors</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">:</span> <span class="s2">&quot;NF4Tensor&quot;</span><span class="p">,</span> <span class="n">method_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">attr_to_tensor</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="n">func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">inner_tensor</span><span class="p">,</span> <span class="n">method_name</span><span class="p">)</span>
        <span class="n">attr_to_tensor</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attr_to_tensor</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CompareOp</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">EQ</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">LT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">expect_num_of_args</span><span class="p">(</span><span class="n">op</span><span class="p">:</span> <span class="n">CompareOp</span><span class="p">,</span> <span class="n">num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">LT</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wrapper</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">expect_arg_value_at_k</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">CompareOp</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">EQ</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">==</span> <span class="n">value</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">msg</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wrapper</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">expect_args_len_at_k</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">CompareOp</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">LT</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">value</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">msg</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>
            <span class="k">elif</span> <span class="n">op</span> <span class="o">==</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">EQ</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">==</span> <span class="n">value</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">msg</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wrapper</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">noop_detach</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>


<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">clone</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">to_nf4</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_original_weight</span><span class="p">())</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_detach</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="n">apply_to_inner_tensors</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">empty_like</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_empty_like</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="n">apply_to_inner_tensors</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">updated_attrs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">split</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_split</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aten.split(NF4Tensor, dim=</span><span class="si">{</span><span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_chunks</span> <span class="o">=</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">//</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">attr_to_chunks</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">inner_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">num_chunks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s2">.numel() not divisible by </span><span class="si">{</span><span class="n">num_chunks</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="n">aten_op</span><span class="p">(</span><span class="n">inner_tensor</span><span class="p">,</span> <span class="n">inner_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">num_chunks</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">attr_to_chunks</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunks</span>

    <span class="n">orig_dim</span> <span class="o">=</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">orig_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">chunked_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_chunks</span><span class="p">,)</span>
    <span class="k">elif</span> <span class="n">orig_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">chunked_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_chunks</span><span class="p">,</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">chunked_size</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aten.split(NF4Tensor) wherer NF4Tensor.dim() = </span><span class="si">{</span><span class="n">orig_dim</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">nf4_chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks</span><span class="p">):</span>
        <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="n">chunked_size</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">attr</span><span class="p">,</span> <span class="n">chunks</span> <span class="ow">in</span> <span class="n">attr_to_chunks</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">nf4_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">nf4_chunks</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">new_zeros</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nd">@expect_args_len_at_k</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">LT</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;aten.view(NF4Tensor) with len(size)=&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_new_zeros</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aten.new_zeros(NF4Tensor) with new size </span><span class="si">{</span><span class="n">new_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span>

    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">inner_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">ratio</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s2">.numel() must be divisible by </span><span class="si">{</span><span class="n">ratio</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="n">aten_op</span><span class="p">(</span><span class="n">inner_tensor</span><span class="p">,</span> <span class="p">[</span><span class="n">inner_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">//</span> <span class="n">ratio</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">inner_tensor</span>
    <span class="n">updated_attrs</span><span class="p">[</span><span class="s2">&quot;size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_size</span>

    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nd">@expect_num_of_args</span><span class="p">(</span><span class="n">CompareOp</span><span class="o">.</span><span class="n">LT</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;aten.slice(NF4Tensor) with customized step&quot;</span><span class="p">)</span>
<span class="nd">@expect_arg_value_at_k</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;aten.slice(NF4Tensor) with dim=&quot;</span><span class="p">)</span>
<span class="nd">@expect_arg_value_at_k</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;aten.slice(NF4Tensor) with start=&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_slice</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># for tensor 512 x 512, tensor[:, :512] dispatch to</span>
    <span class="c1"># aten.slice(dim = 0, end=sys.maxsize)</span>
    <span class="k">if</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aten.slice(NF4Tensor) with end=</span><span class="si">{</span><span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">view</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nd">@expect_args_len_at_k</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">LT</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;aten.view(NF4Tensor) with len(size)=&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_view</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aten.view(NF4Tensor) with size=</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">updated_attrs</span> <span class="o">=</span> <span class="n">apply_to_inner_tensors</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">kwargs</span><span class="p">)</span>
            <span class="n">updated_attrs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()],</span>
                    <span class="s2">&quot;stride&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                <span class="p">}</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;NF4Tensor size does not match view size.&quot;</span><span class="p">)</span>
        <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
            <span class="n">attr_size</span> <span class="o">=</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()]</span>
            <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">aten_op</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="o">*</span><span class="n">attr_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">updated_attrs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;stride&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;aten.view(NF4Tensor) with empty size&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">as_strided</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nd">@expect_args_len_at_k</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">CompareOp</span><span class="o">.</span><span class="n">LT</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;aten.as_strided(NF4Tensor) only support dim &lt;= 2 but got dim=&quot;</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_as_strided</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">storage_offset</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aten.as_strided(NF4Tensor) different numel=</span><span class="si">{</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and size=</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="n">make_contiguous_strides_for</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aten.as_strided(NF4Tensor) only support continuous stride=</span><span class="si">{</span><span class="n">make_contiguous_strides_for</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="si">}</span><span class="s2"> but got stride=</span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">nf4tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">!=</span> <span class="n">storage_offset</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;aten.as_strided(NF4Tensor) only support original storage offset </span><span class="si">{</span><span class="n">nf4tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">storage_offset</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">size</span><span class="p">),</span>
        <span class="s2">&quot;stride&quot;</span><span class="p">:</span> <span class="n">stride</span><span class="p">,</span>
        <span class="s2">&quot;storage_offset&quot;</span><span class="p">:</span> <span class="n">storage_offset</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span><span class="o">.</span><span class="n">default</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_to_copy</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
        <span class="k">assert</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_original_weight</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;dtype&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="s2">&quot;device&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;device&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="n">dtype</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">to_dtype</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
        <span class="k">assert</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_original_weight</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>


<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">default</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">t_default</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">tensor_meta</span> <span class="o">=</span> <span class="n">SubclassTensorArgs</span><span class="p">(</span>
        <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
        <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span>
        <span class="n">a</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
        <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="p">(</span>
        <span class="n">tensor_meta</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">n_blocks</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">scaler_block_size</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">quantized_scalers</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">quantization_factor</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">scaler_mean</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">quantized_data</span><span class="p">,</span>
        <span class="n">a</span><span class="o">.</span><span class="n">nf4</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>


<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mm</span><span class="o">.</span><span class="n">default</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mm_default</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">linear_nf4</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">copy_</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">copy_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">original</span><span class="p">:</span> <span class="n">NF4Tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">copy_in</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Base Case</span>

    <span class="k">if</span> <span class="n">same_metadata</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">copy_in</span><span class="p">):</span>
        <span class="n">original_tensors</span> <span class="o">=</span> <span class="n">original</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">original_tensors</span><span class="p">:</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">copy_in</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">))</span>
        <span class="k">return</span>

    <span class="c1"># Convert Non NF4Tensor into NF4 for copy in</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">copy_in</span><span class="p">,</span> <span class="n">NF4Tensor</span><span class="p">):</span>
        <span class="n">copy_in_nf4</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span>
            <span class="n">copy_in</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">original</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">original</span><span class="o">.</span><span class="n">scaler_block_size</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">original</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">copy_in_nf4</span><span class="p">)</span>

    <span class="c1"># Other Tensor is not a NF4Tensor</span>
    <span class="n">full_precision</span> <span class="o">=</span> <span class="n">copy_in</span><span class="o">.</span><span class="n">get_original_weight</span><span class="p">()</span>
    <span class="n">same_meta_nf4</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span>
        <span class="n">full_precision</span><span class="p">,</span> <span class="n">original</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">original</span><span class="o">.</span><span class="n">scaler_block_size</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">original</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">same_meta_nf4</span><span class="p">)</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">is_pinned</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_is_pinned</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">aten_op</span><span class="p">(</span><span class="n">inner_tensor</span><span class="p">,</span> <span class="o">*</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">_pin_memory</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_pin_memory</span><span class="p">(</span><span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="n">apply_to_inner_tensors</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">aten_op</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">aten</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_cat</span><span class="p">(</span><span class="n">aten_op</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">tensors_to_cat</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors_to_cat</span><span class="p">)</span>
    <span class="n">remaining_args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">ts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors_to_cat</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">NF4Tensor</span><span class="p">):</span>
            <span class="n">ts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">get_original_weight</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtype</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">ts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">tensors</span> <span class="o">=</span> <span class="n">aten_op</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="o">*</span><span class="n">remaining_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensors</span>


<span class="nd">@implements</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">_c10d_functional</span><span class="o">.</span><span class="n">wait_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">wait_tensor</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
        <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span>
    <span class="n">updatedNF4Tensor</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">updatedNF4Tensor</span>


<span class="c1"># _wrap_tensor_autograd was added in PyTorch 2.11.0.dev and later</span>
<span class="k">if</span> <span class="n">torch_version_at_least</span><span class="p">(</span><span class="s2">&quot;2.11.0.dev&quot;</span><span class="p">):</span>

    <span class="nd">@implements</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">_c10d_functional</span><span class="o">.</span><span class="n">_wrap_tensor_autograd</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrap_tensor_autograd_nf4</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Handle _wrap_tensor_autograd for NF4Tensor.</span>
<span class="sd">        This wraps the underlying nf4 data in AsyncCollectiveTensor while</span>
<span class="sd">        preserving the NF4Tensor wrapper with its metadata.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">_INNER_TENSOR_NAMES_FOR_SHARDING</span><span class="p">:</span>
            <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">updatedNF4Tensor</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">nf4tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">updatedNF4Tensor</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SubclassTensorArgs</span><span class="p">:</span>
    <span class="n">original_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span>

    <span class="n">original_strides</span><span class="p">:</span> <span class="n">Tuple</span>
    <span class="n">storage_offset</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_block_absmax</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterate through a flattened tensor getting the absmax scalers for each block</span>

<span class="sd">    Args:</span>
<span class="sd">        input_tensor: Input tensor to get scalers for</span>
<span class="sd">        block_size: Block size for the scanning window</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Tensor of scalers for each block</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Input tensor must be flattened&quot;</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Input tensor must be divisible by block size, got </span><span class="si">{</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="n">n_blocks</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">block_size</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>
    <span class="n">block_scalers</span> <span class="o">=</span> <span class="n">blocks</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="k">return</span> <span class="n">block_scalers</span>


<div class="viewcode-block" id="NF4Tensor"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">NF4Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;NF4Tensor class for converting a weight to the QLoRA NF4 format&quot;&quot;&quot;</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">disable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="c1"># Args related for base tensor construction</span>
        <span class="n">tensor_meta</span><span class="p">:</span> <span class="n">SubclassTensorArgs</span><span class="p">,</span>
        <span class="c1"># Args stored on the instance</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">quantized_scalers</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">quantization_factor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scaler_mean</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">quantized_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">nf4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new NF4Tensor object</span>
<span class="sd">        Args:</span>
<span class="sd">            tensor_meta: Metadata for the tensor</span>
<span class="sd">            block_size: Size of the quantization block</span>
<span class="sd">            n_blocks: Number of blocks to cover the full tensor</span>
<span class="sd">            scaler_block_size: Block size for the scalar quantization</span>
<span class="sd">            quantized_scalers: Quantized scalers data&#39; represented a uint8 tensor</span>
<span class="sd">            quantization_factor: Quantization factor, single scalar represented as torch.Tensor</span>
<span class="sd">            scaler_mean: Mean of the scalers</span>
<span class="sd">            quantized_data: Quantized data represented as uint8 tensor</span>
<span class="sd">            nf4: NF4 tensor LUT for the quantization and dequantization</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">nf4tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span>
            <span class="bp">cls</span><span class="p">,</span>
            <span class="n">tensor_meta</span><span class="o">.</span><span class="n">original_shape</span><span class="p">,</span>
            <span class="n">tensor_meta</span><span class="o">.</span><span class="n">original_strides</span><span class="p">,</span>
            <span class="n">tensor_meta</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">,</span>
            <span class="c1"># Picked some floating dtype, but we need dtype extensibility</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">nf4tensor</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">disable</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tensor_meta</span><span class="p">:</span> <span class="n">SubclassTensorArgs</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">quantized_scalers</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">quantization_factor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scaler_mean</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">quantized_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">nf4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the NF4Tensor class&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler_block_size</span> <span class="o">=</span> <span class="n">scaler_block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantized_scalers</span> <span class="o">=</span> <span class="n">quantized_scalers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantization_factor</span> <span class="o">=</span> <span class="n">quantization_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler_mean</span> <span class="o">=</span> <span class="n">scaler_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantized_data</span> <span class="o">=</span> <span class="n">quantized_data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nf4</span> <span class="o">=</span> <span class="n">nf4</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_tensor</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;expect input tensor dim &lt;= 2 but got dim = </span><span class="si">{</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Input tensor must be divisible by block size, got </span><span class="si">{</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">,</span> <span class="s2">&quot;Input tensor must be contiguous!&quot;</span>
        <span class="c1"># I think I want do this</span>
        <span class="c1"># assert not input_tensor.requires_grad, &quot;Input tensor must not require grad&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">device</span>
        <span class="c1"># Cache the tensor on the class def</span>
        <span class="n">nf4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="o">-</span><span class="mf">1.0000</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.6962</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.5251</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.3949</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.2844</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.1848</span><span class="p">,</span>
                <span class="o">-</span><span class="mf">0.0911</span><span class="p">,</span>
                <span class="mf">0.0000</span><span class="p">,</span>
                <span class="mf">0.0796</span><span class="p">,</span>
                <span class="mf">0.1609</span><span class="p">,</span>
                <span class="mf">0.2461</span><span class="p">,</span>
                <span class="mf">0.3379</span><span class="p">,</span>
                <span class="mf">0.4407</span><span class="p">,</span>
                <span class="mf">0.5626</span><span class="p">,</span>
                <span class="mf">0.7230</span><span class="p">,</span>
                <span class="mf">1.0000</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">n_blocks</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">block_size</span>
        <span class="c1"># Double quantization</span>
        <span class="p">(</span>
            <span class="n">quantized_scalers</span><span class="p">,</span>
            <span class="n">quantization_factor</span><span class="p">,</span>
            <span class="n">scaler_mean</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">double_quantize_scalers</span><span class="p">(</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">scaler_block_size</span>
        <span class="p">)</span>
        <span class="n">quantized_data</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">convert_to_norm_float_weight</span><span class="p">(</span>
            <span class="n">input_tensor</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">nf4</span>
        <span class="p">)</span>
        <span class="n">tensor_meta</span> <span class="o">=</span> <span class="n">SubclassTensorArgs</span><span class="p">(</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">tensor_meta</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">n_blocks</span><span class="p">,</span>
            <span class="n">scaler_block_size</span><span class="p">,</span>
            <span class="n">quantized_scalers</span><span class="p">,</span>
            <span class="n">quantization_factor</span><span class="p">,</span>
            <span class="n">scaler_mean</span><span class="p">,</span>
            <span class="n">quantized_data</span><span class="p">,</span>
            <span class="n">nf4</span><span class="o">=</span><span class="n">nf4</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="NF4Tensor.double_quantize_scalers"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor.double_quantize_scalers">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">double_quantize_scalers</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Used to achieve the double quantization of the scalers</span>
<span class="sd">        We take the input tensor first calculate the absmax quantization factors for each block.</span>
<span class="sd">        We then find the mean of our positive absmax scalers. We subtract this mean from the scalers</span>
<span class="sd">        And then we calculate the absmax quantization factors for each block again. We then quantize the scalers to int8.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_tensor: Input tensor to convert to QLoRA format, typically a weight tensor</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Tensor of per_block quantization factors stored in int8 format</span>
<span class="sd">                size: (n_blocks)</span>
<span class="sd">            torch.Tensor: Tensor of per_scaler_block quantization factors stored in int16 format</span>
<span class="sd">                size: (n_scaler_blocks)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Input tensor must be flattened&quot;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">scaler_block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Input tensor must be divisible by block size, got </span><span class="si">{</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">scaler_block_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># First round of quantization</span>
        <span class="c1"># Produces: A tensor of size (n_blocks) of input_tensor.dtype</span>
        <span class="n">scalers_1</span> <span class="o">=</span> <span class="n">get_block_absmax</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>
        <span class="n">scalers_1_mean</span> <span class="o">=</span> <span class="n">scalers_1</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">scalers_1</span> <span class="o">=</span> <span class="n">scalers_1</span> <span class="o">-</span> <span class="n">scalers_1_mean</span>
        <span class="c1"># Second round of quantization</span>
        <span class="k">assert</span> <span class="n">scalers_1</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">scaler_block_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Number of scalers must be divisible by scaler block size, got </span><span class="si">{</span><span class="n">scalers_1</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> scaler_block_size </span><span class="si">{</span><span class="n">scaler_block_size</span><span class="si">}</span><span class="s2"> &quot;</span>
        <span class="p">)</span>
        <span class="n">n_scaler_blocks</span> <span class="o">=</span> <span class="n">scalers_1</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">scaler_block_size</span>
        <span class="n">scaler_blocks</span> <span class="o">=</span> <span class="n">scalers_1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_scaler_blocks</span><span class="p">,</span> <span class="n">scaler_block_size</span><span class="p">)</span>

        <span class="n">scaler_absmax</span> <span class="o">=</span> <span class="n">get_block_absmax</span><span class="p">(</span><span class="n">scalers_1</span><span class="p">,</span> <span class="n">scaler_block_size</span><span class="p">)</span>
        <span class="n">scaler_absmax</span> <span class="o">=</span> <span class="n">scaler_absmax</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
            <span class="n">n_scaler_blocks</span><span class="p">,</span> <span class="n">scaler_block_size</span>
        <span class="p">)</span>

        <span class="n">quantization_factor</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">scaler_absmax</span><span class="p">)</span>
        <span class="c1"># Length equal to weight numel // block_size</span>
        <span class="n">quantized_scaler_blocks</span> <span class="o">=</span> <span class="n">scaler_blocks</span> <span class="o">*</span> <span class="n">quantization_factor</span>
        <span class="n">quantized_scaler_blocks</span> <span class="o">=</span> <span class="n">quantized_scaler_blocks</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
        <span class="n">quantized_scaler_blocks</span> <span class="o">=</span> <span class="n">quantized_scaler_blocks</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span>

        <span class="c1"># This is needed to make sure that quantization_factor remains a repeated view of n_scaler_blocks</span>
        <span class="c1"># For some reason the 127/scaler_absmax realizes n_scaler entries when only n_scaler_blocks are needed</span>
        <span class="c1"># The following will grab the first entry for the n_scaler_blocks which is the same across the scaler_block_size</span>

        <span class="n">quantization_factor</span> <span class="o">=</span> <span class="n">quantization_factor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">quantized_scaler_blocks</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">),</span>
            <span class="n">quantization_factor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_scaler_blocks</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
            <span class="n">scalers_1_mean</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="NF4Tensor.dequantize_scalers"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor.dequantize_scalers">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">dequantize_scalers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">quantization_factor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Used to unpack the double quantized scalers</span>

<span class="sd">        Args:</span>
<span class="sd">            input_tensor: Input tensor to convert to QLoRA format this is the quantized scalers in int8 format</span>
<span class="sd">            quantization_factor: Tensor of per_scaler_block quantization factors stored in inpt_weight.dtype</span>
<span class="sd">            scaler_block_size: Scaler block size to use for double quantization.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Input tensor must be flattened&quot;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">%</span> <span class="n">scaler_block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Input tensor must be divisible by block size, got </span><span class="si">{</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">scaler_block_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">n_scaler_blocks</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">scaler_block_size</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_scaler_blocks</span><span class="p">,</span> <span class="n">scaler_block_size</span><span class="p">)</span>
        <span class="n">dequantized</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_tensor</span> <span class="o">/</span> <span class="n">quantization_factor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler_mean</span>
        <span class="k">return</span> <span class="n">dequantized</span></div>

<div class="viewcode-block" id="NF4Tensor.convert_to_norm_float_weight"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor.convert_to_norm_float_weight">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">convert_to_norm_float_weight</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nf4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert a tensor to the normalized float weight format&quot;&quot;&quot;</span>
        <span class="n">flattened_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="c1">#  Since we are using uint8 we will encode 2 entries per byte</span>
        <span class="n">numel</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">numel</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Number of elements must be even just to not have to think about the end&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Reshape the flattened tensor into blocks of size self.block_size</span>
        <span class="n">blocks</span> <span class="o">=</span> <span class="n">flattened_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>

        <span class="c1"># Scale the blocks</span>
        <span class="n">scalers</span> <span class="o">=</span> <span class="n">get_block_absmax</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">block_size</span><span class="p">)</span>
        <span class="n">scales</span> <span class="o">=</span> <span class="n">scalers</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>
        <span class="n">scaled_blocks</span> <span class="o">=</span> <span class="n">blocks</span> <span class="o">/</span> <span class="n">scales</span>

        <span class="c1"># Returns a flattened tensor with each element quantized to nf4 index</span>
        <span class="c1"># See Note: Quantize in Chunks</span>
        <span class="n">quantized_blocks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">numel</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">flattened</span> <span class="o">=</span> <span class="n">scaled_blocks</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">chunk_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">numel</span> <span class="o">/</span> <span class="n">CHUNK_SIZE</span><span class="p">)):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">chunk_num</span> <span class="o">*</span> <span class="n">CHUNK_SIZE</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">CHUNK_SIZE</span><span class="p">,</span> <span class="n">numel</span><span class="p">)</span>
            <span class="n">quantized_blocks</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">NF4Tensor</span><span class="o">.</span><span class="n">quantize_tensor_nearest</span><span class="p">(</span>
                <span class="n">flattened</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">nf4</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

        <span class="c1"># Combine the quantized elements into uint8 values</span>
        <span class="c1"># This lays out two consecutive elements in the same byte</span>
        <span class="c1"># [a, b, c, d] -&gt; [ab, cd]</span>
        <span class="c1"># The size of combined blocks will be half the size of the original tensor</span>
        <span class="n">combined_blocks</span> <span class="o">=</span> <span class="n">quantized_blocks</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span> <span class="o">|</span> <span class="n">quantized_blocks</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">combined_blocks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span></div>

<div class="viewcode-block" id="NF4Tensor.get_original_weight"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor.get_original_weight">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_original_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the original weight from the normalized float weight format&quot;&quot;&quot;</span>
        <span class="c1"># Since we are using uint8 we will decode 2 entries per byte</span>
        <span class="c1"># Shift elements down 4 and select out the bottom 4 bits</span>
        <span class="n">first_elements</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantized_data</span> <span class="o">&gt;&gt;</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">second_elements</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantized_data</span> <span class="o">&amp;</span> <span class="mb">0b1111</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

        <span class="c1"># Dequantize every element</span>
        <span class="n">dequantized_first</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">first_elements</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf4</span><span class="p">)</span>
        <span class="n">dequantized_second</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">second_elements</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf4</span><span class="p">)</span>

        <span class="c1"># Build up matrix of scalers repeated for each element in the block</span>
        <span class="c1"># Since first and second elements make up a full block</span>
        <span class="c1"># we expand out to half the size of the full block</span>
        <span class="n">scalers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequantize_scalers</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantized_scalers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantization_factor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler_block_size</span>
        <span class="p">)</span>
        <span class="n">repeated</span> <span class="o">=</span> <span class="n">scalers</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">scalers</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">scaled_first</span> <span class="o">=</span> <span class="n">dequantized_first</span> <span class="o">*</span> <span class="n">repeated</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">scaled_second</span> <span class="o">=</span> <span class="n">dequantized_second</span> <span class="o">*</span> <span class="n">repeated</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="c1"># Flip them to be vertical and them stack them together horizontally</span>
        <span class="c1"># Upon flattening this will interleave the elements</span>
        <span class="n">scaled_first</span> <span class="o">=</span> <span class="n">scaled_first</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">scaled_second</span> <span class="o">=</span> <span class="n">scaled_second</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">scaled_first</span><span class="p">,</span> <span class="n">scaled_second</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="NF4Tensor.quantize_tensor_nearest"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor.quantize_tensor_nearest">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">quantize_tensor_nearest</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">nf4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Quantize a float16 tensor to nf4 format to nearest and not rounded up&quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (numel, 1)</span>
        <span class="c1"># Compare the value tensor with the nf4 tensor element-wise</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">nf4</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
        <span class="n">closest_nf4</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span>
        <span class="k">return</span> <span class="n">closest_nf4</span></div>

<div class="viewcode-block" id="NF4Tensor.dequantize"><a class="viewcode-back" href="../../../generated/torchao.dtypes.NF4Tensor.html#torchao.dtypes.NF4Tensor.dequantize">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dequantize</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">nf4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Dequantize a nf4 value to bfloat16 format&quot;&quot;&quot;</span>
        <span class="c1"># return nf4.index_select(0, value)</span>
        <span class="k">return</span> <span class="n">nf4</span><span class="p">[</span><span class="n">value</span><span class="p">]</span></div>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Quantized Data: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">quantized_data</span><span class="si">}</span><span class="se">\n</span><span class="s2">Scalers: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">quantized_scalers</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;NF4Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tensor_meta</span> <span class="o">=</span> <span class="n">SubclassTensorArgs</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;block_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
            <span class="s2">&quot;n_blocks&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span><span class="p">,</span>
            <span class="s2">&quot;scaler_block_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler_block_size</span><span class="p">,</span>
            <span class="s2">&quot;tensor_meta&quot;</span><span class="p">:</span> <span class="n">tensor_meta</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="s2">&quot;quantized_data&quot;</span><span class="p">,</span>
            <span class="s2">&quot;scaler_mean&quot;</span><span class="p">,</span>
            <span class="s2">&quot;quantization_factor&quot;</span><span class="p">,</span>
            <span class="s2">&quot;quantized_scalers&quot;</span><span class="p">,</span>
            <span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
        <span class="p">],</span> <span class="n">ctx</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__tensor_unflatten__</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inner_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;Expected 5 inner tensors&quot;</span>
        <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span>
            <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;tensor_meta&quot;</span><span class="p">],</span>
            <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;block_size&quot;</span><span class="p">],</span>
            <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;n_blocks&quot;</span><span class="p">],</span>
            <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;scaler_block_size&quot;</span><span class="p">],</span>
            <span class="n">inner_tensors</span><span class="p">[</span><span class="s2">&quot;quantized_scalers&quot;</span><span class="p">],</span>
            <span class="n">inner_tensors</span><span class="p">[</span><span class="s2">&quot;quantization_factor&quot;</span><span class="p">],</span>
            <span class="n">inner_tensors</span><span class="p">[</span><span class="s2">&quot;scaler_mean&quot;</span><span class="p">],</span>
            <span class="n">inner_tensors</span><span class="p">[</span><span class="s2">&quot;quantized_data&quot;</span><span class="p">],</span>
            <span class="n">inner_tensors</span><span class="p">[</span><span class="s2">&quot;nf4&quot;</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">disable</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;TODO we are not supporting torch dispatch at the moment</span>
<span class="sd">        instead we have created a Autograd.Function to handle the linear</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># All ops in the  NF4_OPS_TABLE expect NF4 Tensors as inputs</span>
        <span class="c1"># And don&#39;t support mixed tensor subclasses. This will trigger the handler for</span>
        <span class="c1"># the next type in the dispatch list</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">allowed_subclasses</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">issubclass</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">functional_tensor</span><span class="o">.</span><span class="n">FunctionalTensor</span><span class="p">,</span> <span class="nb">type</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">allowed_subclasses</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">types</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span><span class="p">(</span><span class="s2">&quot;Up to the next one to handle&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">NF4_OPS_TABLE</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">NF4_OPS_TABLE</span><span class="p">[</span><span class="n">func</span><span class="p">](</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;NF4Tensor dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Do not force the Float8TrainingTensor type on the returned tensor</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">NF4_TORCH_FUNCTIONS</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">NF4_TORCH_FUNCTIONS</span><span class="p">[</span><span class="n">func</span><span class="p">](</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fsdp_pre_all_gather</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">DeviceMesh</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantized_scalers</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantization_factor</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantized_data</span><span class="p">,</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="n">SubclassTensorArgs</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler_block_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler_mean</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nf4</span><span class="p">,</span>
            <span class="n">mesh</span><span class="o">.</span><span class="n">get_group</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fsdp_post_all_gather</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">all_gather_outputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">metadata</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">param_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="p">(</span><span class="n">quantized_scalers</span><span class="p">,</span> <span class="n">quantization_factor</span><span class="p">,</span> <span class="n">quantized_data</span><span class="p">)</span> <span class="o">=</span> <span class="n">all_gather_outputs</span>
        <span class="p">(</span>
            <span class="n">tensor_meta</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">n_blocks</span><span class="p">,</span>
            <span class="n">scaler_block_size</span><span class="p">,</span>
            <span class="n">scaler_mean</span><span class="p">,</span>
            <span class="n">nf4</span><span class="p">,</span>
            <span class="n">pg_size</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">metadata</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">original_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;only support 2D shape but got dim=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">original_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span>
            <span class="p">(</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">original_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">pg_size</span><span class="p">,</span> <span class="n">tensor_meta</span><span class="o">.</span><span class="n">original_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="n">new_tensor_meta</span> <span class="o">=</span> <span class="n">replace</span><span class="p">(</span><span class="n">tensor_meta</span><span class="p">,</span> <span class="n">original_shape</span><span class="o">=</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: add param dtype for mixed precision</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">NF4Tensor</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">quantized_scalers</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
                <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">quantized_scalers</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
                <span class="ow">and</span> <span class="n">quantization_factor</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
                <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">quantization_factor</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
                <span class="ow">and</span> <span class="n">quantized_data</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
                <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">quantized_data</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;Expects out&#39;s data to be the all-gather output&quot;</span>
            <span class="k">return</span>

        <span class="k">return</span> <span class="n">nf4_constructor</span><span class="p">(</span>
            <span class="n">new_tensor_meta</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">n_blocks</span><span class="p">,</span>
            <span class="n">scaler_block_size</span><span class="p">,</span>
            <span class="n">quantized_scalers</span><span class="p">,</span>
            <span class="n">quantization_factor</span><span class="p">,</span>
            <span class="n">scaler_mean</span><span class="p">,</span>
            <span class="n">quantized_data</span><span class="p">,</span>
            <span class="n">nf4</span><span class="p">,</span>
        <span class="p">),</span> <span class="p">(</span><span class="n">quantized_scalers</span><span class="p">,</span> <span class="n">quantization_factor</span><span class="p">,</span> <span class="n">quantized_data</span><span class="p">)</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">LinearNF4</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">NF4Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the quantized nf4 weight for backward pass&quot;&quot;&quot;</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The nf4 weight will never require grad so we can just return the grad_output @ weight.to(grad_output.dtype)&quot;&quot;&quot;</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">NF4Tensor</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">linear_nf4</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">NF4Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply a linear operation with the NF4Tensor weight</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Input tensor</span>
<span class="sd">        weight: NF4Tensor weight</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">LinearNF4</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<div class="viewcode-block" id="to_nf4"><a class="viewcode-back" href="../../../generated/torchao.dtypes.to_nf4.html#torchao.dtypes.to_nf4">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">to_nf4</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a given tensor to normalized float 4-bit tensor.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">scaler_block_size</span><span class="p">)</span></div>


<span class="n">NF4_TORCH_FUNCTIONS</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">implements_torch_function</span><span class="p">(</span><span class="n">torch_function</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="n">functools</span><span class="o">.</span><span class="n">update_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch_function</span><span class="p">)</span>
        <span class="n">NF4_TORCH_FUNCTIONS</span><span class="p">[</span><span class="n">torch_function</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span>
        <span class="k">return</span> <span class="n">func</span>

    <span class="k">return</span> <span class="n">decorator</span>


<span class="nd">@implements_torch_function</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">function_to_dtype</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span>
        <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="c1"># dtype is specified -&gt; dequantize</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_original_weight</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span>
        <span class="p">)</span>

    <span class="c1"># dtype is not specified -&gt; keep NF4</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">tensor_attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">tensor_attrs</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">inner_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements_torch_function</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">function_cpu</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Tensor.cpu(self, memory_format)</span>
    <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="nd">@implements_torch_function</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">function_cuda</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Tensor.cuda(self, device, non_blocking, memory_format)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">tensor_attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">tensor_attrs</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">inner_tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">updated_attrs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">updated_attrs</span><span class="p">[</span><span class="n">tensor_attrs</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">device</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@implements_torch_function</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">LinearNF4</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="nd">@implements_torch_function</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">view_as</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">function_view_as</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Handle view_as for NF4Tensor.</span>

<span class="sd">    When view_as is called (typically by autograd internals), we need to return</span>
<span class="sd">    a fresh NF4Tensor without autograd metadata to avoid conflicts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Create a new NF4Tensor with detached inner tensors to avoid autograd conflicts</span>
    <span class="n">updated_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tensor_attrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">__tensor_flatten__</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">tensor_attrs</span><span class="p">:</span>
        <span class="n">inner_tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="c1"># Detach to create a fresh tensor without autograd metadata</span>
        <span class="n">updated_attrs</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">inner_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">construct_nf4_args</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">updated_attrs</span><span class="p">))</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">allow_in_graph</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nf4_constructor</span><span class="p">(</span>
    <span class="n">tensor_meta</span><span class="p">:</span> <span class="n">SubclassTensorArgs</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">scaler_block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">quantized_scalers</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantization_factor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scaler_mean</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">quantized_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">nf4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">NF4Tensor</span><span class="p">(</span>
        <span class="n">tensor_meta</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">,</span>
        <span class="n">scaler_block_size</span><span class="p">,</span>
        <span class="n">quantized_scalers</span><span class="p">,</span>
        <span class="n">quantization_factor</span><span class="p">,</span>
        <span class="n">scaler_mean</span><span class="p">,</span>
        <span class="n">quantized_data</span><span class="p">,</span>
        <span class="n">nf4</span><span class="p">,</span>
    <span class="p">)</span>


<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">NF4Tensor</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">NF4Tensor</span><span class="p">])</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchao.dtypes.nf4tensor",
       "headline": "torchao.dtypes.nf4tensor",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/torchao/dtypes/nf4tensor.html",
       "articleBody": "Source code for torchao.dtypes.nf4tensor # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD 3-Clause license found in the # LICENSE file in the root directory of this source tree. import functools import math import sys from dataclasses import dataclass, replace from enum import Enum, auto from typing import Any, Dict, Optional, Tuple, Union import torch import torch.nn.functional as F from torch._prims_common import make_contiguous_strides_for from torch.distributed.device_mesh import DeviceMesh from torchao.utils import torch_version_at_least aten = torch.ops.aten c10d_functional = torch.ops.c10d_functional def nf4_all_gather_into_tensor(func, *args, **kwargs): assert len(args) \u003e 1, \"Expected valid input\" assert len(args[0]) == 3, \"Expected 3 input args\" nf4tensor = args[0][0] group_size = args[0][1] name = args[0][2] updated_attrs = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: updated_attrs[attr] = func(getattr(nf4tensor, attr), group_size, name) updated_attrs.update( { \"size\": torch.Size((nf4tensor.size()[0] * group_size, nf4tensor.size()[1])), } ) updatedNF4Tensor = NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) return updatedNF4Tensor def scatter_nf4tensor(func, *args, **kwargs): assert len(args) \u003e 1, \"Expected valid input\" assert len(args[0][0]) == 1, \"Expected 1 output tensor\" output_tensor = args[0][0][0] input_tensors = args[0][1] new_attr, update_work = [], [] for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: input_attrs = [] if input_tensors: for input_tensor in input_tensors[0]: assert input_tensor.size() == output_tensor.size(), ( \"Input tensor size must match output tensor size, tensors are not evenly divided.\" ) if hasattr(input_tensor, attr): input_attrs.append(getattr(input_tensor, attr)) input_attrs = [input_attrs] new_attr, update_work = func( [getattr(output_tensor, attr)], input_attrs, *args[0][2:] ) # there are 3 works, return one of them, same as the tensor to fit the required output format return new_attr, update_work NF4_OPS_TABLE: Dict[Any, Any] = { torch.ops._c10d_functional.all_gather_into_tensor.default: nf4_all_gather_into_tensor, torch.ops.c10d.scatter_.default: scatter_nf4tensor, } _INNER_TENSOR_NAMES_FOR_SHARDING = [ \"quantized_scalers\", \"quantization_factor\", \"quantized_data\", ] # Note: Quantize in Chunks # During quantization to NF4, one of the steps to convert from the original float number # to the index of the nearest value in the NF4 format. This can cause a large memory spike # Due to intermediates of the quantization process. Instead we process the original # tensor in chunks. This is a tradeoff between memory and speed. This number seems to # strike a good balance between memory and speed CHUNK_SIZE = 1024**2 def same_metadata(a: \"NF4Tensor\", b: \"NF4Tensor\"): both_nf4 = isinstance(a, NF4Tensor) and isinstance(b, NF4Tensor) return ( both_nf4 and a.block_size == b.block_size and a.scaler_block_size == b.scaler_block_size and a.n_blocks == b.n_blocks ) def implements(aten_ops): \"\"\"Use this decorator to implement a function for an aten op in __torch_dispatch__\"\"\" def decorator(func): for op in aten_ops: NF4_OPS_TABLE[op] = func return func return decorator def construct_nf4_args(nf4tensor: \"NF4Tensor\", kwargs: Optional[Dict[str, Any]] = None): if kwargs is None: kwargs = {} tensor_meta = SubclassTensorArgs( kwargs.get(\"size\", nf4tensor.size()), kwargs.get(\"stride\", nf4tensor.stride()), kwargs.get(\"storage_offset\", nf4tensor.storage_offset()), kwargs.get(\"dtype\", nf4tensor.dtype), kwargs.get(\"device\", nf4tensor.device), kwargs.get(\"requires_grad\", nf4tensor.requires_grad), ) return ( tensor_meta, kwargs.get(\"block_size\", nf4tensor.block_size), kwargs.get(\"n_blocks\", nf4tensor.n_blocks), kwargs.get(\"scaler_block_size\", nf4tensor.scaler_block_size), kwargs.get(\"quantized_scalers\", nf4tensor.quantized_scalers), kwargs.get(\"quantization_factor\", nf4tensor.quantization_factor), kwargs.get(\"scaler_mean\", nf4tensor.scaler_mean), kwargs.get(\"quantized_data\", nf4tensor.quantized_data), kwargs.get(\"nf4\", nf4tensor.nf4), ) # __torch_dispatch__ utils: apply aten op to inner tensors def apply_to_inner_tensors(nf4tensor: \"NF4Tensor\", aten_op, args, kwargs): attr_to_tensor = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: attr_to_tensor[attr] = aten_op(getattr(nf4tensor, attr), *args, **kwargs) return attr_to_tensor # __torch_function__ utils: call tensor ops from inner tensors def call_from_inner_tensors(nf4tensor: \"NF4Tensor\", method_name: str, args, kwargs): attr_to_tensor = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: inner_tensor = getattr(nf4tensor, attr) func = getattr(inner_tensor, method_name) attr_to_tensor[attr] = func(*args, **kwargs) return attr_to_tensor class CompareOp(Enum): EQ = auto() LT = auto() def expect_num_of_args(op: CompareOp, num: int, msg: str): def decorator(func): @functools.wraps(func) def wrapper(aten_op, args, kwargs=None): if op == CompareOp.LT and not (len(args) \u003c num): raise NotImplementedError(msg) return func(aten_op, args, kwargs) return wrapper return decorator def expect_arg_value_at_k(k: int, op: CompareOp, value: Any, msg: str): def decorator(func): @functools.wraps(func) def wrapper(aten_op, args, kwargs=None): if op == CompareOp.EQ and not (args[k] == value): raise NotImplementedError(msg + str(args[k])) return func(aten_op, args, kwargs) return wrapper return decorator def expect_args_len_at_k(k: int, op: CompareOp, value: Any, msg: str): def decorator(func): @functools.wraps(func) def wrapper(aten_op, args, kwargs=None): if op == CompareOp.LT and not (len(args[k]) \u003c value): raise NotImplementedError(msg + str(len(args[k]))) elif op == CompareOp.EQ and not (len(args[k]) == value): raise NotImplementedError(msg + str(len(args[k]))) return func(aten_op, args, kwargs) return wrapper return decorator @implements([torch.ops.aten.detach]) def noop_detach(func, *args, **kwargs): return args[0][0] @implements([torch.ops.aten.clone.default]) def clone(func, *args, **kwargs): return to_nf4(args[0][0].get_original_weight()) @implements( [ aten.detach.default, ] ) def nf4_detach(aten_op, args, kwargs=None): nf4tensor = args[0] updated_attrs = apply_to_inner_tensors(nf4tensor, aten_op, args[1:], kwargs) return NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) @implements( [ aten.empty_like.default, ] ) def nf4_empty_like(aten_op, args, kwargs=None): nf4tensor = args[0] updated_attrs = apply_to_inner_tensors(nf4tensor, aten_op, args[1:], kwargs) if kwargs is not None and len(kwargs): for key, value in kwargs.items(): updated_attrs[key] = value return NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) @implements( [ aten.split.Tensor, ] ) def nf4_split(aten_op, args, kwargs=None): if len(args) == 3 and args[2] != 0: raise NotImplementedError(f\"aten.split(NF4Tensor, dim={args[2]})\") nf4tensor = args[0] num_chunks = nf4tensor.size(0) // args[1] attr_to_chunks = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: inner_tensor = getattr(nf4tensor, attr) assert inner_tensor.numel() % num_chunks == 0, ( f\"{attr}.numel() not divisible by {num_chunks}\" ) chunks = aten_op(inner_tensor, inner_tensor.numel() // num_chunks, **kwargs) attr_to_chunks[attr] = chunks orig_dim = nf4tensor.dim() if orig_dim == 1: chunked_size = (nf4tensor.size(0) // num_chunks,) elif orig_dim == 2: chunked_size = (nf4tensor.size(0) // num_chunks, nf4tensor.size(1)) else: chunked_size = () raise NotImplementedError( f\"aten.split(NF4Tensor) wherer NF4Tensor.dim() = {orig_dim}\" ) nf4_chunks = [] for idx in range(num_chunks): updated_attrs = {\"size\": chunked_size} for attr, chunks in attr_to_chunks.items(): updated_attrs[attr] = chunks[idx] nf4_chunks.append(NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs))) return nf4_chunks @implements( [ aten.new_zeros.default, ] ) @expect_args_len_at_k(1, CompareOp.LT, 3, \"aten.view(NF4Tensor) with len(size)=\") def nf4_new_zeros(aten_op, args, kwargs=None): nf4tensor = args[0] new_size = tuple(args[1]) if nf4tensor.numel() % math.prod(new_size) != 0: raise NotImplementedError(f\"aten.new_zeros(NF4Tensor) with new size {new_size}\") ratio = nf4tensor.numel() // math.prod(new_size) updated_attrs = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: inner_tensor = getattr(nf4tensor, attr) assert inner_tensor.size(0) % ratio == 0, ( f\"{attr}.numel() must be divisible by {ratio}\" ) inner_tensor = aten_op(inner_tensor, [inner_tensor.size(0) // ratio], **kwargs) updated_attrs[attr] = inner_tensor updated_attrs[\"size\"] = new_size return NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) @implements( [ aten.slice.Tensor, ] ) @expect_num_of_args(CompareOp.LT, 5, \"aten.slice(NF4Tensor) with customized step\") @expect_arg_value_at_k(1, CompareOp.EQ, 0, \"aten.slice(NF4Tensor) with dim=\") @expect_arg_value_at_k(2, CompareOp.EQ, 0, \"aten.slice(NF4Tensor) with start=\") def nf4_slice(aten_op, args, kwargs=None): nf4tensor = args[0] # for tensor 512 x 512, tensor[:, :512] dispatch to # aten.slice(dim = 0, end=sys.maxsize) if args[3] not in [nf4tensor.size(0), sys.maxsize]: raise NotImplementedError(f\"aten.slice(NF4Tensor) with end={args[3]}\") return NF4Tensor(*construct_nf4_args(nf4tensor)) @implements( [ aten.view.default, ] ) @expect_args_len_at_k(1, CompareOp.LT, 3, \"aten.view(NF4Tensor) with len(size)=\") def nf4_view(aten_op, args, kwargs=None): nf4tensor = args[0] size = args[1] if len(size) == 1: if size[0] != -1: raise NotImplementedError(f\"aten.view(NF4Tensor) with size={size}\") else: updated_attrs = apply_to_inner_tensors(nf4tensor, aten_op, args[1:], kwargs) updated_attrs.update( { \"size\": [nf4tensor.numel()], \"stride\": (1,), } ) elif len(size) == 2: if nf4tensor.numel() != size[0] * size[1]: raise NotImplementedError(\"NF4Tensor size does not match view size.\") updated_attrs = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: attr_size = [getattr(nf4tensor, attr).size()] updated_attrs[attr] = aten_op( getattr(nf4tensor, attr), *attr_size, **kwargs ) updated_attrs.update( { \"stride\": (size[1], 1), } ) else: raise NotImplementedError(\"aten.view(NF4Tensor) with empty size\") return NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) @implements( [ aten.as_strided.default, ] ) @expect_args_len_at_k( 1, CompareOp.LT, 3, \"aten.as_strided(NF4Tensor) only support dim \u003c= 2 but got dim=\" ) def nf4_as_strided(aten_op, args, kwargs=None): nf4tensor = args[0] size = args[1] stride = tuple(args[2]) storage_offset = args[3] if math.prod(size) != nf4tensor.numel(): raise NotImplementedError( f\"aten.as_strided(NF4Tensor) different numel={nf4tensor.numel()} and size={size}\" ) if stride != make_contiguous_strides_for(size): raise NotImplementedError( f\"aten.as_strided(NF4Tensor) only support continuous stride={make_contiguous_strides_for(size)} but got stride={stride}\" ) if nf4tensor.storage_offset() != storage_offset: raise NotImplementedError( f\"aten.as_strided(NF4Tensor) only support original storage offset {nf4tensor.storage_offset()} but got {storage_offset}\" ) kwargs = { \"size\": torch.Size(size), \"stride\": stride, \"storage_offset\": storage_offset, } return NF4Tensor(*construct_nf4_args(nf4tensor, kwargs)) @implements([torch.ops.aten._to_copy.default]) def _to_copy(func, *args, **kwargs): if not args[0][0].is_contiguous(): assert args[0][0].t().is_contiguous() return func(args[0][0].t()).t() out = args[0][0].get_original_weight().to(args[1][\"dtype\"]) if \"device\" in args[1]: out = out.to(args[1][\"device\"]) return out @implements([torch.ops.aten.to.dtype]) def to_dtype(func, *args, **kwargs): if not args[0][0].is_contiguous(): assert args[0][0].t().is_contiguous() return torch.ops.aten.to.dtype(args[0][0].t(), args[0][1]).t() return args[0][0].get_original_weight().to(args[0][1]) @implements([torch.ops.aten.t.default]) def t_default(func, *args, **kwargs): a = args[0][0] tensor_meta = SubclassTensorArgs( a.size(), (a.stride(1), a.stride(0)), a.storage_offset(), a.dtype, a.device, a.requires_grad, ) b = NF4Tensor( tensor_meta, a.block_size, a.n_blocks, a.scaler_block_size, a.quantized_scalers, a.quantization_factor, a.scaler_mean, a.quantized_data, a.nf4, ) return b @implements([torch.ops.aten.mm.default]) def mm_default(func, *args, **kwargs): return linear_nf4(args[0][0], args[0][1]) @implements( [ aten.copy_.default, ] ) def copy_(func, *args, **kwargs): original: NF4Tensor = args[0][0] copy_in: torch.Tensor = args[0][1] # Base Case if same_metadata(original, copy_in): original_tensors = original.__tensor_flatten__()[0] for tensor_name in original_tensors: getattr(original, tensor_name).copy_(getattr(copy_in, tensor_name)) return # Convert Non NF4Tensor into NF4 for copy in if not isinstance(copy_in, NF4Tensor): copy_in_nf4 = NF4Tensor.from_tensor( copy_in.to(original.device), original.block_size, original.scaler_block_size ) return original.copy_(copy_in_nf4) # Other Tensor is not a NF4Tensor full_precision = copy_in.get_original_weight() same_meta_nf4 = NF4Tensor.from_tensor( full_precision, original.block_size, original.scaler_block_size ) return original.copy_(same_meta_nf4) @implements( [ aten.is_pinned.default, ] ) def nf4_is_pinned(aten_op, args, kwargs=None): nf4tensor = args[0] for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: inner_tensor = getattr(nf4tensor, attr) if not aten_op(inner_tensor, *(args[1:]), **kwargs): return False return True @implements( [ aten._pin_memory.default, ] ) def nf4_pin_memory(aten_op, args, kwargs=None): nf4tensor = args[0] updated_attrs = apply_to_inner_tensors(nf4tensor, aten_op, args[1:], kwargs) return NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) @implements( [ aten.cat.default, ] ) def nf4_cat(aten_op: torch._ops.OpOverload, args, kwargs=None): tensors_to_cat = args[0] assert all(isinstance(t, torch.Tensor) for t in tensors_to_cat) remaining_args = args[1:] ts = [] for t in tensors_to_cat: assert isinstance(t, torch.Tensor) if isinstance(t, NF4Tensor): ts.append(t.get_original_weight()) else: ts.append(t) dtype = ts[0].dtype assert all(t.dtype == dtype for t in ts) if kwargs is None: kwargs = {} tensors = aten_op(ts, *remaining_args, **kwargs) return tensors @implements( [ torch.ops._c10d_functional.wait_tensor.default, ] ) def wait_tensor(func, *args, **kwargs): nf4tensor = args[0][0] updated_attrs = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: updated_attrs[attr] = func(getattr(nf4tensor, attr)) updatedNF4Tensor = NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) return updatedNF4Tensor # _wrap_tensor_autograd was added in PyTorch 2.11.0.dev and later if torch_version_at_least(\"2.11.0.dev\"): @implements( [ torch.ops._c10d_functional._wrap_tensor_autograd.default, ] ) def wrap_tensor_autograd_nf4(func, *args, **kwargs): \"\"\" Handle _wrap_tensor_autograd for NF4Tensor. This wraps the underlying nf4 data in AsyncCollectiveTensor while preserving the NF4Tensor wrapper with its metadata. \"\"\" nf4tensor = args[0][0] updated_attrs = {} for attr in _INNER_TENSOR_NAMES_FOR_SHARDING: updated_attrs[attr] = func(getattr(nf4tensor, attr), *args[0][1:], **kwargs) updatedNF4Tensor = NF4Tensor(*construct_nf4_args(nf4tensor, updated_attrs)) return updatedNF4Tensor @dataclass(frozen=True) class SubclassTensorArgs: original_shape: torch.Size original_strides: Tuple storage_offset: int dtype: torch.dtype device: torch.device requires_grad: bool def get_block_absmax(input_tensor: torch.Tensor, block_size: int) -\u003e torch.Tensor: \"\"\"Iterate through a flattened tensor getting the absmax scalers for each block Args: input_tensor: Input tensor to get scalers for block_size: Block size for the scanning window Returns: torch.Tensor: Tensor of scalers for each block \"\"\" assert input_tensor.dim() == 1, \"Input tensor must be flattened\" assert (input_tensor.numel() % block_size) == 0, ( f\"Input tensor must be divisible by block size, got {input_tensor.numel()} and {block_size}\" ) n_blocks = input_tensor.numel() // block_size blocks = input_tensor.view(n_blocks, block_size) block_scalers = blocks.abs().max(dim=1).values return block_scalers [docs]class NF4Tensor(torch.Tensor): \"\"\"NF4Tensor class for converting a weight to the QLoRA NF4 format\"\"\" @torch._dynamo.disable def __new__( cls, # Args related for base tensor construction tensor_meta: SubclassTensorArgs, # Args stored on the instance block_size: int, n_blocks: int, scaler_block_size: int, quantized_scalers: torch.Tensor, quantization_factor: torch.Tensor, scaler_mean: torch.Tensor, quantized_data: torch.Tensor, nf4: torch.Tensor, ): \"\"\"Create a new NF4Tensor object Args: tensor_meta: Metadata for the tensor block_size: Size of the quantization block n_blocks: Number of blocks to cover the full tensor scaler_block_size: Block size for the scalar quantization quantized_scalers: Quantized scalers data\u0027 represented a uint8 tensor quantization_factor: Quantization factor, single scalar represented as torch.Tensor scaler_mean: Mean of the scalers quantized_data: Quantized data represented as uint8 tensor nf4: NF4 tensor LUT for the quantization and dequantization \"\"\" nf4tensor = torch.Tensor._make_wrapper_subclass( cls, tensor_meta.original_shape, tensor_meta.original_strides, tensor_meta.storage_offset, # Picked some floating dtype, but we need dtype extensibility dtype=tensor_meta.dtype, device=tensor_meta.device, requires_grad=tensor_meta.requires_grad, ) return nf4tensor @torch._dynamo.disable def __init__( self, tensor_meta: SubclassTensorArgs, block_size: int, n_blocks: int, scaler_block_size: int, quantized_scalers: torch.Tensor, quantization_factor: torch.Tensor, scaler_mean: torch.Tensor, quantized_data: torch.Tensor, nf4: torch.Tensor, ): \"\"\"Initialize the NF4Tensor class\"\"\" self.block_size = block_size self.n_blocks = n_blocks self.scaler_block_size = scaler_block_size self.quantized_scalers = quantized_scalers self.quantization_factor = quantization_factor self.scaler_mean = scaler_mean self.quantized_data = quantized_data self.nf4 = nf4 @classmethod @torch.no_grad() def from_tensor( cls, input_tensor: torch.Tensor, block_size: int, scaler_block_size: int, ): assert input_tensor.dim() \u003c= 2, ( f\"expect input tensor dim \u003c= 2 but got dim = {input_tensor.dim()}\" ) assert input_tensor.numel() % block_size == 0, ( f\"Input tensor must be divisible by block size, got {input_tensor.numel()} and {block_size}\" ) assert input_tensor.is_contiguous, \"Input tensor must be contiguous!\" # I think I want do this # assert not input_tensor.requires_grad, \"Input tensor must not require grad\" device = input_tensor.device # Cache the tensor on the class def nf4 = torch.tensor( [ -1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0000, 0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230, 1.0000, ], device=device, dtype=input_tensor.dtype, ) n_blocks = input_tensor.numel() // block_size # Double quantization ( quantized_scalers, quantization_factor, scaler_mean, ) = cls.double_quantize_scalers( input_tensor.flatten(), block_size, scaler_block_size ) quantized_data = cls.convert_to_norm_float_weight( input_tensor, n_blocks, block_size, nf4 ) tensor_meta = SubclassTensorArgs( input_tensor.size(), input_tensor.stride(), input_tensor.storage_offset(), input_tensor.dtype, input_tensor.device, input_tensor.requires_grad, ) return cls( tensor_meta, block_size, n_blocks, scaler_block_size, quantized_scalers, quantization_factor, scaler_mean, quantized_data, nf4=nf4, ) [docs] @staticmethod def double_quantize_scalers( input_tensor: torch.Tensor, block_size: int, scaler_block_size: int, ) -\u003e Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: \"\"\"Used to achieve the double quantization of the scalers We take the input tensor first calculate the absmax quantization factors for each block. We then find the mean of our positive absmax scalers. We subtract this mean from the scalers And then we calculate the absmax quantization factors for each block again. We then quantize the scalers to int8. Args: input_tensor: Input tensor to convert to QLoRA format, typically a weight tensor Returns: torch.Tensor: Tensor of per_block quantization factors stored in int8 format size: (n_blocks) torch.Tensor: Tensor of per_scaler_block quantization factors stored in int16 format size: (n_scaler_blocks) \"\"\" assert input_tensor.dim() == 1, \"Input tensor must be flattened\" assert (input_tensor.numel() % scaler_block_size) == 0, ( f\"Input tensor must be divisible by block size, got {input_tensor.numel()} and {scaler_block_size}\" ) # First round of quantization # Produces: A tensor of size (n_blocks) of input_tensor.dtype scalers_1 = get_block_absmax(input_tensor, block_size) scalers_1_mean = scalers_1.mean() scalers_1 = scalers_1 - scalers_1_mean # Second round of quantization assert scalers_1.numel() % scaler_block_size == 0, ( f\"Number of scalers must be divisible by scaler block size, got {scalers_1.numel()} scaler_block_size {scaler_block_size} \" ) n_scaler_blocks = scalers_1.numel() // scaler_block_size scaler_blocks = scalers_1.view(n_scaler_blocks, scaler_block_size) scaler_absmax = get_block_absmax(scalers_1, scaler_block_size) scaler_absmax = scaler_absmax.unsqueeze(-1).expand( n_scaler_blocks, scaler_block_size ) quantization_factor = 256 / (2 * scaler_absmax) # Length equal to weight numel // block_size quantized_scaler_blocks = scaler_blocks * quantization_factor quantized_scaler_blocks = quantized_scaler_blocks.round() quantized_scaler_blocks = quantized_scaler_blocks.clamp(-128, 127) # This is needed to make sure that quantization_factor remains a repeated view of n_scaler_blocks # For some reason the 127/scaler_absmax realizes n_scaler entries when only n_scaler_blocks are needed # The following will grab the first entry for the n_scaler_blocks which is the same across the scaler_block_size quantization_factor = quantization_factor[:, 0] return ( quantized_scaler_blocks.flatten().to(torch.int8), quantization_factor.view(n_scaler_blocks).contiguous(), scalers_1_mean, ) [docs] def dequantize_scalers( self, input_tensor: torch.Tensor, quantization_factor: torch.Tensor, scaler_block_size: int, ) -\u003e torch.Tensor: \"\"\"Used to unpack the double quantized scalers Args: input_tensor: Input tensor to convert to QLoRA format this is the quantized scalers in int8 format quantization_factor: Tensor of per_scaler_block quantization factors stored in inpt_weight.dtype scaler_block_size: Scaler block size to use for double quantization. \"\"\" assert input_tensor.dim() == 1, \"Input tensor must be flattened\" assert (input_tensor.numel() % scaler_block_size) == 0, ( f\"Input tensor must be divisible by block size, got {input_tensor.numel()} and {scaler_block_size}\" ) n_scaler_blocks = input_tensor.numel() // scaler_block_size input_tensor = input_tensor.view(n_scaler_blocks, scaler_block_size) dequantized = (input_tensor / quantization_factor.unsqueeze(-1)).flatten().to( self.dtype ) + self.scaler_mean return dequantized [docs] @staticmethod def convert_to_norm_float_weight( input_tensor: torch.Tensor, n_blocks: int, block_size: int, nf4: torch.Tensor ) -\u003e torch.Tensor: \"\"\"Convert a tensor to the normalized float weight format\"\"\" flattened_tensor = input_tensor.flatten() # Since we are using uint8 we will encode 2 entries per byte numel = input_tensor.numel() assert numel % 2 == 0, ( \"Number of elements must be even just to not have to think about the end\" ) # Reshape the flattened tensor into blocks of size self.block_size blocks = flattened_tensor.view(n_blocks, block_size) # Scale the blocks scalers = get_block_absmax(input_tensor.flatten(), block_size) scales = scalers.unsqueeze(-1).expand(n_blocks, block_size) scaled_blocks = blocks / scales # Returns a flattened tensor with each element quantized to nf4 index # See Note: Quantize in Chunks quantized_blocks = torch.empty( numel, dtype=torch.uint8, device=input_tensor.device ) flattened = scaled_blocks.flatten() for chunk_num in range(math.ceil(numel / CHUNK_SIZE)): start = chunk_num * CHUNK_SIZE end = min(start + CHUNK_SIZE, numel) quantized_blocks[start:end] = NF4Tensor.quantize_tensor_nearest( flattened[start:end], nf4 ).to(torch.uint8) # Combine the quantized elements into uint8 values # This lays out two consecutive elements in the same byte # [a, b, c, d] -\u003e [ab, cd] # The size of combined blocks will be half the size of the original tensor combined_blocks = quantized_blocks[::2] \u003c\u003c 4 | quantized_blocks[1::2] return combined_blocks.to(torch.uint8) [docs] def get_original_weight(self) -\u003e torch.Tensor: \"\"\"Get the original weight from the normalized float weight format\"\"\" # Since we are using uint8 we will decode 2 entries per byte # Shift elements down 4 and select out the bottom 4 bits first_elements = (self.quantized_data \u003e\u003e 4).to(torch.long) second_elements = (self.quantized_data \u0026 0b1111).to(torch.long) # Dequantize every element dequantized_first = self.dequantize(first_elements, self.nf4) dequantized_second = self.dequantize(second_elements, self.nf4) # Build up matrix of scalers repeated for each element in the block # Since first and second elements make up a full block # we expand out to half the size of the full block scalers = self.dequantize_scalers( self.quantized_scalers, self.quantization_factor, self.scaler_block_size ) repeated = scalers.unsqueeze(-1).expand(scalers.size(0), self.block_size // 2) scaled_first = dequantized_first * repeated.flatten() scaled_second = dequantized_second * repeated.flatten() # Flip them to be vertical and them stack them together horizontally # Upon flattening this will interleave the elements scaled_first = scaled_first.unsqueeze(-1).transpose(0, 1) scaled_second = scaled_second.unsqueeze(-1).transpose(0, 1) return torch.stack([scaled_first, scaled_second], dim=-1).reshape(self.shape) [docs] @staticmethod def quantize_tensor_nearest(value: torch.Tensor, nf4: torch.Tensor) -\u003e torch.Tensor: \"\"\"Quantize a float16 tensor to nf4 format to nearest and not rounded up\"\"\" value = value.unsqueeze(-1) # (numel, 1) # Compare the value tensor with the nf4 tensor element-wise diff = (value - nf4).abs() closest_nf4 = diff.min(dim=-1).indices return closest_nf4 [docs] @staticmethod def dequantize(value: torch.Tensor, nf4: torch.Tensor) -\u003e torch.Tensor: \"\"\"Dequantize a nf4 value to bfloat16 format\"\"\" # return nf4.index_select(0, value) return nf4[value] def __repr__(self) -\u003e str: return f\"Quantized Data: {self.quantized_data}\\nScalers: {self.quantized_scalers}\\n\" def __str__(self) -\u003e str: return f\"NF4Tensor({self.shape}, {self.block_size})\" def __tensor_flatten__(self): tensor_meta = SubclassTensorArgs( self.shape, self.stride(), self.storage_offset(), self.dtype, self.device, self.requires_grad, ) ctx = { \"block_size\": self.block_size, \"n_blocks\": self.n_blocks, \"scaler_block_size\": self.scaler_block_size, \"tensor_meta\": tensor_meta, } return [ \"quantized_data\", \"scaler_mean\", \"quantization_factor\", \"quantized_scalers\", \"nf4\", ], ctx @staticmethod def __tensor_unflatten__(inner_tensors: Dict, metadata, outer_size, outer_stride): assert len(inner_tensors) == 5, \"Expected 5 inner tensors\" return NF4Tensor( metadata[\"tensor_meta\"], metadata[\"block_size\"], metadata[\"n_blocks\"], metadata[\"scaler_block_size\"], inner_tensors[\"quantized_scalers\"], inner_tensors[\"quantization_factor\"], inner_tensors[\"scaler_mean\"], inner_tensors[\"quantized_data\"], inner_tensors[\"nf4\"], ) @classmethod @torch._dynamo.disable def __torch_dispatch__(cls, func, types, args, kwargs=None): \"\"\"TODO we are not supporting torch dispatch at the moment instead we have created a Autograd.Function to handle the linear \"\"\" # All ops in the NF4_OPS_TABLE expect NF4 Tensors as inputs # And don\u0027t support mixed tensor subclasses. This will trigger the handler for # the next type in the dispatch list def allowed_subclasses(type): return ( issubclass(cls, type) or issubclass(torch._subclasses.fake_tensor.FakeTensor, type) or issubclass( torch._subclasses.functional_tensor.FunctionalTensor, type ) ) if not all(allowed_subclasses(t) for t in types): return NotImplemented(\"Up to the next one to handle\") if func in NF4_OPS_TABLE: return NF4_OPS_TABLE[func](func, args, kwargs) raise NotImplementedError( f\"NF4Tensor dispatch: attempting to run {func}, this is not supported\" ) # Do not force the Float8TrainingTensor type on the returned tensor @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} try: if func in NF4_TORCH_FUNCTIONS: return NF4_TORCH_FUNCTIONS[func](*args, **kwargs) except NotImplementedError: pass with torch._C.DisableTorchFunctionSubclass(): return func(*args, **kwargs) def fsdp_pre_all_gather( self, mesh: DeviceMesh ) -\u003e Tuple[Tuple[torch.Tensor, ...], Any]: return ( self.quantized_scalers, self.quantization_factor, self.quantized_data, ), ( SubclassTensorArgs( self.size(), self.stride(), self.storage_offset(), self.dtype, self.device, self.requires_grad, ), self.block_size, self.n_blocks, self.scaler_block_size, self.scaler_mean, self.nf4, mesh.get_group().size(), ) def fsdp_post_all_gather( self, all_gather_outputs: Tuple[torch.Tensor, ...], metadata: Any, param_dtype: torch.dtype, *, out: Optional[torch.Tensor] = None, ) -\u003e Union[Tuple[torch.Tensor, Tuple[torch.Tensor, ...]], None]: (quantized_scalers, quantization_factor, quantized_data) = all_gather_outputs ( tensor_meta, block_size, n_blocks, scaler_block_size, scaler_mean, nf4, pg_size, ) = metadata if len(tensor_meta.original_shape) != 2: raise NotImplementedError( f\"only support 2D shape but got dim={len(tensor_meta.original_shape)}\" ) new_shape = torch.Size( (tensor_meta.original_shape[0] * pg_size, tensor_meta.original_shape[1]) ) new_tensor_meta = replace(tensor_meta, original_shape=new_shape) if out is not None: # TODO: add param dtype for mixed precision assert isinstance(out, NF4Tensor), f\"{type(out)}\" assert ( quantized_scalers.untyped_storage().data_ptr() == out.quantized_scalers.untyped_storage().data_ptr() and quantization_factor.untyped_storage().data_ptr() == out.quantization_factor.untyped_storage().data_ptr() and quantized_data.untyped_storage().data_ptr() == out.quantized_data.untyped_storage().data_ptr() ), \"Expects out\u0027s data to be the all-gather output\" return return nf4_constructor( new_tensor_meta, block_size, n_blocks, scaler_block_size, quantized_scalers, quantization_factor, scaler_mean, quantized_data, nf4, ), (quantized_scalers, quantization_factor, quantized_data) class LinearNF4(torch.autograd.Function): @staticmethod def forward(ctx, input: torch.Tensor, weight: NF4Tensor): \"\"\"Save the quantized nf4 weight for backward pass\"\"\" ctx.save_for_backward(weight) return F.linear(input, weight.to(input.dtype)) @staticmethod def backward(ctx, grad_output): \"\"\"The nf4 weight will never require grad so we can just return the grad_output @ weight.to(grad_output.dtype)\"\"\" weight: NF4Tensor = ctx.saved_tensors[0] return grad_output @ weight.to(grad_output.dtype), None def linear_nf4(input: torch.Tensor, weight: NF4Tensor) -\u003e torch.Tensor: \"\"\"Apply a linear operation with the NF4Tensor weight Args: input: Input tensor weight: NF4Tensor weight \"\"\" return LinearNF4.apply(input, weight) [docs]def to_nf4(tensor, block_size: int = 64, scaler_block_size: int = 256): \"\"\"Convert a given tensor to normalized float 4-bit tensor.\"\"\" return NF4Tensor.from_tensor(tensor, block_size, scaler_block_size) NF4_TORCH_FUNCTIONS = {} def implements_torch_function(torch_function): def decorator(func): functools.update_wrapper(func, torch_function) NF4_TORCH_FUNCTIONS[torch_function] = func return func return decorator @implements_torch_function(torch.Tensor.to) def function_to_dtype(*args, **kwargs): tensor = args[0] device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to( *args[1:], **kwargs ) # dtype is specified -\u003e dequantize if dtype is not None: return tensor.get_original_weight().to( device, dtype, non_blocking, memory_format=convert_to_format ) # dtype is not specified -\u003e keep NF4 updated_attrs = dict(device=device) tensor_attrs, _ = tensor.__tensor_flatten__() for attr in tensor_attrs: inner_tensor = getattr(tensor, attr) updated_attrs[attr] = inner_tensor.to( device, dtype, non_blocking, memory_format=convert_to_format ) return NF4Tensor(*construct_nf4_args(tensor, updated_attrs)) @implements_torch_function(torch.Tensor.cpu) def function_cpu(*args, **kwargs): # Tensor.cpu(self, memory_format) return args[0].to(\"cpu\", *args[1:], **kwargs) @implements_torch_function(torch.Tensor.cuda) def function_cuda(*args, **kwargs): # Tensor.cuda(self, device, non_blocking, memory_format) tensor = args[0] updated_attrs = dict() tensor_attrs, _ = tensor.__tensor_flatten__() for attr in tensor_attrs: inner_tensor = getattr(tensor, attr) updated_attrs[attr] = inner_tensor.cuda(*args[1:], **kwargs) updated_attrs[\"device\"] = updated_attrs[tensor_attrs[0]].device return NF4Tensor(*construct_nf4_args(tensor, updated_attrs)) @implements_torch_function(F.linear) def _(*args, **kwargs): input = args[0] weight = args[1] bias = args[2] if len(args) \u003e 2 else None out = LinearNF4.apply(input, weight) if bias is not None: out = out + bias return out @implements_torch_function(torch.Tensor.view_as) def function_view_as(*args, **kwargs): \"\"\"Handle view_as for NF4Tensor. When view_as is called (typically by autograd internals), we need to return a fresh NF4Tensor without autograd metadata to avoid conflicts. \"\"\" tensor = args[0] # Create a new NF4Tensor with detached inner tensors to avoid autograd conflicts updated_attrs = {} tensor_attrs, _ = tensor.__tensor_flatten__() for attr in tensor_attrs: inner_tensor = getattr(tensor, attr) # Detach to create a fresh tensor without autograd metadata updated_attrs[attr] = inner_tensor.detach() return NF4Tensor(*construct_nf4_args(tensor, updated_attrs)) @torch._dynamo.allow_in_graph def nf4_constructor( tensor_meta: SubclassTensorArgs, block_size: int, n_blocks: int, scaler_block_size: int, quantized_scalers: torch.Tensor, quantization_factor: torch.Tensor, scaler_mean: torch.Tensor, quantized_data: torch.Tensor, nf4: torch.Tensor, ): return NF4Tensor( tensor_meta, block_size, n_blocks, scaler_block_size, quantized_scalers, quantization_factor, scaler_mean, quantized_data, nf4, ) torch.serialization.add_safe_globals([NF4Tensor]) torch.serialization.add_safe_globals([NF4Tensor])",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchao/dtypes/nf4tensor.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>