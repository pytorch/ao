Search.setIndex({"docnames": ["api_ref_dtypes", "api_ref_float8", "api_ref_intro", "api_ref_kernel", "api_ref_quantization", "api_ref_sparsity", "contributor_guide", "dtypes", "finetuning", "generated/torchao.dtypes.AffineQuantizedTensor", "generated/torchao.dtypes.BlockSparseLayout", "generated/torchao.dtypes.CutlassInt4PackedLayout", "generated/torchao.dtypes.CutlassSemiSparseLayout", "generated/torchao.dtypes.Float8Layout", "generated/torchao.dtypes.Int4CPULayout", "generated/torchao.dtypes.Layout", "generated/torchao.dtypes.MarlinQQQLayout", "generated/torchao.dtypes.MarlinQQQTensor", "generated/torchao.dtypes.MarlinSparseLayout", "generated/torchao.dtypes.NF4Tensor", "generated/torchao.dtypes.PlainLayout", "generated/torchao.dtypes.SemiSparseLayout", "generated/torchao.dtypes.TensorCoreTiledLayout", "generated/torchao.dtypes.UintxLayout", "generated/torchao.dtypes.to_affine_quantized_floatx", "generated/torchao.dtypes.to_affine_quantized_floatx_static", "generated/torchao.dtypes.to_affine_quantized_fpx", "generated/torchao.dtypes.to_affine_quantized_intx", "generated/torchao.dtypes.to_affine_quantized_intx_static", "generated/torchao.dtypes.to_marlinqqq_quantized_intx", "generated/torchao.dtypes.to_nf4", "generated/torchao.float8.CastConfig", "generated/torchao.float8.Float8LinearConfig", "generated/torchao.float8.ScalingGranularity", "generated/torchao.float8.ScalingType", "generated/torchao.float8.convert_to_float8_training", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp", "generated/torchao.quantization.FPXWeightOnlyConfig", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig", "generated/torchao.quantization.Float8WeightOnlyConfig", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig", "generated/torchao.quantization.Int4WeightOnlyConfig", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig", "generated/torchao.quantization.Int8WeightOnlyConfig", "generated/torchao.quantization.MappingType", "generated/torchao.quantization.TorchAODType", "generated/torchao.quantization.UIntXWeightOnlyConfig", "generated/torchao.quantization.ZeroPointDomain", "generated/torchao.quantization.autoquant", "generated/torchao.quantization.choose_qparams_affine", "generated/torchao.quantization.choose_qparams_affine_with_min_max", "generated/torchao.quantization.dequantize_affine", "generated/torchao.quantization.int_scaled_matmul", "generated/torchao.quantization.qat.ComposableQATQuantizer", "generated/torchao.quantization.qat.FakeQuantizeConfig", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.initialize_fake_quantizers", "generated/torchao.quantization.quantize_", "generated/torchao.quantization.quantize_affine", "generated/torchao.quantization.safe_int_mm", "generated/torchao.quantization.smooth_fq_linear_to_inference", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear", "generated/torchao.quantization.to_linear_activation_quantized", "generated/torchao.sparsity.PerChannelNormObserver", "generated/torchao.sparsity.WandaSparsifier", "generated/torchao.sparsity.apply_fake_sparsity", "generated/torchao.sparsity.semi_sparse_weight", "generated/torchao.sparsity.sparsify_", "index", "performant_kernels", "pretraining", "quantization", "quick_start", "serialization", "serving", "sg_execution_times", "sparsity", "static_quantization", "subclass_advanced", "subclass_basic", "torchao_vllm_integration", "tutorials/index", "tutorials/sg_execution_times", "tutorials/template_tutorial", "tutorials_source/pt2e_quant_openvino", "tutorials_source/pt2e_quant_ptq", "tutorials_source/pt2e_quant_qat", "tutorials_source/pt2e_quant_x86_inductor", "tutorials_source/pt2e_quant_xpu_inductor", "tutorials_source/pt2e_quantizer"], "filenames": ["api_ref_dtypes.rst", "api_ref_float8.rst", "api_ref_intro.rst", "api_ref_kernel.rst", "api_ref_quantization.rst", "api_ref_sparsity.rst", "contributor_guide.rst", "dtypes.rst", "finetuning.rst", "generated/torchao.dtypes.AffineQuantizedTensor.rst", "generated/torchao.dtypes.BlockSparseLayout.rst", "generated/torchao.dtypes.CutlassInt4PackedLayout.rst", "generated/torchao.dtypes.CutlassSemiSparseLayout.rst", "generated/torchao.dtypes.Float8Layout.rst", "generated/torchao.dtypes.Int4CPULayout.rst", "generated/torchao.dtypes.Layout.rst", "generated/torchao.dtypes.MarlinQQQLayout.rst", "generated/torchao.dtypes.MarlinQQQTensor.rst", "generated/torchao.dtypes.MarlinSparseLayout.rst", "generated/torchao.dtypes.NF4Tensor.rst", "generated/torchao.dtypes.PlainLayout.rst", "generated/torchao.dtypes.SemiSparseLayout.rst", "generated/torchao.dtypes.TensorCoreTiledLayout.rst", "generated/torchao.dtypes.UintxLayout.rst", "generated/torchao.dtypes.to_affine_quantized_floatx.rst", "generated/torchao.dtypes.to_affine_quantized_floatx_static.rst", "generated/torchao.dtypes.to_affine_quantized_fpx.rst", "generated/torchao.dtypes.to_affine_quantized_intx.rst", "generated/torchao.dtypes.to_affine_quantized_intx_static.rst", "generated/torchao.dtypes.to_marlinqqq_quantized_intx.rst", "generated/torchao.dtypes.to_nf4.rst", "generated/torchao.float8.CastConfig.rst", "generated/torchao.float8.Float8LinearConfig.rst", "generated/torchao.float8.ScalingGranularity.rst", "generated/torchao.float8.ScalingType.rst", "generated/torchao.float8.convert_to_float8_training.rst", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp.rst", "generated/torchao.quantization.FPXWeightOnlyConfig.rst", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8WeightOnlyConfig.rst", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.rst", "generated/torchao.quantization.Int4WeightOnlyConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.rst", "generated/torchao.quantization.Int8WeightOnlyConfig.rst", "generated/torchao.quantization.MappingType.rst", "generated/torchao.quantization.TorchAODType.rst", "generated/torchao.quantization.UIntXWeightOnlyConfig.rst", "generated/torchao.quantization.ZeroPointDomain.rst", "generated/torchao.quantization.autoquant.rst", "generated/torchao.quantization.choose_qparams_affine.rst", "generated/torchao.quantization.choose_qparams_affine_with_min_max.rst", "generated/torchao.quantization.dequantize_affine.rst", "generated/torchao.quantization.int_scaled_matmul.rst", "generated/torchao.quantization.qat.ComposableQATQuantizer.rst", "generated/torchao.quantization.qat.FakeQuantizeConfig.rst", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer.rst", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.rst", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.rst", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.initialize_fake_quantizers.rst", "generated/torchao.quantization.quantize_.rst", "generated/torchao.quantization.quantize_affine.rst", "generated/torchao.quantization.safe_int_mm.rst", "generated/torchao.quantization.smooth_fq_linear_to_inference.rst", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear.rst", "generated/torchao.quantization.to_linear_activation_quantized.rst", "generated/torchao.sparsity.PerChannelNormObserver.rst", "generated/torchao.sparsity.WandaSparsifier.rst", "generated/torchao.sparsity.apply_fake_sparsity.rst", "generated/torchao.sparsity.semi_sparse_weight.rst", "generated/torchao.sparsity.sparsify_.rst", "index.rst", "performant_kernels.rst", "pretraining.rst", "quantization.rst", "quick_start.rst", "serialization.rst", "serving.rst", "sg_execution_times.rst", "sparsity.rst", "static_quantization.rst", "subclass_advanced.rst", "subclass_basic.rst", "torchao_vllm_integration.md", "tutorials/index.rst", "tutorials/sg_execution_times.rst", "tutorials/template_tutorial.rst", "tutorials_source/pt2e_quant_openvino.rst", "tutorials_source/pt2e_quant_ptq.rst", "tutorials_source/pt2e_quant_qat.rst", "tutorials_source/pt2e_quant_x86_inductor.rst", "tutorials_source/pt2e_quant_xpu_inductor.rst", "tutorials_source/pt2e_quantizer.rst"], "titles": ["torchao.dtypes", "torchao.float8", "<code class=\"docutils literal notranslate\"><span class=\"pre\">torchao</span></code> API Reference", "torchao.kernel", "torchao.quantization", "torchao.sparsity", "Contributor Guide", "Dtypes", "(Part 2) Fine-tuning with QAT, QLoRA, and float8", "AffineQuantizedTensor", "BlockSparseLayout", "CutlassInt4PackedLayout", "CutlassSemiSparseLayout", "Float8Layout", "Int4CPULayout", "Layout", "MarlinQQQLayout", "MarlinQQQTensor", "MarlinSparseLayout", "NF4Tensor", "PlainLayout", "SemiSparseLayout", "TensorCoreTiledLayout", "UintxLayout", "to_affine_quantized_floatx", "to_affine_quantized_floatx_static", "to_affine_quantized_fpx", "to_affine_quantized_intx", "to_affine_quantized_intx_static", "to_marlinqqq_quantized_intx", "to_nf4", "CastConfig", "Float8LinearConfig", "ScalingGranularity", "ScalingType", "convert_to_float8_training", "precompute_float8_dynamic_scale_for_fsdp", "FPXWeightOnlyConfig", "Float8DynamicActivationFloat8WeightConfig", "Float8StaticActivationFloat8WeightConfig", "Float8WeightOnlyConfig", "GemliteUIntXWeightOnlyConfig", "Int4WeightOnlyConfig", "Int8DynamicActivationInt4WeightConfig", "Int8DynamicActivationInt8WeightConfig", "Int8WeightOnlyConfig", "MappingType", "TorchAODType", "UIntXWeightOnlyConfig", "ZeroPointDomain", "autoquant", "choose_qparams_affine", "choose_qparams_affine_with_min_max", "dequantize_affine", "int_scaled_matmul", "ComposableQATQuantizer", "FakeQuantizeConfig", "FromIntXQuantizationAwareTrainingConfig", "Int4WeightOnlyEmbeddingQATQuantizer", "Int4WeightOnlyQATQuantizer", "Int8DynActInt4WeightQATQuantizer", "IntXQuantizationAwareTrainingConfig", "initialize_fake_quantizers", "quantize", "quantize_affine", "safe_int_mm", "smooth_fq_linear_to_inference", "swap_linear_with_smooth_fq_linear", "to_linear_activation_quantized", "PerChannelNormObserver", "WandaSparsifier", "apply_fake_sparsity", "semi_sparse_weight", "sparsify", "Welcome to the torchao Documentation", "Performant Kernels", "(Part 1) Pre-training with float8", "Quantization Overview", "Quick Start Guide", "Serialization", "(Part 3) Serving on vLLM, SGLang, ExecuTorch", "Computation times", "Sparsity Overview", "Static Quantization", "Writing Your Own Quantized Tensor (advanced)", "Writing Your Own Quantized Tensor", "Integration with VLLM: Architecture and Usage Guide", "&lt;no title&gt;", "Computation times", "Template Tutorial", "PyTorch 2 Export Quantization for OpenVINO torch.compile Backend", "PyTorch 2 Export Post Training Quantization", "PyTorch 2 Export Quantization-Aware Training (QAT)", "PyTorch 2 Export Quantization with X86 Backend through Inductor", "PyTorch 2 Export Quantization with Intel GPU Backend through Inductor", "How to Write a <code class=\"docutils literal notranslate\"><span class=\"pre\">Quantizer</span></code> for PyTorch 2 Export Quantization"], "terms": {"thi": [2, 6, 8, 9, 18, 19, 20, 21, 23, 36, 37, 41, 42, 43, 46, 50, 51, 52, 53, 56, 57, 63, 64, 69, 70, 71, 73, 76, 77, 78, 79, 80, 82, 83, 85, 86, 89, 90, 91, 92, 93, 94, 95], "section": [2, 6, 77, 82, 86, 91, 92, 95], "introduc": [2, 8, 90, 91, 93, 94, 95], "dive": 2, "detail": [2, 6, 8, 37, 50, 76, 77, 78, 82, 83, 85, 90, 91, 92, 93], "how": [2, 6, 8, 9, 15, 23, 42, 46, 51, 56, 57, 64, 74, 76, 78, 79, 82, 83, 85, 86, 90, 93, 94], "integr": [2, 6, 74, 76, 79, 80, 82, 85, 93, 95], "pytorch": [2, 6, 9, 14, 17, 47, 56, 74, 76, 82, 85, 86, 89], "optim": [2, 6, 8, 18, 36, 50, 63, 74, 76, 80, 82, 85, 90, 92, 93, 94], "your": [2, 6, 8, 74, 76, 77, 78, 82, 91, 92, 93, 94, 95], "machin": [2, 92], "learn": [2, 42, 56, 78, 82, 89, 91, 93, 94, 95], "model": [2, 8, 36, 41, 43, 50, 55, 57, 58, 59, 60, 62, 63, 66, 67, 70, 71, 73, 78, 80, 82, 83, 85, 93, 94, 95], "dtype": [2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 47, 48, 50, 51, 52, 53, 56, 58, 59, 60, 64, 73, 74, 76, 78, 79, 83, 85, 86, 91, 93, 94, 95], "quantiz": [2, 6, 9, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 24, 27, 29, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 73, 76, 79, 80, 82], "sparsiti": [2, 8, 12, 18, 21, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80], "tba": [3, 7, 75], "For": [6, 8, 9, 37, 56, 77, 78, 79, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "new": [6, 9, 76, 77, 83, 85, 91, 92, 93, 95], "case": [6, 50, 65, 82, 85, 86, 90, 91, 95], "exampl": [6, 8, 9, 36, 46, 50, 55, 56, 57, 62, 63, 70, 73, 77, 79, 81, 82, 83, 85, 87, 88, 89, 90, 91, 92, 93, 94], "train": [6, 32, 55, 56, 74, 78, 80, 82, 85, 95], "like": [6, 15, 50, 76, 77, 78, 79, 82, 85, 86, 90, 91, 92, 93, 94, 95], "fp4": 6, "s": [6, 8, 9, 46, 50, 51, 53, 64, 76, 77, 78, 82, 83, 85, 91, 92, 93, 94, 95], "fine": [6, 41, 42, 43, 48, 74, 76, 80, 82], "start": [6, 8, 33, 34, 46, 47, 49, 50, 76, 77, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "prototyp": [6, 56, 62, 77, 95], "folder": [6, 91, 92], "you": [6, 8, 56, 70, 76, 77, 78, 79, 82, 85, 86, 89, 90, 91, 92, 93, 94, 95], "could": [6, 77, 85, 90, 91, 93, 94, 95], "also": [6, 8, 50, 56, 63, 77, 78, 79, 82, 83, 85, 86, 91, 94, 95], "take": [6, 19, 63, 69, 73, 77, 82, 90, 91, 92, 93, 94, 95], "look": [6, 9, 76, 77, 82, 90, 91, 92, 93, 94], "affinequantizedtensor": [6, 17, 25, 26, 28, 77, 78, 79, 83, 85], "what": [6, 8, 9, 17, 50, 76, 77, 78, 82, 83, 86, 89, 91, 95], "want": [6, 63, 73, 77, 78, 79, 82, 85, 86, 90, 91, 92, 95], "do": [6, 47, 50, 54, 63, 77, 82, 83, 85, 86, 91, 92, 93, 95], "mostli": [6, 52, 78, 93], "e": [6, 8, 9, 37, 46, 50, 51, 53, 55, 56, 63, 64, 76, 77, 79, 83, 85, 90, 95], "g": [6, 8, 9, 37, 46, 50, 51, 53, 55, 56, 63, 64, 77, 79, 83, 85, 90, 95], "int3": 6, "exact": [6, 8, 91, 92], "same": [6, 8, 9, 38, 51, 52, 53, 64, 65, 73, 76, 77, 82, 83, 85, 92, 93, 94, 95], "affin": [6, 9, 11, 12, 13, 14, 18, 21, 22, 27, 53, 64, 77], "pleas": [6, 8, 9, 17, 37, 42, 74, 77, 78, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "feel": [6, 77, 82, 85, 86], "free": [6, 77, 85], "open": [6, 77, 82], "an": [6, 8, 9, 22, 27, 28, 50, 56, 70, 74, 76, 77, 78, 80, 82, 83, 85, 90, 91, 92, 93, 94, 95], "issu": [6, 77, 78, 85, 93], "have": [6, 8, 41, 42, 46, 50, 58, 59, 60, 64, 70, 77, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "question": [6, 77, 79, 82, 85, 95], "specif": [6, 8, 15, 18, 20, 21, 70, 76, 77, 78, 79, 82, 90, 93, 94, 95], "more": [6, 8, 9, 37, 41, 42, 43, 48, 50, 76, 77, 78, 82, 83, 85, 86, 90, 91, 92, 93, 94], "refer": [6, 8, 9, 76, 82, 83, 85, 86, 90, 91, 92, 93], "our": [6, 8, 19, 76, 78, 80, 82, 83, 85, 91, 92], "overview": [6, 74, 78, 86], "page": [6, 78, 93], "To": [6, 8, 9, 17, 50, 76, 77, 78, 79, 82, 83, 86, 91, 92, 93, 95], "contribut": [6, 78, 82], "exist": [6, 47, 76, 77, 82, 83, 85, 91, 95], "code": [6, 42, 76, 77, 78, 82, 83, 85, 87, 89, 91, 92, 93, 94, 95], "base": [6, 15, 20, 46, 62, 70, 77, 78, 82, 85, 86, 90, 91, 92, 93, 94, 95], "make": [6, 77, 78, 85, 86, 91, 95], "trainabl": [6, 8, 77, 85], "add": [6, 20, 85, 89, 93, 95], "parallel": [6, 76, 85, 86], "etc": [6, 77, 90, 95], "affine_quantized_tensor": [6, 79], "py": [6, 9, 17, 81, 88, 89, 93, 94], "api": [6, 50, 61, 77, 78, 82, 83, 85, 90, 91, 92, 93, 94], "quant_api": [6, 63, 79, 83], "primit": [6, 9, 17, 85, 91], "op": [6, 8, 9, 17, 42, 50, 63, 78, 82, 85, 86, 91, 92, 93, 95], "slight": [6, 82], "variat": [6, 77], "quant_primit": [6, 9, 17, 83], "autotun": [6, 78, 83], "cpu": [6, 9, 14, 79, 82, 83, 86, 90, 91, 92, 93], "cuda": [6, 8, 9, 63, 76, 78, 79, 82, 83, 85, 92], "mp": 6, "csrc": 6, "mayb": [6, 31], "well": [6, 15, 50, 77, 78, 82, 91, 92, 95], "spars": [6, 10, 18, 21, 70, 77, 82], "marlin": [6, 16, 17, 18, 29], "aqt": 6, "621": 6, "we": [6, 8, 9, 19, 46, 48, 50, 51, 52, 53, 56, 63, 64, 73, 76, 77, 78, 79, 82, 83, 86, 90, 91, 92, 93, 94, 95], "ar": [6, 8, 9, 13, 21, 23, 35, 37, 38, 41, 42, 50, 51, 53, 55, 63, 64, 65, 70, 76, 77, 78, 79, 82, 83, 86, 90, 91, 92, 93, 94, 95], "still": [6, 8, 77, 82, 91, 95], "decid": [6, 77, 82, 83], "split": [6, 91, 92], "can": [6, 8, 22, 38, 41, 46, 50, 55, 56, 63, 64, 76, 77, 78, 79, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "implement": [6, 8, 32, 79, 82, 83, 90, 91, 95], "regist": [6, 69, 85], "mai": [6, 52, 56, 77, 79, 83, 91, 92, 93, 94, 95], "need": [6, 38, 69, 70, 77, 78, 79, 82, 85, 86, 91, 92, 93, 95], "defin": [6, 15, 23, 33, 37, 69, 70, 78, 82, 83, 85, 86, 90, 93, 94, 95], "own": [6, 8, 74, 76, 78, 82, 83, 91, 92, 95], "through": [6, 8, 52, 74, 77, 78, 83, 85, 86, 89, 90, 91, 95], "int4": [6, 8, 11, 14, 43, 46, 56, 58, 59, 60, 63, 73, 78, 79, 86], "access": [6, 90], "my_custom_op": 6, "devic": [6, 9, 63, 65, 76, 78, 79, 83, 85, 86, 90, 91, 92, 93, 94], "check": [6, 8, 9, 17, 77, 78, 79, 85, 90, 92, 95], "condit": [6, 77], "__torch_function__": [6, 77, 85], "__torch_dispatch__": [6, 85], "target": [6, 8, 38, 39, 40, 42, 51, 70, 78, 82, 90, 91, 92, 93, 94, 95], "oper": [6, 8, 9, 13, 15, 18, 52, 78, 90, 91, 92, 93, 94], "bfloat16": [6, 19, 59, 64, 76, 77, 78, 79, 82, 83, 86, 93, 94], "activ": [6, 8, 38, 39, 41, 43, 44, 50, 56, 60, 66, 70, 74, 78, 82, 83, 86, 90, 93, 94, 95], "uint4": [6, 42, 77, 78], "weight": [6, 8, 18, 19, 36, 38, 39, 40, 41, 42, 43, 44, 45, 48, 50, 56, 58, 59, 60, 63, 70, 73, 74, 76, 78, 79, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "found": [6, 77, 78, 82, 83, 85], "here": [6, 9, 64, 77, 78, 79, 83, 85, 86, 90, 91, 92, 93, 94, 95], "allow": [6, 78, 82, 85, 90, 91, 92, 93, 95], "peopl": [6, 77, 79, 86, 95], "linear": [6, 8, 18, 32, 35, 38, 40, 42, 43, 44, 45, 48, 50, 55, 59, 60, 63, 67, 71, 73, 76, 77, 78, 79, 82, 83, 85, 90, 91, 92, 93, 95], "two": [6, 8, 17, 21, 38, 77, 78, 82, 85, 90, 91, 92, 93, 95], "dispatch_condit": [6, 77], "impl": [6, 9, 77], "actual": [6, 8, 40, 77, 83, 85, 86, 91, 92, 95], "bia": [6, 77, 78, 79, 83, 85, 86, 92, 95], "run": [6, 8, 36, 50, 63, 66, 69, 76, 77, 78, 82, 85, 89, 90, 91, 92, 93, 94, 95], "both": [6, 9, 38, 77, 78, 82, 83, 85, 91, 93, 94, 95], "input_tensor": [6, 19, 77, 86], "weight_tensor": [6, 77, 86], "argument": [6, 9, 22, 50, 53, 63, 76, 77, 93], "register_aqt_quantized_linear_dispatch": 6, "show": [6, 64, 76, 77, 78, 82, 86, 91, 92], "work": [6, 21, 41, 76, 79, 82, 85, 86, 91, 92, 93], "sometim": [6, 82], "ha": [6, 8, 9, 77, 82, 85, 86, 90, 91, 92, 94, 95], "pack": [6, 9, 11, 22, 23, 37, 41, 48, 77], "order": [6, 50, 55, 77, 82, 85, 95], "yield": [6, 8, 82], "And": [6, 19, 38, 77, 85, 93, 95], "abstract": [6, 77], "see": [6, 8, 9, 17, 37, 76, 77, 78, 79, 82, 83, 85, 86, 90, 91, 95], "full": [6, 8, 78, 83, 89, 90, 92], "after": [6, 8, 36, 50, 76, 77, 79, 82, 90, 91, 92, 93, 94, 95], "wrap": [6, 50, 85, 93, 94], "factori": 6, "convert": [6, 8, 9, 17, 19, 24, 27, 29, 30, 32, 55, 57, 58, 63, 73, 76, 77, 82, 90, 93, 94, 95], "from": [6, 8, 9, 19, 20, 25, 26, 28, 37, 43, 52, 57, 63, 64, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95], "float": [6, 8, 9, 17, 19, 27, 29, 30, 37, 42, 46, 49, 50, 51, 52, 53, 56, 64, 67, 70, 77, 78, 79, 85, 91, 92, 95], "point": [6, 9, 17, 29, 37, 42, 46, 49, 53, 56, 62, 76, 77, 78, 79, 82, 83, 85, 91, 95], "my": [6, 82, 92], "to_my_dtyp": 6, "mydtypetensor": 6, "from_float": [6, 83, 85], "level": [6, 70, 77, 82, 85, 90, 91, 93, 94], "reus": [6, 77, 85], "quantize_": [6, 8, 57, 63, 73, 77, 78, 79, 83], "appli": [6, 8, 9, 38, 39, 40, 41, 42, 43, 44, 45, 48, 50, 55, 63, 73, 77, 78, 82, 86, 92], "convers": [6, 8, 9, 35, 77], "filter": [6, 8, 35, 50, 76, 83], "choos": [6, 77, 82, 85, 91, 93], "which": [6, 8, 17, 23, 50, 76, 77, 78, 79, 82, 83, 86, 90, 91, 92, 93, 94, 95], "modul": [6, 8, 32, 33, 34, 35, 36, 46, 47, 49, 50, 55, 57, 58, 62, 63, 66, 67, 69, 70, 73, 76, 78, 79, 83, 90, 91, 92, 93, 94, 95], "should": [6, 8, 9, 36, 41, 53, 57, 69, 70, 76, 77, 82, 86, 90, 91, 95], "algorithm": [6, 42, 48, 82, 90], "onli": [6, 8, 14, 35, 38, 40, 41, 42, 43, 45, 48, 73, 76, 78, 79, 82, 85, 86, 90, 91, 93, 94, 95], "dynam": [6, 8, 31, 32, 36, 38, 41, 43, 44, 56, 60, 73, 83, 85, 91, 92, 93], "quant": [6, 9, 17, 37, 77, 86, 91, 94, 95], "static": [6, 9, 15, 19, 25, 28, 32, 39, 52, 56, 74, 78, 91, 92, 93, 94, 95], "type": [6, 8, 9, 18, 19, 23, 32, 33, 34, 35, 38, 39, 40, 42, 43, 46, 47, 49, 50, 54, 56, 64, 65, 74, 77, 79, 82, 85, 86, 90, 91, 93, 94, 95], "note": [6, 8, 55, 70, 77, 78, 82, 85, 86, 92, 93, 94], "2": [6, 9, 12, 14, 18, 21, 42, 46, 50, 56, 64, 71, 73, 74, 76, 77, 82, 83, 85, 89], "4": [6, 8, 12, 18, 21, 30, 41, 71, 73, 77, 78, 79, 82, 85, 91, 92], "below": [6, 76, 77, 82, 85, 86, 89, 90], "follow": [6, 8, 42, 56, 76, 77, 78, 82, 83, 85, 90, 91, 92, 93, 94, 95], "util": [6, 41, 76, 77, 78, 79, 85, 86, 90, 91, 92, 93, 94, 95], "import": [6, 8, 57, 63, 73, 78, 79, 82, 83, 85, 86, 89, 90, 93, 94], "unwrap_tensor_subclass": [6, 78], "m_unwrap": 6, "m": [6, 8, 63, 73, 76, 78, 79, 83, 85, 91, 92, 93], "In": [6, 8, 76, 77, 78, 82, 83, 85, 90, 91, 92, 93, 94, 95], "compat": [6, 18, 56, 78], "aim": [6, 77, 82, 94], "fullgraph": [6, 78], "true": [6, 9, 27, 32, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 50, 51, 52, 56, 63, 66, 73, 76, 78, 79, 83, 85, 86, 90, 91, 92, 93, 95], "first": [6, 19, 50, 54, 70, 77, 83, 85, 86, 91, 92, 95], "remov": [6, 51, 70, 76, 82, 86, 91, 92], "ani": [6, 20, 50, 58, 62, 68, 70, 77, 82, 85, 90, 92, 94], "unnecessari": 6, "graph": [6, 78, 91, 92, 95], "break": 6, "torch_log": 6, "output_cod": 6, "when": [6, 8, 9, 20, 51, 53, 64, 76, 77, 82, 83, 86, 90, 91, 92, 93, 94, 95], "script": [6, 78, 83, 85, 89, 92, 93, 94], "inductor": [6, 50, 74, 78, 90, 91], "python": [6, 77, 78, 82, 87, 89, 90, 91, 93, 94], "mode": [6, 41, 42, 50, 78, 83, 90, 92, 93, 94, 95], "max": [6, 46, 77, 78, 83, 85, 91, 92, 95], "checkout": [6, 9, 17, 74, 77], "doc": [6, 76, 77, 78, 85], "huggingfac": 6, "transform": [6, 8, 9, 77, 83, 90, 91, 92, 93, 94], "deseri": [6, 77, 91, 92], "save_pretrain": 6, "push_to_hub": [6, 86], "from_pretrain": [6, 86], "http": [6, 9, 17, 37, 50, 70, 78, 82, 94], "co": 6, "main": [6, 9, 17, 42, 77, 78, 82, 83, 85, 91, 95], "en": [6, 50], "anoth": [6, 77, 82, 85, 91, 95], "diffus": 6, "github": [6, 9, 17, 37, 78], "com": [6, 9, 17, 37], "sayakpaul": 6, "blob": [6, 9, 17], "infer": [6, 8, 9, 66, 74, 77, 78, 79, 82, 83, 85, 90, 91, 92, 93, 94], "serialization_and_load": 6, "md": 6, "The": [6, 8, 9, 10, 15, 18, 23, 35, 37, 38, 39, 40, 50, 54, 63, 65, 66, 67, 70, 76, 77, 78, 79, 82, 85, 86, 90, 91, 92, 93, 94, 95], "abov": [6, 8, 46, 77, 79, 82, 83, 85, 91, 92, 95], "just": [6, 46, 56, 77, 79, 82, 85, 91, 92, 95], "talk": [6, 77], "about": [6, 8, 42, 77, 78, 79, 82, 91, 92, 93, 95], "basic": [6, 20, 78, 83, 85], "provid": [6, 8, 15, 18, 21, 22, 50, 51, 55, 62, 76, 77, 80, 82, 85, 86, 91, 92, 94, 95], "fsdp": [6, 77], "ll": [6, 46, 76, 77, 85, 91, 92, 95], "put": [6, 73, 93, 95], "developer_api_guid": 6, "cover": [6, 77, 89, 91, 94, 95], "executorch": [6, 43, 63, 74, 78, 91, 92], "torchchat": 6, "todo": [6, 77], "qat": [6, 55, 56, 57, 58, 59, 60, 61, 62, 74, 93], "suit": [6, 91, 93], "out": [6, 8, 21, 46, 50, 70, 76, 77, 78, 82, 85, 90, 91, 92, 93], "differ": [6, 8, 15, 42, 52, 55, 64, 65, 76, 77, 78, 79, 82, 85, 86, 91, 92, 93, 95], "system": 6, "dtensor": [6, 85], "recommend": [6, 8, 38, 39, 40, 41, 42, 43, 48, 50, 76, 90, 93, 94], "copi": [6, 9, 70, 78, 79, 82, 83, 85, 90, 92, 93], "past": [6, 82], "adapt": [6, 76, 83], "now": [6, 8, 37, 43, 51, 76, 77, 78, 82, 83, 85, 90, 91, 93, 95], "befor": [6, 8, 63, 77, 79, 82, 83, 85, 91, 92, 95], "some": [6, 50, 63, 70, 77, 78, 82, 83, 85, 90, 91, 92, 93, 94, 95], "singl": [6, 8, 31, 36, 38, 50, 52, 76, 78, 82, 91, 95], "comput": [6, 18, 22, 36, 40, 69, 70, 82, 83, 85, 91, 92, 93, 94], "intens": 6, "memori": [6, 8, 9, 76, 78, 82, 85, 93, 94], "input": [6, 9, 18, 19, 21, 32, 35, 36, 50, 51, 52, 53, 54, 62, 63, 64, 65, 70, 73, 76, 77, 78, 83, 85, 90, 91, 92, 93, 94, 95], "dimens": [6, 9, 23, 48, 51, 53, 54, 64, 76, 85, 86, 91, 92], "get": [6, 8, 19, 76, 77, 78, 82, 86, 90, 91, 92, 93, 95], "sens": [6, 77, 85], "speedup": [6, 8, 42, 76, 77, 78, 82], "d": [6, 77, 92], "creat": [6, 9, 25, 26, 28, 76, 77, 82, 85, 90, 91, 93, 94, 95], "file": [6, 76, 81, 85, 86, 88, 91, 92], "benchmark_aq": 6, "shape": [6, 9, 17, 50, 54, 65, 78, 83, 85, 86, 91, 94], "A": [6, 8, 9, 23, 50, 52, 69, 82, 85, 86, 91], "quick": [6, 74], "wai": [6, 9, 50, 76, 77, 82, 83, 85, 91, 92, 95], "relev": [6, 42, 77, 89], "chang": [6, 63, 76, 77, 78, 79, 82, 83, 85, 90, 91, 92, 94, 95], "interest": [6, 77, 82, 85], "tutori": [6, 8, 9, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95], "print_op_and_shap": 6, "output": [6, 8, 32, 50, 51, 53, 64, 76, 77, 78, 82, 89, 90, 91, 92, 93, 94, 95], "torch_func": 6, "built": [6, 76, 85], "k": [6, 65, 78, 79, 83, 85, 91, 92], "n": [6, 8, 78, 79, 83, 85, 91, 92, 95], "10": [6, 8, 46, 64, 76, 78, 83, 91, 92], "method": [6, 15, 18, 21, 22, 50, 63, 70, 78, 82, 83, 85, 90, 91, 92, 94, 95], "_c": 6, "tensorbas": 6, "object": [6, 23, 57, 63, 73, 85, 91, 92, 95], "arg": [6, 9, 58, 70, 85, 86, 92, 95], "0": [6, 8, 9, 50, 56, 64, 67, 70, 76, 78, 79, 81, 82, 83, 85, 86, 88, 89, 91, 92, 94, 95], "size": [6, 9, 10, 17, 19, 41, 42, 43, 48, 51, 53, 56, 64, 76, 78, 79, 82, 83, 85, 86, 92], "all": [6, 36, 46, 50, 52, 58, 62, 69, 70, 71, 77, 78, 79, 81, 82, 83, 85, 86, 87, 90, 91, 93, 95], "under": [6, 8], "benchmark_your_kernel": 6, "helper": 6, "right": [6, 77, 82, 91], "1": [6, 18, 23, 33, 34, 42, 46, 47, 48, 49, 50, 64, 70, 74, 77, 78, 79, 81, 82, 83, 85, 88, 89, 91, 92], "either": [6, 9, 38, 70, 82, 92, 93, 94], "one": [6, 38, 50, 52, 69, 76, 77, 82, 85, 86, 92, 95], "probabl": 6, "keep": [6, 18, 70, 91], "futur": [6, 37, 83, 86, 91, 92, 93, 95], "llama": [6, 8, 86, 90], "llama2": 6, "llama3": [6, 8, 76], "sam": 6, "alreadi": [6, 9, 50, 85, 95], "modifi": [6, 35, 63, 70, 76, 77, 82, 85], "friendli": [6, 77], "compar": [6, 8, 42, 70, 76, 77, 91, 93, 95], "techniqu": [6, 8, 76, 79, 80, 82, 83, 85, 86], "repres": [6, 9, 10, 13, 15, 26, 32, 56, 64, 70, 77, 79, 85, 91, 92], "bound": [6, 82, 86], "help": [6, 8, 76, 77, 86, 90, 91], "option": [6, 9, 13, 17, 24, 27, 28, 29, 31, 32, 35, 38, 39, 41, 42, 44, 45, 50, 51, 52, 53, 56, 59, 61, 63, 64, 66, 67, 68, 70, 73, 76, 78, 86, 91, 92, 93, 94, 95], "each": [6, 19, 50, 56, 66, 69, 77, 82, 83, 85, 86, 91, 92, 95], "understand": [6, 76, 93, 95], "profil": 6, "profile_path": 6, "chrome": 6, "trace": [6, 77], "let": [6, 46, 64, 77, 78, 82, 83, 85, 95], "know": [6, 50, 57, 85], "end": [8, 76, 77, 80, 82, 85, 86, 89, 92, 95], "pre": [8, 15, 18, 22, 74, 78, 80, 82, 95], "serv": [8, 9, 15, 74, 76, 85, 94], "flow": [8, 43, 76, 80, 82, 83, 90, 91, 92, 93, 94], "leverag": [8, 76, 78, 80, 85, 93, 94], "partner": [8, 76, 80], "framework": [8, 76, 80, 90], "showcas": [8, 76, 80], "focus": [8, 76, 77, 80, 82], "step": [8, 20, 36, 50, 76, 77, 80, 82, 90, 91, 92, 93, 94, 95], "domain": [8, 9, 42, 49, 51, 53, 56, 76], "data": [8, 9, 10, 15, 18, 23, 38, 39, 40, 42, 52, 74, 77, 79, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95], "demonstr": [8, 76, 77, 78, 85, 90, 92], "dure": [8, 9, 17, 50, 53, 56, 67, 76, 78, 82, 83, 85, 90, 92], "numer": [8, 50, 76, 82, 91, 92, 93], "goal": 8, "mitig": [8, 82], "degrad": [8, 82], "eventu": [8, 76], "blog": 8, "readm": [8, 74, 78, 82], "reduc": [8, 36, 76, 82, 93], "resourc": [8, 85], "requir": [8, 20, 22, 76, 77, 78, 82, 85, 90, 93, 95], "small": 8, "matric": [8, 21, 82], "freez": [8, 92, 93, 94], "origin": [8, 9, 19, 40, 57, 64, 70, 77, 78, 79, 82, 90, 91, 95], "checkpoint": [8, 76, 86], "paramet": [8, 9, 15, 18, 19, 25, 28, 35, 36, 38, 39, 40, 41, 42, 43, 46, 48, 50, 51, 53, 54, 56, 63, 64, 65, 66, 67, 70, 73, 76, 77, 79, 82, 85, 86, 90, 91], "effici": [8, 22, 78, 82, 83, 94], "peft": 8, "paper": [8, 37, 82, 89], "speed": [8, 63, 82, 90], "up": [8, 19, 63, 76, 77, 78, 82, 90, 91, 92, 95], "high": [8, 9, 24, 25, 26, 27, 28, 76, 77, 82, 83, 85, 90, 91, 93, 94], "precis": [8, 9, 24, 25, 26, 27, 28, 40, 59, 60, 77, 83, 85, 90, 93, 94], "similar": [8, 77, 82, 83, 92, 93], "so": [8, 50, 76, 77, 78, 79, 82, 85, 91, 92, 95], "inevit": 8, "presum": 8, "support": [8, 9, 26, 38, 43, 56, 73, 76, 78, 79, 82, 85, 90, 91, 92, 93, 94, 95], "been": [8, 50, 85, 92, 93, 94, 95], "us": [8, 9, 13, 14, 15, 18, 19, 20, 23, 25, 28, 38, 39, 42, 43, 46, 48, 50, 51, 52, 53, 55, 56, 57, 64, 70, 74, 76, 77, 78, 79, 82, 83, 85, 86, 90, 91, 92, 93, 94], "successfulli": [8, 82], "recent": [8, 74], "releas": [8, 78, 93], "1b": [8, 86], "3b": 8, "llamaguard": 8, "8b": [8, 76], "improv": [8, 76, 82, 91, 94, 95], "qualiti": [8, 82], "involv": [8, 13, 82], "separ": [8, 56, 82, 86, 91, 95], "prepar": [8, 50, 55, 58, 66, 70, 77, 82, 90, 93, 94, 95], "fake": [8, 56, 57, 58, 59, 60, 76, 91, 92, 95], "mean": [8, 9, 19, 46, 51, 53, 64, 76, 77, 78, 82, 91, 92, 95], "valu": [8, 9, 19, 32, 33, 34, 38, 39, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 53, 64, 66, 70, 77, 82, 83, 85, 90, 91, 92, 95], "bf16": [8, 51, 77, 78, 82, 93, 94], "map": [8, 46, 56, 77, 85, 91, 95], "correspond": [8, 57, 63, 77, 79, 82, 85, 94, 95], "without": [8, 57, 77, 82, 86, 93, 95], "cast": [8, 31, 33], "them": [8, 50, 69, 77, 95], "lower": [8, 43, 77, 78, 82, 83, 92], "replac": [8, 67, 82, 86], "real": [8, 78, 91, 95], "doe": [8, 20, 42, 77, 82, 85, 91, 93, 94], "perform": [8, 9, 22, 36, 41, 50, 54, 58, 59, 60, 65, 66, 69, 76, 78, 82, 83, 85, 86, 90, 92, 93, 94], "There": [8, 77, 83, 85, 91, 95], "multipl": [8, 13, 38, 39, 50, 54, 55, 65, 78, 82, 83, 85, 86, 93, 95], "directli": [8, 46, 52, 77, 82, 83, 85], "loop": [8, 76, 82], "distribut": [8, 76, 83, 85, 86, 90], "recip": [8, 32, 69], "instead": [8, 42, 52, 56, 69, 76, 77, 78, 82, 85, 92, 93, 94, 95], "command": [8, 76, 78], "regular": [8, 90, 93, 94], "nnode": 8, "nproc_per_nod": 8, "full_finetune_distribut": 8, "config": [8, 32, 35, 50, 56, 63, 70, 73, 78, 82, 83, 86, 91, 93, 94], "llama3_2": 8, "3b_full": 8, "batch_siz": [8, 79, 83, 91, 92], "16": [8, 76], "user": [8, 50, 55, 76, 77, 78, 82, 83, 85, 89, 91, 92, 93, 94, 95], "equival": [8, 56, 67, 82, 92, 93, 95], "specifi": [8, 9, 32, 35, 48, 55, 63, 64, 70, 73, 76, 82, 90, 91, 92, 95], "default": [8, 9, 10, 13, 20, 22, 23, 38, 39, 40, 41, 42, 48, 50, 51, 53, 56, 63, 66, 67, 76, 78, 85, 86, 90, 91, 92, 93, 94, 95], "asymmetr": [8, 41, 42, 43, 46, 48, 51, 56, 77, 78, 83, 90, 94, 95], "per": [8, 9, 40, 42, 43, 44, 45, 48, 51, 53, 56, 58, 59, 60, 64, 70, 76, 77, 78, 82, 83, 94], "token": [8, 43, 44, 56, 60, 76], "int8": [8, 19, 43, 44, 45, 56, 60, 63, 73, 77, 85, 91, 93, 94, 95], "symmetr": [8, 38, 39, 40, 41, 43, 44, 45, 46, 51, 56, 85, 90, 91, 94, 95], "group": [8, 42, 43, 48, 56, 58, 59, 60, 77, 78], "configur": [8, 13, 31, 32, 35, 38, 39, 40, 42, 43, 44, 45, 48, 63, 73, 76, 77, 78, 93, 94, 95], "_component_": 8, "qat_distribut": 8, "3b_qat_ful": 8, "evalu": [8, 92], "result": [8, 50, 54, 65, 77, 82, 83, 91, 92, 93, 94, 95], "whether": [8, 42, 48, 49, 50, 51, 56, 85], "wa": [8, 85, 92], "process": [8, 15, 18, 20, 22, 23, 50, 67, 77, 82, 89, 90, 94], "llama3_2_3b": 8, "fullmodelhfcheckpoint": 8, "checkpoint_fil": 8, "00001": 8, "00002": 8, "safetensor": 8, "model_typ": [8, 86, 90], "int8dynactint4weightquant": 8, "groupsiz": [8, 59, 60, 64], "32": [8, 41, 42, 43, 56, 63, 73, 76, 78, 79, 83, 85, 92], "hellaswag": 8, "wikitext": 8, "eleuther_ev": 8, "eleuther_evalu": 8, "task": 8, "fullmodeltorchtunecheckpoint": 8, "8da4w": 8, "ckpt": 8, "llama3_token": 8, "path": [8, 63, 65, 78, 90, 91, 92, 93, 95], "tmp": [8, 78], "meta": [8, 79, 86, 95], "instruct": [8, 78, 91, 92, 93], "print": [8, 70, 78, 79, 85, 89, 91, 92], "version": [8, 14, 56, 76, 78, 85, 86, 91, 92, 95], "shot": [8, 82], "metric": [8, 70], "stderr": 8, "none": [8, 9, 13, 17, 24, 27, 28, 29, 31, 32, 33, 34, 35, 36, 38, 39, 41, 42, 45, 46, 47, 49, 50, 51, 52, 53, 56, 61, 62, 63, 64, 66, 67, 68, 70, 73, 83, 85, 86, 90, 91, 92, 94], "acc": [8, 91, 92], "5021": 8, "0050": 8, "acc_norm": 8, "6797": 8, "0047": 8, "bits_per_byt": 8, "6965": 8, "byte_perplex": 8, "6206": 8, "word_perplex": 8, "13": 8, "2199": 8, "much": [8, 78, 82, 95], "openassist": 8, "oasst1": 8, "dataset": [8, 76, 77, 90, 93, 94], "find": [8, 19, 82, 91, 95], "achiev": [8, 19, 76, 82, 83, 85, 92, 93], "higher": [8, 76, 77, 85, 90, 91, 93, 94], "accuraci": [8, 76, 82, 83, 90, 92, 93], "than": [8, 23, 56, 76, 77, 82, 85, 91], "recov": [8, 82, 92], "69": [8, 83], "8": [8, 22, 23, 41, 42, 46, 59, 76, 77, 78, 86, 93, 94], "overal": [8, 74, 78, 91, 95], "addit": [8, 15, 20, 50, 76, 82, 85, 90, 91, 94, 95], "vanilla": 8, "compos": [8, 55, 77, 82, 85, 91, 92, 95], "lora": 8, "89x": 8, "usag": [8, 9, 36, 50, 55, 56, 57, 74, 76, 93, 94], "36": [8, 76], "qat_lora_finetune_distribut": 8, "3b_qat_lora": 8, "set": [8, 9, 13, 38, 39, 40, 41, 42, 43, 48, 50, 52, 56, 63, 66, 70, 78, 82, 90, 92, 93, 94], "ad": [8, 9, 53, 70, 82, 83, 85, 92], "try": [8, 77, 82, 85, 91], "fsdp2": [8, 76], "yaml": 8, "onc": [8, 50, 82], "complet": [8, 50, 90, 94], "save": [8, 70, 76, 78, 79, 86], "qat_out": 8, "quatiz": 8, "document": [8, 85, 86, 90, 91, 93], "If": [8, 9, 13, 35, 38, 50, 54, 56, 65, 66, 70, 77, 78, 82, 85, 91, 92], "prefer": [8, 77, 78, 85], "custom": [8, 15, 69, 74, 76, 77, 78, 82, 85, 86, 90, 91, 93, 95], "call": [8, 9, 50, 69, 77, 78, 79, 82, 83, 85, 86, 92, 94], "These": [8, 82, 85, 90, 91, 92, 95], "hood": 8, "mini": 8, "gpu": [8, 74, 76, 78, 86, 89, 90], "torch": [8, 9, 18, 19, 23, 25, 32, 35, 38, 39, 40, 42, 48, 50, 51, 53, 54, 56, 58, 59, 60, 63, 64, 65, 66, 67, 73, 76, 77, 78, 79, 82, 83, 85, 86, 89, 93, 94, 95], "smaller": [8, 23, 41, 42, 43, 48, 78, 79], "fit": [8, 22, 77, 79], "a100": [8, 78], "adjust": [8, 38, 39, 40, 41, 42, 43, 48, 50], "attribut": [8, 85, 86, 93, 94], "accordingli": 8, "def": [8, 73, 76, 77, 78, 79, 83, 85, 86, 90, 91, 92, 93, 94, 95], "get_model": 8, "return": [8, 9, 17, 18, 19, 35, 50, 54, 56, 63, 65, 66, 67, 73, 76, 77, 78, 79, 83, 85, 86, 90, 91, 92, 93, 94, 95], "vocab_s": 8, "4096": [8, 76], "num_lay": 8, "num_head": 8, "num_kv_head": 8, "embed_dim": 8, "2048": [8, 76], "max_seq_len": 8, "train_loop": 8, "nn": [8, 32, 35, 50, 55, 58, 63, 66, 67, 73, 76, 77, 78, 79, 82, 83, 85, 86, 91, 92, 93, 95], "sgd": 8, "lr": [8, 76], "001": 8, "momentum": [8, 92], "9": [8, 76], "weight_decai": 8, "1e": [8, 76], "5": [8, 46, 67, 70, 76, 78, 82, 86, 89, 91, 92], "loss_fn": 8, "crossentropyloss": [8, 91, 92], "i": [8, 65, 76, 82, 90, 91, 92], "rang": [8, 46, 76, 82, 83, 91, 92], "randint": 8, "randn": [8, 9, 76, 78, 79, 83, 85, 90, 91, 92, 93, 94], "loss": [8, 76, 82, 91, 92], "backward": [8, 36, 76, 82, 92], "zero_grad": [8, 76, 92], "next": [8, 76, 77, 83, 91, 92, 93, 94], "scheme": [8, 90], "although": [8, 69, 85], "integ": [8, 9, 27, 28, 41, 42, 46, 49, 51, 53, 54, 56, 65, 83, 91, 92, 93], "arithmet": 8, "float32": [8, 9, 25, 53, 56, 58, 60, 64, 79, 82, 83, 85, 93, 94, 95], "becaus": [8, 18, 76, 77, 79, 82, 85, 92, 95], "fakequantizeconfig": [8, 61], "intxquantizationawaretrainingconfig": 8, "insert": [8, 78, 83, 90, 91, 92, 93, 94, 95], "swap": [8, 35, 58, 76, 77, 82, 83, 92], "fakequantizedlinear": [8, 57], "activation_config": [8, 61], "per_token": [8, 56], "is_symmetr": [8, 56], "fals": [8, 9, 27, 32, 42, 44, 48, 50, 56, 60, 66, 70, 76, 77, 78, 79, 83, 85, 86, 90, 91, 92, 94, 95], "weight_config": [8, 61], "group_siz": [8, 41, 42, 43, 45, 48, 56, 58, 63, 78, 86], "qat_config": 8, "structur": [8, 21, 73, 78, 79, 82, 85, 91], "attun": 8, "benefici": 8, "later": [8, 77, 85, 91, 92, 94], "int8dynamicactivationint4weightconfig": 8, "fromintxquantizationawaretrainingconfig": 8, "back": [8, 57, 85], "tensor": [8, 9, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 39, 40, 42, 43, 50, 51, 52, 53, 54, 64, 65, 68, 70, 74, 76, 78, 79, 82, 83, 89, 91, 93, 94], "subclass": [8, 9, 17, 35, 50, 69, 73, 78, 79, 82], "readi": [8, 76, 78, 83, 85, 92], "typic": [8, 19, 20, 77, 78, 79, 83, 86, 95], "did": [8, 43], "altern": [8, 56, 77, 83, 85, 93, 94], "legaci": 8, "offer": [8, 85, 91], "customiz": [8, 63], "unlik": [8, 83], "int8dynactint4weightqatquant": 8, "qat_quant": 8, "int8dynactint4weightqatlinear": 8, "int8dynactint4weightlinear": 8, "come": [8, 76, 77, 80, 82, 83, 84, 92, 93, 94], "soon": [8, 80, 84, 92], "class": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 55, 56, 57, 58, 59, 60, 61, 69, 70, 77, 78, 79, 83, 85, 91, 92, 93, 95], "torchao": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 82, 83, 85, 90, 91, 92, 93, 94], "tensor_impl": [9, 17, 77, 83], "aqttensorimpl": [9, 17], "block_siz": [9, 15, 17, 19, 24, 25, 27, 28, 29, 30, 51, 52, 53, 64, 78, 83], "tupl": [9, 17, 19, 24, 25, 27, 28, 29, 38, 39, 51, 52, 53, 62, 64, 70, 85, 86, 91, 92, 95], "int": [9, 10, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 37, 41, 42, 43, 45, 48, 51, 52, 53, 56, 58, 59, 60, 63, 64, 70, 78, 83, 85, 86], "quant_min": [9, 17, 27, 28, 29, 46, 51, 52, 53, 64, 77, 78, 85, 94, 95], "union": [9, 17, 32, 38, 39, 51, 53, 56, 63, 64], "quant_max": [9, 17, 27, 28, 29, 46, 51, 52, 53, 64, 77, 78, 85, 94, 95], "zero_point_domain": [9, 17, 27, 28, 29, 42, 51, 52, 56], "zeropointdomain": [9, 17, 27, 28, 29, 42, 51, 52, 56], "stride": [9, 17, 77, 85], "sourc": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 87, 89], "quantized_tensor": 9, "float_tensor": [9, 85], "scale": [9, 15, 18, 25, 28, 33, 36, 39, 46, 49, 51, 52, 53, 54, 56, 62, 64, 66, 67, 77, 82, 83, 85, 86, 95], "zero_point": [9, 15, 28, 42, 49, 51, 52, 53, 64, 77, 82, 83, 85, 95], "happen": [9, 17, 50, 77, 85, 91, 93], "choose_qparam": [9, 77], "dequant": [9, 17, 19, 42, 53, 77, 78, 85, 86, 91, 93, 94, 95], "ao": [9, 17, 82, 86], "three": [9, 50, 70, 73, 77, 93, 94], "choose_qparams_affin": [9, 42, 52, 77], "quantize_affin": [9, 42, 77], "qand": 9, "dequantize_affin": [9, 42], "extern": [9, 93], "regardless": 9, "intern": [9, 22], "represent": [9, 15, 26, 42, 77, 82, 86, 91, 95], "orient": 9, "field": [9, 56, 95], "gener": [9, 77, 78, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 95], "storag": [9, 18, 77, 82], "store": [9, 18, 19, 23, 69, 77, 82, 86, 91, 92], "plain": [9, 86], "int_data": [9, 85], "format": [9, 18, 19, 37, 41, 77, 82, 91, 92, 95], "depend": [9, 41, 50, 79, 82, 85, 91, 92, 94], "kernel": [9, 11, 12, 14, 18, 22, 37, 41, 42, 63, 78, 82, 90, 93, 94], "granular": [9, 33, 38, 39, 41, 42, 43, 48, 51, 53, 56, 64, 76, 77, 83, 86], "element": [9, 21, 23, 50, 51, 53, 64, 82], "share": [9, 51, 53, 64, 82], "qparam": [9, 51, 53, 64], "minimum": [9, 50, 51, 53, 64], "deriv": [9, 52, 64], "maximum": [9, 51, 53, 64, 66], "zero": [9, 21, 42, 51, 53, 56, 62, 70, 82, 83, 95], "subtract": [9, 19], "unquant": [9, 95], "given": [9, 17, 30, 76, 82, 86, 95], "classmethod": [9, 17, 83, 85, 86], "from_hp_to_floatx": 9, "input_float": [9, 17, 24, 25, 26, 27, 28, 29, 68], "target_dtyp": [9, 24, 25, 27, 28, 31, 32, 51, 52, 77, 83], "_layout": [9, 17, 24, 25, 26, 27, 28, 29, 77, 78, 83], "layout": [9, 10, 11, 12, 13, 14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 42, 43, 44, 73, 82], "scale_dtyp": [9, 24, 25, 27, 51, 52, 83], "float8": [9, 12, 13, 24, 25, 31, 32, 33, 34, 35, 36, 38, 39, 40, 74, 77, 83], "from_hp_to_floatx_stat": 9, "from_hp_to_fpx": 9, "floatx": [9, 26, 77], "ebit": [9, 26, 37], "mbit": [9, 26, 37], "float1": [9, 26], "float7": [9, 26], "from_hp_to_intx": [9, 17], "mapping_typ": [9, 27, 43, 51, 52, 56], "mappingtyp": [9, 27, 43, 44, 51, 52, 56, 83], "ep": [9, 27, 51, 52, 56, 83, 92, 94, 95], "zero_point_dtyp": [9, 27, 51, 52, 83], "preserve_zero": [9, 27, 42, 51, 52], "bool": [9, 27, 32, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 50, 51, 52, 56, 60, 63, 66, 73, 83], "plainlayout": [9, 27, 28, 43, 44, 83], "use_hqq": [9, 27, 42, 48, 86], "from_hp_to_intx_stat": 9, "kwarg": [9, 56, 58, 69, 70, 71, 85, 86], "self": [9, 77, 78, 79, 83, 85, 86, 91, 92, 93], "correct": [9, 18, 91, 92], "otherwis": [9, 55, 56, 77, 92], "desir": [9, 50, 83], "non_block": 9, "memory_format": [9, 93, 94], "preserve_format": 9, "function": [9, 22, 35, 50, 63, 69, 70, 71, 73, 76, 78, 79, 82, 83, 85, 86, 90, 95], "attempt": 9, "asynchron": 9, "respect": [9, 82, 92], "host": [9, 86], "possibl": [9, 82, 91, 92, 93, 95], "behavior": [9, 15, 55, 86, 91, 92], "pin": 9, "pageabl": 9, "howev": [9, 82, 86, 92, 95], "caution": 9, "advis": [9, 77], "featur": [9, 85, 90, 93, 94], "inform": [9, 82, 86, 90, 91], "good": [9, 78, 85, 95], "pin_memori": 9, "even": [9, 76, 82, 95], "match": [9, 53, 54, 82, 91], "other": [9, 15, 70, 76, 79, 82, 85, 86, 89, 91, 92, 93, 95], "initi": [9, 62, 77, 78, 79, 92], "float64": 9, "5044": 9, "0005": 9, "3310": 9, "0584": 9, "cuda0": 9, "blocksiz": 10, "64": [10, 30, 42, 48, 79, 83, 85, 86], "block": [10, 19, 70, 82], "matrix": [10, 13, 38, 39, 54, 65, 70, 78, 82, 93], "variabl": [10, 13, 22, 23, 70, 82], "cutlass": [11, 12], "mm_config": [13, 38, 39], "float8mmconfig": [13, 38, 39], "tinygemm": [14, 42, 63, 77, 78], "_weight_int4pack_mm_for_cpu": [14, 42], "least": 14, "6": [14, 56, 76, 77, 78, 82, 91, 92, 93], "It": [15, 18, 20, 22, 36, 78, 82, 85, 95], "post": [15, 22, 74, 78, 85, 92, 95], "design": [15, 18, 21, 86, 90, 91, 95], "extend": [15, 77, 82, 93], "conjunct": 15, "tensorimpl": 15, "interact": [15, 77, 91], "qqq": [16, 17, 29], "marlinqqq": 17, "inherit": [17, 20, 85, 86, 93, 94], "_choose_qparams_and_quantize_affine_qqq": 17, "_dequantize_affine_qqq": 17, "handl": [18, 21, 22, 50, 77], "pattern": [18, 21, 77, 78, 86, 90, 91], "ensur": [18, 92], "preprocess": [18, 21], "manag": 18, "pre_process": 18, "1\u00ba": 18, "transpos": [18, 77, 85], "sinc": [18, 69, 77, 79, 82, 83, 85, 91, 92, 93, 94, 95], "layer": [18, 35, 38, 40, 42, 44, 45, 48, 50, 58, 59, 60, 66, 67, 70, 71, 76, 82, 83, 85, 86, 90, 95], "2\u00ba": 18, "inject": 18, "3\u00ba": 18, "again": [18, 19, 82, 91, 95], "dim": [18, 83, 85, 86, 91, 92], "tensor_meta": 19, "subclasstensorarg": 19, "n_block": 19, "scaler_block_s": [19, 30], "quantized_scal": 19, "quantization_factor": 19, "scaler_mean": 19, "quantized_data": [19, 86], "nf4": 19, "qlora": [19, 74], "convert_to_norm_float_weight": 19, "normal": [19, 30, 50, 82, 91, 92], "dequantize_scal": 19, "unpack": [19, 77], "doubl": 19, "scaler": 19, "per_scaler_block": 19, "factor": [19, 54, 67, 76, 82], "inpt_weight": 19, "double_quantize_scal": 19, "calcul": [19, 36, 46, 51, 52, 66, 77, 82, 91, 95], "absmax": 19, "posit": 19, "per_block": 19, "int16": [19, 91], "n_scaler_block": 19, "get_original_weight": 19, "quantize_tensor_nearest": 19, "float16": [19, 64, 82], "nearest": 19, "round": [19, 46, 85], "most": [20, 77, 82, 86, 91, 92, 95], "metadata": [20, 77, 85, 86], "semi": [21, 73, 82], "where": [21, 46, 48, 52, 58, 59, 60, 77, 82, 86, 95], "everi": [21, 69, 82, 85, 91, 92], "four": [21, 90], "prune": [21, 70], "conform": 21, "inner_k_til": [22, 42, 59, 78], "core": [22, 47, 77, 83, 86, 91], "tile": [22, 77], "affect": [22, 82], "matmul": [22, 40, 77, 82, 85], "pack_dim": [23, 48], "uintx": [23, 48, 77], "bit": [23, 30, 37, 41, 48, 85, 86, 91, 93, 94], "width": [23, 41], "standard": [23, 77, 86], "byte": [23, 37, 48], "uintxtensor": 23, "determin": [23, 51, 76, 82, 86], "along": [23, 82, 86, 90], "indic": [23, 49, 82, 95], "last": [23, 76, 90], "256": [30, 42, 58, 59, 60, 91, 92, 95], "scaling_typ": [31, 32], "scalingtyp": [31, 32], "scaling_granular": [31, 32], "scalinggranular": [31, 32], "tensorwis": [31, 32], "cast_config_input": 32, "castconfig": 32, "cast_config_input_for_grad_weight": 32, "cast_config_weight": 32, "cast_config_weight_for_grad_input": 32, "cast_config_grad_output": 32, "cast_config_grad_output_for_grad_weight": 32, "gemm_config_output": 32, "float8gemmconfig": 32, "use_fast_accum": 32, "gemm_config_grad_input": 32, "gemm_config_grad_weight": 32, "enable_fsdp_float8_all_gath": 32, "pad_inner_dim": 32, "emul": 32, "force_recompute_fp8_weight_in_bwd": 32, "round_scales_to_power_of_2": 32, "from_recipe_nam": 32, "recipe_nam": [32, 76], "float8linearrecipenam": 32, "str": [32, 35, 41, 56, 63, 67, 68, 70, 73, 76, 85, 86, 94], "string": [32, 56, 70], "name": [33, 34, 46, 47, 49, 63, 67, 70, 73, 82, 85, 86, 90, 91, 92, 95], "qualnam": [33, 34, 46, 47, 49], "boundari": [33, 34, 46, 47, 49], "strategi": 33, "module_filter_fn": [35, 76], "callabl": [35, 50, 63, 68, 73, 86], "float8linearconfig": 35, "float8linear": [35, 76], "pass": [35, 50, 52, 69, 77, 83, 85, 86, 92, 95], "instanc": [35, 63, 69, 73, 79, 85, 91, 93, 94, 95], "fqn": [35, 70, 73, 76, 83], "sum": [36, 91, 92], "set_inductor_config": [37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 50], "sub": [37, 48, 82], "expon": 37, "mantissa": 37, "fp6_e3_m2": 37, "fp6_e2_m3": 37, "fp6": 37, "llm": 37, "arxiv": [37, 70, 82], "org": [37, 50, 70, 77, 78, 82, 94], "ab": [37, 70, 82], "2401": 37, "14112": 37, "repo": 37, "usyd": 37, "fsalab": 37, "fp6_llm": 37, "renam": [37, 91, 92], "fpxtensorcoreaqttensorimpl": 37, "experiment": [37, 90], "merg": 37, "to_affine_quantized_floatx": 37, "activation_dtyp": [38, 39], "float8_e4m3fn": [38, 39, 40, 77], "weight_dtyp": [38, 39, 40], "pertensor": [38, 39, 83], "perrow": [38, 39], "list": [38, 50, 53, 55, 67, 70, 77, 78, 85, 86, 90, 92, 95], "current": [38, 43, 63, 67, 70, 73, 76, 78, 82, 85, 86, 91, 92, 94], "fast": [38, 39, 82], "accumul": [38, 39], "torchinductor": [38, 39, 40, 41, 42, 43, 48, 93, 94], "float8_e4m": 39, "channel": [40, 44, 45, 56, 58, 59, 60, 69, 83, 94], "128": [41, 42, 76, 83, 85, 86, 94, 95], "bit_width": 41, "packing_bitwidth": 41, "weight_onli": 41, "gemlit": 41, "triton": [41, 77, 93, 94], "its": [41, 82, 85, 86, 91, 95], "associ": [41, 83], "fp16": [41, 51], "control": [41, 42, 43, 48, 70, 82, 86, 91], "grain": [41, 42, 43, 48, 85], "impact": [41, 50, 76, 86], "hardwar": [41, 77, 78, 82], "runtim": [41, 77, 91], "tensorcoretiledlayout": [42, 77, 78], "tensor_core_til": [42, 77], "int4mm": [42, 78], "aten": [42, 77, 78, 85, 86, 90, 91, 92, 93, 94], "_weight_int4pack_mm": [42, 77], "tradit": 42, "exactli": [42, 85], "chosen": [42, 82], "choic": 42, "hqq": [42, 48, 77], "preserv": [42, 51, 70, 82, 90], "Will": 42, "act_mapping_typ": [43, 44], "produc": [43, 78, 90, 91, 92, 93, 94], "backend": [43, 74, 78, 82, 95], "yet": [43, 47, 85, 86, 92, 93, 94], "marlinqqqlayout": 43, "cutlassint4packedlayout": 43, "weight_only_decod": 44, "number": [46, 48, 50, 70, 82, 85, 92, 93], "sai": [46, 64, 77, 86, 95], "3": [46, 50, 64, 74, 76, 77, 78, 82, 89, 91, 92], "7": [46, 76, 93, 94], "symmetric_no_clipping_err": 46, "variant": [46, 52, 85], "smin": 46, "smax": 46, "min_val_neg": [46, 85], "max_val_po": [46, 85], "By": [46, 82], "individu": [46, 82], "less": [46, 82, 85, 91], "error": [46, 50, 56, 76, 85, 91], "neg": 46, "placehold": [47, 94], "x": [48, 76, 78, 79, 83, 85, 86, 89, 90, 91, 92, 93, 94], "uint1": [48, 77], "uint7": [48, 77], "enum": 49, "quantized_v": 49, "float_val": 49, "mid_point": 49, "example_input": [50, 62, 78, 79, 83, 90, 91, 92, 93, 94, 95], "qtensor_class_list": 50, "aqdefaultlinearweight": 50, "aqint8weightonlyquantizedlinearweight": 50, "aqint8weightonlyquantizedlinearweight2": 50, "aqint8dynamicallyquantizedlinearweight": 50, "filter_fn": [50, 63, 73], "interpol": 50, "85": 50, "manual": [50, 92], "supress_autoquant_error": 50, "min_sqnr": 50, "aq_kwarg": 50, "autoquant": 50, "identifi": [50, 83, 95], "fastest": 50, "over": [50, 76, 82, 91, 92], "potenti": [50, 82, 83, 90, 91, 93, 94], "qtensor": 50, "search": [50, 82], "whose": [50, 95], "exchang": 50, "autoquantizablelinearweight": 50, "calibr": [50, 52, 78, 90, 92, 93, 94], "seen": 50, "record": [50, 77, 83], "final": [50, 63, 77, 78, 82, 90, 91, 92, 93, 94, 95], "benchmark": [50, 66, 76, 78, 90, 93, 94], "member": 50, "pick": 50, "highli": 50, "simpli": [50, 82, 83, 85], "had": [50, 85, 91], "compil": [50, 63, 65, 76, 77, 78, 83, 85, 93, 94], "proce": 50, "combin": [50, 56, 82, 85, 91, 93], "finalize_autoqu": 50, "log": [50, 85], "forward": [50, 69, 77, 78, 79, 82, 83, 85, 86, 91, 92, 93], "fulli": [50, 63, 67, 73, 82, 91], "unless": [50, 86], "default_autoquant_class_list": 50, "contain": [50, 66, 67, 82, 85, 92, 95], "second": [50, 54, 76, 77, 89, 95], "stop": 50, "wait": [50, 77], "sever": [50, 76, 86, 90, 95], "automat": [50, 76, 85, 86, 89], "suppress": 50, "accept": [50, 95], "signal": 50, "nois": 50, "ration": 50, "wikipedia": 50, "wiki": 50, "noise_ratio": 50, "v": [50, 95], "non": [50, 77, 82, 85, 90, 93, 94], "caus": [50, 76], "too": 50, "larg": [50, 85, 93], "resaon": 50, "40": [50, 76], "keyword": 50, "example_input1": 50, "example_input2": 50, "int32": [51, 56, 58, 77, 78, 91, 95], "fp32": [51, 53, 56, 83, 85, 91, 93], "optioanl": 51, "param": [51, 52, 70], "request": [51, 53, 64], "min_val": [52, 77, 85], "max_val": [52, 77, 85], "observ": [52, 69, 82, 83, 90, 91, 92, 93, 94, 95], "obtain": 52, "track": [52, 77, 86], "input_dtyp": 53, "output_dtyp": [53, 64], "uint8": [53, 64, 77, 83, 95], "b": 54, "scales1": 54, "multipli": [54, 65, 82], "row": [54, 76, 82], "rais": [54, 65, 76, 85, 86], "assertionerror": [54, 65, 76, 85], "expect": [54, 76, 82, 85, 90, 91, 93, 94, 95], "twostepquant": 55, "easili": [55, 90], "thei": [55, 76, 77, 78, 82, 85, 91, 92, 95], "constructor": [55, 85], "must": [55, 56, 76, 82, 86, 92, 94, 95], "embed": [55, 58], "undefin": [55, 70], "my_quant": 55, "qatquantizer1": 55, "qatquantizer2": 55, "qatquantizer3": 55, "torchaodtyp": 56, "scale_precis": [56, 58], "zero_point_precis": [56, 58], "is_dynam": [56, 93, 94, 95], "range_learn": 56, "simul": [56, 71, 77, 82], "older": 56, "int1": [56, 77], "int7": 56, "pergroup": 56, "pertoken": 56, "per_channel": 56, "peraxi": [56, 83], "per_group": [56, 64], "leav": 56, "empti": [56, 77], "properti": [56, 77], "throw": 56, "els": [56, 86, 91, 92], "fakequantizedembed": 57, "model_with_fake_quantized_linear": 57, "int4weightonlyqatembed": 58, "int4weightonlyembed": 58, "scales_precis": [59, 60], "padding_allow": 60, "fakequant": 62, "aobaseconfig": [63, 73, 83, 86], "inplac": [63, 70, 78], "workflow": [63, 73, 76, 78, 82, 95], "qualifi": [63, 67, 73, 82], "move": [63, 77, 83, 86, 92, 93], "predefin": [63, 95], "execut": [63, 81, 85, 88], "int8_dynamic_activation_int4_weight": 63, "int8_dynamic_activation_int8_weight": [63, 73], "mm": [63, 85, 91], "int4_weight_onli": 63, "int8_weight_onli": 63, "sequenti": [63, 73, 76], "1024": [63, 73, 78, 79, 93], "tabl": [64, 76, 77, 82], "per_tensor": 64, "per_axi": 64, "axi": [64, 83], "mat2": 65, "safe": 65, "consid": [65, 77, 82], "cubla": 65, "fallback": [65, 86], "j": 65, "debug_skip_calibr": 66, "smoothquant": [66, 67, 90], "smoothfakedynamicallyquantizedlinear": [66, 67], "debug": 66, "skip_fqn_list": 67, "cur_fqn": 67, "alpha": 67, "skip": [67, 70, 82], "being": [67, 76, 77, 82, 86, 93, 94], "input_quant_func": [68, 77], "quant_kwarg": 68, "dict": [68, 70, 85, 86, 94, 95], "l2": [69, 82], "norm": [69, 70, 82], "buffer": 69, "x_orig": 69, "overridden": 69, "within": [69, 82, 86, 93, 94], "afterward": 69, "former": 69, "care": [69, 79, 82, 91], "hook": [69, 77], "while": [69, 70, 82, 85, 90, 91, 95], "latter": [69, 92], "silent": [69, 93], "ignor": [69, 76, 91, 92], "sparsity_level": [70, 82], "semi_structured_block_s": 70, "wanda": 70, "sparsifi": [70, 74, 79, 82], "propos": 70, "2306": 70, "11695": 70, "awar": [70, 74, 78, 82, 85], "product": [70, 86, 93, 95], "magnitud": [70, 82], "parametr": 70, "deepcopi": [70, 78, 83, 85, 92], "squash_mask": [70, 82], "params_to_keep": 70, "params_to_keep_per_lay": 70, "squash": 70, "mask": [70, 82], "appropri": [70, 77, 90, 91, 92, 93, 94], "sparse_param": 70, "attach": [70, 82, 95], "kei": [70, 82, 89], "xdoctest": 70, "local": [70, 82], "don": [70, 76, 78, 82, 86, 95], "t": [70, 76, 77, 78, 82, 83, 85, 86, 91, 92, 95], "hasattr": [70, 86], "submodule1": 70, "linear1": [70, 78, 79, 83, 85], "foo": [70, 91], "bar": [70, 91], "submodule2": 70, "linear42": 70, "baz": 70, "42": [70, 83], "24": 70, "ones": [70, 77, 92], "update_mask": 70, "tensor_nam": [70, 86], "statist": [70, 77, 82, 83, 91, 92], "retriev": 70, "act_per_input": 70, "Then": [70, 85, 94, 95], "across": [70, 82, 85, 86], "whole": [70, 95], "alia": [72, 86], "semisparseweightconfig": 72, "sparsify_": 73, "apply_tensor_subclass": [73, 77], "essenti": [73, 86, 90], "semi_sparse_weight": 73, "semisparselayout": 73, "sparsemarlinlayout": 73, "isinst": [73, 76, 82, 83, 85, 86, 92, 95], "sparse_api": 73, "librari": [74, 79], "gradient": [74, 82], "nativ": [74, 76, 85, 91], "introduct": [74, 77], "highlight": [74, 85, 89], "updat": [74, 78, 79, 82, 91, 92, 93, 95], "guid": [74, 77, 90], "contributor": [74, 78], "part": [74, 77, 82, 85, 92], "tune": [74, 76, 80, 82, 90], "vllm": 74, "sglang": 74, "architectur": [74, 82, 90, 91, 93, 94], "serial": [74, 77, 91, 92], "write": [74, 78, 90, 91, 92], "advanc": [74, 83, 85, 90, 93, 94], "export": 74, "x86": [74, 78], "intel": [74, 90, 93], "5x": 76, "512": 76, "cluster": 76, "34": 76, "43x": 76, "2k": 76, "h200": 76, "latest": [76, 78], "offic": 76, "offici": 76, "popular": [76, 77], "flagship": 76, "common": [76, 77, 82], "form": [76, 77, 82], "quickli": [76, 85], "batteri": 76, "includ": [76, 77, 85, 90, 93, 94, 95], "experi": [76, 94], "commonli": [76, 82], "fork": 76, "build": [76, 77, 82, 85, 86, 91], "top": [76, 77, 85, 90, 91, 92, 93, 94], "re": [76, 79, 85, 91, 92], "virtual": 76, "environ": 76, "conda": 76, "venv": 76, "instal": [76, 78, 91, 94], "download": [76, 78, 87, 89, 91, 92, 94], "job": 76, "root": 76, "directori": 76, "launch": 76, "ngpu": 76, "config_fil": 76, "train_config": 76, "llama3_8b": 76, "toml": 76, "run_train": 76, "sh": 76, "hyperparamet": 76, "edit": 76, "line": [76, 82], "flag": [76, 92], "termin": 76, "rank0": 76, "titan": 76, "2025": 76, "06": 76, "04": 76, "08": 76, "51": 76, "48": 76, "074": 76, "info": 76, "12": [76, 94, 95], "2254": 76, "27": 76, "34gib": 76, "28": 76, "78": 76, "tp": [76, 86], "375": 76, "tflop": 76, "21": 76, "73": [76, 83], "mfu": 76, "20": [76, 92], "58": 76, "557": 76, "7069": 76, "30": [76, 78, 91], "99gib": 76, "62": 76, "034": 76, "407": 76, "35": [76, 83], "41": 76, "19": 76, "52": 76, "224": [76, 83, 90, 91, 92, 93, 94], "9196": 76, "022": 76, "406": [76, 91, 92], "65": 76, "904": 76, "1423": 76, "014": 76, "23": [76, 83], "As": [76, 77, 91, 95], "warmup": 76, "around": [76, 78, 79, 91], "7k": 76, "99gb": 76, "peak": 76, "against": 76, "baselin": [76, 91], "11": 76, "02": 76, "37": 76, "404": 76, "2611": 76, "22gib": 76, "595": 76, "47": 76, "49": [76, 83], "027": 76, "4260": 76, "89gib": 76, "344": 76, "367": 76, "39": 76, "15": [76, 78], "03": 76, "01": 76, "988": 76, "9482": 76, "321": 76, "366": 76, "14": 76, "991": 76, "1183": 76, "300": 76, "364": 76, "89": 76, "013": 76, "4659": 76, "291": 76, "84": 76, "769": 76, "gc": 76, "peform": 76, "period": 76, "collect": [76, 77, 82], "3k": 76, "89gb": 76, "11x": 76, "throughput": 76, "nearli": 76, "ident": [76, 82], "performan": 76, "vs": [76, 82, 91, 95], "curv": [76, 82], "omit": [76, 91, 92, 93], "648": 76, "2648": 76, "28gib": 76, "71": 76, "29": 76, "26": 76, "475": 76, "9106": 76, "91gib": 76, "53": 76, "503": 76, "434": 76, "43": 76, "94": [76, 91], "166": 76, "0774": 76, "663": 76, "443": 76, "44": [76, 83], "87": 76, "50": [76, 82, 83, 90, 91, 93, 94], "885": 76, "3233": 76, "643": 76, "442": 76, "66": [76, 83], "76": 76, "613": 76, "6150": 76, "637": 76, "72": 76, "6k": 76, "91gb": 76, "21x": 76, "tl": 76, "dr": 76, "better": [76, 85, 91, 92, 93, 94, 95], "priorit": 76, "accur": [76, 82, 90], "stabil": 76, "cost": [76, 83], "slightli": [76, 85], "limit": [76, 85, 86, 91], "outlier": 76, "underflow": 76, "8xh100": 76, "box": [76, 82, 93], "toi": [76, 78, 83, 85, 93], "convert_to_float8_train": 76, "recurs": 76, "kind": [76, 91], "gemm": [76, 93, 94], "snippet": [76, 91, 92], "f": [76, 77, 79, 82, 83, 85, 86, 91, 92], "float8_linear_util": 76, "float8_linear": 76, "torch_version_at_least_2_5": [76, 78], "greater": 76, "sampl": [76, 77, 91, 93, 94], "adamw": 76, "elig": 76, "mod": [76, 82, 85], "divis": 76, "in_featur": [76, 78, 79, 83, 85], "out_featur": [76, 78, 83, 85], "enabl": [76, 77, 86, 93], "competit": 76, "_": [76, 83, 86, 90, 91, 92, 93], "label": 76, "purpos": [76, 77, 85, 91], "fake_label": 76, "ones_lik": 76, "mse_loss": 76, "model_state_dict": 76, "state_dict": [76, 79, 91, 92], "optimizer_state_dict": 76, "pth": [76, 91, 92], "explor": [76, 78, 94], "few": [76, 85, 91, 92], "lai": 77, "stack": 77, "awq": 77, "gptq": 77, "codebookquantizedtensor": 77, "float3": 77, "compon": [77, 85, 86], "overload": [77, 82], "term": [77, 82, 91, 95], "extra": 77, "dev": 77, "discuss": [77, 85], "1833": 77, "No": [77, 79, 82], "matter": [77, 82], "avail": [77, 90, 91, 92, 93, 94], "float3_e2_m0": 77, "float4_e2_m1": 77, "float4_e3_m0": 77, "float5_e2_m2": 77, "float5_e3_m1": 77, "float6_e2_m3": 77, "float6_e3_m2": 77, "float8_e5m2": 77, "float8_e4m3fnuz": 77, "float8_e5m2fnuz": 77, "plan": [77, 92], "float4": 77, "float6": 77, "becom": [77, 91], "uint2": 77, "117208": 77, "outsid": 77, "mention": [77, 91], "criteria": 77, "wide": 77, "adopt": 77, "fundament": [77, 82, 92], "until": 77, "evid": 77, "hopefulli": 77, "amen": 77, "haven": 77, "enough": 77, "ont": 77, "revisit": 77, "intx": 77, "connect": [77, 95], "int4tensor": 77, "previou": [77, 91, 92, 93, 94], "between": [77, 82, 85, 86, 90, 92, 93, 95], "preicison": 77, "mainli": [77, 90, 93, 95], "accommod": 77, "choose_qparams_affine_with_min_max": 77, "min": [77, 83, 85, 91, 95], "int_matmul": 77, "int_scaled_matmul": 77, "reli": [77, 78, 82, 83, 85], "On": [77, 78], "glue": 77, "everyth": 77, "togeth": [77, 91, 93, 95], "construct": [77, 91, 95], "low_precision_v": 77, "high_precision_v": 77, "procedur": 77, "veri": [77, 82, 86, 92], "straightforward": [77, 95], "high_preicsion_v": 77, "especi": [77, 79, 82, 93, 94], "bitwidth": [77, 95], "codebook": 77, "hardcod": [77, 95], "select": [77, 91], "multi": 77, "dimension": [77, 82], "view": [77, 85, 91, 92], "mkldnn": 77, "coo": [77, 82], "sparse_coo": [77, 82], "sparsetensorimpl": 77, "idea": [77, 82], "nice": [77, 82], "concept": [77, 89, 91, 93, 94, 95], "why": [77, 85, 89], "c": [77, 78, 85, 93, 94], "conflict": 77, "quantized_linear": [77, 83, 91], "semant": 77, "stai": [77, 78, 85, 93], "develop": [77, 78, 91, 92, 95], "tradition": 77, "to_affine_quant": 77, "simplic": 77, "explain": [77, 90, 93], "simplest": [77, 82], "easi": 77, "linear_modul": 77, "to_affine_quantized_intx": 77, "requires_grad": [77, 83, 85, 86], "to_linear_activation_quant": 77, "quantized_weight": [77, 86], "activation_and_weight_quant": 77, "encount": 77, "input_qunat_func": 77, "redispatch": 77, "fx": [77, 91, 95], "symbolic_trac": 77, "But": [77, 85, 86, 95], "easier": [77, 95], "further": [77, 85, 90, 91, 92, 93], "modif": 77, "figur": [77, 82, 91], "At": [77, 82, 91], "thing": [77, 79, 82, 85, 91], "address": [77, 91], "stat": [77, 92], "averag": [77, 83, 91, 92], "calculate_qparam": [77, 83, 95], "affinequantizedminmaxobserv": [77, 83], "insert_observer_": 77, "observedlinear": [77, 83], "complic": [77, 82, 91], "done": [77, 85], "manner": 77, "intend": [77, 91], "autoround": 77, "multitensor": 77, "sure": [77, 95], "describ": [77, 79, 82, 89, 91, 92], "todai": 77, "low_bit_optim": 77, "quantized_train": 77, "progress": [77, 86], "lot": [77, 82], "walk": [77, 83, 85, 89, 90, 93], "int4weightonlyconfig": [77, 78, 79, 86], "_convert_weight_to_int4pack": 77, "tensorcoretiledaqttensorimpl": 77, "_quantized_linear_op": 77, "goe": 77, "_aqt_qlinear_dispatch_t": 77, "dispatch": 77, "explan": 77, "wint4": 77, "stabl": 78, "pip": [78, 90, 91], "nightli": 78, "index": [78, 82, 94], "url": [78, 94], "whl": [78, 94], "cu121": 78, "major": 78, "entri": 78, "mutat": 78, "logic": [78, 85, 86], "toylinearmodel": [78, 79, 83], "__init__": [78, 79, 83, 85, 86, 91, 92, 93], "super": [78, 79, 83, 85, 91, 92, 93], "linear2": [78, 79, 83, 85], "eval": [78, 79, 83, 90, 92, 93, 94], "faster": [78, 82], "model_bf16": 78, "mix": [78, 90, 93, 94], "tensor_impl_dtyp": 78, "verifi": [78, 79, 85], "roughli": [78, 82], "quarter": 78, "os": [78, 91, 92], "int4_model": 78, "pt": 78, "bfloat16_model": 78, "int4_model_size_mb": 78, "getsiz": [78, 91, 92], "bfloat16_model_size_mb": 78, "2f": [78, 91, 92], "mb": [78, 79, 81, 88, 91, 92], "25": 78, "00": [78, 81, 88], "benchmark_model": 78, "temporari": 78, "workaround": [78, 86], "num_run": 78, "100": [78, 85, 91, 92], "_dynamo": [78, 85], "reset": [78, 91, 92], "bf16_time": 78, "int4_tim": 78, "time": [78, 82, 85, 89, 90, 91, 92], "3f": [78, 92], "ms": 78, "1fx": 78, "80gb": 78, "393": 78, "410": 78, "9x": 78, "recogn": [78, 95], "decis": 78, "relu": [78, 90, 95], "pt2e": [78, 90, 91, 92, 93, 94], "fuse": [78, 82, 85, 92], "deleg": [78, 91], "x86inductorquant": [78, 93], "quantize_pt2": [78, 90, 91, 92, 93, 94], "prepare_pt2": [78, 90, 91, 93, 94], "x86_inductor_quant": [78, 93], "get_default_x86_inductor_quantization_config": [78, 93], "float_model": [78, 85, 90, 91, 92, 93], "data_load": [78, 91, 92, 93, 94], "no_grad": [78, 85, 90, 91, 92, 93, 94], "imag": [78, 90, 91, 92, 93, 94], "program": [78, 91, 92, 93, 95], "captur": [78, 91, 92, 95], "expos": [78, 91, 92], "express": [78, 85, 90, 91, 92, 95], "set_glob": [78, 91, 92, 93, 94], "xiq": [78, 93], "prepare_qat_pt2": [78, 92, 93], "sample_inference_data": 78, "convert_pt2": [78, 90, 91, 92, 93, 94], "wrapper": [78, 85, 93], "_inductor": [78, 93], "cpp_wrapper": [78, 93], "optimized_model": [78, 90, 93, 94], "converted_model": [78, 93, 94], "xpu": [78, 94], "openvino": 78, "simpl": [78, 82, 83, 85, 90, 93, 94], "visit": 78, "would": [78, 82, 85, 92, 94], "forget": 78, "tempfil": 79, "get_model_size_in_byt": 79, "ref": [79, 91], "namedtemporaryfil": 79, "seek": [79, 82], "load": [79, 86], "m_load": 79, "load_state_dict": [79, 91, 92], "assign": 79, "assert": [79, 83, 85, 86, 95], "equal": [79, 82], "float_weight1": 79, "float_weight2": 79, "quantized_weight1": 79, "quantized_weight2": 79, "go": [79, 85, 89, 95], "techinqu": 79, "reduct": [79, 82, 85], "4x": 79, "0625": 79, "reason": [79, 82], "avoid": [79, 82], "properli": 79, "003": [81, 88, 89], "total": [81, 88, 89], "galleri": [81, 87, 89], "mem": [81, 88], "templat": [81, 87, 88], "tutorials_sourc": 81, "template_tutori": [81, 88, 89], "neural": [82, 90, 93], "network": [82, 85, 90, 93], "overhead": [82, 86, 93], "latenc": 82, "carefulli": 82, "signific": 82, "pai": 82, "low": [82, 85, 90], "price": 82, "f1": 82, "problem": [82, 85], "research": [82, 89], "face": [82, 91], "fragment": 82, "rightfulli": 82, "spent": 82, "compress": [82, 90], "place": [82, 90, 91, 92, 93, 94], "dens": 82, "solv": [82, 85], "focu": [82, 85], "realli": 82, "push": [82, 86], "concret": [82, 95], "hope": 82, "modular": 82, "acceler": 82, "scratch": [82, 89], "minim": [82, 90, 93, 94], "algorthim": 82, "realiz": 82, "trade": 82, "off": 82, "theoret": 82, "gain": [82, 94], "2x": 82, "analog": 82, "fix": [82, 83], "unstructur": 82, "One": [82, 85, 86, 95], "close": 82, "relat": 82, "retrain": 82, "neglig": 82, "area": 82, "agre": 82, "upon": 82, "consensu": 82, "mind": 82, "thought": 82, "subproblem": 82, "satisfi": 82, "consist": [82, 85, 93, 94, 95], "answer": 82, "independ": 82, "frontend": [82, 93], "arbitrari": 82, "handoff": 82, "piec": 82, "miss": 82, "natur": [82, 85, 91, 95], "present": 82, "clear": 82, "contract": 82, "7x": 82, "advantag": 82, "anticip": 82, "mani": [82, 85], "solut": 82, "third": 82, "parti": 82, "to_sparse_semi_structur": 82, "sparsesemistructuredtensor": 82, "weightnormsparsifi": 82, "half": 82, "subnetwork": 82, "sparse_config": 82, "named_modul": 82, "append": [82, 91, 92], "tensor_fqn": 82, "sparse_block_shap": 82, "zeros_per_block": 82, "fakespars": 82, "manipul": 82, "dictionari": 82, "paramer": 82, "parameter": 82, "necessari": [82, 83, 85, 90, 91, 92, 93, 94], "ve": 82, "suitabl": [82, 93], "0s": 82, "spot": 82, "definit": [82, 86], "academia": 82, "industri": 82, "often": [82, 85], "interchang": 82, "confus": [82, 91], "distinct": 82, "pretrain": [82, 90, 91, 92, 93], "behind": 82, "doesn": [82, 92, 95], "itself": [82, 85], "those": [82, 83, 85], "loos": 82, "speak": 82, "tightli": 82, "coupl": [82, 85], "nvidia": 82, "csc": 82, "fbgemm": 82, "qnnpack": 82, "descript": [82, 90], "coordin": 82, "vector": [82, 93], "locat": 82, "bsr": 82, "sparse_bsr": 82, "except": [82, 85, 95], "scalar": [82, 91], "csr": 82, "sparse_csr": 82, "sparse_csc": 82, "column": 82, "compact": 82, "sparse_matrix": 82, "1d": 82, "indexptr": 82, "\u00bd": 82, "bitmask": 82, "2bit": 82, "unprun": 82, "quit": [82, 85], "broken": 82, "down": 82, "Not": 82, "sensit": 82, "effect": [82, 83, 85, 93, 94, 95], "best": [82, 93], "subsequ": [82, 85, 93, 94], "infinit": 82, "lost": 82, "degre": 82, "analysi": 82, "drop": 82, "give": [82, 85], "proxi": 82, "aforement": 82, "smallest": 82, "absolut": 82, "global": [82, 85], "scope": 82, "impli": 82, "pro": 82, "con": 82, "tradeoff": 82, "span": 82, "threshold": 82, "increas": [82, 91], "complex": 82, "constant": [82, 85, 91], "ctr_mobile_fe": 82, "score": 82, "w": [82, 86], "tenosr": 82, "udpat": 82, "cannot": [82, 83, 86], "histori": 82, "regrow": 82, "dw": 82, "via": [82, 90], "backprop": 82, "pat": 82, "unmask": 82, "resid": 82, "salienc": 82, "lowest": 82, "l1": 82, "shown": [82, 92, 95], "abl": [82, 85, 86, 91, 95], "repeat": [82, 91, 92], "movement": 82, "2005": 82, "07683": 82, "rank": [82, 85], "wx": 82, "sqx": 82, "q": [82, 91], "usual": 82, "sort": 82, "wise": 82, "reconstruct": [82, 86], "random": [82, 91, 92], "randomli": 82, "tri": 82, "remedi": 82, "item": [82, 89], "ultim": [82, 83], "literatur": 82, "vision": 82, "nlp": [82, 89, 93], "iter": [82, 91, 92], "ctr_feed": 82, "na": 82, "multimask": 82, "pyspeech": 82, "fastna": 82, "approach": [82, 85, 90, 93, 94], "knowledg": [82, 89], "distil": 82, "pdf": 82, "2204": 82, "09656": 82, "arrang": 82, "recal": 82, "counterpart": 82, "slower": 82, "suffici": 82, "flexibl": [82, 85, 90, 93], "98": 82, "benefit": [82, 85, 91, 94], "special": [82, 90, 91], "exhibit": 82, "maintain": 82, "penalti": 82, "expens": [82, 85], "dictat": 82, "characterist": 82, "highest": 82, "wouldn": [82, 85], "visual": 82, "fig": 82, "4x4": 82, "benchmak": 82, "batch": [83, 92], "fly": 83, "welcom": 83, "histogram": [83, 91], "act_ob": 83, "finfo": 83, "weight_ob": 83, "observed_input": 83, "observed_weight": 83, "cl": [83, 85, 86], "float_linear": 83, "observed_linear": 83, "_replace_with_custom_fn_if_matches_filt": 83, "insert_observers_": 83, "_is_linear": 83, "lambda": [83, 86], "replacement_fn": 83, "copied_act_ob": 83, "copied_weight_ob": 83, "popul": 83, "feed": 83, "simpler": [83, 91], "quantizedlinear": [83, 85], "isn": 83, "strictli": 83, "to_affine_quantized_intx_stat": 83, "act_scal": [83, 95], "act_zero_point": 83, "weight_scal": [83, 91, 95], "weight_zero_point": [83, 91], "qweight": 83, "qinput": 83, "from_observ": 83, "begin": [83, 85], "dataclass": [83, 86, 95], "transform_modul": [83, 86], "register_quantize_module_handl": [83, 86], "staticquantconfig": 83, "_apply_static_qu": 83, "is_observed_linear": 83, "optimizedmodul": 83, "_orig_mod": 83, "0237": 83, "plainaqttensorimpl": 83, "142": 83, "31": [83, 95], "113": 83, "157": 83, "57": 83, "59": 83, "160": 83, "70": 83, "150": 83, "67": 83, "241": 83, "238": 83, "235": 83, "228": 83, "255": [83, 95], "201": 83, "114": 83, "236": 83, "88": [83, 91], "83": 83, "109": 83, "209": 83, "92": 83, "184": 83, "141": 83, "110": 83, "0009": 83, "0010": 83, "130": 83, "122": 83, "132": 83, "125": 83, "126": 83, "129": 83, "127": [83, 85, 94, 95], "133": 83, "124": 83, "131": 83, "135": 83, "136": 83, "foundat": 85, "extens": [85, 91, 93], "autograd": [85, 95], "interpos": 85, "namespac": 85, "continu": [85, 92, 93, 94, 95], "seamlessli": [85, 93, 94], "obviou": 85, "int8quantizedlinear": 85, "finer": 85, "intercept": 85, "contrast": 85, "long": [85, 91], "clunki": 85, "distributedlinear": 85, "duplic": 85, "bypass": 85, "outer": 85, "inner": 85, "allgath": 85, "bandwidth": 85, "rest": [85, 92], "read": 85, "zoo": 85, "podcast": 85, "edward": 85, "yang": 85, "int8_symmetric_quant": 85, "fp32_tensor": 85, "amin": 85, "keepdim": [85, 91, 92], "amax": 85, "zeros_lik": 85, "clamp": [85, 91], "w_int8": 85, "new_linear": 85, "left": [85, 95], "toymodel": 85, "quantized_model": [85, 90, 91, 92], "child": 85, "named_children": 85, "setattr": 85, "drawback": 85, "won": 85, "suppos": 85, "clean": 85, "eleg": 85, "pretti": 85, "power": [85, 86], "overrid": 85, "almost": 85, "shard": [85, 86], "ragged": 85, "rag": 85, "nestedtensor": 85, "who": 85, "link": [85, 89], "googl": 85, "collab": 85, "flopcount": 85, "memorytrack": 85, "With": [85, 91, 93, 95], "bare": 85, "bone": 85, "int8symmetrictensor": 85, "hold": 85, "staticmethod": 85, "disabl": [85, 92], "__new__": [85, 86], "_make_wrapper_subclass": [85, 86], "storage_offset": 85, "ndim": 85, "__tensor_flatten__": [85, 86], "pt2": [85, 93], "__tensor_unflatten__": [85, 86], "tensor_data_dict": [85, 86], "extra_metadata": 85, "outer_s": [85, 86], "outer_strid": [85, 86], "undo": 85, "__repr__": 85, "repr": 85, "ahead": 85, "insid": 85, "int8_tensor": 85, "func": [85, 86], "op_implementations_dict": 85, "conveni": 85, "register_op": 85, "_op": 85, "opoverload": 85, "impl_decor": 85, "op_impl": 85, "particular": 85, "largest": 85, "tell": 85, "desugar": 85, "decor": [85, 86], "surfac": 85, "coverag": [85, 90, 91, 93, 94], "though": 85, "brute": 85, "forc": 85, "repeatedli": 85, "loggingtensor": 85, "_python_dispatch": [85, 86], "return_and_correct_alias": [85, 86], "int8_mm": 85, "detach": [85, 86], "int8_view_op": 85, "out_data": 85, "out_scal": [85, 91], "notic": 85, "hit": 85, "background": 85, "decomposit": 85, "live": 85, "decomp": 85, "shrink": 85, "author": [85, 89, 90, 91, 92, 93, 94, 95], "pain": 85, "rather": 85, "underli": 85, "worth": 85, "written": 85, "differenti": 85, "nuanc": 85, "longer": [85, 91, 92], "That": 85, "transposit": 85, "got": [85, 91, 95], "propag": [85, 91, 93, 94], "fact": 85, "themselv": [85, 91], "pointwis": [85, 93, 94], "alwai": 85, "were": 85, "might": [85, 86, 91, 95], "unwrap": 85, "dim0": 85, "dim1": 85, "confirm": 85, "quantized_model_module_swap": 85, "quantized_model_subclass": 85, "subclass_param": 85, "out_module_swap": 85, "allclos": 85, "out_compil": 85, "seri": 85, "comprehens": [86, 93], "e2": 86, "json": 86, "quant_typ": 86, "_type": 86, "_data": 86, "valid": [86, 95], "capabl": [86, 91, 93], "modulefqntoconfig": 86, "int8weightonlyconfig": 86, "self_attn": 86, "q_proj": 86, "k_proj": 86, "mlp": 86, "gate_proj": 86, "_default": 86, "torchaoconfig": 86, "automodelforcausallm": 86, "quantization_config": [86, 94], "torch_dtyp": 86, "auto": 86, "device_map": 86, "safe_seri": 86, "usernam": 86, "server": 86, "narrow": 86, "copy_": 86, "state": 86, "slice": 86, "chunk": 86, "_apply_fn_to_data": 86, "heavi": 86, "codebas": 86, "fn": 86, "ctx": 86, "new_tensor": 86, "getattr": 86, "__class__": 86, "principl": 86, "torchaobasetensor": 86, "mynewquantconfig": 86, "classvar": 86, "myquantizedtensor": 86, "fbgemmfp8tensor": 86, "tensor_data_attr": 86, "tensor_attribut": 86, "attr": 86, "_to_copi": 86, "clone": 86, "fill_default": 86, "notimplementederror": 86, "_my_quant_transform": 86, "my_quantization_funct": 86, "len": [86, 91, 92, 95], "use_cutlass_kernel": 86, "my_cutlass_linear": 86, "elif": 86, "use_triton_kernel": 86, "my_triton_linear": 86, "disappear": 86, "extrem": 86, "sole": 86, "think": 86, "littl": 86, "world": 86, "explicitli": [86, 95], "spooki": 86, "action": [86, 91, 92], "distanc": 86, "statu": 86, "due": [86, 90, 95], "hub": 86, "team": 86, "2338": 86, "creation": 86, "detect": 86, "illustr": 86, "tutorials_python": 87, "zip": [87, 89], "jupyt": [87, 89], "notebook": [87, 89], "tutorials_jupyt": 87, "sphinx": [87, 89], "firstnam": 89, "lastnam": 89, "prerequisit": [89, 91], "v2": 89, "topic": 89, "rand": [89, 91, 92], "1666": 89, "7885": 89, "3268": 89, "3481": 89, "8334": 89, "0496": 89, "4242": 89, "0912": 89, "1320": 89, "0865": 89, "4726": 89, "5110": 89, "7426": 89, "5090": 89, "2999": 89, "practic": 89, "test": [89, 91, 93], "summar": 89, "takeawai": 89, "link1": 89, "link2": 89, "minut": 89, "ipynb": 89, "daniil": 90, "lyakhov": 90, "aamir": 90, "nazir": 90, "alexand": 90, "suslov": 90, "yamini": 90, "nimmagadda": 90, "kozlov": 90, "subject": [90, 92], "openvinoquant": 90, "unlock": 90, "placement": 90, "significantli": [90, 91, 93, 94], "simplifi": [90, 91, 93, 94], "ux": [90, 91, 93], "torchdynamo": [90, 93, 94, 95], "eager": [90, 91, 92, 93, 94, 95], "mechan": [90, 93, 94], "torchvis": [90, 91, 92, 93, 94, 95], "resnet18": [90, 91, 92, 93, 94], "u": 90, "model_nam": [90, 93, 94], "__dict__": [90, 91, 92, 93, 94], "dummi": [90, 93, 94], "traced_b": [90, 93, 94], "disable_patch": 90, "exported_model": [90, 91, 92, 93, 94], "preset": 90, "elu": 90, "prelu": 90, "gelu": 90, "quantizationpreset": 90, "bert": [90, 93], "modeltyp": 90, "ignored_scop": 90, "exclud": 90, "layer_1": 90, "layer_2": 90, "layer_3": 90, "ignoredscop": 90, "conv2d": [90, 91, 92, 93, 94, 95], "regex": 90, "layer_": 90, "subgraph": [90, 92], "node": [90, 92, 93, 94, 95], "target_devic": 90, "taken": 90, "account": 90, "cpu_spr": 90, "npu": 90, "targetdevic": 90, "fold": [90, 91, 93, 94], "batchnorm": [90, 91, 92, 93, 94], "preced": [90, 91, 93, 94], "prepared_model": [90, 91, 92, 93, 94], "fold_quant": 90, "finish": [90, 93], "comparison": 90, "biascorrect": 90, "discrep": 90, "calibration_load": 90, "dataload": [90, 91, 92], "transform_fn": 90, "data_item": 90, "calibration_dataset": 90, "smooth_quant": 90, "fast_bias_correct": 90, "deploy": [90, 93], "jerri": [91, 93, 95], "zhang": [91, 93, 94, 95], "_export": [91, 92, 93], "14k": 91, "programm": [91, 93, 94], "db": 91, "xnnpack": [91, 92, 95], "xnnpack_quant": [91, 92], "get_symmetric_quantization_config": [91, 92], "xnnpackquant": [91, 92, 95], "prior": 91, "qconfigmap": [91, 95], "backendconfig": [91, 95], "rel": 91, "intent": [91, 95], "qconfig": [91, 95], "3d": [91, 95], "incompat": 91, "great": 91, "ideal": 91, "fake_qu": 91, "hidden": 91, "summari": 91, "thu": 91, "queri": [91, 95], "previous": 91, "embedding_byt": 91, "executorchquant": 91, "concaten": 91, "prone": 91, "cleaner": 91, "composed_quant": 91, "quantization_cap": 91, "concern": 91, "decoupl": 91, "minmax": 91, "freed": 91, "identitc": 91, "entir": [91, 92], "imagenet": [91, 92], "unzip": [91, 92], "data_path": [91, 92], "resnet18_pretrained_float": [91, 92], "sy": [91, 92], "numpi": [91, 92], "np": [91, 92], "resnet": [91, 92, 93], "warn": [91, 92], "filterwarn": [91, 92], "categori": [91, 92], "deprecationwarn": [91, 92], "r": [91, 92], "seed": [91, 92], "manual_se": [91, 92], "191009": [91, 92], "averagemet": [91, 92], "fmt": [91, 92], "val": [91, 92], "avg": [91, 92], "count": [91, 92], "__str__": [91, 92], "fmtstr": [91, 92], "topk": [91, 92], "predict": [91, 92], "maxk": [91, 92], "pred": [91, 92], "eq": [91, 92], "expand_a": [91, 92], "correct_k": [91, 92], "reshap": [91, 92], "mul_": [91, 92], "criterion": [91, 92], "top1": [91, 92], "top5": [91, 92], "cnt": [91, 92], "acc1": [91, 92], "acc5": [91, 92], "load_model": [91, 92], "model_fil": [91, 92], "weights_onli": [91, 92], "print_size_of_model": [91, 92], "temp": [91, 92], "p": [91, 92], "1e6": [91, 92], "prepare_data_load": [91, 92], "485": [91, 92], "456": [91, 92], "std": [91, 92], "229": [91, 92], "225": [91, 92], "randomresizedcrop": [91, 92], "randomhorizontalflip": [91, 92], "totensor": [91, 92], "dataset_test": [91, 92], "resiz": [91, 92], "centercrop": [91, 92], "train_sampl": [91, 92], "randomsampl": [91, 92], "test_sampl": [91, 92], "sequentialsampl": [91, 92], "train_batch_s": [91, 92], "sampler": [91, 92], "data_loader_test": [91, 92, 93, 94], "eval_batch_s": [91, 92], "saved_model_dir": [91, 92], "float_model_fil": [91, 92], "model_to_quant": [91, 92], "capture_pre_autograd_graph": [91, 92, 93], "dynamic_shap": [91, 92], "export_for_train": 91, "vari": [91, 92, 93, 94], "dynamic_dim": [91, 92], "constraint": [91, 92, 95], "qconfig_opt": 91, "set_object_typ": 91, "set_module_nam": 91, "workload": 91, "themodel": 91, "feedback": 91, "dq": 91, "fp32_op": 91, "qauntiz": 91, "x_int8": 91, "x_scale": 91, "x_zero_point": 91, "weight_int8": 91, "bias_fp32": 91, "output_scal": 91, "output_zero_point": 91, "x_fp32": 91, "quantized_decompos": 91, "dequantize_per_tensor": 91, "x_i8": 91, "x_quant_min": 91, "x_quant_max": 91, "weight_fp32": 91, "weight_i8": 91, "weight_quant_min": 91, "weight_quant_max": 91, "weight_permut": 91, "permute_copi": 91, "out_fp32": 91, "addmm": 91, "out_i8": 91, "quantize_per_tensor": 91, "out_zero_point": 91, "out_quant_min": 91, "out_quant_max": 91, "float32_op": 91, "decompos": 91, "use_reference_represent": 91, "x_int16": 91, "weight_int16": 91, "acc_int32": 91, "out_dtyp": 91, "bias_scal": 91, "bias_int32": 91, "div": 91, "mul": 91, "out_int8": 91, "qmin": 91, "qmax": 91, "date": 91, "unus": 91, "serila": 91, "consult": 91, "exportedprogram": 91, "pt2e_quantized_model_file_path": 91, "resnet18_pt2e_quant": 91, "quantized_ep": 91, "loaded_quantized_ep": 91, "loaded_quantized_model": 91, "diff": 91, "79": 91, "82": 91, "55": 91, "edg": [91, 95], "went": 91, "andrew": 92, "Or": 92, "ptq": [92, 93], "move_exported_model_to_ev": [92, 93], "correctli": 92, "certain": 92, "dropout": 92, "move_exported_model_to_train": 92, "jit": 92, "recursivescriptmodul": 92, "train_one_epoch": 92, "ntrain_batch": 92, "avgloss": 92, "5f": 92, "start_tim": 92, "global_avg": 92, "is_qat": [92, 93], "fusion": 92, "batchnorm2d": 92, "_native_batch_norm_legit": 92, "cudnn_batch_norm": 92, "mobilenetv2": 92, "recompil": 92, "consolid": 92, "epoch": 92, "far": 92, "num_epoch": 92, "num_train_batch": 92, "num_eval_batch": 92, "num_observer_update_epoch": 92, "num_batch_norm_update_epoch": 92, "num_epochs_between_ev": 92, "nepoch": 92, "subseq": 92, "disable_observ": 92, "bn": 92, "running_mean": 92, "running_var": 92, "new_arg": 92, "wish": 92, "prepared_model_copi": 92, "neval_batch": 92, "paus": 92, "resum": 92, "fail": [92, 95], "checkpoint_path": 92, "checkpoint_": 92, "behav": 92, "incorrectli": 92, "lesli": [93, 95], "fang": [93, 95], "weiwen": [93, 95], "xia": [93, 95], "jiong": [93, 95], "gong": [93, 95], "cnn": 93, "rnn": 93, "outstand": 93, "fourth": 93, "spr": 93, "xeon": 93, "processor": 93, "boost": 93, "contigu": [93, 94], "channels_last": [93, 94], "onednn": [93, 94], "assum": [93, 95], "word": 93, "satur": 93, "invok": 93, "addition": [93, 94], "pure": 93, "seamless": 93, "dedic": 93, "scenario": [93, 94], "plai": [93, 94], "convolut": [93, 94, 95], "absenc": [93, 94], "enhanc": [93, 94], "mirror": [93, 94], "autocast": [93, 94], "context": [93, 94], "device_typ": [93, 94], "turn": [93, 94], "cpp": 93, "qconvolut": [93, 94], "qlinear": [93, 94], "presenc": [93, 94], "pair": [93, 94], "remain": [93, 94], "conting": [93, 94], "qmaxpool2d": [93, 94], "torchinductor_freez": [93, 94], "example_x86inductorquantizer_pytorch_2_1": 93, "torchbench": 93, "measur": 93, "proven": 93, "depth": 93, "1000": 93, "shoud": 93, "example_x86inductorquantizer_qat": 93, "yan": 94, "zhiwei": 94, "wang": 94, "eikan": 94, "liangang": 94, "liu": 94, "river": 94, "cui": 94, "yifeng": 94, "xpuinductorquant": 94, "pip3": 94, "torchaudio": 94, "xpu_inductor_quantizer_exampl": 94, "xpu_inductor_quant": 94, "xpuiq": 94, "resnet18_weight": 94, "get_default_xpu_inductor_quantization_config": 94, "sign": 94, "wherea": 94, "histogramobserv": [94, 95], "perchannelminmaxobserv": 94, "quantizationspec": [94, 95], "quantizationconfig": [94, 95], "type_check": 94, "observerorfakequantizeconstructor": 94, "get_xpu_inductor_symm_quantization_config": 94, "extra_arg": 94, "act_observer_or_fake_quant_ctr": 94, "act_quantization_spec": [94, 95], "qscheme": [94, 95], "per_tensor_symmetr": [94, 95], "observer_or_fake_quant_ctr": [94, 95], "with_arg": [94, 95], "weight_observer_or_fake_quant_ctr": 94, "weight_quantization_spec": [94, 95], "per_channel_symmetr": 94, "ch_axi": 94, "oc": 94, "ic": 94, "kh": 94, "kw": 94, "conv": [94, 95], "bias_quantization_spec": 94, "amp": 94, "indcutor": 94, "kimish": 95, "patel": 95, "made": 95, "explicit": 95, "quantiat": 95, "encod": 95, "convei": 95, "quantizationannot": 95, "furthermor": 95, "minmaxobserv": 95, "input_qspec_map": 95, "output_qspec": 95, "_annot": 95, "conclud": 95, "matcher": 95, "get_source_partit": 95, "add_partit": 95, "gm": 95, "itertool": 95, "chain": 95, "add_nod": 95, "output_nod": 95, "per_tensor_affin": 95, "input_act_qspec": 95, "output_act_qspec": 95, "input_act0": 95, "input_act1": 95, "quantization_annot": 95, "phase": 95, "substitut": 95, "among": 95, "sharedquantizationspec": 95, "maxpool": 95, "average_pool": 95, "concat": 95, "edgeornod": 95, "transit": 95, "spec": 95, "conv1": 95, "conv2": 95, "fed": 95, "cat": 95, "conv1_out": 95, "conv2_out": 95, "qspec1": 95, "cat_input0": 95, "cat_input1": 95, "implicitli": 95, "therefor": 95, "ob": 95, "consum": 95, "rewrit": 95, "share_qparams_with_input_act0_qspec": 95, "known": 95, "beforehand": 95, "sigmoid": 95, "fixedqparamsquantizationspec": 95, "act_qspec": 95, "sigmoid_nod": 95, "input_act": 95, "derivedquantizationspec": 95, "derive_qparams_fn": 95, "observerorfakequant": 95, "observerbas": 95, "fakequantizebas": 95, "heurist": 95, "obejct": 95, "obs_or_fq": 95, "fq": 95, "act_obs_or_fq": 95, "weight_obs_or_fq": 95, "act_zp": 95, "weight_zp": 95, "bias_qspec": 95, "derived_from": 95, "backendquant": 95, "get_input_act_qspec": 95, "get_output_act_qspec": 95, "get_weight_qspec": 95, "get_bias_qspec": 95, "intermedi": 95, "call_funct": 95, "relu_": 95, "relu_nod": 95, "maybe_conv_nod": 95, "conv1d": 95, "unexpect": 95, "recognz": 95, "subgraphmatch": 95, "conv_relu_pattern": 95, "name_node_map": 95, "input_nod": 95, "weight_nod": 95, "bias_nod": 95, "caveat": 95, "exhaust": 95, "2d": 95, "4d": 95, "symbol": 95, "outcom": 95}, "objects": {"torchao.dtypes": [[9, 0, 1, "", "AffineQuantizedTensor"], [10, 0, 1, "", "BlockSparseLayout"], [11, 0, 1, "", "CutlassInt4PackedLayout"], [12, 0, 1, "", "CutlassSemiSparseLayout"], [13, 0, 1, "", "Float8Layout"], [14, 0, 1, "", "Int4CPULayout"], [15, 0, 1, "", "Layout"], [16, 0, 1, "", "MarlinQQQLayout"], [17, 0, 1, "", "MarlinQQQTensor"], [18, 0, 1, "", "MarlinSparseLayout"], [19, 0, 1, "", "NF4Tensor"], [20, 0, 1, "", "PlainLayout"], [21, 0, 1, "", "SemiSparseLayout"], [22, 0, 1, "", "TensorCoreTiledLayout"], [23, 0, 1, "", "UintxLayout"], [24, 2, 1, "", "to_affine_quantized_floatx"], [25, 2, 1, "", "to_affine_quantized_floatx_static"], [26, 2, 1, "", "to_affine_quantized_fpx"], [27, 2, 1, "", "to_affine_quantized_intx"], [28, 2, 1, "", "to_affine_quantized_intx_static"], [29, 2, 1, "", "to_marlinqqq_quantized_intx"], [30, 2, 1, "", "to_nf4"]], "torchao.dtypes.AffineQuantizedTensor": [[9, 1, 1, "", "dequantize"], [9, 1, 1, "", "from_hp_to_floatx"], [9, 1, 1, "", "from_hp_to_floatx_static"], [9, 1, 1, "", "from_hp_to_fpx"], [9, 1, 1, "", "from_hp_to_intx"], [9, 1, 1, "", "from_hp_to_intx_static"], [9, 1, 1, "", "to"]], "torchao.dtypes.MarlinQQQTensor": [[17, 1, 1, "", "dequantize"], [17, 1, 1, "", "from_hp_to_intx"]], "torchao.dtypes.MarlinSparseLayout": [[18, 1, 1, "", "pre_process"]], "torchao.dtypes.NF4Tensor": [[19, 1, 1, "", "convert_to_norm_float_weight"], [19, 1, 1, "", "dequantize"], [19, 1, 1, "", "dequantize_scalers"], [19, 1, 1, "", "double_quantize_scalers"], [19, 1, 1, "", "get_original_weight"], [19, 1, 1, "", "quantize_tensor_nearest"]], "torchao.float8": [[31, 0, 1, "", "CastConfig"], [32, 0, 1, "", "Float8LinearConfig"], [33, 0, 1, "", "ScalingGranularity"], [34, 0, 1, "", "ScalingType"], [35, 2, 1, "", "convert_to_float8_training"], [36, 2, 1, "", "precompute_float8_dynamic_scale_for_fsdp"]], "torchao.float8.Float8LinearConfig": [[32, 1, 1, "", "from_recipe_name"]], "torchao.quantization": [[37, 0, 1, "", "FPXWeightOnlyConfig"], [38, 0, 1, "", "Float8DynamicActivationFloat8WeightConfig"], [39, 0, 1, "", "Float8StaticActivationFloat8WeightConfig"], [40, 0, 1, "", "Float8WeightOnlyConfig"], [41, 0, 1, "", "GemliteUIntXWeightOnlyConfig"], [42, 0, 1, "", "Int4WeightOnlyConfig"], [43, 0, 1, "", "Int8DynamicActivationInt4WeightConfig"], [44, 0, 1, "", "Int8DynamicActivationInt8WeightConfig"], [45, 0, 1, "", "Int8WeightOnlyConfig"], [46, 0, 1, "", "MappingType"], [47, 0, 1, "", "TorchAODType"], [48, 0, 1, "", "UIntXWeightOnlyConfig"], [49, 0, 1, "", "ZeroPointDomain"], [50, 2, 1, "", "autoquant"], [51, 2, 1, "", "choose_qparams_affine"], [52, 2, 1, "", "choose_qparams_affine_with_min_max"], [53, 2, 1, "", "dequantize_affine"], [54, 2, 1, "", "int_scaled_matmul"], [63, 2, 1, "", "quantize_"], [64, 2, 1, "", "quantize_affine"], [65, 2, 1, "", "safe_int_mm"], [66, 2, 1, "", "smooth_fq_linear_to_inference"], [67, 2, 1, "", "swap_linear_with_smooth_fq_linear"], [68, 2, 1, "", "to_linear_activation_quantized"]], "torchao.quantization.qat": [[55, 0, 1, "", "ComposableQATQuantizer"], [56, 0, 1, "", "FakeQuantizeConfig"], [57, 0, 1, "", "FromIntXQuantizationAwareTrainingConfig"], [58, 0, 1, "", "Int4WeightOnlyEmbeddingQATQuantizer"], [59, 0, 1, "", "Int4WeightOnlyQATQuantizer"], [60, 0, 1, "", "Int8DynActInt4WeightQATQuantizer"], [61, 0, 1, "", "IntXQuantizationAwareTrainingConfig"], [62, 2, 1, "", "initialize_fake_quantizers"]], "torchao.quantization.qat.FakeQuantizeConfig": [[56, 3, 1, "", "group_size"], [56, 3, 1, "", "is_symmetric"]], "torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer": [[58, 1, 1, "", "convert"], [58, 1, 1, "", "prepare"]], "torchao": [[5, 4, 0, "-", "sparsity"]], "torchao.sparsity": [[69, 0, 1, "", "PerChannelNormObserver"], [70, 0, 1, "", "WandaSparsifier"], [71, 2, 1, "", "apply_fake_sparsity"], [72, 5, 1, "", "semi_sparse_weight"], [73, 2, 1, "", "sparsify_"]], "torchao.sparsity.PerChannelNormObserver": [[69, 1, 1, "", "forward"]], "torchao.sparsity.WandaSparsifier": [[70, 1, 1, "", "prepare"], [70, 1, 1, "", "squash_mask"], [70, 1, 1, "", "update_mask"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:module", "5": "py:attribute"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "module", "Python module"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchao": [0, 1, 2, 3, 4, 5, 6, 8, 74, 76, 77, 86], "dtype": [0, 7, 77], "layout": [0, 6, 15, 77], "tensor": [0, 6, 77, 84, 85, 86, 95], "subclass": [0, 6, 77, 85, 86], "quantiz": [0, 4, 8, 63, 74, 77, 78, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95], "techniqu": 0, "float8": [1, 8, 76], "main": [1, 4], "train": [1, 8, 76, 77, 90, 91, 92, 93, 94], "api": [1, 2, 4, 8, 74, 76, 95], "other": [1, 4, 6, 77], "type": 1, "refer": [2, 74], "python": 2, "kernel": [3, 6, 75, 77, 86], "infer": 4, "quantize_": 4, "qat": [4, 8, 92], "primit": [4, 77], "sparsiti": [5, 82], "contributor": 6, "guid": [6, 78, 86], "gener": 6, "extend": 6, "ad": [6, 77, 86], "effici": [6, 77], "custom": 6, "triton": 6, "hand": 6, "written": 6, "dispatch": [6, 86], "tensorimpl": [6, 77], "flow": [6, 77, 79, 86, 95], "us": [6, 95], "torch": [6, 90, 91, 92], "compil": [6, 86, 90], "perform": [6, 75, 91], "serial": [6, 79, 86], "featur": 6, "support": [6, 77, 86], "function": [6, 77, 91, 92], "compos": 6, "test": 6, "microbenchmark": 6, "model": [6, 76, 77, 79, 86, 90, 91, 92], "benchmark": 6, "eval": [6, 91], "part": [8, 76, 80], "2": [8, 78, 86, 90, 91, 92, 93, 94, 95], "fine": 8, "tune": 8, "qlora": 8, "awar": [8, 77, 92, 93], "option": [8, 89, 90], "1": [8, 76, 86, 90, 93, 94, 95], "torchtun": 8, "integr": [8, 77, 86], "axolotl": 8, "3": [8, 80, 86, 90, 93, 94, 95], "low": [8, 77], "rank": 8, "adapt": 8, "affinequantizedtensor": 9, "blocksparselayout": 10, "cutlassint4packedlayout": 11, "cutlasssemisparselayout": 12, "float8layout": 13, "int4cpulayout": 14, "marlinqqqlayout": 16, "marlinqqqtensor": 17, "marlinsparselayout": 18, "nf4tensor": 19, "plainlayout": 20, "semisparselayout": 21, "tensorcoretiledlayout": 22, "uintxlayout": 23, "to_affine_quantized_floatx": 24, "to_affine_quantized_floatx_stat": 25, "to_affine_quantized_fpx": 26, "to_affine_quantized_intx": 27, "to_affine_quantized_intx_stat": 28, "to_marlinqqq_quantized_intx": 29, "to_nf4": 30, "castconfig": 31, "float8linearconfig": 32, "scalinggranular": 33, "scalingtyp": 34, "convert_to_float8_train": 35, "precompute_float8_dynamic_scale_for_fsdp": 36, "fpxweightonlyconfig": 37, "float8dynamicactivationfloat8weightconfig": 38, "float8staticactivationfloat8weightconfig": 39, "float8weightonlyconfig": 40, "gemliteuintxweightonlyconfig": 41, "int4weightonlyconfig": 42, "int8dynamicactivationint4weightconfig": 43, "int8dynamicactivationint8weightconfig": 44, "int8weightonlyconfig": 45, "mappingtyp": 46, "torchaodtyp": 47, "uintxweightonlyconfig": 48, "zeropointdomain": 49, "autoqu": 50, "choose_qparams_affin": 51, "choose_qparams_affine_with_min_max": 52, "dequantize_affin": 53, "int_scaled_matmul": 54, "composableqatquant": 55, "fakequantizeconfig": 56, "fromintxquantizationawaretrainingconfig": 57, "int4weightonlyembeddingqatquant": 58, "int4weightonlyqatquant": 59, "int8dynactint4weightqatquant": 60, "intxquantizationawaretrainingconfig": 61, "initialize_fake_quant": 62, "quantize_affin": 64, "safe_int_mm": 65, "smooth_fq_linear_to_infer": 66, "swap_linear_with_smooth_fq_linear": 67, "to_linear_activation_quant": 68, "perchannelnormobserv": 69, "wandasparsifi": 70, "apply_fake_spars": 71, "semi_sparse_weight": 72, "sparsifi": 73, "welcom": 74, "document": 74, "get": 74, "start": [74, 78], "develop": 74, "note": [74, 76, 95], "eager": 74, "tutori": [74, 89], "pt2e": [74, 95], "pre": 76, "torchtitan": 76, "prerequisit": [76, 90, 93, 94, 95], "rowwis": 76, "scale": 76, "tensorwis": 76, "pick": 76, "recip": 76, "import": [76, 91, 92], "directli": [76, 95], "convers": 76, "overview": [77, 82, 89], "basic": 77, "current": 77, "placehold": 77, "pytorch": [77, 78, 90, 91, 92, 93, 94, 95], "implement": [77, 85, 86], "oper": [77, 85, 86, 95], "nativ": 77, "factori": 77, "op": 77, "deriv": [77, 95], "algorithm": 77, "weight": 77, "onli": 77, "dynam": 77, "activ": 77, "static": [77, 83], "insert": 77, "observ": 77, "how": [77, 91, 92, 95], "defin": [77, 91, 92], "modul": [77, 85, 86], "add": [77, 86], "calibr": [77, 83, 91], "bit": 77, "optim": [77, 79], "case": 77, "studi": 77, "int4": 77, "work": 77, "dure": 77, "execut": 77, "save": [77, 91, 92], "load": [77, 91, 92], "quick": 78, "first": 78, "exampl": [78, 86, 95], "export": [78, 90, 91, 92, 93, 94, 95], "next": [78, 85], "step": [78, 85, 86, 89], "deseri": 79, "what": [79, 85], "happen": 79, "when": 79, "an": 79, "serv": [80, 86], "vllm": [80, 86], "sglang": 80, "executorch": 80, "comput": [81, 88], "time": [81, 88], "goal": 82, "design": 82, "context": 82, "prune": 82, "configur": [82, 86, 91, 92], "criteria": 82, "strategi": 82, "pattern": [82, 95], "phase": 83, "write": [84, 85, 95], "your": [84, 85, 86], "own": [84, 85], "advanc": 84, "ar": 85, "swap": 85, "which": 85, "should": 85, "we": 85, "compar": 85, "output": 85, "architectur": 86, "usag": 86, "system": 86, "huggingfac": 86, "class": 86, "level": 86, "new": 86, "method": 86, "minim": 86, "requir": 86, "compat": 86, "why": 86, "creat": 86, "regist": 86, "s": 86, "kei": 86, "detail": 86, "hardwar": 86, "specif": [86, 91, 92], "linear": 86, "benefit": 86, "trade": 86, "off": 86, "share": [86, 95], "safetensor": 86, "diagram": 86, "high": 86, "transform": 86, "point": 86, "bring": 86, "extern": 86, "templat": 89, "addit": 89, "exercis": 89, "conclus": [89, 90, 91, 92, 93, 94, 95], "further": 89, "read": 89, "openvino": 90, "backend": [90, 91, 92, 93, 94], "introduct": [90, 93, 94, 95], "post": [90, 91, 93, 94], "nncf": 90, "instal": 90, "captur": [90, 93, 94], "fx": [90, 93, 94], "graph": [90, 93, 94], "appli": [90, 93, 94], "lower": [90, 91, 93, 94], "represent": 90, "4": [90, 95], "improv": 90, "metric": 90, "motiv": [91, 95], "helper": [91, 92], "prepar": [91, 92], "dataset": [91, 92], "set": 91, "mode": 91, "convert": [91, 92], "check": 91, "size": 91, "accuraci": 91, "evalu": 91, "debug": 91, "loop": 92, "checkpoint": 92, "x86": 93, "through": [93, 94], "inductor": [93, 94], "intel": 94, "gpu": 94, "annot": 95, "common": 95, "param": 95, "fix": 95, "paramet": 95, "5": 95, "A": 95, "toi": 95, "resnet18": 95, "ir": 95, "problem": 95, "match": 95, "aten": 95, "recommend": 95, "subgraphmatcherwithnamenodemap": 95}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})