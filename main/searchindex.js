Search.setIndex({"docnames": ["api_ref_dtypes", "api_ref_float8", "api_ref_intro", "api_ref_kernel", "api_ref_quantization", "api_ref_sparsity", "contributor_guide", "dtypes", "generated/torchao.dtypes.AffineQuantizedTensor", "generated/torchao.dtypes.BlockSparseLayout", "generated/torchao.dtypes.CutlassInt4PackedLayout", "generated/torchao.dtypes.CutlassSemiSparseLayout", "generated/torchao.dtypes.Float8Layout", "generated/torchao.dtypes.Int4CPULayout", "generated/torchao.dtypes.Layout", "generated/torchao.dtypes.MarlinQQQLayout", "generated/torchao.dtypes.MarlinQQQTensor", "generated/torchao.dtypes.MarlinSparseLayout", "generated/torchao.dtypes.NF4Tensor", "generated/torchao.dtypes.PlainLayout", "generated/torchao.dtypes.SemiSparseLayout", "generated/torchao.dtypes.TensorCoreTiledLayout", "generated/torchao.dtypes.UintxLayout", "generated/torchao.dtypes.to_affine_quantized_floatx", "generated/torchao.dtypes.to_affine_quantized_floatx_static", "generated/torchao.dtypes.to_affine_quantized_fpx", "generated/torchao.dtypes.to_affine_quantized_intx", "generated/torchao.dtypes.to_affine_quantized_intx_static", "generated/torchao.dtypes.to_marlinqqq_quantized_intx", "generated/torchao.dtypes.to_nf4", "generated/torchao.float8.CastConfig", "generated/torchao.float8.Float8LinearConfig", "generated/torchao.float8.ScalingGranularity", "generated/torchao.float8.ScalingType", "generated/torchao.float8.convert_to_float8_training", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp", "generated/torchao.quantization.FPXWeightOnlyConfig", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig", "generated/torchao.quantization.Float8WeightOnlyConfig", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig", "generated/torchao.quantization.Int4WeightOnlyConfig", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig", "generated/torchao.quantization.Int8WeightOnlyConfig", "generated/torchao.quantization.MappingType", "generated/torchao.quantization.TorchAODType", "generated/torchao.quantization.UIntXWeightOnlyConfig", "generated/torchao.quantization.ZeroPointDomain", "generated/torchao.quantization.autoquant", "generated/torchao.quantization.choose_qparams_affine", "generated/torchao.quantization.choose_qparams_affine_floatx", "generated/torchao.quantization.choose_qparams_affine_with_min_max", "generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq", "generated/torchao.quantization.dequantize_affine", "generated/torchao.quantization.dequantize_affine_floatx", "generated/torchao.quantization.fake_quantize_affine", "generated/torchao.quantization.fake_quantize_affine_cachemask", "generated/torchao.quantization.int_scaled_matmul", "generated/torchao.quantization.qat.ComposableQATQuantizer", "generated/torchao.quantization.qat.FakeQuantizeConfig", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.initialize_fake_quantizers", "generated/torchao.quantization.quantize_", "generated/torchao.quantization.quantize_affine", "generated/torchao.quantization.quantize_affine_floatx", "generated/torchao.quantization.safe_int_mm", "generated/torchao.quantization.smooth_fq_linear_to_inference", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear", "generated/torchao.quantization.to_linear_activation_quantized", "generated/torchao.sparsity.PerChannelNormObserver", "generated/torchao.sparsity.WandaSparsifier", "generated/torchao.sparsity.apply_fake_sparsity", "generated/torchao.sparsity.semi_sparse_weight", "generated/torchao.sparsity.sparsify_", "index", "performant_kernels", "pretraining", "quantization", "quick_start", "serialization", "sg_execution_times", "sparsity", "static_quantization", "subclass_advanced", "subclass_basic", "torchao_vllm_integration", "tutorials/index", "tutorials/sg_execution_times", "tutorials/template_tutorial", "tutorials_source/pt2e_quant_openvino", "tutorials_source/pt2e_quant_ptq", "tutorials_source/pt2e_quant_qat", "tutorials_source/pt2e_quant_x86_inductor", "tutorials_source/pt2e_quant_xpu_inductor", "tutorials_source/pt2e_quantizer"], "filenames": ["api_ref_dtypes.rst", "api_ref_float8.rst", "api_ref_intro.rst", "api_ref_kernel.rst", "api_ref_quantization.rst", "api_ref_sparsity.rst", "contributor_guide.rst", "dtypes.rst", "generated/torchao.dtypes.AffineQuantizedTensor.rst", "generated/torchao.dtypes.BlockSparseLayout.rst", "generated/torchao.dtypes.CutlassInt4PackedLayout.rst", "generated/torchao.dtypes.CutlassSemiSparseLayout.rst", "generated/torchao.dtypes.Float8Layout.rst", "generated/torchao.dtypes.Int4CPULayout.rst", "generated/torchao.dtypes.Layout.rst", "generated/torchao.dtypes.MarlinQQQLayout.rst", "generated/torchao.dtypes.MarlinQQQTensor.rst", "generated/torchao.dtypes.MarlinSparseLayout.rst", "generated/torchao.dtypes.NF4Tensor.rst", "generated/torchao.dtypes.PlainLayout.rst", "generated/torchao.dtypes.SemiSparseLayout.rst", "generated/torchao.dtypes.TensorCoreTiledLayout.rst", "generated/torchao.dtypes.UintxLayout.rst", "generated/torchao.dtypes.to_affine_quantized_floatx.rst", "generated/torchao.dtypes.to_affine_quantized_floatx_static.rst", "generated/torchao.dtypes.to_affine_quantized_fpx.rst", "generated/torchao.dtypes.to_affine_quantized_intx.rst", "generated/torchao.dtypes.to_affine_quantized_intx_static.rst", "generated/torchao.dtypes.to_marlinqqq_quantized_intx.rst", "generated/torchao.dtypes.to_nf4.rst", "generated/torchao.float8.CastConfig.rst", "generated/torchao.float8.Float8LinearConfig.rst", "generated/torchao.float8.ScalingGranularity.rst", "generated/torchao.float8.ScalingType.rst", "generated/torchao.float8.convert_to_float8_training.rst", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp.rst", "generated/torchao.quantization.FPXWeightOnlyConfig.rst", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8WeightOnlyConfig.rst", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.rst", "generated/torchao.quantization.Int4WeightOnlyConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.rst", "generated/torchao.quantization.Int8WeightOnlyConfig.rst", "generated/torchao.quantization.MappingType.rst", "generated/torchao.quantization.TorchAODType.rst", "generated/torchao.quantization.UIntXWeightOnlyConfig.rst", "generated/torchao.quantization.ZeroPointDomain.rst", "generated/torchao.quantization.autoquant.rst", "generated/torchao.quantization.choose_qparams_affine.rst", "generated/torchao.quantization.choose_qparams_affine_floatx.rst", "generated/torchao.quantization.choose_qparams_affine_with_min_max.rst", "generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq.rst", "generated/torchao.quantization.dequantize_affine.rst", "generated/torchao.quantization.dequantize_affine_floatx.rst", "generated/torchao.quantization.fake_quantize_affine.rst", "generated/torchao.quantization.fake_quantize_affine_cachemask.rst", "generated/torchao.quantization.int_scaled_matmul.rst", "generated/torchao.quantization.qat.ComposableQATQuantizer.rst", "generated/torchao.quantization.qat.FakeQuantizeConfig.rst", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer.rst", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.rst", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.rst", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.initialize_fake_quantizers.rst", "generated/torchao.quantization.quantize_.rst", "generated/torchao.quantization.quantize_affine.rst", "generated/torchao.quantization.quantize_affine_floatx.rst", "generated/torchao.quantization.safe_int_mm.rst", "generated/torchao.quantization.smooth_fq_linear_to_inference.rst", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear.rst", "generated/torchao.quantization.to_linear_activation_quantized.rst", "generated/torchao.sparsity.PerChannelNormObserver.rst", "generated/torchao.sparsity.WandaSparsifier.rst", "generated/torchao.sparsity.apply_fake_sparsity.rst", "generated/torchao.sparsity.semi_sparse_weight.rst", "generated/torchao.sparsity.sparsify_.rst", "index.rst", "performant_kernels.rst", "pretraining.rst", "quantization.rst", "quick_start.rst", "serialization.rst", "sg_execution_times.rst", "sparsity.rst", "static_quantization.rst", "subclass_advanced.rst", "subclass_basic.rst", "torchao_vllm_integration.md", "tutorials/index.rst", "tutorials/sg_execution_times.rst", "tutorials/template_tutorial.rst", "tutorials_source/pt2e_quant_openvino.rst", "tutorials_source/pt2e_quant_ptq.rst", "tutorials_source/pt2e_quant_qat.rst", "tutorials_source/pt2e_quant_x86_inductor.rst", "tutorials_source/pt2e_quant_xpu_inductor.rst", "tutorials_source/pt2e_quantizer.rst"], "titles": ["torchao.dtypes", "torchao.float8", "<code class=\"docutils literal notranslate\"><span class=\"pre\">torchao</span></code> API Reference", "torchao.kernel", "torchao.quantization", "torchao.sparsity", "Contributor Guide", "Dtypes", "AffineQuantizedTensor", "BlockSparseLayout", "CutlassInt4PackedLayout", "CutlassSemiSparseLayout", "Float8Layout", "Int4CPULayout", "Layout", "MarlinQQQLayout", "MarlinQQQTensor", "MarlinSparseLayout", "NF4Tensor", "PlainLayout", "SemiSparseLayout", "TensorCoreTiledLayout", "UintxLayout", "to_affine_quantized_floatx", "to_affine_quantized_floatx_static", "to_affine_quantized_fpx", "to_affine_quantized_intx", "to_affine_quantized_intx_static", "to_marlinqqq_quantized_intx", "to_nf4", "CastConfig", "Float8LinearConfig", "ScalingGranularity", "ScalingType", "convert_to_float8_training", "precompute_float8_dynamic_scale_for_fsdp", "FPXWeightOnlyConfig", "Float8DynamicActivationFloat8WeightConfig", "Float8StaticActivationFloat8WeightConfig", "Float8WeightOnlyConfig", "GemliteUIntXWeightOnlyConfig", "Int4WeightOnlyConfig", "Int8DynamicActivationInt4WeightConfig", "Int8DynamicActivationInt8WeightConfig", "Int8WeightOnlyConfig", "MappingType", "TorchAODType", "UIntXWeightOnlyConfig", "ZeroPointDomain", "autoquant", "choose_qparams_affine", "choose_qparams_affine_floatx", "choose_qparams_affine_with_min_max", "choose_qparams_and_quantize_affine_hqq", "dequantize_affine", "dequantize_affine_floatx", "fake_quantize_affine", "fake_quantize_affine_cachemask", "int_scaled_matmul", "ComposableQATQuantizer", "FakeQuantizeConfig", "FromIntXQuantizationAwareTrainingConfig", "Int4WeightOnlyEmbeddingQATQuantizer", "Int4WeightOnlyQATQuantizer", "Int8DynActInt4WeightQATQuantizer", "IntXQuantizationAwareTrainingConfig", "initialize_fake_quantizers", "quantize", "quantize_affine", "quantize_affine_floatx", "safe_int_mm", "smooth_fq_linear_to_inference", "swap_linear_with_smooth_fq_linear", "to_linear_activation_quantized", "PerChannelNormObserver", "WandaSparsifier", "apply_fake_sparsity", "semi_sparse_weight", "sparsify", "Welcome to the torchao Documentation", "Performant Kernels", "Pretraining with float8", "Quantization Overview", "Quick Start Guide", "Serialization", "Computation times", "Sparsity Overview", "Static Quantization", "Writing Your Own Quantized Tensor (advanced)", "Writing Your Own Quantized Tensor", "Integration with VLLM: Architecture and Usage Guide", "&lt;no title&gt;", "Computation times", "Template Tutorial", "PyTorch 2 Export Quantization for OpenVINO torch.compile Backend", "PyTorch 2 Export Post Training Quantization", "PyTorch 2 Export Quantization-Aware Training (QAT)", "PyTorch 2 Export Quantization with X86 Backend through Inductor", "PyTorch 2 Export Quantization with Intel GPU Backend through Inductor", "How to Write a <code class=\"docutils literal notranslate\"><span class=\"pre\">Quantizer</span></code> for PyTorch 2 Export Quantization"], "terms": {"thi": [2, 6, 8, 17, 18, 19, 20, 22, 35, 36, 40, 41, 42, 45, 49, 50, 52, 54, 56, 57, 60, 61, 67, 68, 74, 75, 76, 78, 81, 82, 83, 84, 86, 87, 89, 90, 93, 94, 95, 96, 97, 98, 99], "section": [2, 6, 82, 86, 90, 95, 96, 99], "introduc": [2, 94, 95, 97, 98, 99], "dive": 2, "detail": [2, 6, 36, 49, 81, 82, 83, 86, 87, 89, 94, 95, 96, 97], "how": [2, 6, 8, 14, 22, 41, 45, 50, 60, 61, 68, 79, 81, 83, 84, 86, 87, 89, 90, 94, 97, 98], "integr": [2, 6, 79, 81, 84, 86, 89, 97, 99], "pytorch": [2, 6, 8, 13, 16, 46, 60, 79, 81, 86, 89, 90, 93], "optim": [2, 6, 17, 35, 49, 53, 67, 79, 81, 86, 89, 94, 96, 97, 98], "your": [2, 6, 79, 81, 82, 83, 86, 95, 96, 97, 98, 99], "machin": [2, 96], "learn": [2, 41, 60, 83, 86, 93, 95, 97, 98, 99], "model": [2, 35, 40, 42, 49, 59, 61, 62, 63, 64, 66, 67, 71, 72, 75, 76, 78, 83, 86, 87, 89, 97, 98, 99], "dtype": [2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 46, 47, 49, 50, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 68, 78, 79, 81, 83, 84, 87, 89, 90, 95, 97, 98, 99], "quantiz": [2, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 20, 21, 23, 26, 28, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 78, 81, 84, 86], "sparsiti": [2, 11, 17, 20, 74, 75, 76, 77, 78, 79, 82, 84], "tba": [3, 7, 80], "For": [6, 8, 36, 60, 82, 83, 84, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "new": [6, 8, 81, 82, 87, 89, 95, 96, 97, 99], "case": [6, 49, 70, 86, 89, 90, 94, 95, 99], "exampl": [6, 8, 35, 45, 49, 59, 60, 61, 66, 67, 75, 78, 82, 84, 85, 86, 87, 89, 91, 92, 93, 94, 95, 96, 97, 98], "train": [6, 31, 56, 57, 59, 60, 79, 81, 83, 86, 89, 99], "like": [6, 14, 49, 81, 82, 83, 84, 86, 89, 90, 94, 95, 96, 97, 98, 99], "fp4": 6, "s": [6, 8, 45, 49, 50, 54, 56, 68, 69, 81, 82, 83, 86, 87, 89, 95, 96, 97, 98, 99], "fine": [6, 40, 41, 42, 47, 86], "start": [6, 32, 33, 45, 46, 48, 49, 81, 82, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "prototyp": [6, 60, 66, 82, 99], "folder": [6, 95, 96], "you": [6, 60, 75, 81, 82, 83, 84, 86, 89, 90, 93, 94, 95, 96, 97, 98, 99], "could": [6, 82, 89, 94, 95, 97, 98, 99], "also": [6, 49, 60, 67, 82, 83, 84, 86, 87, 89, 90, 95, 98, 99], "take": [6, 18, 67, 74, 78, 82, 86, 94, 95, 96, 97, 98, 99], "look": [6, 8, 81, 82, 86, 94, 95, 96, 97, 98], "affinequantizedtensor": [6, 16, 24, 25, 27, 82, 83, 84, 87, 89], "what": [6, 8, 16, 49, 81, 82, 83, 86, 87, 90, 93, 95, 99], "want": [6, 67, 78, 82, 83, 84, 86, 89, 90, 94, 95, 96, 99], "do": [6, 46, 49, 58, 67, 82, 86, 87, 89, 90, 95, 96, 97, 99], "mostli": [6, 52, 83, 97], "e": [6, 8, 36, 45, 49, 50, 54, 56, 59, 60, 67, 68, 69, 81, 82, 84, 87, 89, 94, 99], "g": [6, 8, 36, 45, 49, 50, 54, 56, 59, 60, 67, 68, 82, 84, 87, 89, 94, 99], "int3": 6, "exact": [6, 95, 96], "same": [6, 8, 37, 50, 52, 54, 56, 57, 68, 70, 78, 81, 82, 86, 87, 89, 96, 97, 98, 99], "affin": [6, 8, 10, 11, 12, 13, 17, 20, 21, 26, 54, 56, 68, 82], "pleas": [6, 8, 16, 36, 41, 79, 82, 83, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "feel": [6, 82, 86, 89, 90], "free": [6, 82, 89], "open": [6, 82, 86], "an": [6, 8, 21, 26, 27, 49, 57, 60, 75, 79, 82, 83, 86, 87, 89, 94, 95, 96, 97, 98, 99], "issu": [6, 82, 83, 89, 97], "have": [6, 40, 41, 45, 49, 62, 63, 64, 68, 75, 82, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "question": [6, 82, 84, 86, 89, 99], "specif": [6, 14, 17, 19, 20, 75, 82, 83, 84, 86, 94, 97, 98, 99], "more": [6, 8, 36, 40, 41, 42, 47, 49, 57, 81, 82, 83, 86, 87, 89, 90, 94, 95, 96, 97, 98], "refer": [6, 8, 81, 86, 87, 89, 90, 94, 95, 96, 97], "our": [6, 18, 81, 83, 86, 87, 89, 95, 96], "overview": [6, 79, 83, 90], "page": [6, 83, 97], "To": [6, 8, 16, 49, 81, 82, 83, 84, 86, 87, 90, 95, 96, 97, 99], "contribut": [6, 83, 86], "exist": [6, 46, 81, 82, 86, 87, 89, 95, 99], "code": [6, 41, 81, 82, 83, 86, 87, 89, 91, 93, 95, 96, 97, 98, 99], "base": [6, 14, 19, 45, 66, 75, 82, 83, 86, 89, 90, 94, 95, 96, 97, 98, 99], "make": [6, 82, 83, 89, 90, 95, 99], "trainabl": [6, 82, 89], "add": [6, 19, 89, 93, 97, 99], "parallel": [6, 81, 89, 90], "etc": [6, 82, 94, 99], "affine_quantized_tensor": [6, 84], "py": [6, 8, 16, 85, 92, 93, 97, 98], "api": [6, 49, 65, 82, 83, 86, 87, 89, 94, 95, 96, 97, 98], "quant_api": [6, 67, 84, 87], "primit": [6, 8, 16, 89, 95], "op": [6, 8, 16, 41, 49, 56, 57, 67, 83, 86, 89, 90, 95, 96, 97, 99], "slight": [6, 86], "variat": [6, 82], "quant_primit": [6, 8, 16, 87], "autotun": [6, 83, 87], "cpu": [6, 8, 13, 84, 86, 87, 90, 94, 95, 96, 97], "cuda": [6, 8, 53, 67, 81, 83, 84, 86, 87, 89, 96], "mp": 6, "csrc": 6, "mayb": [6, 30], "well": [6, 14, 49, 82, 83, 86, 95, 96, 99], "spars": [6, 9, 17, 20, 75, 82, 86], "marlin": [6, 15, 16, 17, 28], "aqt": 6, "621": 6, "we": [6, 8, 18, 45, 47, 49, 50, 52, 54, 56, 60, 67, 68, 78, 81, 82, 83, 84, 86, 87, 90, 94, 95, 96, 97, 98, 99], "ar": [6, 8, 12, 20, 22, 34, 36, 37, 40, 41, 49, 50, 54, 56, 59, 67, 68, 70, 75, 81, 82, 83, 84, 86, 87, 90, 94, 95, 96, 97, 98, 99], "still": [6, 82, 86, 95, 99], "decid": [6, 82, 86, 87], "split": [6, 95, 96], "can": [6, 21, 37, 40, 45, 49, 59, 60, 67, 68, 81, 82, 83, 84, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "implement": [6, 31, 84, 86, 87, 94, 95, 99], "regist": [6, 74, 89], "mai": [6, 52, 60, 82, 84, 87, 95, 96, 97, 98, 99], "need": [6, 37, 74, 75, 82, 83, 84, 86, 89, 90, 95, 96, 97, 99], "defin": [6, 14, 22, 32, 36, 74, 75, 83, 86, 87, 89, 90, 94, 97, 98, 99], "own": [6, 79, 81, 83, 86, 87, 95, 96, 99], "through": [6, 52, 79, 82, 83, 87, 89, 90, 93, 94, 95, 99], "int4": [6, 10, 13, 42, 45, 60, 62, 63, 64, 67, 78, 83, 84, 90], "access": [6, 94], "my_custom_op": 6, "devic": [6, 8, 53, 67, 70, 81, 83, 84, 87, 89, 90, 94, 95, 96, 97, 98], "check": [6, 8, 16, 82, 83, 84, 89, 94, 96, 99], "condit": [6, 82], "__torch_function__": [6, 82, 89], "__torch_dispatch__": [6, 89], "target": [6, 37, 38, 39, 41, 50, 75, 83, 86, 94, 95, 96, 97, 98, 99], "oper": [6, 8, 12, 14, 17, 52, 83, 94, 95, 96, 97, 98], "bfloat16": [6, 18, 56, 63, 68, 81, 82, 83, 84, 86, 87, 90, 97, 98], "activ": [6, 37, 38, 40, 42, 43, 49, 60, 64, 71, 75, 79, 83, 86, 87, 90, 94, 97, 98, 99], "uint4": [6, 41, 82, 83], "weight": [6, 17, 18, 35, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 60, 62, 63, 64, 67, 75, 78, 79, 81, 83, 84, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "found": [6, 82, 83, 86, 87, 89], "here": [6, 8, 68, 82, 83, 84, 87, 89, 90, 94, 95, 96, 97, 98, 99], "allow": [6, 83, 86, 89, 94, 95, 96, 97, 99], "peopl": [6, 82, 84, 90, 99], "linear": [6, 17, 31, 34, 37, 39, 41, 42, 43, 44, 47, 49, 59, 63, 64, 67, 72, 76, 78, 81, 82, 83, 84, 86, 87, 89, 94, 95, 96, 97, 99], "two": [6, 16, 20, 37, 82, 83, 86, 89, 94, 95, 96, 97, 99], "dispatch_condit": [6, 82], "impl": [6, 8, 82], "actual": [6, 39, 82, 87, 89, 90, 95, 96, 99], "bia": [6, 82, 83, 84, 87, 89, 90, 96, 99], "run": [6, 35, 49, 67, 71, 74, 81, 82, 83, 86, 89, 93, 94, 95, 96, 97, 98, 99], "both": [6, 8, 37, 82, 83, 86, 87, 89, 95, 97, 98, 99], "input_tensor": [6, 18, 82, 90], "weight_tensor": [6, 82, 90], "argument": [6, 8, 21, 49, 54, 67, 81, 82, 97], "register_aqt_quantized_linear_dispatch": 6, "show": [6, 68, 81, 82, 83, 86, 90, 95, 96], "work": [6, 20, 40, 81, 84, 86, 89, 90, 95, 96, 97], "sometim": [6, 86], "ha": [6, 8, 82, 86, 89, 90, 94, 95, 96, 98, 99], "pack": [6, 8, 10, 21, 22, 36, 40, 47, 82], "order": [6, 49, 59, 82, 86, 89, 99], "yield": [6, 86], "And": [6, 18, 37, 82, 89, 97, 99], "abstract": [6, 82], "see": [6, 8, 16, 36, 81, 82, 83, 84, 86, 87, 89, 90, 94, 95, 99], "full": [6, 83, 87, 93, 94, 96], "after": [6, 35, 49, 82, 84, 86, 94, 95, 96, 97, 98, 99], "wrap": [6, 49, 89, 97, 98], "factori": 6, "convert": [6, 8, 16, 18, 23, 26, 28, 29, 31, 59, 61, 62, 67, 69, 78, 81, 82, 86, 94, 97, 98, 99], "from": [6, 8, 18, 19, 24, 25, 27, 36, 42, 52, 56, 61, 67, 68, 78, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99], "float": [6, 8, 16, 18, 26, 28, 29, 36, 41, 45, 48, 49, 50, 52, 53, 54, 56, 57, 60, 68, 69, 72, 75, 82, 83, 84, 89, 95, 96, 99], "point": [6, 8, 16, 28, 36, 41, 45, 48, 54, 56, 60, 66, 69, 81, 82, 83, 84, 86, 87, 89, 95, 99], "my": [6, 86, 96], "to_my_dtyp": 6, "mydtypetensor": 6, "from_float": [6, 87, 89], "level": [6, 75, 82, 86, 89, 94, 95, 97, 98], "reus": [6, 82, 89], "quantize_": [6, 61, 67, 78, 82, 83, 84, 87], "appli": [6, 8, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 59, 67, 78, 82, 83, 86, 90, 96], "convers": [6, 8, 34, 82], "filter": [6, 34, 49, 81, 87], "choos": [6, 82, 86, 89, 95, 97], "which": [6, 16, 22, 49, 81, 82, 83, 84, 86, 87, 90, 94, 95, 96, 97, 98, 99], "modul": [6, 31, 32, 33, 34, 35, 45, 46, 48, 49, 59, 61, 62, 66, 67, 71, 72, 74, 75, 78, 81, 83, 84, 87, 94, 95, 96, 97, 98, 99], "should": [6, 8, 35, 40, 54, 56, 61, 74, 75, 81, 82, 86, 90, 94, 95, 99], "algorithm": [6, 41, 47, 86, 94], "onli": [6, 13, 34, 37, 39, 40, 41, 42, 44, 47, 78, 81, 83, 84, 86, 89, 90, 94, 95, 97, 98, 99], "dynam": [6, 30, 31, 35, 37, 40, 42, 43, 60, 64, 78, 87, 89, 95, 96, 97], "quant": [6, 8, 16, 36, 82, 90, 95, 98, 99], "static": [6, 8, 14, 18, 24, 27, 31, 38, 52, 60, 79, 83, 95, 96, 97, 98, 99], "type": [6, 8, 17, 18, 22, 31, 32, 33, 34, 37, 38, 39, 41, 42, 45, 46, 48, 49, 53, 58, 60, 68, 70, 79, 82, 84, 86, 89, 90, 94, 95, 97, 98, 99], "note": [6, 57, 59, 75, 82, 83, 86, 89, 90, 96, 97, 98], "2": [6, 8, 11, 13, 17, 20, 41, 45, 49, 57, 60, 68, 76, 78, 79, 81, 82, 86, 87, 89, 93], "4": [6, 11, 17, 20, 29, 40, 53, 76, 78, 82, 83, 84, 86, 89, 95, 96], "below": [6, 81, 82, 86, 89, 90, 93, 94], "follow": [6, 41, 60, 81, 82, 83, 86, 87, 89, 94, 95, 96, 97, 98, 99], "util": [6, 40, 81, 82, 83, 84, 89, 90, 94, 95, 96, 97, 98, 99], "import": [6, 61, 67, 78, 83, 84, 86, 87, 89, 90, 93, 94, 97, 98], "unwrap_tensor_subclass": [6, 83], "m_unwrap": 6, "m": [6, 67, 69, 78, 81, 83, 84, 87, 89, 95, 96, 97], "In": [6, 81, 82, 83, 86, 87, 89, 94, 95, 96, 97, 98, 99], "compat": [6, 17, 60, 83], "aim": [6, 82, 86, 98], "fullgraph": [6, 83], "true": [6, 8, 26, 31, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 50, 52, 53, 60, 67, 71, 78, 81, 83, 84, 87, 89, 90, 94, 95, 96, 97, 99], "first": [6, 18, 49, 58, 75, 82, 87, 89, 90, 95, 96, 99], "remov": [6, 50, 75, 81, 86, 90, 95, 96], "ani": [6, 19, 49, 62, 66, 73, 75, 82, 86, 89, 94, 96, 98], "unnecessari": 6, "graph": [6, 83, 95, 96, 99], "break": 6, "torch_log": 6, "output_cod": 6, "when": [6, 8, 19, 50, 54, 56, 68, 81, 82, 86, 87, 90, 94, 95, 96, 97, 98, 99], "script": [6, 83, 87, 89, 93, 96, 97, 98], "inductor": [6, 49, 79, 83, 94, 95], "python": [6, 82, 83, 86, 91, 93, 94, 95, 97, 98], "mode": [6, 40, 41, 49, 83, 87, 94, 96, 97, 98, 99], "max": [6, 45, 82, 83, 87, 89, 95, 96, 99], "checkout": [6, 8, 16, 79, 82], "doc": [6, 81, 82, 83, 89], "huggingfac": 6, "transform": [6, 8, 82, 87, 94, 95, 96, 97, 98], "deseri": [6, 82, 95, 96], "save_pretrain": 6, "push_to_hub": [6, 90], "from_pretrain": [6, 90], "http": [6, 8, 16, 36, 49, 75, 83, 86, 98], "co": 6, "main": [6, 8, 16, 41, 82, 83, 86, 87, 89, 95, 99], "en": [6, 49], "anoth": [6, 82, 86, 89, 95, 99], "diffus": 6, "github": [6, 8, 16, 36, 83], "com": [6, 8, 16, 36], "sayakpaul": 6, "blob": [6, 8, 16], "infer": [6, 8, 71, 79, 82, 83, 84, 86, 87, 89, 94, 95, 96, 97, 98], "serialization_and_load": 6, "md": 6, "The": [6, 8, 9, 14, 17, 22, 34, 36, 37, 38, 39, 49, 58, 67, 70, 71, 72, 75, 81, 82, 83, 84, 86, 89, 90, 94, 95, 96, 97, 98, 99], "abov": [6, 45, 82, 84, 86, 87, 89, 95, 96, 99], "just": [6, 45, 60, 82, 84, 86, 89, 95, 96, 99], "talk": [6, 82], "about": [6, 41, 82, 83, 84, 86, 95, 96, 97, 99], "basic": [6, 19, 83, 87, 89], "provid": [6, 14, 17, 20, 21, 49, 50, 59, 66, 81, 82, 86, 89, 90, 95, 96, 98, 99], "fsdp": [6, 82], "ll": [6, 45, 81, 82, 89, 95, 96, 99], "put": [6, 78, 97, 99], "developer_api_guid": 6, "cover": [6, 82, 93, 95, 98, 99], "executorch": [6, 42, 67, 83, 95, 96], "torchchat": 6, "todo": [6, 82], "qat": [6, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 79, 97], "suit": [6, 95, 97], "out": [6, 20, 45, 49, 75, 81, 82, 83, 86, 89, 94, 95, 96, 97], "differ": [6, 14, 41, 52, 59, 68, 70, 81, 82, 83, 84, 86, 89, 90, 95, 96, 97, 99], "system": 6, "dtensor": [6, 89], "recommend": [6, 37, 38, 39, 40, 41, 42, 47, 49, 81, 94, 97, 98], "copi": [6, 8, 75, 83, 84, 86, 87, 89, 94, 96, 97], "past": [6, 86], "adapt": [6, 87], "now": [6, 36, 42, 50, 81, 82, 83, 86, 87, 89, 94, 95, 97, 99], "befor": [6, 67, 82, 84, 86, 87, 89, 95, 96, 99], "some": [6, 49, 67, 75, 82, 83, 86, 87, 89, 94, 95, 96, 97, 98, 99], "singl": [6, 30, 35, 37, 49, 52, 81, 83, 86, 95, 99], "comput": [6, 17, 21, 35, 39, 74, 75, 86, 87, 89, 95, 96, 97, 98], "intens": 6, "memori": [6, 8, 57, 81, 83, 86, 89, 97, 98], "input": [6, 8, 17, 18, 20, 31, 34, 35, 49, 50, 52, 54, 56, 57, 58, 66, 67, 68, 70, 75, 78, 81, 82, 83, 87, 89, 94, 95, 96, 97, 98, 99], "dimens": [6, 8, 22, 47, 50, 54, 56, 58, 68, 81, 89, 90, 95, 96], "get": [6, 18, 81, 82, 83, 86, 90, 94, 95, 96, 97, 99], "sens": [6, 82, 89], "speedup": [6, 41, 81, 82, 83, 86], "d": [6, 82, 96], "creat": [6, 8, 24, 25, 27, 81, 82, 86, 89, 94, 95, 97, 98, 99], "file": [6, 81, 85, 89, 90, 92, 95, 96], "benchmark_aq": 6, "shape": [6, 8, 16, 49, 58, 70, 83, 87, 89, 90, 95, 98], "A": [6, 8, 22, 49, 52, 57, 74, 86, 89, 90, 95], "quick": [6, 79], "wai": [6, 8, 49, 81, 82, 86, 87, 89, 95, 96, 99], "relev": [6, 41, 82, 93], "chang": [6, 67, 81, 82, 83, 84, 86, 87, 89, 94, 95, 96, 98, 99], "interest": [6, 82, 86, 89], "tutori": [6, 8, 81, 82, 83, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99], "print_op_and_shap": 6, "output": [6, 31, 49, 50, 54, 56, 68, 81, 82, 83, 86, 93, 94, 95, 96, 97, 98, 99], "torch_func": 6, "built": [6, 81, 89], "k": [6, 70, 83, 84, 87, 89, 95, 96], "n": [6, 83, 84, 87, 89, 95, 96, 99], "10": [6, 45, 68, 81, 83, 87, 95, 96], "method": [6, 14, 17, 20, 21, 49, 67, 75, 83, 86, 87, 89, 94, 95, 96, 98, 99], "_c": 6, "tensorbas": 6, "object": [6, 22, 61, 67, 78, 89, 95, 96, 99], "arg": [6, 8, 62, 75, 89, 90, 96, 99], "0": [6, 8, 49, 60, 68, 72, 75, 81, 83, 84, 85, 86, 87, 89, 90, 92, 93, 95, 96, 98, 99], "size": [6, 8, 9, 16, 18, 40, 41, 42, 47, 50, 54, 56, 60, 68, 81, 83, 84, 86, 87, 89, 90, 96], "all": [6, 35, 45, 49, 52, 62, 66, 74, 75, 76, 82, 83, 84, 85, 86, 87, 89, 90, 91, 94, 95, 97, 99], "under": 6, "benchmark_your_kernel": 6, "helper": 6, "right": [6, 82, 86, 95], "1": [6, 17, 22, 32, 33, 41, 45, 46, 47, 48, 49, 53, 68, 75, 81, 82, 83, 84, 85, 86, 87, 89, 92, 93, 95, 96], "either": [6, 8, 37, 56, 75, 86, 96, 97, 98], "one": [6, 37, 49, 52, 74, 81, 82, 86, 89, 90, 96, 99], "probabl": 6, "keep": [6, 17, 75, 95], "futur": [6, 36, 87, 90, 95, 96, 97, 99], "llama": [6, 90, 94], "llama2": 6, "llama3": [6, 81], "sam": 6, "alreadi": [6, 8, 49, 89, 99], "modifi": [6, 34, 67, 75, 81, 82, 86, 89], "friendli": [6, 82], "compar": [6, 41, 57, 75, 81, 82, 95, 97, 99], "techniqu": [6, 84, 86, 87, 89, 90], "repres": [6, 8, 9, 12, 14, 25, 31, 60, 68, 75, 82, 84, 89, 95, 96], "bound": [6, 86, 90], "help": [6, 81, 82, 90, 94, 95], "option": [6, 8, 12, 16, 23, 26, 27, 28, 30, 31, 34, 37, 38, 40, 41, 43, 44, 49, 50, 52, 54, 56, 57, 60, 63, 65, 67, 68, 71, 72, 73, 75, 78, 81, 83, 90, 95, 96, 97, 98, 99], "each": [6, 18, 49, 60, 71, 74, 82, 86, 87, 89, 90, 95, 96, 99], "understand": [6, 81, 97, 99], "profil": 6, "profile_path": 6, "chrome": 6, "trace": [6, 82], "let": [6, 45, 68, 82, 83, 86, 87, 89, 99], "know": [6, 49, 61, 89], "class": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 30, 31, 32, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 59, 60, 61, 62, 63, 64, 65, 74, 75, 82, 83, 84, 87, 89, 95, 96, 97, 99], "torchao": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 83, 84, 86, 87, 89, 94, 95, 96, 97, 98], "tensor_impl": [8, 16, 82, 87], "aqttensorimpl": [8, 16], "block_siz": [8, 14, 16, 18, 23, 24, 26, 27, 28, 29, 50, 52, 54, 56, 57, 68, 83, 87], "tupl": [8, 16, 18, 23, 24, 26, 27, 28, 37, 38, 50, 52, 53, 54, 56, 57, 66, 68, 75, 89, 90, 95, 96, 99], "int": [8, 9, 16, 18, 21, 22, 23, 24, 26, 27, 28, 29, 36, 40, 41, 42, 44, 47, 50, 51, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 67, 68, 69, 75, 83, 87, 89, 90], "quant_min": [8, 16, 26, 27, 28, 45, 50, 52, 54, 56, 57, 68, 82, 83, 89, 98, 99], "union": [8, 16, 31, 37, 38, 50, 54, 56, 57, 60, 67, 68], "none": [8, 12, 16, 23, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 44, 45, 46, 48, 49, 50, 52, 54, 56, 57, 60, 65, 66, 67, 68, 71, 72, 73, 75, 78, 87, 89, 90, 94, 95, 96, 98], "quant_max": [8, 16, 26, 27, 28, 45, 50, 52, 54, 56, 57, 68, 82, 83, 89, 98, 99], "zero_point_domain": [8, 16, 26, 27, 28, 41, 50, 52, 56, 57, 60], "zeropointdomain": [8, 16, 26, 27, 28, 41, 50, 52, 56, 57, 60], "stride": [8, 16, 82, 89], "sourc": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 91, 93], "tensor": [8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 38, 39, 41, 42, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 68, 69, 70, 73, 75, 79, 81, 83, 84, 86, 87, 93, 95, 97, 98], "subclass": [8, 16, 34, 49, 74, 78, 83, 84, 86], "mean": [8, 18, 45, 50, 54, 56, 68, 69, 81, 82, 83, 86, 95, 96, 99], "quantized_tensor": 8, "float_tensor": [8, 89], "scale": [8, 14, 17, 24, 27, 32, 35, 38, 45, 48, 50, 52, 54, 55, 56, 57, 58, 60, 66, 68, 69, 71, 72, 82, 86, 87, 89, 90, 99], "zero_point": [8, 14, 27, 41, 48, 50, 52, 54, 56, 57, 68, 82, 86, 87, 89, 99], "happen": [8, 16, 49, 82, 89, 95, 97], "dure": [8, 16, 49, 54, 56, 60, 72, 81, 83, 86, 87, 89, 94, 96], "choose_qparam": [8, 82], "dequant": [8, 16, 18, 41, 54, 82, 83, 89, 90, 95, 97, 98, 99], "ao": [8, 16, 86, 90], "three": [8, 49, 75, 78, 82, 97, 98], "choose_qparams_affin": [8, 41, 52, 82], "quantize_affin": [8, 41, 56, 57, 82], "qand": 8, "dequantize_affin": [8, 41, 56, 57], "extern": [8, 97], "regardless": 8, "intern": [8, 21], "represent": [8, 14, 25, 41, 82, 86, 90, 95, 99], "orient": 8, "field": [8, 60, 99], "serv": [8, 14, 89, 98], "gener": [8, 56, 57, 82, 83, 86, 87, 89, 90, 91, 93, 94, 96, 97, 98, 99], "storag": [8, 17, 82, 86], "data": [8, 9, 14, 17, 22, 37, 38, 39, 41, 52, 79, 82, 84, 86, 87, 89, 90, 94, 95, 96, 97, 98, 99], "store": [8, 17, 18, 22, 74, 82, 86, 90, 95, 96], "plain": [8, 90], "int_data": [8, 89], "format": [8, 17, 18, 36, 40, 69, 82, 86, 95, 96, 99], "depend": [8, 40, 49, 84, 86, 89, 95, 96, 98], "kernel": [8, 10, 11, 13, 17, 21, 36, 40, 41, 67, 83, 86, 94, 97, 98], "granular": [8, 32, 37, 38, 40, 41, 42, 47, 50, 54, 56, 60, 68, 81, 82, 87, 90], "element": [8, 20, 22, 49, 50, 54, 56, 68, 86], "share": [8, 50, 54, 56, 68, 86], "qparam": [8, 50, 54, 56, 68], "us": [8, 12, 13, 14, 17, 18, 19, 22, 24, 27, 37, 38, 41, 42, 45, 47, 49, 50, 52, 54, 56, 59, 60, 61, 68, 75, 79, 81, 82, 83, 84, 86, 87, 89, 90, 94, 95, 96, 97, 98], "per": [8, 39, 41, 42, 43, 44, 47, 50, 54, 56, 60, 62, 63, 64, 68, 75, 81, 82, 83, 86, 87, 98], "torch": [8, 17, 18, 22, 24, 31, 34, 37, 38, 39, 41, 47, 49, 50, 53, 54, 55, 56, 58, 60, 62, 63, 64, 67, 68, 70, 71, 72, 78, 81, 82, 83, 84, 86, 87, 89, 90, 93, 97, 98, 99], "origin": [8, 18, 39, 56, 61, 68, 75, 82, 83, 84, 86, 94, 95, 99], "high": [8, 23, 24, 25, 26, 27, 69, 81, 82, 86, 87, 89, 94, 95, 97, 98], "precis": [8, 23, 24, 25, 26, 27, 39, 63, 64, 69, 82, 87, 89, 94, 97, 98], "minimum": [8, 49, 50, 54, 56, 68], "valu": [8, 18, 31, 32, 33, 37, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 54, 56, 57, 68, 71, 75, 82, 86, 87, 89, 94, 95, 96, 99], "specifi": [8, 31, 34, 47, 56, 59, 67, 68, 75, 78, 81, 86, 94, 95, 96, 99], "deriv": [8, 52, 56, 68], "maximum": [8, 50, 54, 56, 68, 71], "domain": [8, 41, 48, 50, 54, 56, 60], "integ": [8, 26, 27, 40, 41, 45, 48, 50, 54, 56, 58, 60, 70, 87, 95, 96, 97], "zero": [8, 20, 41, 50, 54, 56, 60, 66, 75, 86, 87, 99], "ad": [8, 54, 56, 75, 86, 87, 89, 96], "subtract": [8, 18, 56], "unquant": [8, 56, 99], "default": [8, 9, 12, 19, 21, 22, 37, 38, 39, 40, 41, 47, 49, 50, 54, 56, 60, 67, 71, 72, 81, 83, 89, 90, 94, 95, 96, 97, 98, 99], "float32": [8, 24, 54, 55, 56, 60, 62, 64, 68, 69, 84, 86, 87, 89, 97, 98, 99], "given": [8, 16, 29, 81, 86, 90, 99], "return": [8, 16, 17, 18, 34, 49, 57, 58, 60, 67, 70, 71, 72, 78, 81, 82, 83, 84, 87, 89, 90, 94, 95, 96, 97, 98, 99], "classmethod": [8, 16, 87, 89, 90], "from_hp_to_floatx": 8, "input_float": [8, 16, 23, 24, 25, 26, 27, 28, 73], "target_dtyp": [8, 23, 24, 26, 27, 30, 31, 50, 52, 82, 87], "_layout": [8, 16, 23, 24, 25, 26, 27, 28, 82, 83, 87], "layout": [8, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 41, 42, 43, 78, 86], "scale_dtyp": [8, 23, 24, 26, 50, 52, 87], "float8": [8, 11, 12, 23, 24, 30, 31, 32, 33, 34, 35, 37, 38, 39, 79, 82, 87], "from_hp_to_floatx_stat": 8, "paramet": [8, 14, 17, 18, 24, 27, 34, 35, 37, 38, 39, 40, 41, 42, 45, 47, 49, 50, 54, 56, 58, 60, 67, 68, 70, 71, 72, 75, 78, 81, 82, 84, 86, 89, 90, 94, 95], "from_hp_to_fpx": 8, "floatx": [8, 25, 82], "ebit": [8, 25, 36, 51, 55, 69], "mbit": [8, 25, 36, 51, 55, 69], "support": [8, 25, 37, 42, 60, 78, 81, 83, 84, 86, 89, 94, 95, 96, 97, 98, 99], "float1": [8, 25], "float7": [8, 25], "from_hp_to_intx": [8, 16], "mapping_typ": [8, 26, 42, 50, 52, 60], "mappingtyp": [8, 26, 42, 43, 50, 52, 60, 87], "ep": [8, 26, 50, 52, 60, 87, 96, 98, 99], "zero_point_dtyp": [8, 26, 50, 52, 87], "preserve_zero": [8, 26, 41, 50, 52], "bool": [8, 26, 31, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 50, 52, 53, 60, 64, 67, 71, 78, 87], "plainlayout": [8, 26, 27, 42, 43, 87], "use_hqq": [8, 26, 41, 47, 90], "fals": [8, 26, 31, 41, 43, 47, 49, 53, 60, 64, 71, 75, 81, 82, 83, 84, 87, 89, 90, 94, 95, 96, 98, 99], "from_hp_to_intx_stat": 8, "kwarg": [8, 60, 62, 74, 75, 76, 89, 90], "perform": [8, 21, 35, 40, 49, 58, 62, 63, 64, 70, 71, 74, 81, 83, 86, 87, 89, 90, 94, 96, 97, 98], "self": [8, 82, 83, 84, 87, 89, 90, 95, 96, 97], "If": [8, 12, 34, 37, 49, 58, 60, 70, 71, 75, 82, 83, 86, 89, 95, 96], "correct": [8, 17, 95, 96], "otherwis": [8, 59, 60, 82, 96], "desir": [8, 49, 56, 87], "call": [8, 49, 56, 57, 74, 82, 83, 84, 86, 87, 89, 90, 96, 98], "non_block": 8, "memory_format": [8, 97, 98], "preserve_format": 8, "set": [8, 12, 37, 38, 39, 40, 41, 42, 47, 49, 52, 60, 67, 71, 75, 83, 86, 94, 96, 97, 98], "function": [8, 21, 34, 49, 53, 67, 74, 75, 76, 78, 81, 83, 84, 86, 87, 89, 90, 94, 99], "attempt": 8, "asynchron": 8, "respect": [8, 86, 96], "host": [8, 90], "possibl": [8, 86, 95, 96, 97, 99], "behavior": [8, 14, 59, 90, 95, 96], "pin": 8, "pageabl": 8, "howev": [8, 86, 90, 96, 99], "caution": 8, "advis": [8, 82], "featur": [8, 89, 94, 97, 98], "inform": [8, 86, 90, 94, 95], "good": [8, 83, 89, 99], "usag": [8, 35, 49, 59, 60, 61, 79, 81, 97, 98], "pin_memori": 8, "even": [8, 81, 86, 99], "match": [8, 54, 58, 86, 95], "other": [8, 14, 75, 81, 84, 86, 89, 90, 93, 95, 96, 97, 99], "randn": [8, 81, 83, 84, 87, 89, 94, 95, 96, 97, 98], "initi": [8, 66, 82, 83, 84, 96], "float64": 8, "5044": 8, "0005": 8, "3310": 8, "0584": 8, "cuda0": 8, "blocksiz": 9, "64": [9, 29, 41, 47, 53, 84, 87, 89, 90], "block": [9, 18, 75, 86], "matrix": [9, 12, 37, 38, 58, 70, 75, 83, 86, 97], "variabl": [9, 12, 21, 22, 75, 86], "cutlass": [10, 11], "mm_config": [12, 37, 38], "float8mmconfig": [12, 37, 38], "configur": [12, 30, 31, 34, 37, 38, 39, 41, 42, 43, 44, 47, 67, 78, 81, 82, 83, 97, 98, 99], "multipl": [12, 37, 38, 49, 58, 59, 70, 83, 86, 87, 89, 90, 97, 99], "involv": [12, 86], "tinygemm": [13, 41, 67, 82, 83], "_weight_int4pack_mm_for_cpu": [13, 41], "version": [13, 60, 81, 83, 89, 90, 95, 96, 99], "least": 13, "6": [13, 60, 81, 82, 83, 86, 95, 96, 97], "It": [14, 17, 19, 21, 35, 83, 86, 89, 99], "pre": [14, 17, 21, 83, 86, 99], "process": [14, 17, 19, 21, 22, 49, 72, 82, 86, 93, 94, 98], "post": [14, 21, 79, 83, 89, 96, 99], "addit": [14, 19, 49, 57, 81, 86, 89, 94, 95, 98, 99], "design": [14, 17, 20, 90, 94, 95, 99], "extend": [14, 82, 86, 97], "conjunct": 14, "tensorimpl": 14, "custom": [14, 74, 79, 81, 82, 83, 86, 89, 90, 94, 95, 97, 99], "interact": [14, 82, 95], "qqq": [15, 16, 28], "marlinqqq": 16, "inherit": [16, 19, 89, 90, 97, 98], "choose_qparams_and_quantize_affine_qqq": 16, "dequantize_affine_qqq": 16, "handl": [17, 20, 21, 49, 82], "pattern": [17, 20, 82, 83, 90, 94, 95], "ensur": [17, 96], "preprocess": [17, 20], "manag": 17, "pre_process": 17, "1\u00ba": 17, "transpos": [17, 82, 89], "sinc": [17, 74, 82, 84, 86, 87, 89, 95, 96, 97, 98, 99], "layer": [17, 34, 37, 39, 41, 43, 44, 47, 49, 62, 63, 64, 71, 72, 75, 76, 81, 86, 87, 89, 90, 94, 99], "2\u00ba": 17, "inject": 17, "3\u00ba": 17, "again": [17, 18, 86, 95, 99], "becaus": [17, 81, 82, 84, 86, 89, 96, 99], "dim": [17, 87, 89, 90, 95, 96], "tensor_meta": 18, "subclasstensorarg": 18, "n_block": 18, "scaler_block_s": [18, 29], "quantized_scal": 18, "quantization_factor": 18, "scaler_mean": 18, "quantized_data": [18, 90], "nf4": 18, "qlora": 18, "convert_to_norm_float_weight": 18, "normal": [18, 29, 49, 86, 95, 96], "dequantize_scal": 18, "unpack": [18, 69, 82], "doubl": 18, "scaler": 18, "int8": [18, 42, 43, 44, 60, 64, 67, 78, 82, 89, 95, 97, 98, 99], "per_scaler_block": 18, "factor": [18, 58, 72, 81, 86], "inpt_weight": 18, "double_quantize_scal": 18, "achiev": [18, 81, 86, 87, 89, 96, 97], "calcul": [18, 35, 45, 50, 52, 71, 82, 86, 95, 99], "absmax": 18, "find": [18, 86, 95, 99], "posit": 18, "typic": [18, 19, 82, 83, 84, 87, 90, 99], "per_block": 18, "int16": [18, 95], "n_scaler_block": 18, "get_original_weight": 18, "quantize_tensor_nearest": 18, "float16": [18, 53, 56, 68, 86], "nearest": 18, "round": [18, 45, 89], "up": [18, 67, 81, 82, 83, 86, 94, 95, 96, 99], "most": [19, 82, 86, 90, 95, 96, 99], "doe": [19, 41, 82, 86, 89, 95, 97, 98], "metadata": [19, 82, 89, 90], "step": [19, 35, 49, 81, 82, 86, 94, 95, 96, 97, 98, 99], "requir": [19, 21, 81, 82, 83, 86, 89, 94, 97, 99], "semi": [20, 78, 86], "structur": [20, 78, 83, 84, 86, 89, 95], "matric": [20, 86], "where": [20, 45, 47, 52, 62, 63, 64, 69, 82, 86, 90, 99], "everi": [20, 74, 86, 89, 95, 96], "four": [20, 94], "prune": [20, 75], "conform": 20, "inner_k_til": [21, 41, 63, 83], "8": [21, 22, 40, 41, 45, 63, 81, 82, 83, 90, 97, 98], "core": [21, 46, 82, 87, 90, 95], "tile": [21, 82], "fit": [21, 82, 84], "effici": [21, 83, 86, 87, 98], "affect": [21, 86], "matmul": [21, 39, 82, 86, 89], "pack_dim": [22, 47], "uintx": [22, 47, 82], "smaller": [22, 40, 41, 42, 47, 83, 84], "bit": [22, 29, 36, 40, 47, 69, 89, 90, 95, 97, 98], "width": [22, 40], "than": [22, 60, 81, 82, 86, 89, 95], "standard": [22, 82, 90], "byte": [22, 36, 47], "uintxtensor": 22, "determin": [22, 50, 56, 81, 86, 90], "along": [22, 86, 90, 94], "indic": [22, 48, 86, 99], "last": [22, 81, 94], "256": [29, 41, 62, 63, 64, 95, 96, 99], "scaling_typ": [30, 31], "scalingtyp": [30, 31], "scaling_granular": [30, 31], "scalinggranular": [30, 31], "tensorwis": [30, 31], "cast": [30, 32, 56, 57], "cast_config_input": 31, "config": [31, 34, 49, 60, 67, 75, 78, 83, 86, 87, 90, 95, 97, 98], "castconfig": 31, "cast_config_input_for_grad_weight": 31, "cast_config_weight": 31, "cast_config_weight_for_grad_input": 31, "cast_config_grad_output": 31, "cast_config_grad_output_for_grad_weight": 31, "gemm_config_output": 31, "float8gemmconfig": 31, "use_fast_accum": 31, "gemm_config_grad_input": 31, "gemm_config_grad_weight": 31, "enable_fsdp_float8_all_gath": 31, "pad_inner_dim": 31, "emul": 31, "force_recompute_fp8_weight_in_bwd": 31, "round_scales_to_power_of_2": 31, "nn": [31, 34, 49, 59, 62, 67, 71, 72, 78, 81, 82, 83, 84, 86, 87, 89, 90, 95, 96, 97, 99], "from_recipe_nam": 31, "recipe_nam": [31, 81], "float8linearrecipenam": 31, "str": [31, 34, 40, 53, 60, 67, 72, 73, 75, 78, 81, 89, 90, 98], "string": [31, 60, 75], "recip": [31, 74], "name": [32, 33, 45, 46, 48, 67, 72, 75, 78, 86, 89, 90, 94, 95, 96, 99], "qualnam": [32, 33, 45, 46, 48], "boundari": [32, 33, 45, 46, 48], "strategi": 32, "module_filter_fn": [34, 81], "callabl": [34, 49, 53, 67, 73, 78, 90], "float8linearconfig": 34, "swap": [34, 62, 81, 82, 86, 87, 96], "float8linear": [34, 81], "pass": [34, 49, 52, 74, 82, 87, 89, 90, 96, 99], "instanc": [34, 67, 74, 78, 84, 89, 95, 97, 98, 99], "fqn": [34, 75, 78, 81, 87], "reduc": [35, 81, 86, 97], "sum": [35, 95, 96], "backward": [35, 81, 86, 96], "set_inductor_config": [36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49], "sub": [36, 47, 86], "expon": [36, 69], "mantissa": [36, 69], "fp6_e3_m2": 36, "fp6_e2_m3": 36, "fp6": 36, "llm": 36, "paper": [36, 86, 93], "arxiv": [36, 75, 86], "org": [36, 49, 75, 82, 83, 86, 98], "ab": [36, 75, 86], "2401": 36, "14112": 36, "repo": 36, "usyd": 36, "fsalab": 36, "fp6_llm": 36, "renam": [36, 95, 96], "fpxtensorcoreaqttensorimpl": 36, "experiment": [36, 94], "merg": 36, "to_affine_quantized_floatx": 36, "activation_dtyp": [37, 38], "float8_e4m3fn": [37, 38, 39, 82], "weight_dtyp": [37, 38, 39], "pertensor": [37, 38, 87], "perrow": [37, 38], "list": [37, 49, 54, 59, 72, 75, 82, 83, 89, 90, 94, 96, 99], "symmetr": [37, 38, 39, 40, 42, 43, 44, 45, 50, 60, 89, 94, 95, 98, 99], "current": [37, 42, 67, 72, 75, 78, 81, 83, 86, 89, 90, 95, 96, 98], "fast": [37, 38, 86], "accumul": [37, 38], "adjust": [37, 38, 39, 40, 41, 42, 47, 49], "torchinductor": [37, 38, 39, 40, 41, 42, 47, 97, 98], "float8_e4m": 38, "channel": [39, 43, 44, 60, 62, 63, 64, 74, 87, 98], "group_siz": [40, 41, 42, 44, 47, 53, 60, 62, 67, 83, 90], "128": [40, 41, 81, 87, 89, 90, 98, 99], "bit_width": 40, "packing_bitwidth": 40, "weight_onli": 40, "gemlit": 40, "triton": [40, 82, 97, 98], "its": [40, 86, 89, 90, 95, 99], "associ": [40, 87], "fp16": [40, 50], "asymmetr": [40, 41, 42, 45, 47, 50, 60, 82, 83, 87, 94, 98, 99], "control": [40, 41, 42, 47, 75, 86, 90, 95], "grain": [40, 41, 42, 47, 89], "32": [40, 41, 42, 60, 67, 78, 81, 83, 84, 87, 89, 96], "impact": [40, 49, 81, 90], "hardwar": [40, 82, 83, 86], "runtim": [40, 82, 95], "tensorcoretiledlayout": [41, 82, 83], "group": [41, 42, 47, 60, 62, 63, 64, 82, 83], "tensor_core_til": [41, 82], "int4mm": [41, 83], "aten": [41, 82, 83, 89, 90, 94, 95, 96, 97, 98], "_weight_int4pack_mm": [41, 82], "tradit": 41, "instead": [41, 52, 60, 74, 81, 82, 83, 86, 89, 96, 97, 98, 99], "exactli": [41, 89], "chosen": [41, 86], "choic": 41, "whether": [41, 47, 48, 49, 50, 60, 89], "hqq": [41, 47, 82], "preserv": [41, 50, 75, 86, 94], "Will": 41, "act_mapping_typ": [42, 43], "token": [42, 43, 60, 64, 81], "produc": [42, 83, 94, 95, 96, 97, 98], "backend": [42, 79, 83, 86, 99], "did": 42, "lower": [42, 82, 83, 86, 87, 96], "flow": [42, 86, 87, 94, 95, 96, 97, 98], "yet": [42, 46, 89, 90, 96, 97, 98], "marlinqqqlayout": 42, "cutlassint4packedlayout": 42, "weight_only_decod": 43, "number": [45, 47, 49, 69, 75, 86, 89, 96, 97], "map": [45, 60, 82, 89, 95, 99], "rang": [45, 81, 86, 87, 95, 96], "sai": [45, 68, 82, 90, 99], "3": [45, 49, 68, 81, 82, 83, 86, 93, 95, 96], "5": [45, 72, 75, 81, 83, 86, 90, 93, 95, 96], "7": [45, 81, 97, 98], "symmetric_no_clipping_err": 45, "variant": [45, 52, 89], "smin": 45, "smax": 45, "min_val_neg": [45, 89], "max_val_po": [45, 89], "By": [45, 86], "individu": [45, 86], "less": [45, 86, 89, 95], "error": [45, 49, 60, 81, 89, 95], "neg": 45, "directli": [45, 52, 82, 86, 87, 89], "placehold": [46, 98], "x": [47, 81, 83, 84, 87, 89, 90, 93, 94, 95, 96, 97, 98], "uint1": [47, 82], "uint7": [47, 82], "enum": 48, "quantized_v": 48, "float_val": 48, "mid_point": 48, "example_input": [49, 66, 83, 84, 87, 94, 95, 96, 97, 98, 99], "qtensor_class_list": 49, "aqdefaultlinearweight": 49, "aqint8weightonlyquantizedlinearweight": 49, "aqint8weightonlyquantizedlinearweight2": 49, "aqint8dynamicallyquantizedlinearweight": 49, "filter_fn": [49, 67, 78], "interpol": 49, "85": 49, "manual": [49, 96], "supress_autoquant_error": 49, "min_sqnr": 49, "aq_kwarg": 49, "autoquant": 49, "identifi": [49, 87, 99], "fastest": 49, "over": [49, 81, 86, 95, 96], "potenti": [49, 86, 87, 94, 95, 97, 98], "qtensor": 49, "prepar": [49, 59, 62, 71, 75, 82, 86, 94, 97, 98, 99], "search": [49, 86], "whose": [49, 99], "exchang": 49, "autoquantizablelinearweight": 49, "calibr": [49, 52, 83, 94, 96, 97, 98], "user": [49, 59, 81, 82, 83, 86, 87, 89, 93, 95, 96, 97, 98, 99], "seen": 49, "record": [49, 82, 87], "so": [49, 81, 82, 83, 84, 86, 89, 95, 96, 99], "final": [49, 57, 67, 82, 83, 86, 94, 95, 96, 97, 98, 99], "benchmark": [49, 71, 81, 83, 94, 97, 98], "member": 49, "pick": 49, "result": [49, 58, 69, 70, 82, 86, 87, 95, 96, 97, 98, 99], "highli": 49, "complet": [49, 94, 98], "simpli": [49, 86, 87, 89], "had": [49, 89, 95], "compil": [49, 67, 70, 81, 82, 83, 87, 89, 97, 98], "them": [49, 74, 82, 99], "onc": [49, 86], "proce": 49, "combin": [49, 60, 86, 89, 95, 97], "finalize_autoqu": 49, "been": [49, 89, 96, 97, 98, 99], "log": [49, 89], "forward": [49, 74, 82, 83, 84, 86, 87, 89, 90, 95, 96, 97], "fulli": [49, 67, 72, 78, 86, 95], "unless": [49, 90], "default_autoquant_class_list": 49, "contain": [49, 71, 72, 86, 89, 96, 99], "second": [49, 58, 81, 82, 93, 99], "stop": 49, "wait": [49, 82], "sever": [49, 81, 90, 94, 99], "automat": [49, 81, 89, 90, 93], "suppress": 49, "accept": [49, 99], "signal": 49, "nois": 49, "ration": 49, "wikipedia": 49, "wiki": 49, "noise_ratio": 49, "v": [49, 99], "non": [49, 82, 86, 89, 94, 97, 98], "caus": [49, 81], "too": 49, "larg": [49, 89, 97], "numer": [49, 81, 86, 95, 96, 97], "resaon": 49, "40": [49, 81], "keyword": 49, "example_input1": 49, "example_input2": 49, "int32": [50, 60, 62, 82, 83, 95, 99], "fp32": [50, 54, 60, 87, 89, 95, 97], "bf16": [50, 82, 83, 86, 97, 98], "optioanl": 50, "param": [50, 52, 57, 75], "request": [50, 54, 68], "min_val": [52, 82, 89], "max_val": [52, 82, 89], "observ": [52, 74, 86, 87, 94, 95, 96, 97, 98, 99], "obtain": 52, "track": [52, 82, 90], "nbit": 53, "axi": [53, 68, 87], "compute_dtyp": 53, "verbos": 53, "raw_output": 53, "optimize_weight": 53, "optimize_weights_proximal_legaci": 53, "input_dtyp": 54, "output_dtyp": [54, 55, 68], "uint8": [54, 68, 82, 87, 99], "quant_dtyp": [56, 57], "fake": [56, 57, 60, 61, 62, 63, 64, 81, 95, 96, 99], "awar": [56, 57, 75, 79, 83, 86, 89], "equival": [56, 57, 60, 72, 86, 96, 97, 99], "without": [56, 57, 61, 82, 86, 90, 97, 99], "valid": [56, 90, 99], "fake_quantize_affin": 57, "consum": [57, 99], "outlier": [57, 81], "mask": [57, 75, 86], "intermedi": [57, 99], "b": 58, "scales1": 58, "multipli": [58, 70, 86], "row": [58, 81, 86], "rais": [58, 70, 81, 89, 90], "assertionerror": [58, 70, 81, 89], "expect": [58, 81, 86, 89, 94, 95, 97, 98, 99], "twostepquant": 59, "compos": [59, 82, 86, 89, 95, 96, 99], "easili": [59, 94], "thei": [59, 81, 82, 83, 86, 89, 95, 96, 99], "constructor": [59, 89], "must": [59, 60, 81, 86, 90, 96, 98, 99], "embed": [59, 62], "undefin": [59, 75], "my_quant": 59, "qatquantizer1": 59, "qatquantizer2": 59, "qatquantizer3": 59, "torchaodtyp": 60, "scale_precis": [60, 62], "zero_point_precis": [60, 62], "is_dynam": [60, 97, 98, 99], "range_learn": 60, "is_symmetr": 60, "simul": [60, 76, 82, 86], "older": 60, "int1": [60, 82], "int7": 60, "pergroup": 60, "per_token": 60, "pertoken": 60, "per_channel": 60, "peraxi": [60, 87], "per_group": [60, 68], "separ": [60, 86, 90, 95, 99], "altern": [60, 82, 87, 89, 97, 98], "leav": 60, "empti": [60, 82], "properti": [60, 82], "throw": 60, "els": [60, 90, 95, 96], "fakequantizedlinear": 61, "fakequantizedembed": 61, "back": [61, 89], "correspond": [61, 67, 82, 84, 86, 89, 98, 99], "model_with_fake_quantized_linear": 61, "int4weightonlyqatembed": 62, "int4weightonlyembed": 62, "groupsiz": [63, 64, 68], "scales_precis": [63, 64], "padding_allow": 64, "activation_config": 65, "fakequantizeconfig": 65, "weight_config": 65, "fakequant": 66, "aobaseconfig": [67, 78, 87, 90], "inplac": [67, 75, 83], "workflow": [67, 78, 81, 83, 86, 99], "qualifi": [67, 72, 78, 86], "move": [67, 82, 87, 90, 96, 97], "speed": [67, 86, 94], "predefin": [67, 99], "execut": [67, 85, 89, 92], "path": [67, 70, 83, 94, 95, 96, 97, 99], "customiz": 67, "int8_dynamic_activation_int4_weight": 67, "int8_dynamic_activation_int8_weight": [67, 78], "mm": [67, 89, 95], "int4_weight_onli": 67, "int8_weight_onli": 67, "sequenti": [67, 78, 81], "1024": [67, 78, 83, 84, 97], "tabl": [68, 81, 82, 86], "per_tensor": 68, "per_axi": 68, "low": [69, 86, 89, 94], "00seeemm": 69, "fp6_e3m2": 69, "sign": [69, 98], "mat2": 70, "safe": 70, "consid": [70, 82, 86], "cubla": 70, "fallback": [70, 90], "i": [70, 81, 86, 94, 95, 96], "j": 70, "debug_skip_calibr": 71, "smoothquant": [71, 72, 94], "smoothfakedynamicallyquantizedlinear": [71, 72], "debug": 71, "skip_fqn_list": 72, "cur_fqn": 72, "alpha": 72, "replac": [72, 86, 90], "skip": [72, 75, 86], "being": [72, 81, 82, 86, 90, 97, 98], "input_quant_func": [73, 82], "quant_kwarg": 73, "dict": [73, 75, 89, 90, 98, 99], "l2": [74, 86], "norm": [74, 75, 86], "buffer": 74, "x_orig": 74, "overridden": 74, "although": [74, 89], "within": [74, 86, 90, 97, 98], "afterward": 74, "former": 74, "care": [74, 84, 86, 95], "hook": [74, 82], "while": [74, 75, 86, 89, 94, 95, 99], "latter": [74, 96], "silent": [74, 97], "ignor": [74, 81, 95, 96], "sparsity_level": [75, 86], "semi_structured_block_s": 75, "wanda": 75, "sparsifi": [75, 79, 84, 86], "propos": 75, "2306": 75, "11695": 75, "product": [75, 90, 97, 99], "magnitud": [75, 86], "parametr": 75, "deepcopi": [75, 83, 87, 89, 96], "squash_mask": [75, 86], "params_to_keep": 75, "params_to_keep_per_lay": 75, "squash": 75, "appropri": [75, 82, 94, 95, 96, 97, 98], "sparse_param": 75, "attach": [75, 86, 99], "kei": [75, 86, 93], "save": [75, 81, 83, 84, 90], "xdoctest": 75, "local": [75, 86], "don": [75, 81, 83, 86, 90, 99], "t": [75, 81, 82, 83, 86, 87, 89, 90, 95, 96, 99], "hasattr": [75, 90], "submodule1": 75, "linear1": [75, 83, 84, 87, 89], "foo": [75, 95], "bar": [75, 95], "submodule2": 75, "linear42": 75, "baz": 75, "print": [75, 83, 84, 89, 93, 95, 96], "42": [75, 87], "24": 75, "ones": [75, 82, 96], "update_mask": 75, "tensor_nam": [75, 90], "statist": [75, 82, 86, 87, 95, 96], "retriev": 75, "act_per_input": 75, "Then": [75, 89, 98, 99], "metric": 75, "across": [75, 86, 89, 90], "whole": [75, 99], "alia": [77, 90], "semisparseweightconfig": 77, "sparsify_": 78, "apply_tensor_subclass": [78, 82], "essenti": [78, 90, 94], "semi_sparse_weight": 78, "semisparselayout": 78, "sparsemarlinlayout": 78, "def": [78, 81, 82, 83, 84, 87, 89, 90, 94, 95, 96, 97, 98, 99], "isinst": [78, 81, 86, 87, 89, 90, 96, 99], "sparse_api": 78, "librari": [79, 84], "gradient": [79, 86], "nativ": [79, 81, 89, 95], "readm": [79, 83, 86], "overal": [79, 83, 95, 99], "introduct": [79, 82], "recent": 79, "highlight": [79, 89, 93], "updat": [79, 83, 84, 86, 95, 96, 97, 99], "guid": [79, 82, 94], "contributor": [79, 83], "serial": [79, 82, 95, 96], "write": [79, 83, 94, 95, 96], "advanc": [79, 87, 89, 94, 97, 98], "pretrain": [79, 86, 94, 95, 96, 97], "vllm": 79, "architectur": [79, 86, 94, 95, 97, 98], "export": 79, "x86": [79, 83], "intel": [79, 94, 97], "gpu": [79, 81, 83, 90, 93, 94], "5x": 81, "512": 81, "cluster": 81, "34": 81, "43x": 81, "2k": 81, "h200": 81, "latest": [81, 83], "offic": 81, "framework": [81, 94], "8b": 81, "offici": 81, "popular": [81, 82], "flagship": 81, "common": [81, 82, 86], "form": [81, 82, 86], "distribut": [81, 87, 89, 90, 94], "checkpoint": [81, 90], "quickli": [81, 89], "batteri": 81, "includ": [81, 82, 89, 94, 97, 98, 99], "experi": [81, 98], "commonli": [81, 86], "fork": 81, "build": [81, 82, 86, 89, 90, 95], "top": [81, 82, 89, 94, 95, 96, 97, 98], "re": [81, 84, 89, 95, 96], "readi": [81, 83, 87, 89, 96], "virtual": 81, "environ": 81, "conda": 81, "venv": 81, "instal": [81, 83, 95, 98], "download": [81, 83, 91, 93, 95, 96, 98], "job": 81, "command": [81, 83], "root": 81, "directori": 81, "launch": 81, "ngpu": 81, "config_fil": 81, "train_config": 81, "llama3_8b": 81, "toml": 81, "run_train": 81, "sh": 81, "fsdp2": 81, "hyperparamet": 81, "edit": 81, "line": [81, 86], "flag": [81, 96], "termin": 81, "rank0": 81, "titan": 81, "2025": 81, "06": 81, "04": 81, "08": 81, "51": 81, "48": 81, "074": 81, "info": 81, "loss": [81, 86, 95, 96], "12": [81, 98, 99], "2254": 81, "27": 81, "34gib": 81, "28": 81, "78": 81, "tp": [81, 90], "375": 81, "tflop": 81, "21": 81, "73": [81, 87], "mfu": 81, "20": [81, 96], "58": 81, "557": 81, "7069": 81, "30": [81, 83, 95], "99gib": 81, "62": 81, "034": 81, "407": 81, "35": [81, 87], "41": 81, "19": 81, "52": 81, "224": [81, 87, 94, 95, 96, 97, 98], "9196": 81, "022": 81, "406": [81, 95, 96], "65": 81, "904": 81, "1423": 81, "014": 81, "23": [81, 87], "As": [81, 82, 95, 99], "warmup": 81, "around": [81, 83, 84, 95], "7k": 81, "99gb": 81, "peak": 81, "against": 81, "baselin": [81, 95], "11": 81, "02": 81, "37": 81, "404": 81, "2611": 81, "22gib": 81, "595": 81, "47": 81, "49": [81, 87], "027": 81, "4260": 81, "89gib": 81, "344": 81, "367": 81, "39": 81, "15": [81, 83], "03": 81, "01": 81, "988": 81, "9482": 81, "321": 81, "366": 81, "14": 81, "991": 81, "1183": 81, "300": 81, "364": 81, "89": 81, "36": 81, "013": 81, "4659": 81, "291": 81, "84": 81, "769": 81, "gc": 81, "peform": 81, "period": 81, "collect": [81, 82, 86], "3k": 81, "89gb": 81, "11x": 81, "higher": [81, 82, 89, 94, 95, 97, 98], "throughput": 81, "nearli": 81, "ident": [81, 86], "improv": [81, 86, 95, 98, 99], "performan": 81, "vs": [81, 86, 95, 99], "accuraci": [81, 86, 87, 94, 96, 97], "curv": [81, 86], "omit": [81, 95, 96, 97], "648": 81, "2648": 81, "28gib": 81, "71": 81, "29": 81, "26": 81, "475": 81, "9106": 81, "91gib": 81, "53": 81, "503": 81, "434": 81, "43": 81, "94": [81, 95], "166": 81, "9": 81, "0774": 81, "663": 81, "443": 81, "44": [81, 87], "87": 81, "50": [81, 86, 87, 94, 95, 97, 98], "885": 81, "3233": 81, "643": 81, "442": 81, "66": [81, 87], "76": 81, "613": 81, "6150": 81, "637": 81, "72": 81, "6k": 81, "91gb": 81, "21x": 81, "tl": 81, "dr": 81, "better": [81, 89, 95, 96, 97, 98, 99], "priorit": 81, "accur": [81, 86, 94], "stabil": 81, "come": [81, 82, 86, 87, 88, 96, 97, 98], "cost": [81, 87], "slightli": [81, 89], "limit": [81, 89, 90, 95], "underflow": 81, "8xh100": 81, "box": [81, 86, 97], "toi": [81, 83, 87, 89, 97], "convert_to_float8_train": 81, "recurs": 81, "kind": [81, 95], "gemm": [81, 97, 98], "snippet": [81, 95, 96], "f": [81, 82, 84, 86, 87, 89, 90, 95, 96], "float8_linear_util": 81, "float8_linear": 81, "torch_version_at_least_2_5": [81, 83], "greater": 81, "sampl": [81, 82, 95, 97, 98], "2048": 81, "4096": 81, "adamw": 81, "lr": 81, "1e": 81, "elig": 81, "mod": [81, 86, 89], "divis": 81, "16": 81, "in_featur": [81, 83, 84, 87, 89], "out_featur": [81, 83, 87, 89], "enabl": [81, 82, 90, 97], "competit": 81, "loop": [81, 86], "_": [81, 87, 90, 94, 95, 96, 97], "zero_grad": [81, 96], "label": 81, "demonstr": [81, 82, 83, 89, 94, 96], "purpos": [81, 82, 89, 95], "fake_label": 81, "ones_lik": 81, "mse_loss": 81, "model_state_dict": 81, "state_dict": [81, 84, 95, 96], "optimizer_state_dict": 81, "pth": [81, 95, 96], "lai": 82, "stack": 82, "awq": 82, "gptq": 82, "codebookquantizedtensor": 82, "float3": 82, "compon": [82, 89, 90], "overload": [82, 86], "term": [82, 86, 95, 99], "extra": 82, "dev": 82, "discuss": [82, 89], "1833": 82, "No": [82, 84, 86], "matter": [82, 86], "end": [82, 86, 89, 90, 93, 96, 99], "avail": [82, 94, 95, 96, 97, 98], "later": [82, 89, 95, 96, 98], "float3_e2_m0": 82, "float4_e2_m1": 82, "float4_e3_m0": 82, "float5_e2_m2": 82, "float5_e3_m1": 82, "float6_e2_m3": 82, "float6_e3_m2": 82, "float8_e5m2": 82, "float8_e4m3fnuz": 82, "float8_e5m2fnuz": 82, "plan": [82, 96], "float4": 82, "float6": 82, "becom": [82, 95], "part": [82, 86, 89, 96], "uint2": 82, "117208": 82, "outsid": 82, "mention": [82, 95], "criteria": 82, "wide": 82, "adopt": 82, "fundament": [82, 86, 96], "until": 82, "evid": 82, "hopefulli": 82, "amen": 82, "haven": 82, "enough": 82, "ont": 82, "revisit": 82, "intx": 82, "connect": [82, 99], "int4tensor": 82, "previou": [82, 95, 96, 97, 98], "between": [82, 86, 89, 90, 94, 96, 97, 99], "preicison": 82, "mainli": [82, 94, 97, 99], "There": [82, 87, 89, 95, 99], "accommod": 82, "choose_qparams_affine_with_min_max": 82, "min": [82, 87, 89, 95, 99], "int_matmul": 82, "int_scaled_matmul": 82, "reli": [82, 83, 86, 87, 89], "On": [82, 83], "glue": 82, "everyth": 82, "togeth": [82, 95, 97, 99], "construct": [82, 95, 99], "low_precision_v": 82, "high_precision_v": 82, "procedur": 82, "veri": [82, 86, 90, 96], "straightforward": [82, 99], "try": [82, 86, 89, 95], "high_preicsion_v": 82, "especi": [82, 84, 86, 97, 98], "bitwidth": [82, 99], "codebook": 82, "hardcod": [82, 99], "select": [82, 95], "multi": 82, "dimension": [82, 86], "view": [82, 89, 95, 96], "mkldnn": 82, "coo": [82, 86], "sparse_coo": [82, 86], "sparsetensorimpl": 82, "idea": [82, 86], "nice": [82, 86], "concept": [82, 93, 95, 97, 98, 99], "why": [82, 89, 93], "c": [82, 83, 89, 97, 98], "conflict": 82, "quantized_linear": [82, 87, 95], "semant": 82, "stai": [82, 83, 89, 97], "develop": [82, 83, 95, 96, 99], "tradition": 82, "to_affine_quant": 82, "simplic": 82, "explain": [82, 94, 97], "simplest": [82, 86], "easi": 82, "linear_modul": 82, "to_affine_quantized_intx": 82, "requires_grad": [82, 87, 89, 90], "to_linear_activation_quant": 82, "quantized_weight": [82, 90], "activation_and_weight_quant": 82, "encount": 82, "input_qunat_func": 82, "redispatch": 82, "fx": [82, 95, 99], "symbolic_trac": 82, "But": [82, 89, 90, 99], "prefer": [82, 83, 89], "easier": [82, 99], "further": [82, 89, 94, 95, 96, 97], "modif": 82, "figur": [82, 86, 95], "At": [82, 86, 95], "thing": [82, 84, 86, 89, 95], "address": [82, 95], "stat": [82, 96], "averag": [82, 87, 95, 96], "calculate_qparam": [82, 87, 99], "affinequantizedminmaxobserv": [82, 87], "insert_observer_": 82, "observedlinear": [82, 87], "dataset": [82, 94, 97, 98], "complic": [82, 86, 95], "next": [82, 87, 95, 96, 97, 98], "done": [82, 89], "manner": 82, "intend": [82, 95], "autoround": 82, "multitensor": 82, "sure": [82, 99], "describ": [82, 84, 86, 93, 95, 96], "focus": [82, 86], "todai": 82, "low_bit_optim": 82, "similar": [82, 86, 87, 96, 97], "quantized_train": 82, "progress": [82, 90], "lot": [82, 86], "walk": [82, 87, 89, 93, 94, 97], "int4weightonlyconfig": [82, 83, 84, 90], "_convert_weight_to_int4pack": 82, "tensorcoretiledaqttensorimpl": 82, "_quantized_linear_op": 82, "goe": 82, "_aqt_qlinear_dispatch_t": 82, "dispatch": 82, "explan": 82, "wint4": 82, "explor": [83, 98], "stabl": 83, "releas": [83, 97], "pip": [83, 94, 95], "nightli": 83, "index": [83, 86, 98], "url": [83, 98], "whl": [83, 98], "cu121": 83, "major": 83, "instruct": [83, 95, 96, 97], "entri": 83, "mutat": 83, "insert": [83, 87, 94, 95, 96, 97, 98, 99], "logic": [83, 89, 90], "toylinearmodel": [83, 84, 87], "__init__": [83, 84, 87, 89, 90, 95, 96, 97], "super": [83, 84, 87, 89, 95, 96, 97], "linear2": [83, 84, 87, 89], "eval": [83, 84, 87, 94, 96, 97, 98], "faster": [83, 86], "model_bf16": 83, "leverag": [83, 89, 97, 98], "mix": [83, 94, 97, 98], "tensor_impl_dtyp": 83, "verifi": [83, 84, 89], "roughli": [83, 86], "quarter": 83, "os": [83, 95, 96], "tmp": 83, "int4_model": 83, "pt": 83, "bfloat16_model": 83, "int4_model_size_mb": 83, "getsiz": [83, 95, 96], "bfloat16_model_size_mb": 83, "2f": [83, 95, 96], "mb": [83, 84, 85, 92, 95, 96], "25": 83, "00": [83, 85, 92], "much": [83, 86, 99], "benchmark_model": 83, "temporari": 83, "workaround": [83, 90], "num_run": 83, "100": [83, 89, 95, 96], "_dynamo": [83, 89], "reset": [83, 95, 96], "bf16_time": 83, "int4_tim": 83, "time": [83, 86, 89, 93, 94, 95, 96], "3f": [83, 96], "ms": 83, "1fx": 83, "a100": 83, "80gb": 83, "393": 83, "410": 83, "9x": 83, "recogn": [83, 99], "decis": 83, "relu": [83, 94, 99], "pt2e": [83, 94, 95, 96, 97, 98], "fuse": [83, 86, 89, 96], "real": [83, 95, 99], "deleg": [83, 95], "x86inductorquant": [83, 97], "quantize_pt2": [83, 94, 95, 96, 97, 98], "prepare_pt2": [83, 94, 95, 97, 98], "x86_inductor_quant": [83, 97], "get_default_x86_inductor_quantization_config": [83, 97], "float_model": [83, 89, 94, 95, 96, 97], "data_load": [83, 95, 96, 97, 98], "no_grad": [83, 89, 94, 95, 96, 97, 98], "imag": [83, 94, 95, 96, 97, 98], "program": [83, 95, 96, 97, 99], "captur": [83, 95, 96, 99], "expos": [83, 95, 96], "express": [83, 89, 94, 95, 96, 99], "set_glob": [83, 95, 96, 97, 98], "xiq": [83, 97], "prepare_qat_pt2": [83, 96, 97], "sample_inference_data": 83, "convert_pt2": [83, 94, 95, 96, 97, 98], "wrapper": [83, 89, 97], "_inductor": [83, 97], "cpp_wrapper": [83, 97], "optimized_model": [83, 94, 97, 98], "converted_model": [83, 97, 98], "xpu": [83, 98], "openvino": 83, "simpl": [83, 86, 87, 89, 94, 97, 98], "visit": 83, "would": [83, 86, 89, 96, 98], "forget": 83, "tempfil": 84, "get_model_size_in_byt": 84, "batch_siz": [84, 87, 95, 96], "ref": [84, 95], "namedtemporaryfil": 84, "seek": [84, 86], "load": [84, 90], "meta": [84, 90, 99], "m_load": 84, "load_state_dict": [84, 95, 96], "assign": 84, "assert": [84, 87, 89, 90, 99], "equal": [84, 86], "float_weight1": 84, "float_weight2": 84, "quantized_weight1": 84, "quantized_weight2": 84, "go": [84, 89, 93, 99], "techinqu": 84, "reduct": [84, 86, 89], "4x": 84, "0625": 84, "reason": [84, 86], "avoid": [84, 86], "properli": 84, "003": [85, 92, 93], "total": [85, 92, 93], "galleri": [85, 91, 93], "mem": [85, 92], "templat": [85, 91, 92], "tutorials_sourc": 85, "template_tutori": [85, 92, 93], "neural": [86, 94, 97], "network": [86, 89, 94, 97], "overhead": [86, 90, 97], "latenc": 86, "carefulli": 86, "signific": 86, "pai": 86, "price": 86, "qualiti": 86, "f1": 86, "problem": [86, 89], "research": [86, 93], "face": [86, 95], "fragment": 86, "rightfulli": 86, "spent": 86, "compress": [86, 94], "place": [86, 94, 95, 96, 97, 98], "dens": 86, "solv": [86, 89], "focu": [86, 89], "realli": 86, "push": [86, 90], "concret": [86, 99], "hope": 86, "modular": 86, "acceler": 86, "scratch": [86, 93], "minim": [86, 94, 97, 98], "recov": [86, 96], "algorthim": 86, "realiz": 86, "trade": 86, "off": 86, "degrad": 86, "theoret": 86, "gain": [86, 98], "2x": 86, "analog": 86, "fix": [86, 87], "unstructur": 86, "One": [86, 89, 90, 99], "close": 86, "relat": 86, "mitig": 86, "retrain": 86, "neglig": 86, "area": 86, "agre": 86, "upon": 86, "consensu": 86, "mind": 86, "thought": 86, "subproblem": 86, "satisfi": 86, "consist": [86, 89, 97, 98, 99], "answer": 86, "independ": 86, "frontend": [86, 97], "arbitrari": 86, "handoff": 86, "piec": 86, "miss": 86, "natur": [86, 89, 95, 99], "present": 86, "clear": 86, "contract": 86, "7x": 86, "advantag": 86, "anticip": 86, "mani": [86, 89], "solut": 86, "third": 86, "parti": 86, "to_sparse_semi_structur": 86, "sparsesemistructuredtensor": 86, "weightnormsparsifi": 86, "half": 86, "subnetwork": 86, "sparse_config": 86, "named_modul": 86, "append": [86, 95, 96], "tensor_fqn": 86, "sparse_block_shap": 86, "zeros_per_block": 86, "fakespars": 86, "manipul": 86, "dictionari": 86, "paramer": 86, "parameter": 86, "necessari": [86, 87, 89, 94, 95, 96, 97, 98], "ve": 86, "suitabl": [86, 97], "0s": 86, "spot": 86, "definit": [86, 90], "academia": 86, "industri": 86, "often": [86, 89], "interchang": 86, "confus": [86, 95], "distinct": 86, "behind": 86, "doesn": [86, 96, 99], "itself": [86, 89], "those": [86, 87, 89], "loos": 86, "speak": 86, "tightli": 86, "coupl": [86, 89], "nvidia": 86, "csc": 86, "fbgemm": 86, "qnnpack": 86, "descript": [86, 94], "coordin": 86, "vector": [86, 97], "locat": 86, "bsr": 86, "sparse_bsr": 86, "except": [86, 89, 99], "scalar": [86, 95], "csr": 86, "sparse_csr": 86, "sparse_csc": 86, "column": 86, "compact": 86, "sparse_matrix": 86, "1d": 86, "indexptr": 86, "\u00bd": 86, "bitmask": 86, "2bit": 86, "unprun": 86, "quit": [86, 89], "successfulli": 86, "These": [86, 89, 94, 95, 96, 99], "broken": 86, "down": 86, "Not": 86, "sensit": 86, "effect": [86, 87, 89, 97, 98, 99], "best": [86, 97], "subsequ": [86, 89, 97, 98], "infinit": 86, "lost": 86, "degre": 86, "analysi": 86, "drop": 86, "give": [86, 89], "proxi": 86, "aforement": 86, "smallest": 86, "absolut": 86, "global": [86, 89], "scope": 86, "impli": 86, "pro": 86, "con": 86, "tradeoff": 86, "span": 86, "threshold": 86, "increas": [86, 95], "complex": 86, "constant": [86, 89, 95], "ctr_mobile_fe": 86, "score": 86, "w": [86, 90], "tenosr": 86, "udpat": 86, "cannot": [86, 87, 90], "histori": 86, "regrow": 86, "dw": 86, "via": [86, 94], "backprop": 86, "pat": 86, "unmask": 86, "resid": 86, "salienc": 86, "lowest": 86, "l1": 86, "shown": [86, 96, 99], "abl": [86, 89, 90, 95, 99], "repeat": [86, 95, 96], "shot": 86, "movement": 86, "tune": [86, 94], "2005": 86, "07683": 86, "rank": [86, 89], "wx": 86, "sqx": 86, "q": [86, 95], "usual": 86, "sort": 86, "wise": 86, "reconstruct": [86, 90], "random": [86, 95, 96], "randomli": 86, "tri": 86, "remedi": 86, "item": [86, 93], "ultim": [86, 87], "literatur": 86, "vision": 86, "nlp": [86, 93, 97], "iter": [86, 95, 96], "ctr_feed": 86, "na": 86, "multimask": 86, "pyspeech": 86, "fastna": 86, "approach": [86, 89, 94, 97, 98], "knowledg": [86, 93], "distil": 86, "pdf": 86, "2204": 86, "09656": 86, "arrang": 86, "recal": 86, "counterpart": 86, "slower": 86, "suffici": 86, "flexibl": [86, 89, 94, 97], "98": 86, "benefit": [86, 89, 95, 98], "special": [86, 94, 95], "exhibit": 86, "maintain": 86, "penalti": 86, "expens": [86, 89], "dictat": 86, "characterist": 86, "highest": 86, "wouldn": [86, 89], "visual": 86, "fig": 86, "4x4": 86, "benchmak": 86, "unlik": 87, "batch": [87, 96], "fly": 87, "welcom": 87, "histogram": [87, 95], "act_ob": 87, "finfo": 87, "weight_ob": 87, "observed_input": 87, "observed_weight": 87, "cl": [87, 89, 90], "float_linear": 87, "observed_linear": 87, "_replace_with_custom_fn_if_matches_filt": 87, "insert_observers_": 87, "_is_linear": 87, "lambda": [87, 90], "replacement_fn": 87, "copied_act_ob": 87, "copied_weight_ob": 87, "popul": 87, "feed": 87, "simpler": [87, 95], "quantizedlinear": [87, 89], "isn": 87, "strictli": 87, "to_affine_quantized_intx_stat": 87, "act_scal": [87, 99], "act_zero_point": 87, "weight_scal": [87, 95, 99], "weight_zero_point": [87, 95], "qweight": 87, "qinput": 87, "from_observ": 87, "begin": [87, 89], "dataclass": [87, 90, 99], "transform_modul": [87, 90], "register_quantize_module_handl": [87, 90], "staticquantconfig": 87, "_apply_static_qu": 87, "is_observed_linear": 87, "optimizedmodul": 87, "_orig_mod": 87, "0237": 87, "plainaqttensorimpl": 87, "142": 87, "31": [87, 99], "113": 87, "157": 87, "57": 87, "59": 87, "160": 87, "70": 87, "150": 87, "67": 87, "241": 87, "238": 87, "69": 87, "235": 87, "228": 87, "255": [87, 99], "201": 87, "114": 87, "236": 87, "88": [87, 95], "83": 87, "109": 87, "209": 87, "92": 87, "184": 87, "141": 87, "110": 87, "0009": 87, "0010": 87, "130": 87, "122": 87, "132": 87, "125": 87, "126": 87, "129": 87, "127": [87, 89, 98, 99], "133": 87, "124": 87, "131": 87, "135": 87, "136": 87, "soon": [88, 96], "foundat": 89, "extens": [89, 95, 97], "autograd": [89, 99], "interpos": 89, "namespac": 89, "continu": [89, 96, 97, 98, 99], "seamlessli": [89, 97, 98], "obviou": 89, "int8quantizedlinear": 89, "few": [89, 95, 96], "finer": 89, "intercept": 89, "contrast": 89, "long": [89, 95], "clunki": 89, "distributedlinear": 89, "duplic": 89, "bypass": 89, "offer": [89, 95], "outer": 89, "inner": 89, "allgath": 89, "bandwidth": 89, "rest": [89, 96], "read": 89, "document": [89, 90, 94, 95, 97], "zoo": 89, "podcast": 89, "edward": 89, "yang": 89, "int8_symmetric_quant": 89, "fp32_tensor": 89, "amin": 89, "keepdim": [89, 95, 96], "amax": 89, "zeros_lik": 89, "clamp": [89, 95], "w_int8": 89, "new_linear": 89, "left": [89, 99], "toymodel": 89, "quantized_model": [89, 94, 95, 96], "child": 89, "named_children": 89, "setattr": 89, "drawback": 89, "won": 89, "suppos": 89, "clean": 89, "eleg": 89, "pretti": 89, "power": [89, 90], "overrid": 89, "almost": 89, "shard": [89, 90], "ragged": 89, "rag": 89, "nestedtensor": 89, "resourc": 89, "who": 89, "link": [89, 93], "googl": 89, "collab": 89, "flopcount": 89, "memorytrack": 89, "With": [89, 95, 97, 99], "bare": 89, "bone": 89, "int8symmetrictensor": 89, "hold": 89, "staticmethod": 89, "disabl": [89, 96], "__new__": [89, 90], "_make_wrapper_subclass": [89, 90], "storage_offset": 89, "ndim": 89, "__tensor_flatten__": [89, 90], "attribut": [89, 90, 97, 98], "pt2": [89, 97], "__tensor_unflatten__": [89, 90], "tensor_data_dict": [89, 90], "extra_metadata": 89, "outer_s": [89, 90], "outer_strid": [89, 90], "undo": 89, "__repr__": 89, "repr": 89, "ahead": 89, "insid": 89, "int8_tensor": 89, "func": [89, 90], "op_implementations_dict": 89, "conveni": 89, "register_op": 89, "_op": 89, "opoverload": 89, "impl_decor": 89, "op_impl": 89, "particular": 89, "largest": 89, "tell": 89, "desugar": 89, "decor": [89, 90], "surfac": 89, "coverag": [89, 94, 95, 97, 98], "though": 89, "brute": 89, "forc": 89, "repeatedli": 89, "loggingtensor": 89, "_python_dispatch": [89, 90], "return_and_correct_alias": [89, 90], "int8_mm": 89, "detach": [89, 90], "int8_view_op": 89, "out_data": 89, "out_scal": [89, 95], "notic": 89, "hit": 89, "background": 89, "decomposit": 89, "live": 89, "decomp": 89, "shrink": 89, "author": [89, 93, 94, 95, 96, 97, 98, 99], "pain": 89, "rather": 89, "underli": 89, "worth": 89, "written": 89, "differenti": 89, "nuanc": 89, "longer": [89, 95, 96], "That": 89, "transposit": 89, "got": [89, 95, 99], "propag": [89, 95, 97, 98], "fact": 89, "themselv": [89, 95], "pointwis": [89, 97, 98], "alwai": 89, "were": 89, "might": [89, 90, 95, 99], "unwrap": 89, "dim0": 89, "dim1": 89, "confirm": 89, "quantized_model_module_swap": 89, "quantized_model_subclass": 89, "subclass_param": 89, "out_module_swap": 89, "allclos": 89, "out_compil": 89, "seri": 89, "wa": [89, 96], "comprehens": [90, 97], "e2": 90, "json": 90, "model_typ": [90, 94], "quant_typ": 90, "_type": 90, "_data": 90, "capabl": [90, 95, 97], "modulefqntoconfig": 90, "int8weightonlyconfig": 90, "self_attn": 90, "q_proj": 90, "k_proj": 90, "mlp": 90, "gate_proj": 90, "_default": 90, "torchaoconfig": 90, "automodelforcausallm": 90, "quantization_config": [90, 98], "1b": 90, "torch_dtyp": 90, "auto": 90, "device_map": 90, "safe_seri": 90, "usernam": 90, "server": 90, "narrow": 90, "copy_": 90, "state": 90, "slice": 90, "chunk": 90, "_apply_fn_to_data": 90, "heavi": 90, "codebas": 90, "fn": 90, "ctx": 90, "new_tensor": 90, "getattr": 90, "__class__": 90, "principl": 90, "torchaobasetensor": 90, "mynewquantconfig": 90, "classvar": 90, "myquantizedtensor": 90, "fbgemmfp8tensor": 90, "tensor_data_attr": 90, "tensor_attribut": 90, "attr": 90, "_to_copi": 90, "clone": 90, "fill_default": 90, "notimplementederror": 90, "_my_quant_transform": 90, "my_quantization_funct": 90, "len": [90, 95, 96, 99], "use_cutlass_kernel": 90, "my_cutlass_linear": 90, "elif": 90, "use_triton_kernel": 90, "my_triton_linear": 90, "disappear": 90, "extrem": 90, "sole": 90, "think": 90, "littl": 90, "world": 90, "explicitli": [90, 99], "spooki": 90, "action": [90, 95, 96], "distanc": 90, "statu": 90, "due": [90, 94, 99], "hub": 90, "team": 90, "2338": 90, "creation": 90, "detect": 90, "illustr": 90, "tutorials_python": 91, "zip": [91, 93], "jupyt": [91, 93], "notebook": [91, 93], "tutorials_jupyt": 91, "sphinx": [91, 93], "firstnam": 93, "lastnam": 93, "prerequisit": [93, 95], "v2": 93, "topic": 93, "rand": [93, 95, 96], "0714": 93, "5888": 93, "9336": 93, "4129": 93, "6234": 93, "3005": 93, "7315": 93, "5622": 93, "0342": 93, "7048": 93, "7587": 93, "7943": 93, "0808": 93, "9594": 93, "2546": 93, "practic": 93, "test": [93, 95, 97], "summar": 93, "takeawai": 93, "link1": 93, "link2": 93, "minut": 93, "ipynb": 93, "daniil": 94, "lyakhov": 94, "aamir": 94, "nazir": 94, "alexand": 94, "suslov": 94, "yamini": 94, "nimmagadda": 94, "kozlov": 94, "subject": [94, 96], "openvinoquant": 94, "unlock": 94, "placement": 94, "significantli": [94, 95, 97, 98], "simplifi": [94, 95, 97, 98], "ux": [94, 95, 97], "torchdynamo": [94, 97, 98, 99], "eager": [94, 95, 96, 97, 98, 99], "mechan": [94, 97, 98], "torchvis": [94, 95, 96, 97, 98, 99], "resnet18": [94, 95, 96, 97, 98], "u": 94, "model_nam": [94, 97, 98], "__dict__": [94, 95, 96, 97, 98], "dummi": [94, 97, 98], "traced_b": [94, 97, 98], "disable_patch": 94, "exported_model": [94, 95, 96, 97, 98], "preset": 94, "scheme": 94, "elu": 94, "prelu": 94, "gelu": 94, "quantizationpreset": 94, "bert": [94, 97], "modeltyp": 94, "ignored_scop": 94, "exclud": 94, "layer_1": 94, "layer_2": 94, "layer_3": 94, "ignoredscop": 94, "conv2d": [94, 95, 96, 97, 98, 99], "regular": [94, 97, 98], "regex": 94, "layer_": 94, "subgraph": [94, 96], "node": [94, 96, 97, 98, 99], "target_devic": 94, "taken": 94, "account": 94, "cpu_spr": 94, "npu": 94, "targetdevic": 94, "fold": [94, 95, 97, 98], "batchnorm": [94, 95, 96, 97, 98], "preced": [94, 95, 97, 98], "prepared_model": [94, 95, 96, 97, 98], "fold_quant": 94, "finish": [94, 97], "comparison": 94, "biascorrect": 94, "discrep": 94, "calibration_load": 94, "dataload": [94, 95, 96], "transform_fn": 94, "data_item": 94, "calibration_dataset": 94, "smooth_quant": 94, "fast_bias_correct": 94, "deploy": [94, 97], "jerri": [95, 97, 99], "zhang": [95, 97, 98, 99], "_export": [95, 96, 97], "14k": 95, "programm": [95, 97, 98], "db": 95, "xnnpack": [95, 96, 99], "xnnpack_quant": [95, 96], "get_symmetric_quantization_config": [95, 96], "xnnpackquant": [95, 96, 99], "prior": 95, "qconfigmap": [95, 99], "backendconfig": [95, 99], "rel": 95, "intent": [95, 99], "qconfig": [95, 99], "3d": [95, 99], "incompat": 95, "great": 95, "ideal": 95, "fake_qu": 95, "hidden": 95, "summari": 95, "thu": 95, "queri": [95, 99], "previous": 95, "embedding_byt": 95, "executorchquant": 95, "concaten": 95, "prone": 95, "cleaner": 95, "composed_quant": 95, "quantization_cap": 95, "concern": 95, "decoupl": 95, "minmax": 95, "freed": 95, "identitc": 95, "entir": [95, 96], "imagenet": [95, 96], "unzip": [95, 96], "data_path": [95, 96], "resnet18_pretrained_float": [95, 96], "sy": [95, 96], "numpi": [95, 96], "np": [95, 96], "resnet": [95, 96, 97], "warn": [95, 96], "filterwarn": [95, 96], "categori": [95, 96], "deprecationwarn": [95, 96], "r": [95, 96], "seed": [95, 96], "manual_se": [95, 96], "191009": [95, 96], "averagemet": [95, 96], "fmt": [95, 96], "val": [95, 96], "avg": [95, 96], "count": [95, 96], "__str__": [95, 96], "fmtstr": [95, 96], "topk": [95, 96], "predict": [95, 96], "maxk": [95, 96], "pred": [95, 96], "eq": [95, 96], "expand_a": [95, 96], "correct_k": [95, 96], "reshap": [95, 96], "mul_": [95, 96], "criterion": [95, 96], "top1": [95, 96], "acc": [95, 96], "top5": [95, 96], "cnt": [95, 96], "acc1": [95, 96], "acc5": [95, 96], "load_model": [95, 96], "model_fil": [95, 96], "weights_onli": [95, 96], "print_size_of_model": [95, 96], "temp": [95, 96], "p": [95, 96], "1e6": [95, 96], "prepare_data_load": [95, 96], "485": [95, 96], "456": [95, 96], "std": [95, 96], "229": [95, 96], "225": [95, 96], "randomresizedcrop": [95, 96], "randomhorizontalflip": [95, 96], "totensor": [95, 96], "dataset_test": [95, 96], "resiz": [95, 96], "centercrop": [95, 96], "train_sampl": [95, 96], "randomsampl": [95, 96], "test_sampl": [95, 96], "sequentialsampl": [95, 96], "train_batch_s": [95, 96], "sampler": [95, 96], "data_loader_test": [95, 96, 97, 98], "eval_batch_s": [95, 96], "saved_model_dir": [95, 96], "float_model_fil": [95, 96], "crossentropyloss": [95, 96], "model_to_quant": [95, 96], "capture_pre_autograd_graph": [95, 96, 97], "dynamic_shap": [95, 96], "export_for_train": 95, "vari": [95, 96, 97, 98], "dynamic_dim": [95, 96], "constraint": [95, 96, 99], "qconfig_opt": 95, "set_object_typ": 95, "set_module_nam": 95, "workload": 95, "themodel": 95, "feedback": 95, "dq": 95, "fp32_op": 95, "qauntiz": 95, "x_int8": 95, "x_scale": 95, "x_zero_point": 95, "weight_int8": 95, "bias_fp32": 95, "output_scal": 95, "output_zero_point": 95, "x_fp32": 95, "quantized_decompos": 95, "dequantize_per_tensor": 95, "x_i8": 95, "x_quant_min": 95, "x_quant_max": 95, "weight_fp32": 95, "weight_i8": 95, "weight_quant_min": 95, "weight_quant_max": 95, "weight_permut": 95, "permute_copi": 95, "out_fp32": 95, "addmm": 95, "out_i8": 95, "quantize_per_tensor": 95, "out_zero_point": 95, "out_quant_min": 95, "out_quant_max": 95, "float32_op": 95, "decompos": 95, "use_reference_represent": 95, "x_int16": 95, "weight_int16": 95, "acc_int32": 95, "out_dtyp": 95, "bias_scal": 95, "bias_int32": 95, "div": 95, "mul": 95, "out_int8": 95, "qmin": 95, "qmax": 95, "date": 95, "unus": 95, "serila": 95, "consult": 95, "exportedprogram": 95, "pt2e_quantized_model_file_path": 95, "resnet18_pt2e_quant": 95, "quantized_ep": 95, "loaded_quantized_ep": 95, "loaded_quantized_model": 95, "diff": 95, "79": 95, "82": 95, "55": 95, "edg": [95, 99], "went": 95, "andrew": 96, "Or": 96, "ptq": [96, 97], "move_exported_model_to_ev": [96, 97], "correctli": 96, "certain": 96, "dropout": 96, "move_exported_model_to_train": 96, "evalu": 96, "jit": 96, "recursivescriptmodul": 96, "train_one_epoch": 96, "ntrain_batch": 96, "avgloss": 96, "5f": 96, "start_tim": 96, "global_avg": 96, "is_qat": [96, 97], "fusion": 96, "batchnorm2d": 96, "_native_batch_norm_legit": 96, "cudnn_batch_norm": 96, "mobilenetv2": 96, "recompil": 96, "consolid": 96, "epoch": 96, "far": 96, "num_epoch": 96, "num_train_batch": 96, "num_eval_batch": 96, "num_observer_update_epoch": 96, "num_batch_norm_update_epoch": 96, "num_epochs_between_ev": 96, "nepoch": 96, "subseq": 96, "disable_observ": 96, "freez": [96, 97, 98], "bn": 96, "running_mean": 96, "running_var": 96, "momentum": 96, "new_arg": 96, "wish": 96, "prepared_model_copi": 96, "neval_batch": 96, "paus": 96, "resum": 96, "fail": [96, 99], "checkpoint_path": 96, "checkpoint_": 96, "behav": 96, "incorrectli": 96, "lesli": [97, 99], "fang": [97, 99], "weiwen": [97, 99], "xia": [97, 99], "jiong": [97, 99], "gong": [97, 99], "cnn": 97, "rnn": 97, "outstand": 97, "fourth": 97, "spr": 97, "xeon": 97, "processor": 97, "boost": 97, "contigu": [97, 98], "channels_last": [97, 98], "onednn": [97, 98], "assum": [97, 99], "word": 97, "satur": 97, "invok": 97, "addition": [97, 98], "pure": 97, "seamless": 97, "dedic": 97, "scenario": [97, 98], "plai": [97, 98], "convolut": [97, 98, 99], "absenc": [97, 98], "enhanc": [97, 98], "mirror": [97, 98], "autocast": [97, 98], "context": [97, 98], "device_typ": [97, 98], "turn": [97, 98], "cpp": 97, "qconvolut": [97, 98], "qlinear": [97, 98], "presenc": [97, 98], "pair": [97, 98], "remain": [97, 98], "conting": [97, 98], "qmaxpool2d": [97, 98], "torchinductor_freez": [97, 98], "example_x86inductorquantizer_pytorch_2_1": 97, "torchbench": 97, "measur": 97, "proven": 97, "depth": 97, "1000": 97, "shoud": 97, "example_x86inductorquantizer_qat": 97, "yan": 98, "zhiwei": 98, "wang": 98, "eikan": 98, "liangang": 98, "liu": 98, "river": 98, "cui": 98, "yifeng": 98, "xpuinductorquant": 98, "pip3": 98, "torchaudio": 98, "xpu_inductor_quantizer_exampl": 98, "xpu_inductor_quant": 98, "xpuiq": 98, "resnet18_weight": 98, "get_default_xpu_inductor_quantization_config": 98, "wherea": 98, "histogramobserv": [98, 99], "perchannelminmaxobserv": 98, "quantizationspec": [98, 99], "quantizationconfig": [98, 99], "type_check": 98, "observerorfakequantizeconstructor": 98, "get_xpu_inductor_symm_quantization_config": 98, "extra_arg": 98, "act_observer_or_fake_quant_ctr": 98, "act_quantization_spec": [98, 99], "qscheme": [98, 99], "per_tensor_symmetr": [98, 99], "observer_or_fake_quant_ctr": [98, 99], "with_arg": [98, 99], "weight_observer_or_fake_quant_ctr": 98, "weight_quantization_spec": [98, 99], "per_channel_symmetr": 98, "ch_axi": 98, "oc": 98, "ic": 98, "kh": 98, "kw": 98, "conv": [98, 99], "bias_quantization_spec": 98, "amp": 98, "indcutor": 98, "kimish": 99, "patel": 99, "made": 99, "explicit": 99, "quantiat": 99, "encod": 99, "qnnpackquant": 99, "convei": 99, "quantizationannot": 99, "furthermor": 99, "minmaxobserv": 99, "input_qspec_map": 99, "output_qspec": 99, "_annot": 99, "conclud": 99, "matcher": 99, "get_source_partit": 99, "add_partit": 99, "gm": 99, "itertool": 99, "chain": 99, "add_nod": 99, "output_nod": 99, "per_tensor_affin": 99, "input_act_qspec": 99, "output_act_qspec": 99, "input_act0": 99, "input_act1": 99, "quantization_annot": 99, "phase": 99, "substitut": 99, "among": 99, "sharedquantizationspec": 99, "maxpool": 99, "average_pool": 99, "concat": 99, "edgeornod": 99, "transit": 99, "spec": 99, "conv1": 99, "conv2": 99, "fed": 99, "cat": 99, "conv1_out": 99, "conv2_out": 99, "qspec1": 99, "cat_input0": 99, "cat_input1": 99, "implicitli": 99, "therefor": 99, "ob": 99, "rewrit": 99, "share_qparams_with_input_act0_qspec": 99, "known": 99, "beforehand": 99, "sigmoid": 99, "fixedqparamsquantizationspec": 99, "act_qspec": 99, "sigmoid_nod": 99, "input_act": 99, "derivedquantizationspec": 99, "derive_qparams_fn": 99, "observerorfakequant": 99, "observerbas": 99, "fakequantizebas": 99, "heurist": 99, "obejct": 99, "obs_or_fq": 99, "fq": 99, "act_obs_or_fq": 99, "weight_obs_or_fq": 99, "act_zp": 99, "weight_zp": 99, "bias_qspec": 99, "derived_from": 99, "backendquant": 99, "get_input_act_qspec": 99, "get_output_act_qspec": 99, "get_weight_qspec": 99, "get_bias_qspec": 99, "call_funct": 99, "relu_": 99, "relu_nod": 99, "maybe_conv_nod": 99, "conv1d": 99, "unexpect": 99, "recognz": 99, "subgraphmatch": 99, "conv_relu_pattern": 99, "name_node_map": 99, "input_nod": 99, "weight_nod": 99, "bias_nod": 99, "caveat": 99, "exhaust": 99, "2d": 99, "4d": 99, "symbol": 99, "outcom": 99}, "objects": {"torchao.dtypes": [[8, 0, 1, "", "AffineQuantizedTensor"], [9, 0, 1, "", "BlockSparseLayout"], [10, 0, 1, "", "CutlassInt4PackedLayout"], [11, 0, 1, "", "CutlassSemiSparseLayout"], [12, 0, 1, "", "Float8Layout"], [13, 0, 1, "", "Int4CPULayout"], [14, 0, 1, "", "Layout"], [15, 0, 1, "", "MarlinQQQLayout"], [16, 0, 1, "", "MarlinQQQTensor"], [17, 0, 1, "", "MarlinSparseLayout"], [18, 0, 1, "", "NF4Tensor"], [19, 0, 1, "", "PlainLayout"], [20, 0, 1, "", "SemiSparseLayout"], [21, 0, 1, "", "TensorCoreTiledLayout"], [22, 0, 1, "", "UintxLayout"], [23, 2, 1, "", "to_affine_quantized_floatx"], [24, 2, 1, "", "to_affine_quantized_floatx_static"], [25, 2, 1, "", "to_affine_quantized_fpx"], [26, 2, 1, "", "to_affine_quantized_intx"], [27, 2, 1, "", "to_affine_quantized_intx_static"], [28, 2, 1, "", "to_marlinqqq_quantized_intx"], [29, 2, 1, "", "to_nf4"]], "torchao.dtypes.AffineQuantizedTensor": [[8, 1, 1, "", "dequantize"], [8, 1, 1, "", "from_hp_to_floatx"], [8, 1, 1, "", "from_hp_to_floatx_static"], [8, 1, 1, "", "from_hp_to_fpx"], [8, 1, 1, "", "from_hp_to_intx"], [8, 1, 1, "", "from_hp_to_intx_static"], [8, 1, 1, "", "to"]], "torchao.dtypes.MarlinQQQTensor": [[16, 1, 1, "", "dequantize"], [16, 1, 1, "", "from_hp_to_intx"]], "torchao.dtypes.MarlinSparseLayout": [[17, 1, 1, "", "pre_process"]], "torchao.dtypes.NF4Tensor": [[18, 1, 1, "", "convert_to_norm_float_weight"], [18, 1, 1, "", "dequantize"], [18, 1, 1, "", "dequantize_scalers"], [18, 1, 1, "", "double_quantize_scalers"], [18, 1, 1, "", "get_original_weight"], [18, 1, 1, "", "quantize_tensor_nearest"]], "torchao.float8": [[30, 0, 1, "", "CastConfig"], [31, 0, 1, "", "Float8LinearConfig"], [32, 0, 1, "", "ScalingGranularity"], [33, 0, 1, "", "ScalingType"], [34, 2, 1, "", "convert_to_float8_training"], [35, 2, 1, "", "precompute_float8_dynamic_scale_for_fsdp"]], "torchao.float8.Float8LinearConfig": [[31, 1, 1, "", "from_recipe_name"]], "torchao.quantization": [[36, 0, 1, "", "FPXWeightOnlyConfig"], [37, 0, 1, "", "Float8DynamicActivationFloat8WeightConfig"], [38, 0, 1, "", "Float8StaticActivationFloat8WeightConfig"], [39, 0, 1, "", "Float8WeightOnlyConfig"], [40, 0, 1, "", "GemliteUIntXWeightOnlyConfig"], [41, 0, 1, "", "Int4WeightOnlyConfig"], [42, 0, 1, "", "Int8DynamicActivationInt4WeightConfig"], [43, 0, 1, "", "Int8DynamicActivationInt8WeightConfig"], [44, 0, 1, "", "Int8WeightOnlyConfig"], [45, 0, 1, "", "MappingType"], [46, 0, 1, "", "TorchAODType"], [47, 0, 1, "", "UIntXWeightOnlyConfig"], [48, 0, 1, "", "ZeroPointDomain"], [49, 2, 1, "", "autoquant"], [50, 2, 1, "", "choose_qparams_affine"], [51, 2, 1, "", "choose_qparams_affine_floatx"], [52, 2, 1, "", "choose_qparams_affine_with_min_max"], [53, 2, 1, "", "choose_qparams_and_quantize_affine_hqq"], [54, 2, 1, "", "dequantize_affine"], [55, 2, 1, "", "dequantize_affine_floatx"], [56, 2, 1, "", "fake_quantize_affine"], [57, 2, 1, "", "fake_quantize_affine_cachemask"], [58, 2, 1, "", "int_scaled_matmul"], [67, 2, 1, "", "quantize_"], [68, 2, 1, "", "quantize_affine"], [69, 2, 1, "", "quantize_affine_floatx"], [70, 2, 1, "", "safe_int_mm"], [71, 2, 1, "", "smooth_fq_linear_to_inference"], [72, 2, 1, "", "swap_linear_with_smooth_fq_linear"], [73, 2, 1, "", "to_linear_activation_quantized"]], "torchao.quantization.qat": [[59, 0, 1, "", "ComposableQATQuantizer"], [60, 0, 1, "", "FakeQuantizeConfig"], [61, 0, 1, "", "FromIntXQuantizationAwareTrainingConfig"], [62, 0, 1, "", "Int4WeightOnlyEmbeddingQATQuantizer"], [63, 0, 1, "", "Int4WeightOnlyQATQuantizer"], [64, 0, 1, "", "Int8DynActInt4WeightQATQuantizer"], [65, 0, 1, "", "IntXQuantizationAwareTrainingConfig"], [66, 2, 1, "", "initialize_fake_quantizers"]], "torchao.quantization.qat.FakeQuantizeConfig": [[60, 3, 1, "", "group_size"], [60, 3, 1, "", "is_symmetric"]], "torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer": [[62, 1, 1, "", "convert"], [62, 1, 1, "", "prepare"]], "torchao": [[5, 4, 0, "-", "sparsity"]], "torchao.sparsity": [[74, 0, 1, "", "PerChannelNormObserver"], [75, 0, 1, "", "WandaSparsifier"], [76, 2, 1, "", "apply_fake_sparsity"], [77, 5, 1, "", "semi_sparse_weight"], [78, 2, 1, "", "sparsify_"]], "torchao.sparsity.PerChannelNormObserver": [[74, 1, 1, "", "forward"]], "torchao.sparsity.WandaSparsifier": [[75, 1, 1, "", "prepare"], [75, 1, 1, "", "squash_mask"], [75, 1, 1, "", "update_mask"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:module", "5": "py:attribute"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "module", "Python module"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchao": [0, 1, 2, 3, 4, 5, 6, 79, 81, 82, 90], "dtype": [0, 7, 82], "layout": [0, 6, 14, 82], "tensor": [0, 6, 82, 88, 89, 90, 99], "subclass": [0, 6, 82, 89, 90], "quantiz": [0, 4, 67, 79, 82, 83, 87, 88, 89, 90, 94, 95, 96, 97, 98, 99], "techniqu": 0, "float8": [1, 81], "main": [1, 4], "train": [1, 82, 94, 95, 96, 97, 98], "api": [1, 2, 4, 79, 81, 99], "other": [1, 4, 6, 82], "type": 1, "refer": [2, 79], "python": 2, "kernel": [3, 6, 80, 82, 90], "infer": 4, "quantize_": 4, "qat": [4, 96], "primit": [4, 82], "sparsiti": [5, 86], "contributor": 6, "guid": [6, 83, 90], "gener": 6, "extend": 6, "ad": [6, 82, 90], "effici": [6, 82], "custom": 6, "triton": 6, "hand": 6, "written": 6, "dispatch": [6, 90], "tensorimpl": [6, 82], "flow": [6, 82, 84, 90, 99], "us": [6, 99], "torch": [6, 94, 95, 96], "compil": [6, 90, 94], "perform": [6, 80, 95], "serial": [6, 84, 90], "featur": 6, "support": [6, 82, 90], "function": [6, 82, 95, 96], "compos": 6, "test": 6, "microbenchmark": 6, "model": [6, 81, 82, 84, 90, 94, 95, 96], "benchmark": 6, "eval": [6, 95], "affinequantizedtensor": 8, "blocksparselayout": 9, "cutlassint4packedlayout": 10, "cutlasssemisparselayout": 11, "float8layout": 12, "int4cpulayout": 13, "marlinqqqlayout": 15, "marlinqqqtensor": 16, "marlinsparselayout": 17, "nf4tensor": 18, "plainlayout": 19, "semisparselayout": 20, "tensorcoretiledlayout": 21, "uintxlayout": 22, "to_affine_quantized_floatx": 23, "to_affine_quantized_floatx_stat": 24, "to_affine_quantized_fpx": 25, "to_affine_quantized_intx": 26, "to_affine_quantized_intx_stat": 27, "to_marlinqqq_quantized_intx": 28, "to_nf4": 29, "castconfig": 30, "float8linearconfig": 31, "scalinggranular": 32, "scalingtyp": 33, "convert_to_float8_train": 34, "precompute_float8_dynamic_scale_for_fsdp": 35, "fpxweightonlyconfig": 36, "float8dynamicactivationfloat8weightconfig": 37, "float8staticactivationfloat8weightconfig": 38, "float8weightonlyconfig": 39, "gemliteuintxweightonlyconfig": 40, "int4weightonlyconfig": 41, "int8dynamicactivationint4weightconfig": 42, "int8dynamicactivationint8weightconfig": 43, "int8weightonlyconfig": 44, "mappingtyp": 45, "torchaodtyp": 46, "uintxweightonlyconfig": 47, "zeropointdomain": 48, "autoqu": 49, "choose_qparams_affin": 50, "choose_qparams_affine_floatx": 51, "choose_qparams_affine_with_min_max": 52, "choose_qparams_and_quantize_affine_hqq": 53, "dequantize_affin": 54, "dequantize_affine_floatx": 55, "fake_quantize_affin": 56, "fake_quantize_affine_cachemask": 57, "int_scaled_matmul": 58, "composableqatquant": 59, "fakequantizeconfig": 60, "fromintxquantizationawaretrainingconfig": 61, "int4weightonlyembeddingqatquant": 62, "int4weightonlyqatquant": 63, "int8dynactint4weightqatquant": 64, "intxquantizationawaretrainingconfig": 65, "initialize_fake_quant": 66, "quantize_affin": 68, "quantize_affine_floatx": 69, "safe_int_mm": 70, "smooth_fq_linear_to_infer": 71, "swap_linear_with_smooth_fq_linear": 72, "to_linear_activation_quant": 73, "perchannelnormobserv": 74, "wandasparsifi": 75, "apply_fake_spars": 76, "semi_sparse_weight": 77, "sparsifi": 78, "welcom": 79, "document": 79, "get": 79, "start": [79, 83], "develop": 79, "note": [79, 81, 99], "eager": 79, "tutori": [79, 93], "pt2e": [79, 99], "pretrain": 81, "torchtitan": 81, "prerequisit": [81, 94, 97, 98, 99], "rowwis": 81, "scale": 81, "tensorwis": 81, "pick": 81, "recip": 81, "import": [81, 95, 96], "directli": [81, 99], "convers": 81, "overview": [82, 86, 93], "basic": 82, "current": 82, "placehold": 82, "pytorch": [82, 83, 94, 95, 96, 97, 98, 99], "implement": [82, 89, 90], "oper": [82, 89, 90, 99], "integr": [82, 90], "nativ": 82, "factori": 82, "op": 82, "deriv": [82, 99], "algorithm": 82, "weight": 82, "onli": 82, "dynam": 82, "activ": 82, "static": [82, 87], "insert": 82, "observ": 82, "how": [82, 95, 96, 99], "defin": [82, 95, 96], "modul": [82, 89, 90], "add": [82, 90], "calibr": [82, 87, 95], "awar": [82, 96, 97], "low": 82, "bit": 82, "optim": [82, 84], "case": 82, "studi": 82, "int4": 82, "work": 82, "dure": 82, "execut": 82, "save": [82, 95, 96], "load": [82, 95, 96], "quick": 83, "first": 83, "exampl": [83, 90, 99], "2": [83, 90, 94, 95, 96, 97, 98, 99], "export": [83, 94, 95, 96, 97, 98, 99], "next": [83, 89], "step": [83, 89, 90, 93], "deseri": 84, "what": [84, 89], "happen": 84, "when": 84, "an": 84, "comput": [85, 92], "time": [85, 92], "goal": 86, "design": 86, "context": 86, "prune": 86, "configur": [86, 90, 95, 96], "criteria": 86, "strategi": 86, "pattern": [86, 99], "phase": 87, "write": [88, 89, 99], "your": [88, 89, 90], "own": [88, 89], "advanc": 88, "ar": 89, "swap": 89, "which": 89, "should": 89, "we": 89, "compar": 89, "output": 89, "vllm": 90, "architectur": 90, "usag": 90, "system": 90, "1": [90, 94, 97, 98, 99], "huggingfac": 90, "class": 90, "3": [90, 94, 97, 98, 99], "level": 90, "serv": 90, "new": 90, "method": 90, "minim": 90, "requir": 90, "compat": 90, "why": 90, "creat": 90, "regist": 90, "s": 90, "kei": 90, "detail": 90, "hardwar": 90, "specif": [90, 95, 96], "linear": 90, "benefit": 90, "trade": 90, "off": 90, "share": [90, 99], "safetensor": 90, "diagram": 90, "high": 90, "transform": 90, "point": 90, "bring": 90, "extern": 90, "templat": 93, "option": [93, 94], "addit": 93, "exercis": 93, "conclus": [93, 94, 95, 96, 97, 98, 99], "further": 93, "read": 93, "openvino": 94, "backend": [94, 95, 96, 97, 98], "introduct": [94, 97, 98, 99], "post": [94, 95, 97, 98], "nncf": 94, "instal": 94, "captur": [94, 97, 98], "fx": [94, 97, 98], "graph": [94, 97, 98], "appli": [94, 97, 98], "lower": [94, 95, 97, 98], "represent": 94, "4": [94, 99], "improv": 94, "metric": 94, "motiv": [95, 99], "helper": [95, 96], "prepar": [95, 96], "dataset": [95, 96], "set": 95, "mode": 95, "convert": [95, 96], "check": 95, "size": 95, "accuraci": 95, "evalu": 95, "debug": 95, "loop": 96, "checkpoint": 96, "x86": 97, "through": [97, 98], "inductor": [97, 98], "intel": 98, "gpu": 98, "annot": 99, "common": 99, "param": 99, "fix": 99, "paramet": 99, "5": 99, "A": 99, "toi": 99, "resnet18": 99, "ir": 99, "problem": 99, "match": 99, "aten": 99, "recommend": 99, "subgraphmatcherwithnamenodemap": 99}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})