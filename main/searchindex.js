Search.setIndex({"docnames": ["api_ref_dtypes", "api_ref_float8", "api_ref_intro", "api_ref_kernel", "api_ref_quantization", "api_ref_sparsity", "contributor_guide", "dtypes", "generated/torchao.dtypes.AffineQuantizedTensor", "generated/torchao.dtypes.BlockSparseLayout", "generated/torchao.dtypes.CutlassInt4PackedLayout", "generated/torchao.dtypes.CutlassSemiSparseLayout", "generated/torchao.dtypes.Float8Layout", "generated/torchao.dtypes.Int4CPULayout", "generated/torchao.dtypes.Layout", "generated/torchao.dtypes.MarlinQQQLayout", "generated/torchao.dtypes.MarlinQQQTensor", "generated/torchao.dtypes.MarlinSparseLayout", "generated/torchao.dtypes.NF4Tensor", "generated/torchao.dtypes.PlainLayout", "generated/torchao.dtypes.SemiSparseLayout", "generated/torchao.dtypes.TensorCoreTiledLayout", "generated/torchao.dtypes.UintxLayout", "generated/torchao.dtypes.to_affine_quantized_floatx", "generated/torchao.dtypes.to_affine_quantized_floatx_static", "generated/torchao.dtypes.to_affine_quantized_fpx", "generated/torchao.dtypes.to_affine_quantized_intx", "generated/torchao.dtypes.to_affine_quantized_intx_static", "generated/torchao.dtypes.to_marlinqqq_quantized_intx", "generated/torchao.dtypes.to_nf4", "generated/torchao.float8.CastConfig", "generated/torchao.float8.Float8LinearConfig", "generated/torchao.float8.ScalingGranularity", "generated/torchao.float8.ScalingType", "generated/torchao.float8.convert_to_float8_training", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp", "generated/torchao.quantization.FPXWeightOnlyConfig", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig", "generated/torchao.quantization.Float8WeightOnlyConfig", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig", "generated/torchao.quantization.Int4WeightOnlyConfig", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig", "generated/torchao.quantization.Int8WeightOnlyConfig", "generated/torchao.quantization.MappingType", "generated/torchao.quantization.TorchAODType", "generated/torchao.quantization.UIntXWeightOnlyConfig", "generated/torchao.quantization.ZeroPointDomain", "generated/torchao.quantization.autoquant", "generated/torchao.quantization.choose_qparams_affine", "generated/torchao.quantization.choose_qparams_affine_floatx", "generated/torchao.quantization.choose_qparams_affine_with_min_max", "generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq", "generated/torchao.quantization.dequantize_affine", "generated/torchao.quantization.dequantize_affine_floatx", "generated/torchao.quantization.fake_quantize_affine", "generated/torchao.quantization.fake_quantize_affine_cachemask", "generated/torchao.quantization.int_scaled_matmul", "generated/torchao.quantization.qat.ComposableQATQuantizer", "generated/torchao.quantization.qat.FakeQuantizeConfig", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.initialize_fake_quantizers", "generated/torchao.quantization.quantize_", "generated/torchao.quantization.quantize_affine", "generated/torchao.quantization.quantize_affine_floatx", "generated/torchao.quantization.safe_int_mm", "generated/torchao.quantization.smooth_fq_linear_to_inference", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear", "generated/torchao.quantization.to_linear_activation_quantized", "generated/torchao.sparsity.PerChannelNormObserver", "generated/torchao.sparsity.WandaSparsifier", "generated/torchao.sparsity.apply_fake_sparsity", "generated/torchao.sparsity.semi_sparse_weight", "generated/torchao.sparsity.sparsify_", "index", "performant_kernels", "pretraining", "quantization", "quick_start", "serialization", "sg_execution_times", "sparsity", "static_quantization", "subclass_advanced", "subclass_basic", "torchao_vllm_integration", "tutorials/index", "tutorials/sg_execution_times", "tutorials/template_tutorial"], "filenames": ["api_ref_dtypes.rst", "api_ref_float8.rst", "api_ref_intro.rst", "api_ref_kernel.rst", "api_ref_quantization.rst", "api_ref_sparsity.rst", "contributor_guide.rst", "dtypes.rst", "generated/torchao.dtypes.AffineQuantizedTensor.rst", "generated/torchao.dtypes.BlockSparseLayout.rst", "generated/torchao.dtypes.CutlassInt4PackedLayout.rst", "generated/torchao.dtypes.CutlassSemiSparseLayout.rst", "generated/torchao.dtypes.Float8Layout.rst", "generated/torchao.dtypes.Int4CPULayout.rst", "generated/torchao.dtypes.Layout.rst", "generated/torchao.dtypes.MarlinQQQLayout.rst", "generated/torchao.dtypes.MarlinQQQTensor.rst", "generated/torchao.dtypes.MarlinSparseLayout.rst", "generated/torchao.dtypes.NF4Tensor.rst", "generated/torchao.dtypes.PlainLayout.rst", "generated/torchao.dtypes.SemiSparseLayout.rst", "generated/torchao.dtypes.TensorCoreTiledLayout.rst", "generated/torchao.dtypes.UintxLayout.rst", "generated/torchao.dtypes.to_affine_quantized_floatx.rst", "generated/torchao.dtypes.to_affine_quantized_floatx_static.rst", "generated/torchao.dtypes.to_affine_quantized_fpx.rst", "generated/torchao.dtypes.to_affine_quantized_intx.rst", "generated/torchao.dtypes.to_affine_quantized_intx_static.rst", "generated/torchao.dtypes.to_marlinqqq_quantized_intx.rst", "generated/torchao.dtypes.to_nf4.rst", "generated/torchao.float8.CastConfig.rst", "generated/torchao.float8.Float8LinearConfig.rst", "generated/torchao.float8.ScalingGranularity.rst", "generated/torchao.float8.ScalingType.rst", "generated/torchao.float8.convert_to_float8_training.rst", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp.rst", "generated/torchao.quantization.FPXWeightOnlyConfig.rst", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8WeightOnlyConfig.rst", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.rst", "generated/torchao.quantization.Int4WeightOnlyConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.rst", "generated/torchao.quantization.Int8WeightOnlyConfig.rst", "generated/torchao.quantization.MappingType.rst", "generated/torchao.quantization.TorchAODType.rst", "generated/torchao.quantization.UIntXWeightOnlyConfig.rst", "generated/torchao.quantization.ZeroPointDomain.rst", "generated/torchao.quantization.autoquant.rst", "generated/torchao.quantization.choose_qparams_affine.rst", "generated/torchao.quantization.choose_qparams_affine_floatx.rst", "generated/torchao.quantization.choose_qparams_affine_with_min_max.rst", "generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq.rst", "generated/torchao.quantization.dequantize_affine.rst", "generated/torchao.quantization.dequantize_affine_floatx.rst", "generated/torchao.quantization.fake_quantize_affine.rst", "generated/torchao.quantization.fake_quantize_affine_cachemask.rst", "generated/torchao.quantization.int_scaled_matmul.rst", "generated/torchao.quantization.qat.ComposableQATQuantizer.rst", "generated/torchao.quantization.qat.FakeQuantizeConfig.rst", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer.rst", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.rst", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.rst", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.initialize_fake_quantizers.rst", "generated/torchao.quantization.quantize_.rst", "generated/torchao.quantization.quantize_affine.rst", "generated/torchao.quantization.quantize_affine_floatx.rst", "generated/torchao.quantization.safe_int_mm.rst", "generated/torchao.quantization.smooth_fq_linear_to_inference.rst", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear.rst", "generated/torchao.quantization.to_linear_activation_quantized.rst", "generated/torchao.sparsity.PerChannelNormObserver.rst", "generated/torchao.sparsity.WandaSparsifier.rst", "generated/torchao.sparsity.apply_fake_sparsity.rst", "generated/torchao.sparsity.semi_sparse_weight.rst", "generated/torchao.sparsity.sparsify_.rst", "index.rst", "performant_kernels.rst", "pretraining.rst", "quantization.rst", "quick_start.rst", "serialization.rst", "sg_execution_times.rst", "sparsity.rst", "static_quantization.rst", "subclass_advanced.rst", "subclass_basic.rst", "torchao_vllm_integration.md", "tutorials/index.rst", "tutorials/sg_execution_times.rst", "tutorials/template_tutorial.rst"], "titles": ["torchao.dtypes", "torchao.float8", "<code class=\"docutils literal notranslate\"><span class=\"pre\">torchao</span></code> API Reference", "torchao.kernel", "torchao.quantization", "torchao.sparsity", "Contributor Guide", "Dtypes", "AffineQuantizedTensor", "BlockSparseLayout", "CutlassInt4PackedLayout", "CutlassSemiSparseLayout", "Float8Layout", "Int4CPULayout", "Layout", "MarlinQQQLayout", "MarlinQQQTensor", "MarlinSparseLayout", "NF4Tensor", "PlainLayout", "SemiSparseLayout", "TensorCoreTiledLayout", "UintxLayout", "to_affine_quantized_floatx", "to_affine_quantized_floatx_static", "to_affine_quantized_fpx", "to_affine_quantized_intx", "to_affine_quantized_intx_static", "to_marlinqqq_quantized_intx", "to_nf4", "CastConfig", "Float8LinearConfig", "ScalingGranularity", "ScalingType", "convert_to_float8_training", "precompute_float8_dynamic_scale_for_fsdp", "FPXWeightOnlyConfig", "Float8DynamicActivationFloat8WeightConfig", "Float8StaticActivationFloat8WeightConfig", "Float8WeightOnlyConfig", "GemliteUIntXWeightOnlyConfig", "Int4WeightOnlyConfig", "Int8DynamicActivationInt4WeightConfig", "Int8DynamicActivationInt8WeightConfig", "Int8WeightOnlyConfig", "MappingType", "TorchAODType", "UIntXWeightOnlyConfig", "ZeroPointDomain", "autoquant", "choose_qparams_affine", "choose_qparams_affine_floatx", "choose_qparams_affine_with_min_max", "choose_qparams_and_quantize_affine_hqq", "dequantize_affine", "dequantize_affine_floatx", "fake_quantize_affine", "fake_quantize_affine_cachemask", "int_scaled_matmul", "ComposableQATQuantizer", "FakeQuantizeConfig", "FromIntXQuantizationAwareTrainingConfig", "Int4WeightOnlyEmbeddingQATQuantizer", "Int4WeightOnlyQATQuantizer", "Int8DynActInt4WeightQATQuantizer", "IntXQuantizationAwareTrainingConfig", "initialize_fake_quantizers", "quantize", "quantize_affine", "quantize_affine_floatx", "safe_int_mm", "smooth_fq_linear_to_inference", "swap_linear_with_smooth_fq_linear", "to_linear_activation_quantized", "PerChannelNormObserver", "WandaSparsifier", "apply_fake_sparsity", "semi_sparse_weight", "sparsify", "Welcome to the torchao Documentation", "Performant Kernels", "Pretraining with float8", "Quantization Overview", "Quick Start Guide", "Serialization", "Computation times", "Sparsity Overview", "Static Quantization", "Writing Your Own Quantized Tensor (advanced)", "Writing Your Own Quantized Tensor", "Integration with VLLM: Architecture and Usage Guide", "&lt;no title&gt;", "Computation times", "Template Tutorial"], "terms": {"thi": [2, 6, 8, 17, 18, 19, 20, 22, 35, 36, 40, 41, 42, 45, 49, 50, 52, 54, 56, 57, 60, 61, 67, 68, 74, 75, 76, 78, 81, 82, 83, 84, 86, 87, 89, 90, 93], "section": [2, 6, 82, 86, 90], "introduc": 2, "dive": 2, "detail": [2, 6, 36, 49, 81, 82, 83, 86, 87, 89], "how": [2, 6, 8, 14, 22, 41, 45, 50, 60, 61, 68, 81, 83, 84, 86, 87, 89, 90], "integr": [2, 6, 79, 81, 84, 86, 89], "pytorch": [2, 6, 8, 13, 16, 46, 60, 79, 81, 83, 86, 89, 90, 93], "optim": [2, 6, 17, 35, 49, 53, 67, 79, 81, 86, 89], "your": [2, 6, 79, 81, 82, 83, 86], "machin": 2, "learn": [2, 41, 60, 83, 86, 93], "model": [2, 35, 40, 42, 49, 59, 61, 62, 63, 64, 66, 67, 71, 72, 75, 76, 78, 83, 86, 87, 89], "dtype": [2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 46, 47, 49, 50, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 68, 78, 79, 81, 83, 84, 87, 89, 90], "quantiz": [2, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 20, 21, 23, 26, 28, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 78, 79, 81, 84, 86], "sparsiti": [2, 11, 17, 20, 74, 75, 76, 77, 78, 79, 82, 84], "tba": [3, 7, 80], "For": [6, 8, 36, 60, 82, 83, 84, 86, 87, 89, 90], "new": [6, 8, 81, 82, 87, 89], "case": [6, 49, 70, 86, 89, 90], "exampl": [6, 8, 35, 45, 49, 59, 60, 61, 66, 67, 75, 78, 82, 84, 85, 86, 87, 89, 91, 92, 93], "train": [6, 31, 56, 57, 59, 60, 79, 81, 86, 89], "like": [6, 14, 49, 81, 82, 83, 84, 86, 89, 90], "fp4": 6, "s": [6, 8, 45, 49, 50, 54, 56, 68, 69, 81, 82, 83, 86, 87, 89], "fine": [6, 40, 41, 42, 47, 86], "start": [6, 32, 33, 45, 46, 48, 49, 81, 82, 86, 87, 89, 90], "prototyp": [6, 60, 66, 82], "folder": 6, "you": [6, 60, 75, 81, 82, 83, 84, 86, 89, 90, 93], "could": [6, 82, 89], "also": [6, 49, 60, 67, 82, 83, 84, 86, 87, 89, 90], "take": [6, 18, 67, 74, 78, 82, 86], "look": [6, 8, 81, 82, 86], "affinequantizedtensor": [6, 16, 24, 25, 27, 82, 83, 84, 87, 89], "what": [6, 8, 16, 49, 81, 82, 83, 86, 87, 90, 93], "want": [6, 67, 78, 82, 84, 86, 89, 90], "do": [6, 46, 49, 58, 67, 82, 86, 87, 89, 90], "mostli": [6, 52], "e": [6, 8, 36, 45, 49, 50, 54, 56, 59, 60, 67, 68, 69, 81, 82, 84, 87, 89], "g": [6, 8, 36, 45, 49, 50, 54, 56, 59, 60, 67, 68, 82, 84, 87, 89], "int3": 6, "exact": 6, "same": [6, 8, 37, 50, 52, 54, 56, 57, 68, 70, 78, 81, 82, 86, 87, 89], "affin": [6, 8, 10, 11, 12, 13, 17, 20, 21, 26, 54, 56, 68, 82], "pleas": [6, 8, 16, 36, 41, 79, 82, 86, 87, 89, 90], "feel": [6, 82, 86, 89, 90], "free": [6, 82, 89], "open": [6, 82, 86], "an": [6, 8, 21, 26, 27, 49, 57, 60, 75, 79, 82, 86, 87, 89], "issu": [6, 82, 83, 89], "have": [6, 40, 41, 45, 49, 62, 63, 64, 68, 75, 82, 86, 87, 89, 90], "question": [6, 82, 84, 86, 89], "specif": [6, 14, 17, 19, 20, 75, 82, 83, 84, 86], "more": [6, 8, 36, 40, 41, 42, 47, 49, 57, 81, 82, 83, 86, 87, 89, 90], "refer": [6, 8, 81, 86, 87, 89, 90], "our": [6, 18, 81, 83, 86, 87, 89], "overview": [6, 79, 83, 90], "page": [6, 83], "To": [6, 8, 16, 49, 81, 82, 83, 84, 86, 87, 90], "contribut": [6, 83, 86], "exist": [6, 46, 81, 82, 86, 87, 89], "code": [6, 41, 81, 82, 83, 86, 87, 89, 91, 93], "base": [6, 14, 19, 45, 66, 75, 82, 83, 86, 89, 90], "make": [6, 82, 89, 90], "trainabl": [6, 82, 89], "add": [6, 19, 89, 93], "parallel": [6, 81, 89, 90], "etc": [6, 82], "affine_quantized_tensor": [6, 84], "py": [6, 8, 16, 85, 92, 93], "api": [6, 49, 65, 82, 83, 86, 87, 89], "quant_api": [6, 67, 84, 87], "primit": [6, 8, 16, 89], "op": [6, 8, 16, 41, 49, 56, 57, 67, 86, 89, 90], "slight": [6, 86], "variat": [6, 82], "quant_primit": [6, 8, 16, 87], "autotun": [6, 83, 87], "cpu": [6, 8, 13, 84, 86, 87, 90], "cuda": [6, 8, 53, 67, 81, 83, 84, 86, 87, 89], "mp": 6, "csrc": 6, "mayb": [6, 30], "well": [6, 14, 49, 82, 86], "spars": [6, 9, 17, 20, 75, 82, 86], "marlin": [6, 15, 16, 17, 28], "aqt": 6, "621": 6, "we": [6, 8, 18, 45, 47, 49, 50, 52, 54, 56, 60, 67, 68, 78, 81, 82, 83, 84, 86, 87, 90], "ar": [6, 8, 12, 20, 22, 34, 36, 37, 40, 41, 49, 50, 54, 56, 59, 67, 68, 70, 75, 81, 82, 83, 84, 86, 87, 90], "still": [6, 82, 86], "decid": [6, 82, 86, 87], "split": 6, "can": [6, 21, 37, 40, 45, 49, 59, 60, 67, 68, 81, 82, 83, 84, 86, 87, 89, 90], "implement": [6, 31, 84, 86, 87], "regist": [6, 74, 89], "mai": [6, 52, 60, 82, 84, 87], "need": [6, 37, 74, 75, 82, 83, 84, 86, 89, 90], "defin": [6, 14, 22, 32, 36, 74, 75, 86, 87, 89, 90], "own": [6, 79, 81, 86, 87], "through": [6, 52, 82, 83, 87, 89, 90, 93], "int4": [6, 10, 13, 42, 45, 60, 62, 63, 64, 67, 78, 83, 84, 90], "access": 6, "my_custom_op": 6, "devic": [6, 8, 53, 67, 70, 81, 83, 84, 87, 89, 90], "check": [6, 8, 16, 82, 83, 84, 89], "condit": [6, 82], "__torch_function__": [6, 82, 89], "__torch_dispatch__": [6, 89], "target": [6, 37, 38, 39, 41, 50, 75, 86], "oper": [6, 8, 12, 14, 17, 52], "bfloat16": [6, 18, 56, 63, 68, 81, 82, 83, 84, 86, 87, 90], "activ": [6, 37, 38, 40, 42, 43, 49, 60, 64, 71, 75, 79, 86, 87, 90], "uint4": [6, 41, 82, 83], "weight": [6, 17, 18, 35, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 60, 62, 63, 64, 67, 75, 78, 79, 81, 83, 84, 86, 87, 89, 90], "found": [6, 82, 83, 86, 87, 89], "here": [6, 8, 68, 82, 84, 87, 89, 90], "allow": [6, 86, 89], "peopl": [6, 82, 84, 90], "linear": [6, 17, 31, 34, 37, 39, 41, 42, 43, 44, 47, 49, 59, 63, 64, 67, 72, 76, 78, 81, 82, 83, 84, 86, 87, 89], "two": [6, 16, 20, 37, 82, 86, 89], "dispatch_condit": [6, 82], "impl": [6, 8, 82], "actual": [6, 39, 82, 87, 89, 90], "bia": [6, 82, 83, 84, 87, 89, 90], "run": [6, 35, 49, 67, 71, 74, 81, 82, 86, 89, 93], "both": [6, 8, 37, 82, 86, 87, 89], "input_tensor": [6, 18, 82, 90], "weight_tensor": [6, 82, 90], "argument": [6, 8, 21, 49, 54, 67, 81, 82], "register_aqt_quantized_linear_dispatch": 6, "show": [6, 68, 81, 82, 86, 90], "work": [6, 20, 40, 81, 84, 86, 89, 90], "sometim": [6, 86], "ha": [6, 8, 82, 86, 89, 90], "pack": [6, 8, 10, 21, 22, 36, 40, 47, 82], "order": [6, 49, 59, 82, 86, 89], "yield": [6, 86], "And": [6, 18, 37, 82, 89], "abstract": [6, 82], "see": [6, 8, 16, 36, 81, 82, 83, 84, 86, 87, 89, 90], "full": [6, 87, 93], "after": [6, 35, 49, 82, 84, 86], "wrap": [6, 49, 89], "factori": 6, "convert": [6, 8, 16, 18, 23, 26, 28, 29, 31, 59, 61, 62, 67, 69, 78, 81, 82, 86], "from": [6, 8, 18, 19, 24, 25, 27, 36, 42, 52, 56, 61, 67, 68, 78, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93], "float": [6, 8, 16, 18, 26, 28, 29, 36, 41, 45, 48, 49, 50, 52, 53, 54, 56, 57, 60, 68, 69, 72, 75, 82, 84, 89], "point": [6, 8, 16, 28, 36, 41, 45, 48, 54, 56, 60, 66, 69, 81, 82, 83, 84, 86, 87, 89], "my": [6, 86], "to_my_dtyp": 6, "mydtypetensor": 6, "from_float": [6, 87, 89], "level": [6, 75, 82, 86, 89], "reus": [6, 82, 89], "quantize_": [6, 61, 67, 78, 82, 83, 84, 87], "appli": [6, 8, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 59, 67, 78, 82, 83, 86, 90], "convers": [6, 8, 34, 82], "filter": [6, 34, 49, 81, 87], "choos": [6, 82, 86, 89], "which": [6, 16, 22, 49, 81, 82, 83, 84, 86, 87, 90], "modul": [6, 31, 32, 33, 34, 35, 45, 46, 48, 49, 59, 61, 62, 66, 67, 71, 72, 74, 75, 78, 81, 83, 84, 87], "should": [6, 8, 35, 40, 54, 56, 61, 74, 75, 81, 82, 86, 90], "algorithm": [6, 41, 47, 86], "onli": [6, 13, 34, 37, 39, 40, 41, 42, 44, 47, 78, 81, 83, 84, 86, 89, 90], "dynam": [6, 30, 31, 35, 37, 40, 42, 43, 60, 64, 78, 87, 89], "quant": [6, 8, 16, 36, 82, 90], "static": [6, 8, 14, 18, 24, 27, 31, 38, 52, 60, 79], "type": [6, 8, 17, 18, 22, 31, 32, 33, 34, 37, 38, 39, 41, 42, 45, 46, 48, 49, 53, 58, 60, 68, 70, 79, 82, 84, 86, 89, 90], "note": [6, 57, 59, 75, 82, 83, 86, 89, 90], "2": [6, 8, 11, 13, 17, 20, 41, 45, 49, 57, 60, 68, 76, 78, 81, 82, 83, 86, 87, 89, 93], "4": [6, 11, 17, 20, 29, 40, 53, 76, 78, 82, 83, 84, 86, 89], "below": [6, 81, 82, 86, 89, 90, 93], "follow": [6, 41, 60, 81, 82, 83, 86, 87, 89], "util": [6, 40, 81, 82, 83, 84, 89, 90], "import": [6, 61, 67, 78, 83, 84, 86, 87, 89, 90, 93], "unwrap_tensor_subclass": [6, 83], "m_unwrap": 6, "m": [6, 67, 69, 78, 81, 83, 84, 87, 89], "In": [6, 81, 82, 83, 86, 87, 89], "compat": [6, 17, 60, 83], "aim": [6, 82, 86], "fullgraph": [6, 83], "true": [6, 8, 26, 31, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 50, 52, 53, 60, 67, 71, 78, 81, 83, 84, 87, 89, 90], "first": [6, 18, 49, 58, 75, 82, 87, 89, 90], "remov": [6, 50, 75, 81, 86, 90], "ani": [6, 19, 49, 62, 66, 73, 75, 82, 86, 89], "unnecessari": 6, "graph": 6, "break": 6, "torch_log": 6, "output_cod": 6, "when": [6, 8, 19, 50, 54, 56, 68, 81, 82, 86, 87, 90], "script": [6, 83, 87, 89, 93], "inductor": [6, 49], "python": [6, 82, 86, 91, 93], "mode": [6, 40, 41, 49, 83, 87], "max": [6, 45, 82, 83, 87, 89], "checkout": [6, 8, 16, 79, 82], "doc": [6, 81, 82, 89], "huggingfac": 6, "transform": [6, 8, 82, 87], "deseri": [6, 82], "save_pretrain": 6, "push_to_hub": [6, 90], "from_pretrain": [6, 90], "http": [6, 8, 16, 36, 49, 75, 83, 86], "co": 6, "main": [6, 8, 16, 41, 82, 83, 86, 87, 89], "en": [6, 49], "anoth": [6, 82, 86, 89], "diffus": 6, "github": [6, 8, 16, 36, 83], "com": [6, 8, 16, 36], "sayakpaul": 6, "blob": [6, 8, 16], "infer": [6, 8, 71, 79, 82, 83, 84, 86, 87, 89], "serialization_and_load": 6, "md": 6, "The": [6, 8, 9, 14, 17, 22, 34, 36, 37, 38, 39, 49, 58, 67, 70, 71, 72, 75, 81, 82, 83, 84, 86, 89, 90], "abov": [6, 45, 82, 84, 86, 87, 89], "just": [6, 45, 60, 82, 84, 86, 89], "talk": [6, 82], "about": [6, 41, 82, 83, 84, 86], "basic": [6, 19, 83, 87, 89], "provid": [6, 14, 17, 20, 21, 49, 50, 59, 66, 81, 82, 86, 89, 90], "fsdp": [6, 82], "ll": [6, 45, 81, 82, 89], "put": [6, 78], "developer_api_guid": 6, "cover": [6, 82, 93], "executorch": [6, 42, 67], "torchchat": 6, "todo": [6, 82], "qat": [6, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66], "suit": 6, "out": [6, 20, 45, 49, 75, 81, 82, 83, 86, 89], "differ": [6, 14, 41, 52, 59, 68, 70, 81, 82, 83, 84, 86, 89, 90], "system": 6, "dtensor": [6, 89], "recommend": [6, 37, 38, 39, 40, 41, 42, 47, 49, 81], "copi": [6, 8, 75, 83, 84, 86, 87, 89], "past": [6, 86], "adapt": [6, 87], "now": [6, 36, 42, 50, 81, 82, 83, 86, 87, 89], "befor": [6, 67, 82, 84, 86, 87, 89], "some": [6, 49, 67, 75, 82, 86, 87, 89], "singl": [6, 30, 35, 37, 49, 52, 81, 83, 86], "comput": [6, 17, 21, 35, 39, 74, 75, 86, 87, 89], "intens": 6, "memori": [6, 8, 57, 81, 83, 86, 89], "input": [6, 8, 17, 18, 20, 31, 34, 35, 49, 50, 52, 54, 56, 57, 58, 66, 67, 68, 70, 75, 78, 81, 82, 87, 89], "dimens": [6, 8, 22, 47, 50, 54, 56, 58, 68, 81, 89, 90], "get": [6, 18, 81, 82, 86, 90], "sens": [6, 82, 89], "speedup": [6, 41, 81, 82, 83, 86], "d": [6, 82], "creat": [6, 8, 24, 25, 27, 81, 82, 86, 89], "file": [6, 81, 85, 89, 90, 92], "benchmark_aq": 6, "shape": [6, 8, 16, 49, 58, 70, 83, 87, 89, 90], "A": [6, 8, 22, 49, 52, 57, 74, 86, 89, 90], "quick": [6, 79], "wai": [6, 8, 49, 81, 82, 86, 87, 89], "relev": [6, 41, 82, 93], "chang": [6, 67, 81, 82, 83, 84, 86, 87, 89], "interest": [6, 82, 86, 89], "tutori": [6, 8, 81, 82, 85, 86, 87, 89, 90, 91, 92], "print_op_and_shap": 6, "output": [6, 31, 49, 50, 54, 56, 68, 81, 82, 86, 93], "torch_func": 6, "built": [6, 81, 89], "k": [6, 70, 83, 84, 87, 89], "n": [6, 83, 84, 87, 89], "10": [6, 45, 68, 81, 87], "method": [6, 14, 17, 20, 21, 49, 67, 75, 86, 87, 89], "_c": 6, "tensorbas": 6, "object": [6, 22, 61, 67, 78, 89], "arg": [6, 8, 62, 75, 89, 90], "0": [6, 8, 49, 60, 68, 72, 75, 81, 83, 84, 85, 86, 87, 89, 90, 92, 93], "size": [6, 8, 9, 16, 18, 40, 41, 42, 47, 50, 54, 56, 60, 68, 81, 83, 84, 86, 87, 89, 90], "all": [6, 35, 45, 49, 52, 62, 66, 74, 75, 76, 82, 83, 84, 85, 86, 87, 89, 90, 91], "under": 6, "benchmark_your_kernel": 6, "helper": 6, "right": [6, 82, 86], "1": [6, 17, 22, 32, 33, 41, 45, 46, 47, 48, 49, 53, 68, 75, 81, 82, 83, 84, 85, 86, 87, 89, 92, 93], "either": [6, 8, 37, 56, 75, 86], "one": [6, 37, 49, 52, 74, 81, 82, 86, 89, 90], "probabl": 6, "keep": [6, 17, 75], "futur": [6, 36, 87, 90], "llama": [6, 90], "llama2": 6, "llama3": [6, 81], "sam": 6, "alreadi": [6, 8, 49, 89], "modifi": [6, 34, 67, 75, 81, 82, 86, 89], "friendli": [6, 82], "compar": [6, 41, 57, 75, 81, 82], "techniqu": [6, 84, 86, 87, 89, 90], "repres": [6, 8, 9, 12, 14, 25, 31, 60, 68, 75, 82, 84, 89], "bound": [6, 86, 90], "help": [6, 81, 82, 90], "option": [6, 8, 12, 16, 23, 26, 27, 28, 30, 31, 34, 37, 38, 40, 41, 43, 44, 49, 50, 52, 54, 56, 57, 60, 63, 65, 67, 68, 71, 72, 73, 75, 78, 81, 83, 90], "each": [6, 18, 49, 60, 71, 74, 82, 86, 87, 89, 90], "understand": [6, 81], "profil": 6, "profile_path": 6, "chrome": 6, "trace": [6, 82], "let": [6, 45, 68, 82, 83, 86, 87, 89], "know": [6, 49, 61, 89], "class": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 30, 31, 32, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 59, 60, 61, 62, 63, 64, 65, 74, 75, 82, 83, 84, 87, 89], "torchao": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 83, 84, 86, 87, 89], "tensor_impl": [8, 16, 82, 87], "aqttensorimpl": [8, 16], "block_siz": [8, 14, 16, 18, 23, 24, 26, 27, 28, 29, 50, 52, 54, 56, 57, 68, 83, 87], "tupl": [8, 16, 18, 23, 24, 26, 27, 28, 37, 38, 50, 52, 53, 54, 56, 57, 66, 68, 75, 89, 90], "int": [8, 9, 16, 18, 21, 22, 23, 24, 26, 27, 28, 29, 36, 40, 41, 42, 44, 47, 50, 51, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 67, 68, 69, 75, 83, 87, 89, 90], "quant_min": [8, 16, 26, 27, 28, 45, 50, 52, 54, 56, 57, 68, 82, 83, 89], "union": [8, 16, 31, 37, 38, 50, 54, 56, 57, 60, 67, 68], "none": [8, 12, 16, 23, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 44, 45, 46, 48, 49, 50, 52, 54, 56, 57, 60, 65, 66, 67, 68, 71, 72, 73, 75, 78, 87, 89, 90], "quant_max": [8, 16, 26, 27, 28, 45, 50, 52, 54, 56, 57, 68, 82, 83, 89], "zero_point_domain": [8, 16, 26, 27, 28, 41, 50, 52, 56, 57, 60], "zeropointdomain": [8, 16, 26, 27, 28, 41, 50, 52, 56, 57, 60], "stride": [8, 16, 82, 89], "sourc": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 91, 93], "tensor": [8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 38, 39, 41, 42, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 68, 69, 70, 73, 75, 79, 81, 83, 84, 86, 87, 93], "subclass": [8, 16, 34, 49, 74, 78, 83, 84, 86], "mean": [8, 18, 45, 50, 54, 56, 68, 69, 81, 82, 83, 86], "quantized_tensor": 8, "float_tensor": [8, 89], "scale": [8, 14, 17, 24, 27, 32, 35, 38, 45, 48, 50, 52, 54, 55, 56, 57, 58, 60, 66, 68, 69, 71, 72, 82, 86, 87, 89, 90], "zero_point": [8, 14, 27, 41, 48, 50, 52, 54, 56, 57, 68, 82, 86, 87, 89], "happen": [8, 16, 49, 82, 89], "dure": [8, 16, 49, 54, 56, 60, 72, 81, 86, 87, 89], "choose_qparam": [8, 82], "dequant": [8, 16, 18, 41, 54, 82, 89, 90], "ao": [8, 16, 86, 90], "three": [8, 49, 75, 78, 82], "choose_qparams_affin": [8, 41, 52, 82], "quantize_affin": [8, 41, 56, 57, 82], "qand": 8, "dequantize_affin": [8, 41, 56, 57], "extern": 8, "regardless": 8, "intern": [8, 21], "represent": [8, 14, 25, 41, 82, 86, 90], "orient": 8, "field": [8, 60], "serv": [8, 14, 89], "gener": [8, 56, 57, 82, 83, 86, 87, 89, 90, 91, 93], "storag": [8, 17, 82, 86], "data": [8, 9, 14, 17, 22, 37, 38, 39, 41, 52, 79, 82, 84, 86, 87, 89, 90], "store": [8, 17, 18, 22, 74, 82, 86, 90], "plain": [8, 90], "int_data": [8, 89], "format": [8, 17, 18, 36, 40, 69, 82, 86], "depend": [8, 40, 49, 84, 86, 89], "kernel": [8, 10, 11, 13, 17, 21, 36, 40, 41, 67, 83, 86], "granular": [8, 32, 37, 38, 40, 41, 42, 47, 50, 54, 56, 60, 68, 81, 82, 87, 90], "element": [8, 20, 22, 49, 50, 54, 56, 68, 86], "share": [8, 50, 54, 56, 68, 86], "qparam": [8, 50, 54, 56, 68], "us": [8, 12, 13, 14, 17, 18, 19, 22, 24, 27, 37, 38, 41, 42, 45, 47, 49, 50, 52, 54, 56, 59, 60, 61, 68, 75, 79, 81, 82, 83, 84, 86, 87, 89, 90], "per": [8, 39, 41, 42, 43, 44, 47, 50, 54, 56, 60, 62, 63, 64, 68, 75, 81, 82, 83, 86, 87], "torch": [8, 17, 18, 22, 24, 31, 34, 37, 38, 39, 41, 47, 49, 50, 53, 54, 55, 56, 58, 60, 62, 63, 64, 67, 68, 70, 71, 72, 78, 81, 82, 83, 84, 86, 87, 89, 90, 93], "origin": [8, 18, 39, 56, 61, 68, 75, 82, 83, 84, 86], "high": [8, 23, 24, 25, 26, 27, 69, 81, 82, 86, 87, 89], "precis": [8, 23, 24, 25, 26, 27, 39, 63, 64, 69, 82, 87, 89], "minimum": [8, 49, 50, 54, 56, 68], "valu": [8, 18, 31, 32, 33, 37, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 54, 56, 57, 68, 71, 75, 82, 86, 87, 89], "specifi": [8, 31, 34, 47, 56, 59, 67, 68, 75, 78, 81, 86], "deriv": [8, 52, 56, 68], "maximum": [8, 50, 54, 56, 68, 71], "domain": [8, 41, 48, 50, 54, 56, 60], "integ": [8, 26, 27, 40, 41, 45, 48, 50, 54, 56, 58, 60, 70, 87], "zero": [8, 20, 41, 50, 54, 56, 60, 66, 75, 86, 87], "ad": [8, 54, 56, 75, 86, 87, 89], "subtract": [8, 18, 56], "unquant": [8, 56], "default": [8, 9, 12, 19, 21, 22, 37, 38, 39, 40, 41, 47, 49, 50, 54, 56, 60, 67, 71, 72, 81, 89, 90], "float32": [8, 24, 54, 55, 56, 60, 62, 64, 68, 69, 84, 86, 87, 89], "given": [8, 16, 29, 81, 86, 90], "return": [8, 16, 17, 18, 34, 49, 57, 58, 60, 67, 70, 71, 72, 78, 81, 82, 83, 84, 87, 89, 90], "classmethod": [8, 16, 87, 89, 90], "from_hp_to_floatx": 8, "input_float": [8, 16, 23, 24, 25, 26, 27, 28, 73], "target_dtyp": [8, 23, 24, 26, 27, 30, 31, 50, 52, 82, 87], "_layout": [8, 16, 23, 24, 25, 26, 27, 28, 82, 83, 87], "layout": [8, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 41, 42, 43, 78, 86], "scale_dtyp": [8, 23, 24, 26, 50, 52, 87], "float8": [8, 11, 12, 23, 24, 30, 31, 32, 33, 34, 35, 37, 38, 39, 79, 82, 87], "from_hp_to_floatx_stat": 8, "paramet": [8, 14, 17, 18, 24, 27, 34, 35, 37, 38, 39, 40, 41, 42, 45, 47, 49, 50, 54, 56, 58, 60, 67, 68, 70, 71, 72, 75, 78, 81, 82, 84, 86, 89, 90], "from_hp_to_fpx": 8, "floatx": [8, 25, 82], "ebit": [8, 25, 36, 51, 55, 69], "mbit": [8, 25, 36, 51, 55, 69], "support": [8, 25, 37, 42, 60, 78, 81, 83, 84, 86, 89], "float1": [8, 25], "float7": [8, 25], "from_hp_to_intx": [8, 16], "mapping_typ": [8, 26, 42, 50, 52, 60], "mappingtyp": [8, 26, 42, 43, 50, 52, 60, 87], "ep": [8, 26, 50, 52, 60, 87], "zero_point_dtyp": [8, 26, 50, 52, 87], "preserve_zero": [8, 26, 41, 50, 52], "bool": [8, 26, 31, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 50, 52, 53, 60, 64, 67, 71, 78, 87], "plainlayout": [8, 26, 27, 42, 43, 87], "use_hqq": [8, 26, 41, 47, 90], "fals": [8, 26, 31, 41, 43, 47, 49, 53, 60, 64, 71, 75, 81, 82, 83, 84, 87, 89, 90], "from_hp_to_intx_stat": 8, "kwarg": [8, 60, 62, 74, 75, 76, 89, 90], "perform": [8, 21, 35, 40, 49, 58, 62, 63, 64, 70, 71, 74, 81, 83, 86, 87, 89, 90], "self": [8, 82, 83, 84, 87, 89, 90], "If": [8, 12, 34, 37, 49, 58, 60, 70, 71, 75, 82, 83, 86, 89], "correct": [8, 17], "otherwis": [8, 59, 60, 82], "desir": [8, 49, 56, 87], "call": [8, 49, 56, 57, 74, 82, 83, 84, 86, 87, 89, 90], "non_block": 8, "memory_format": 8, "preserve_format": 8, "set": [8, 12, 37, 38, 39, 40, 41, 42, 47, 49, 52, 60, 67, 71, 75, 83, 86], "function": [8, 21, 34, 49, 53, 67, 74, 75, 76, 78, 81, 83, 84, 86, 87, 89, 90], "attempt": 8, "asynchron": 8, "respect": [8, 86], "host": [8, 90], "possibl": [8, 86], "behavior": [8, 14, 59, 90], "pin": 8, "pageabl": 8, "howev": [8, 86, 90], "caution": 8, "advis": [8, 82], "featur": [8, 89], "inform": [8, 86, 90], "good": [8, 83, 89], "usag": [8, 35, 49, 59, 60, 61, 79, 81], "pin_memori": 8, "even": [8, 81, 86], "match": [8, 54, 58, 86], "other": [8, 14, 75, 81, 84, 86, 89, 90, 93], "randn": [8, 81, 83, 84, 87, 89], "initi": [8, 66, 82, 84], "float64": 8, "5044": 8, "0005": 8, "3310": 8, "0584": 8, "cuda0": 8, "blocksiz": 9, "64": [9, 29, 41, 47, 53, 84, 87, 89, 90], "block": [9, 18, 75, 86], "matrix": [9, 12, 37, 38, 58, 70, 75, 83, 86], "variabl": [9, 12, 21, 22, 75, 86], "cutlass": [10, 11], "mm_config": [12, 37, 38], "float8mmconfig": [12, 37, 38], "configur": [12, 30, 31, 34, 37, 38, 39, 41, 42, 43, 44, 47, 67, 78, 81, 82, 83], "multipl": [12, 37, 38, 49, 58, 59, 70, 83, 86, 87, 89, 90], "involv": [12, 86], "tinygemm": [13, 41, 67, 82, 83], "_weight_int4pack_mm_for_cpu": [13, 41], "version": [13, 60, 81, 83, 89, 90], "least": 13, "6": [13, 60, 81, 82, 83, 86], "It": [14, 17, 19, 21, 35, 86, 89], "pre": [14, 17, 21, 83, 86], "process": [14, 17, 19, 21, 22, 49, 72, 82, 86, 93], "post": [14, 21, 89], "addit": [14, 19, 49, 57, 81, 86, 89], "design": [14, 17, 20, 90], "extend": [14, 82, 86], "conjunct": 14, "tensorimpl": 14, "custom": [14, 74, 79, 81, 82, 83, 86, 89, 90], "interact": [14, 82], "qqq": [15, 16, 28], "marlinqqq": 16, "inherit": [16, 19, 89, 90], "choose_qparams_and_quantize_affine_qqq": 16, "dequantize_affine_qqq": 16, "handl": [17, 20, 21, 49, 82], "pattern": [17, 20, 82, 90], "ensur": 17, "preprocess": [17, 20], "manag": 17, "pre_process": 17, "1\u00ba": 17, "transpos": [17, 82, 89], "sinc": [17, 74, 82, 84, 86, 87, 89], "layer": [17, 34, 37, 39, 41, 43, 44, 47, 49, 62, 63, 64, 71, 72, 75, 76, 81, 86, 87, 89, 90], "2\u00ba": 17, "inject": 17, "3\u00ba": 17, "again": [17, 18, 86], "becaus": [17, 81, 82, 84, 86, 89], "dim": [17, 87, 89, 90], "tensor_meta": 18, "subclasstensorarg": 18, "n_block": 18, "scaler_block_s": [18, 29], "quantized_scal": 18, "quantization_factor": 18, "scaler_mean": 18, "quantized_data": [18, 90], "nf4": 18, "qlora": 18, "convert_to_norm_float_weight": 18, "normal": [18, 29, 49, 86], "dequantize_scal": 18, "unpack": [18, 69, 82], "doubl": 18, "scaler": 18, "int8": [18, 42, 43, 44, 60, 64, 67, 78, 82, 89], "per_scaler_block": 18, "factor": [18, 58, 72, 81, 86], "inpt_weight": 18, "double_quantize_scal": 18, "achiev": [18, 81, 86, 87, 89], "calcul": [18, 35, 45, 50, 52, 71, 82, 86], "absmax": 18, "find": [18, 86], "posit": 18, "typic": [18, 19, 82, 84, 87, 90], "per_block": 18, "int16": 18, "n_scaler_block": 18, "get_original_weight": 18, "quantize_tensor_nearest": 18, "float16": [18, 53, 56, 68, 86], "nearest": 18, "round": [18, 45, 89], "up": [18, 67, 81, 82, 83, 86], "most": [19, 82, 86, 90], "doe": [19, 41, 82, 86, 89], "metadata": [19, 82, 89, 90], "step": [19, 35, 49, 81, 82, 86], "requir": [19, 21, 81, 82, 86, 89], "semi": [20, 78, 86], "structur": [20, 78, 83, 84, 86, 89], "matric": [20, 86], "where": [20, 45, 47, 52, 62, 63, 64, 69, 82, 86, 90], "everi": [20, 74, 86, 89], "four": 20, "prune": [20, 75], "conform": 20, "inner_k_til": [21, 41, 63, 83], "8": [21, 22, 40, 41, 45, 63, 81, 82, 83, 90], "core": [21, 46, 82, 87, 90], "tile": [21, 82], "fit": [21, 82, 84], "effici": [21, 83, 86, 87], "affect": [21, 86], "matmul": [21, 39, 82, 86, 89], "pack_dim": [22, 47], "uintx": [22, 47, 82], "smaller": [22, 40, 41, 42, 47, 83, 84], "bit": [22, 29, 36, 40, 47, 69, 89, 90], "width": [22, 40], "than": [22, 60, 81, 82, 86, 89], "standard": [22, 82, 90], "byte": [22, 36, 47], "uintxtensor": 22, "determin": [22, 50, 56, 81, 86, 90], "along": [22, 86, 90], "indic": [22, 48, 86], "last": [22, 81], "256": [29, 41, 62, 63, 64], "scaling_typ": [30, 31], "scalingtyp": [30, 31], "scaling_granular": [30, 31], "scalinggranular": [30, 31], "tensorwis": [30, 31], "cast": [30, 32, 56, 57], "cast_config_input": 31, "config": [31, 34, 49, 60, 67, 75, 78, 86, 87, 90], "castconfig": 31, "cast_config_input_for_grad_weight": 31, "cast_config_weight": 31, "cast_config_weight_for_grad_input": 31, "cast_config_grad_output": 31, "cast_config_grad_output_for_grad_weight": 31, "gemm_config_output": 31, "float8gemmconfig": 31, "use_fast_accum": 31, "gemm_config_grad_input": 31, "gemm_config_grad_weight": 31, "enable_fsdp_float8_all_gath": 31, "pad_inner_dim": 31, "emul": 31, "force_recompute_fp8_weight_in_bwd": 31, "round_scales_to_power_of_2": 31, "nn": [31, 34, 49, 59, 62, 67, 71, 72, 78, 81, 82, 83, 84, 86, 87, 89, 90], "from_recipe_nam": 31, "recipe_nam": [31, 81], "float8linearrecipenam": 31, "str": [31, 34, 40, 53, 60, 67, 72, 73, 75, 78, 81, 89, 90], "string": [31, 60, 75], "recip": [31, 74], "name": [32, 33, 45, 46, 48, 67, 72, 75, 78, 86, 89, 90], "qualnam": [32, 33, 45, 46, 48], "boundari": [32, 33, 45, 46, 48], "strategi": 32, "module_filter_fn": [34, 81], "callabl": [34, 49, 53, 67, 73, 78, 90], "float8linearconfig": 34, "swap": [34, 62, 81, 82, 86, 87], "float8linear": [34, 81], "pass": [34, 49, 52, 74, 82, 87, 89, 90], "instanc": [34, 67, 74, 78, 84, 89], "fqn": [34, 75, 78, 81, 87], "reduc": [35, 81, 86], "sum": 35, "backward": [35, 81, 86], "set_inductor_config": [36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49], "sub": [36, 47, 86], "expon": [36, 69], "mantissa": [36, 69], "fp6_e3_m2": 36, "fp6_e2_m3": 36, "fp6": 36, "llm": 36, "paper": [36, 86, 93], "arxiv": [36, 75, 86], "org": [36, 49, 75, 82, 83, 86], "ab": [36, 75, 86], "2401": 36, "14112": 36, "repo": 36, "usyd": 36, "fsalab": 36, "fp6_llm": 36, "renam": 36, "fpxtensorcoreaqttensorimpl": 36, "experiment": 36, "merg": 36, "to_affine_quantized_floatx": 36, "activation_dtyp": [37, 38], "float8_e4m3fn": [37, 38, 39, 82], "weight_dtyp": [37, 38, 39], "pertensor": [37, 38, 87], "perrow": [37, 38], "list": [37, 49, 54, 59, 72, 75, 82, 83, 89, 90], "symmetr": [37, 38, 39, 40, 42, 43, 44, 45, 50, 60, 89], "current": [37, 42, 67, 72, 75, 78, 81, 86, 89, 90], "fast": [37, 38, 86], "accumul": [37, 38], "adjust": [37, 38, 39, 40, 41, 42, 47, 49], "torchinductor": [37, 38, 39, 40, 41, 42, 47], "float8_e4m": 38, "channel": [39, 43, 44, 60, 62, 63, 64, 74, 87], "group_siz": [40, 41, 42, 44, 47, 53, 60, 62, 67, 83, 90], "128": [40, 41, 81, 87, 89, 90], "bit_width": 40, "packing_bitwidth": 40, "weight_onli": 40, "gemlit": 40, "triton": [40, 82], "its": [40, 86, 89, 90], "associ": [40, 87], "fp16": [40, 50], "asymmetr": [40, 41, 42, 45, 47, 50, 60, 82, 83, 87], "control": [40, 41, 42, 47, 75, 86, 90], "grain": [40, 41, 42, 47, 89], "32": [40, 41, 42, 60, 67, 78, 81, 83, 84, 87, 89], "impact": [40, 49, 81, 90], "hardwar": [40, 82, 86], "runtim": [40, 82], "tensorcoretiledlayout": [41, 82, 83], "group": [41, 42, 47, 60, 62, 63, 64, 82, 83], "tensor_core_til": [41, 82], "int4mm": [41, 83], "aten": [41, 82, 89, 90], "_weight_int4pack_mm": [41, 82], "tradit": 41, "instead": [41, 52, 60, 74, 81, 82, 83, 86, 89], "exactli": [41, 89], "chosen": [41, 86], "choic": 41, "whether": [41, 47, 48, 49, 50, 60, 89], "hqq": [41, 47, 82], "preserv": [41, 50, 75, 86], "Will": 41, "act_mapping_typ": [42, 43], "token": [42, 43, 60, 64, 81], "produc": 42, "backend": [42, 86], "did": 42, "lower": [42, 82, 86, 87], "flow": [42, 86, 87], "yet": [42, 46, 89, 90], "marlinqqqlayout": 42, "cutlassint4packedlayout": 42, "weight_only_decod": 43, "number": [45, 47, 49, 69, 75, 86, 89], "map": [45, 60, 82, 89], "rang": [45, 81, 86, 87], "sai": [45, 68, 82, 90], "3": [45, 49, 68, 81, 82, 83, 86, 93], "5": [45, 72, 75, 81, 83, 86, 90, 93], "7": [45, 81], "symmetric_no_clipping_err": 45, "variant": [45, 52, 89], "smin": 45, "smax": 45, "min_val_neg": [45, 89], "max_val_po": [45, 89], "By": [45, 86], "individu": [45, 86], "less": [45, 86, 89], "error": [45, 49, 60, 81, 89], "neg": 45, "directli": [45, 52, 82, 86, 87, 89], "placehold": 46, "x": [47, 81, 83, 84, 87, 89, 90, 93], "uint1": [47, 82], "uint7": [47, 82], "enum": 48, "quantized_v": 48, "float_val": 48, "mid_point": 48, "example_input": [49, 66, 83, 84, 87], "qtensor_class_list": 49, "aqdefaultlinearweight": 49, "aqint8weightonlyquantizedlinearweight": 49, "aqint8weightonlyquantizedlinearweight2": 49, "aqint8dynamicallyquantizedlinearweight": 49, "filter_fn": [49, 67, 78], "interpol": 49, "85": 49, "manual": 49, "supress_autoquant_error": 49, "min_sqnr": 49, "aq_kwarg": 49, "autoquant": 49, "identifi": [49, 87], "fastest": 49, "over": [49, 81, 86], "potenti": [49, 86, 87], "qtensor": 49, "prepar": [49, 59, 62, 71, 75, 82, 86], "search": [49, 86], "whose": 49, "exchang": 49, "autoquantizablelinearweight": 49, "calibr": [49, 52], "user": [49, 59, 81, 82, 83, 86, 87, 89, 93], "seen": 49, "record": [49, 82, 87], "so": [49, 81, 82, 83, 84, 86, 89], "final": [49, 57, 67, 82, 83, 86], "benchmark": [49, 71, 81], "member": 49, "pick": 49, "result": [49, 58, 69, 70, 82, 86, 87], "highli": 49, "complet": 49, "simpli": [49, 86, 87, 89], "had": [49, 89], "compil": [49, 67, 70, 81, 82, 83, 87, 89], "them": [49, 74, 82], "onc": [49, 86], "proce": 49, "combin": [49, 60, 86, 89], "finalize_autoqu": 49, "been": [49, 89], "log": [49, 89], "forward": [49, 74, 82, 83, 84, 86, 87, 89, 90], "fulli": [49, 67, 72, 78, 86], "unless": [49, 90], "default_autoquant_class_list": 49, "contain": [49, 71, 72, 86, 89], "second": [49, 58, 81, 82, 93], "stop": 49, "wait": [49, 82], "sever": [49, 81, 90], "automat": [49, 81, 89, 90, 93], "suppress": 49, "accept": 49, "signal": 49, "nois": 49, "ration": 49, "wikipedia": 49, "wiki": 49, "noise_ratio": 49, "v": 49, "non": [49, 82, 86, 89], "caus": [49, 81], "too": 49, "larg": [49, 89], "numer": [49, 81, 86], "resaon": 49, "40": [49, 81], "keyword": 49, "example_input1": 49, "example_input2": 49, "int32": [50, 60, 62, 82, 83], "fp32": [50, 54, 60, 87, 89], "bf16": [50, 82, 83, 86], "optioanl": 50, "param": [50, 52, 57, 75], "request": [50, 54, 68], "min_val": [52, 82, 89], "max_val": [52, 82, 89], "observ": [52, 74, 86, 87], "obtain": 52, "track": [52, 82, 90], "nbit": 53, "axi": [53, 68, 87], "compute_dtyp": 53, "verbos": 53, "raw_output": 53, "optimize_weight": 53, "optimize_weights_proximal_legaci": 53, "input_dtyp": 54, "output_dtyp": [54, 55, 68], "uint8": [54, 68, 82, 87], "quant_dtyp": [56, 57], "fake": [56, 57, 60, 61, 62, 63, 64, 81], "awar": [56, 57, 75, 86, 89], "equival": [56, 57, 60, 72, 86], "without": [56, 57, 61, 82, 86, 90], "valid": [56, 90], "fake_quantize_affin": 57, "consum": 57, "outlier": [57, 81], "mask": [57, 75, 86], "intermedi": 57, "b": 58, "scales1": 58, "multipli": [58, 70, 86], "row": [58, 81, 86], "rais": [58, 70, 81, 89, 90], "assertionerror": [58, 70, 81, 89], "expect": [58, 81, 86, 89], "twostepquant": 59, "compos": [59, 82, 86, 89], "easili": 59, "thei": [59, 81, 82, 86, 89], "constructor": [59, 89], "must": [59, 60, 81, 86, 90], "embed": [59, 62], "undefin": [59, 75], "my_quant": 59, "qatquantizer1": 59, "qatquantizer2": 59, "qatquantizer3": 59, "torchaodtyp": 60, "scale_precis": [60, 62], "zero_point_precis": [60, 62], "is_dynam": 60, "range_learn": 60, "is_symmetr": 60, "simul": [60, 76, 82, 86], "older": 60, "int1": [60, 82], "int7": 60, "pergroup": 60, "per_token": 60, "pertoken": 60, "per_channel": 60, "peraxi": [60, 87], "per_group": [60, 68], "separ": [60, 86, 90], "altern": [60, 82, 87, 89], "leav": 60, "empti": [60, 82], "properti": [60, 82], "throw": 60, "els": [60, 90], "fakequantizedlinear": 61, "fakequantizedembed": 61, "back": [61, 89], "correspond": [61, 67, 82, 84, 86, 89], "model_with_fake_quantized_linear": 61, "int4weightonlyqatembed": 62, "int4weightonlyembed": 62, "groupsiz": [63, 64, 68], "scales_precis": [63, 64], "padding_allow": 64, "activation_config": 65, "fakequantizeconfig": 65, "weight_config": 65, "fakequant": 66, "aobaseconfig": [67, 78, 87, 90], "inplac": [67, 75, 83], "workflow": [67, 78, 81, 83, 86], "qualifi": [67, 72, 78, 86], "move": [67, 82, 87, 90], "speed": [67, 86], "predefin": 67, "execut": [67, 85, 89, 92], "path": [67, 70, 83], "customiz": 67, "int8_dynamic_activation_int4_weight": 67, "int8_dynamic_activation_int8_weight": [67, 78], "mm": [67, 89], "int4_weight_onli": 67, "int8_weight_onli": 67, "sequenti": [67, 78, 81], "1024": [67, 78, 83, 84], "tabl": [68, 81, 82, 86], "per_tensor": 68, "per_axi": 68, "low": [69, 86, 89], "00seeemm": 69, "fp6_e3m2": 69, "sign": 69, "mat2": 70, "safe": 70, "consid": [70, 82, 86], "cubla": 70, "fallback": [70, 90], "i": [70, 81, 86], "j": 70, "debug_skip_calibr": 71, "smoothquant": [71, 72], "smoothfakedynamicallyquantizedlinear": [71, 72], "debug": 71, "skip_fqn_list": 72, "cur_fqn": 72, "alpha": 72, "replac": [72, 86, 90], "skip": [72, 75, 86], "being": [72, 81, 82, 86, 90], "input_quant_func": [73, 82], "quant_kwarg": 73, "dict": [73, 75, 89, 90], "l2": [74, 86], "norm": [74, 75, 86], "buffer": 74, "x_orig": 74, "overridden": 74, "although": [74, 89], "within": [74, 86, 90], "afterward": 74, "former": 74, "care": [74, 84, 86], "hook": [74, 82], "while": [74, 75, 86, 89], "latter": 74, "silent": 74, "ignor": [74, 81], "sparsity_level": [75, 86], "semi_structured_block_s": 75, "wanda": 75, "sparsifi": [75, 79, 84, 86], "propos": 75, "2306": 75, "11695": 75, "product": [75, 90], "magnitud": [75, 86], "parametr": 75, "deepcopi": [75, 83, 87, 89], "squash_mask": [75, 86], "params_to_keep": 75, "params_to_keep_per_lay": 75, "squash": 75, "appropri": [75, 82], "sparse_param": 75, "attach": [75, 86], "kei": [75, 86, 93], "save": [75, 81, 83, 84, 90], "xdoctest": 75, "local": [75, 86], "don": [75, 81, 83, 86, 90], "t": [75, 81, 82, 83, 86, 87, 89, 90], "hasattr": [75, 90], "submodule1": 75, "linear1": [75, 83, 84, 87, 89], "foo": 75, "bar": 75, "submodule2": 75, "linear42": 75, "baz": 75, "print": [75, 83, 84, 89, 93], "42": [75, 87], "24": 75, "ones": [75, 82], "update_mask": 75, "tensor_nam": [75, 90], "statist": [75, 82, 86, 87], "retriev": 75, "act_per_input": 75, "Then": [75, 89], "metric": 75, "across": [75, 86, 89, 90], "whole": 75, "alia": [77, 90], "semisparseweightconfig": 77, "sparsify_": 78, "apply_tensor_subclass": [78, 82], "essenti": [78, 90], "semi_sparse_weight": 78, "semisparselayout": 78, "sparsemarlinlayout": 78, "def": [78, 81, 82, 83, 84, 87, 89, 90], "isinst": [78, 81, 86, 87, 89, 90], "sparse_api": 78, "librari": [79, 84], "gradient": [79, 86], "nativ": [79, 81, 89], "readm": [79, 83, 86], "overal": [79, 83], "introduct": [79, 82], "recent": 79, "highlight": [79, 89, 93], "updat": [79, 83, 84, 86], "guid": [79, 82], "contributor": [79, 83], "serial": [79, 82], "write": 79, "advanc": [79, 87, 89], "pretrain": [79, 86], "vllm": 79, "architectur": [79, 86], "5x": 81, "512": 81, "gpu": [81, 83, 90, 93], "cluster": 81, "34": 81, "43x": 81, "2k": 81, "h200": 81, "latest": [81, 83], "offic": 81, "framework": 81, "8b": 81, "offici": 81, "popular": [81, 82], "flagship": 81, "common": [81, 82, 86], "form": [81, 82, 86], "distribut": [81, 87, 89, 90], "checkpoint": [81, 90], "quickli": [81, 89], "batteri": 81, "includ": [81, 82, 89], "experi": 81, "commonli": [81, 86], "fork": 81, "build": [81, 82, 86, 89, 90], "top": [81, 82, 89], "re": [81, 84, 89], "readi": [81, 83, 87, 89], "virtual": 81, "environ": 81, "conda": 81, "venv": 81, "instal": [81, 83], "download": [81, 83, 91, 93], "job": 81, "command": [81, 83], "root": 81, "directori": 81, "launch": 81, "ngpu": 81, "config_fil": 81, "train_config": 81, "llama3_8b": 81, "toml": 81, "run_train": 81, "sh": 81, "fsdp2": 81, "hyperparamet": 81, "edit": 81, "line": [81, 86], "flag": 81, "termin": 81, "rank0": 81, "titan": 81, "2025": 81, "06": 81, "04": 81, "08": 81, "51": 81, "48": 81, "074": 81, "info": 81, "loss": [81, 86], "12": 81, "2254": 81, "27": 81, "34gib": 81, "28": 81, "78": 81, "tp": [81, 90], "375": 81, "tflop": 81, "21": 81, "73": [81, 87], "mfu": 81, "20": 81, "58": 81, "557": 81, "7069": 81, "30": [81, 83], "99gib": 81, "62": 81, "034": 81, "407": 81, "35": [81, 87], "41": 81, "19": 81, "52": 81, "224": [81, 87], "9196": 81, "022": 81, "406": 81, "65": 81, "904": 81, "1423": 81, "014": 81, "23": [81, 87], "As": [81, 82], "warmup": 81, "around": [81, 84], "7k": 81, "99gb": 81, "peak": 81, "against": 81, "baselin": 81, "11": 81, "02": 81, "37": 81, "404": 81, "2611": 81, "22gib": 81, "595": 81, "47": 81, "49": [81, 87], "027": 81, "4260": 81, "89gib": 81, "344": 81, "367": 81, "39": 81, "15": [81, 83], "03": 81, "01": 81, "988": 81, "9482": 81, "321": 81, "366": 81, "14": 81, "991": 81, "1183": 81, "300": 81, "364": 81, "89": 81, "36": 81, "013": 81, "4659": 81, "291": 81, "84": 81, "769": 81, "gc": 81, "peform": 81, "period": 81, "collect": [81, 82, 86], "3k": 81, "89gb": 81, "11x": 81, "higher": [81, 82, 89], "throughput": 81, "nearli": 81, "ident": [81, 86], "improv": [81, 86], "performan": 81, "vs": [81, 86], "accuraci": [81, 86, 87], "curv": [81, 86], "omit": 81, "648": 81, "2648": 81, "28gib": 81, "71": 81, "29": 81, "26": 81, "475": 81, "9106": 81, "91gib": 81, "53": 81, "503": 81, "434": 81, "43": 81, "94": 81, "166": 81, "9": 81, "0774": 81, "663": 81, "443": 81, "44": [81, 87], "87": 81, "50": [81, 86, 87], "885": 81, "3233": 81, "643": 81, "442": 81, "66": [81, 87], "76": 81, "613": 81, "6150": 81, "637": 81, "72": 81, "6k": 81, "91gb": 81, "21x": 81, "tl": 81, "dr": 81, "better": [81, 89], "priorit": 81, "accur": [81, 86], "stabil": 81, "come": [81, 82, 86, 87, 88], "cost": [81, 87], "slightli": [81, 89], "limit": [81, 89, 90], "underflow": 81, "8xh100": 81, "box": [81, 86], "toi": [81, 83, 87, 89], "convert_to_float8_train": 81, "recurs": 81, "kind": 81, "gemm": 81, "snippet": 81, "f": [81, 82, 84, 86, 87, 89, 90], "float8_linear_util": 81, "float8_linear": 81, "torch_version_at_least_2_5": [81, 83], "greater": 81, "sampl": [81, 82], "2048": 81, "4096": 81, "adamw": 81, "lr": 81, "1e": 81, "elig": 81, "mod": [81, 86, 89], "divis": 81, "16": 81, "in_featur": [81, 83, 84, 87, 89], "out_featur": [81, 83, 87, 89], "enabl": [81, 82, 90], "competit": 81, "loop": [81, 86], "_": [81, 87, 90], "zero_grad": 81, "label": 81, "demonstr": [81, 82, 83, 89], "purpos": [81, 82, 89], "fake_label": 81, "ones_lik": 81, "mse_loss": 81, "model_state_dict": 81, "state_dict": [81, 84], "optimizer_state_dict": 81, "pth": 81, "lai": 82, "stack": 82, "awq": 82, "gptq": 82, "codebookquantizedtensor": 82, "float3": 82, "compon": [82, 89, 90], "overload": [82, 86], "term": [82, 86], "extra": 82, "dev": 82, "discuss": [82, 89], "1833": 82, "No": [82, 84, 86], "matter": [82, 86], "end": [82, 86, 89, 90, 93], "avail": 82, "later": [82, 89], "float3_e2_m0": 82, "float4_e2_m1": 82, "float4_e3_m0": 82, "float5_e2_m2": 82, "float5_e3_m1": 82, "float6_e2_m3": 82, "float6_e3_m2": 82, "float8_e5m2": 82, "float8_e4m3fnuz": 82, "float8_e5m2fnuz": 82, "plan": 82, "float4": 82, "float6": 82, "becom": 82, "part": [82, 86, 89], "uint2": 82, "117208": 82, "outsid": 82, "mention": 82, "criteria": 82, "wide": 82, "adopt": 82, "fundament": [82, 86], "until": 82, "evid": 82, "hopefulli": 82, "amen": 82, "haven": 82, "enough": 82, "ont": 82, "revisit": 82, "intx": 82, "connect": 82, "int4tensor": 82, "previou": 82, "between": [82, 86, 89, 90], "preicison": 82, "mainli": 82, "There": [82, 87, 89], "accommod": 82, "choose_qparams_affine_with_min_max": 82, "min": [82, 87, 89], "int_matmul": 82, "int_scaled_matmul": 82, "reli": [82, 86, 87, 89], "On": [82, 83], "glue": 82, "everyth": 82, "togeth": 82, "construct": 82, "low_precision_v": 82, "high_precision_v": 82, "procedur": 82, "veri": [82, 86, 90], "straightforward": 82, "try": [82, 86, 89], "high_preicsion_v": 82, "especi": [82, 84, 86], "bitwidth": 82, "codebook": 82, "hardcod": 82, "select": 82, "multi": 82, "dimension": [82, 86], "view": [82, 89], "mkldnn": 82, "coo": [82, 86], "sparse_coo": [82, 86], "sparsetensorimpl": 82, "idea": [82, 86], "nice": [82, 86], "concept": [82, 93], "why": [82, 89, 93], "c": [82, 89], "conflict": 82, "quantized_linear": [82, 87], "semant": 82, "stai": [82, 83, 89], "develop": 82, "tradition": 82, "to_affine_quant": 82, "simplic": 82, "explain": 82, "simplest": [82, 86], "easi": 82, "linear_modul": 82, "to_affine_quantized_intx": 82, "requires_grad": [82, 87, 89, 90], "to_linear_activation_quant": 82, "quantized_weight": [82, 90], "activation_and_weight_quant": 82, "encount": 82, "input_qunat_func": 82, "redispatch": 82, "fx": 82, "symbolic_trac": 82, "But": [82, 89, 90], "prefer": [82, 83, 89], "easier": 82, "further": [82, 89], "modif": 82, "figur": [82, 86], "At": [82, 86], "thing": [82, 84, 86, 89], "address": 82, "stat": 82, "averag": [82, 87], "calculate_qparam": [82, 87], "affinequantizedminmaxobserv": [82, 87], "insert_observer_": 82, "observedlinear": [82, 87], "dataset": 82, "complic": [82, 86], "next": [82, 87], "done": [82, 89], "manner": 82, "intend": 82, "autoround": 82, "multitensor": 82, "sure": 82, "describ": [82, 84, 86, 93], "focus": [82, 86], "todai": 82, "low_bit_optim": 82, "similar": [82, 86, 87], "quantized_train": 82, "progress": [82, 90], "lot": [82, 86], "walk": [82, 87, 89, 93], "int4weightonlyconfig": [82, 83, 84, 90], "_convert_weight_to_int4pack": 82, "tensorcoretiledaqttensorimpl": 82, "_quantized_linear_op": 82, "goe": 82, "_aqt_qlinear_dispatch_t": 82, "dispatch": 82, "explan": 82, "wint4": 82, "explor": 83, "stabl": 83, "releas": 83, "pip": 83, "nightli": 83, "index": [83, 86], "url": 83, "whl": 83, "cu121": 83, "major": 83, "instruct": 83, "entri": 83, "mutat": 83, "insert": [83, 87], "logic": [83, 89, 90], "toylinearmodel": [83, 84, 87], "__init__": [83, 84, 87, 89, 90], "super": [83, 84, 87, 89], "linear2": [83, 84, 87, 89], "eval": [83, 84, 87], "faster": [83, 86], "model_bf16": 83, "leverag": [83, 89], "mix": 83, "tensor_impl_dtyp": 83, "verifi": [83, 84, 89], "roughli": [83, 86], "quarter": 83, "os": 83, "tmp": 83, "int4_model": 83, "pt": 83, "bfloat16_model": 83, "int4_model_size_mb": 83, "getsiz": 83, "bfloat16_model_size_mb": 83, "2f": 83, "mb": [83, 84, 85, 92], "25": 83, "00": [83, 85, 92], "much": [83, 86], "benchmark_model": 83, "temporari": 83, "workaround": [83, 90], "num_run": 83, "100": [83, 89], "_dynamo": [83, 89], "reset": 83, "bf16_time": 83, "int4_tim": 83, "time": [83, 86, 89, 93], "3f": 83, "ms": 83, "1fx": 83, "a100": 83, "80gb": 83, "393": 83, "410": 83, "9x": 83, "simpl": [83, 86, 87, 89], "visit": 83, "would": [83, 86, 89], "forget": 83, "tempfil": 84, "get_model_size_in_byt": 84, "batch_siz": [84, 87], "ref": 84, "namedtemporaryfil": 84, "seek": [84, 86], "load": [84, 90], "meta": [84, 90], "m_load": 84, "load_state_dict": 84, "assign": 84, "assert": [84, 87, 89, 90], "equal": [84, 86], "float_weight1": 84, "float_weight2": 84, "quantized_weight1": 84, "quantized_weight2": 84, "go": [84, 89, 93], "techinqu": 84, "reduct": [84, 86, 89], "4x": 84, "0625": 84, "reason": [84, 86], "avoid": [84, 86], "properli": 84, "003": [85, 92, 93], "total": [85, 92, 93], "galleri": [85, 91, 93], "mem": [85, 92], "templat": [85, 91, 92], "tutorials_sourc": 85, "template_tutori": [85, 92, 93], "neural": 86, "network": [86, 89], "overhead": [86, 90], "latenc": 86, "carefulli": 86, "signific": 86, "pai": 86, "price": 86, "qualiti": 86, "f1": 86, "problem": [86, 89], "research": [86, 93], "face": 86, "fragment": 86, "rightfulli": 86, "spent": 86, "compress": 86, "place": 86, "dens": 86, "solv": [86, 89], "focu": [86, 89], "realli": 86, "push": [86, 90], "concret": 86, "hope": 86, "modular": 86, "acceler": 86, "scratch": [86, 93], "minim": 86, "recov": 86, "algorthim": 86, "realiz": 86, "trade": 86, "off": 86, "degrad": 86, "theoret": 86, "gain": 86, "2x": 86, "analog": 86, "fix": [86, 87], "unstructur": 86, "One": [86, 89, 90], "close": 86, "relat": 86, "mitig": 86, "retrain": 86, "neglig": 86, "area": 86, "agre": 86, "upon": 86, "consensu": 86, "mind": 86, "thought": 86, "subproblem": 86, "satisfi": 86, "consist": [86, 89], "answer": 86, "independ": 86, "frontend": 86, "arbitrari": 86, "handoff": 86, "piec": 86, "miss": 86, "natur": [86, 89], "present": 86, "clear": 86, "contract": 86, "7x": 86, "advantag": 86, "anticip": 86, "mani": [86, 89], "solut": 86, "third": 86, "parti": 86, "to_sparse_semi_structur": 86, "sparsesemistructuredtensor": 86, "weightnormsparsifi": 86, "half": 86, "subnetwork": 86, "sparse_config": 86, "named_modul": 86, "append": 86, "tensor_fqn": 86, "sparse_block_shap": 86, "zeros_per_block": 86, "fakespars": 86, "manipul": 86, "dictionari": 86, "paramer": 86, "parameter": 86, "necessari": [86, 87, 89], "ve": 86, "suitabl": 86, "fuse": [86, 89], "0s": 86, "spot": 86, "definit": [86, 90], "academia": 86, "industri": 86, "often": [86, 89], "interchang": 86, "confus": 86, "distinct": 86, "behind": 86, "doesn": 86, "itself": [86, 89], "those": [86, 87, 89], "loos": 86, "speak": 86, "tightli": 86, "coupl": [86, 89], "nvidia": 86, "csc": 86, "fbgemm": 86, "qnnpack": 86, "descript": 86, "coordin": 86, "vector": 86, "locat": 86, "bsr": 86, "sparse_bsr": 86, "except": [86, 89], "scalar": 86, "csr": 86, "sparse_csr": 86, "sparse_csc": 86, "column": 86, "compact": 86, "sparse_matrix": 86, "1d": 86, "indexptr": 86, "\u00bd": 86, "bitmask": 86, "2bit": 86, "unprun": 86, "quit": [86, 89], "successfulli": 86, "These": [86, 89], "broken": 86, "down": 86, "Not": 86, "sensit": 86, "effect": [86, 87, 89], "best": 86, "subsequ": [86, 89], "infinit": 86, "lost": 86, "degre": 86, "analysi": 86, "drop": 86, "give": [86, 89], "proxi": 86, "aforement": 86, "smallest": 86, "absolut": 86, "global": [86, 89], "scope": 86, "impli": 86, "pro": 86, "con": 86, "tradeoff": 86, "span": 86, "threshold": 86, "increas": 86, "complex": 86, "constant": [86, 89], "ctr_mobile_fe": 86, "score": 86, "w": [86, 90], "tenosr": 86, "udpat": 86, "cannot": [86, 87, 90], "histori": 86, "regrow": 86, "dw": 86, "via": 86, "backprop": 86, "pat": 86, "unmask": 86, "resid": 86, "salienc": 86, "lowest": 86, "l1": 86, "shown": 86, "abl": [86, 89, 90], "repeat": 86, "shot": 86, "movement": 86, "tune": 86, "2005": 86, "07683": 86, "rank": [86, 89], "wx": 86, "sqx": 86, "q": 86, "usual": 86, "sort": 86, "wise": 86, "reconstruct": [86, 90], "random": 86, "randomli": 86, "tri": 86, "remedi": 86, "item": [86, 93], "ultim": [86, 87], "literatur": 86, "vision": 86, "nlp": [86, 93], "iter": 86, "ctr_feed": 86, "na": 86, "multimask": 86, "pyspeech": 86, "fastna": 86, "approach": [86, 89], "knowledg": [86, 93], "distil": 86, "pdf": 86, "2204": 86, "09656": 86, "arrang": 86, "recal": 86, "counterpart": 86, "slower": 86, "suffici": 86, "flexibl": [86, 89], "98": 86, "benefit": [86, 89], "special": 86, "exhibit": 86, "maintain": 86, "penalti": 86, "expens": [86, 89], "dictat": 86, "characterist": 86, "highest": 86, "wouldn": [86, 89], "visual": 86, "fig": 86, "4x4": 86, "benchmak": 86, "unlik": 87, "batch": 87, "fly": 87, "welcom": 87, "histogram": 87, "act_ob": 87, "finfo": 87, "weight_ob": 87, "observed_input": 87, "observed_weight": 87, "cl": [87, 89, 90], "float_linear": 87, "observed_linear": 87, "_replace_with_custom_fn_if_matches_filt": 87, "insert_observers_": 87, "_is_linear": 87, "lambda": [87, 90], "replacement_fn": 87, "copied_act_ob": 87, "copied_weight_ob": 87, "popul": 87, "feed": 87, "simpler": 87, "quantizedlinear": [87, 89], "isn": 87, "strictli": 87, "to_affine_quantized_intx_stat": 87, "act_scal": 87, "act_zero_point": 87, "weight_scal": 87, "weight_zero_point": 87, "qweight": 87, "qinput": 87, "from_observ": 87, "begin": [87, 89], "dataclass": [87, 90], "transform_modul": [87, 90], "register_quantize_module_handl": [87, 90], "staticquantconfig": 87, "_apply_static_qu": 87, "is_observed_linear": 87, "optimizedmodul": 87, "_orig_mod": 87, "0237": 87, "plainaqttensorimpl": 87, "142": 87, "31": 87, "113": 87, "157": 87, "57": 87, "59": 87, "160": 87, "70": 87, "150": 87, "67": 87, "241": 87, "238": 87, "69": 87, "235": 87, "228": 87, "255": 87, "201": 87, "114": 87, "236": 87, "88": 87, "83": 87, "109": 87, "209": 87, "92": 87, "184": 87, "141": 87, "110": 87, "0009": 87, "0010": 87, "130": 87, "122": 87, "132": 87, "125": 87, "126": 87, "129": 87, "127": [87, 89], "133": 87, "124": 87, "131": 87, "135": 87, "136": 87, "soon": 88, "foundat": 89, "extens": 89, "autograd": 89, "express": 89, "interpos": 89, "namespac": 89, "continu": 89, "seamlessli": 89, "obviou": 89, "int8quantizedlinear": 89, "few": 89, "finer": 89, "intercept": 89, "contrast": 89, "long": 89, "clunki": 89, "distributedlinear": 89, "duplic": 89, "bypass": 89, "offer": 89, "outer": 89, "inner": 89, "allgath": 89, "bandwidth": 89, "rest": 89, "read": 89, "document": [89, 90], "zoo": 89, "podcast": 89, "edward": 89, "yang": 89, "int8_symmetric_quant": 89, "fp32_tensor": 89, "amin": 89, "keepdim": 89, "amax": 89, "zeros_lik": 89, "clamp": 89, "w_int8": 89, "new_linear": 89, "left": 89, "toymodel": 89, "float_model": 89, "quantized_model": 89, "child": 89, "named_children": 89, "setattr": 89, "drawback": 89, "won": 89, "suppos": 89, "clean": 89, "eleg": 89, "pretti": 89, "power": [89, 90], "overrid": 89, "almost": 89, "shard": [89, 90], "ragged": 89, "rag": 89, "nestedtensor": 89, "resourc": 89, "who": 89, "link": [89, 93], "googl": 89, "collab": 89, "flopcount": 89, "memorytrack": 89, "With": 89, "bare": 89, "bone": 89, "int8symmetrictensor": 89, "hold": 89, "staticmethod": 89, "disabl": 89, "__new__": [89, 90], "_make_wrapper_subclass": [89, 90], "storage_offset": 89, "ndim": 89, "__tensor_flatten__": [89, 90], "attribut": [89, 90], "pt2": 89, "__tensor_unflatten__": [89, 90], "tensor_data_dict": [89, 90], "extra_metadata": 89, "outer_s": [89, 90], "outer_strid": [89, 90], "undo": 89, "__repr__": 89, "repr": 89, "ahead": 89, "insid": 89, "int8_tensor": 89, "func": [89, 90], "op_implementations_dict": 89, "conveni": 89, "register_op": 89, "_op": 89, "opoverload": 89, "impl_decor": 89, "op_impl": 89, "wrapper": 89, "particular": 89, "largest": 89, "tell": 89, "desugar": 89, "decor": [89, 90], "surfac": 89, "coverag": 89, "though": 89, "brute": 89, "forc": 89, "repeatedli": 89, "loggingtensor": 89, "_python_dispatch": [89, 90], "return_and_correct_alias": [89, 90], "int8_mm": 89, "detach": [89, 90], "int8_view_op": 89, "out_data": 89, "out_scal": 89, "notic": 89, "hit": 89, "background": 89, "decomposit": 89, "live": 89, "decomp": 89, "shrink": 89, "author": [89, 93], "pain": 89, "rather": 89, "underli": 89, "worth": 89, "written": 89, "differenti": 89, "nuanc": 89, "longer": 89, "That": 89, "transposit": 89, "got": 89, "propag": 89, "fact": 89, "themselv": 89, "pointwis": 89, "alwai": 89, "were": 89, "might": [89, 90], "unwrap": 89, "dim0": 89, "dim1": 89, "confirm": 89, "quantized_model_module_swap": 89, "quantized_model_subclass": 89, "subclass_param": 89, "no_grad": 89, "out_module_swap": 89, "allclos": 89, "out_compil": 89, "seri": 89, "wa": 89, "comprehens": 90, "e2": 90, "json": 90, "model_typ": 90, "quant_typ": 90, "_type": 90, "_data": 90, "capabl": 90, "modulefqntoconfig": 90, "int8weightonlyconfig": 90, "self_attn": 90, "q_proj": 90, "k_proj": 90, "mlp": 90, "gate_proj": 90, "_default": 90, "torchaoconfig": 90, "automodelforcausallm": 90, "quantization_config": 90, "1b": 90, "torch_dtyp": 90, "auto": 90, "device_map": 90, "safe_seri": 90, "usernam": 90, "server": 90, "narrow": 90, "copy_": 90, "state": 90, "slice": 90, "chunk": 90, "_apply_fn_to_data": 90, "heavi": 90, "codebas": 90, "fn": 90, "ctx": 90, "new_tensor": 90, "getattr": 90, "__class__": 90, "principl": 90, "torchaobasetensor": 90, "mynewquantconfig": 90, "classvar": 90, "myquantizedtensor": 90, "fbgemmfp8tensor": 90, "tensor_data_attr": 90, "tensor_attribut": 90, "attr": 90, "_to_copi": 90, "clone": 90, "fill_default": 90, "notimplementederror": 90, "_my_quant_transform": 90, "my_quantization_funct": 90, "len": 90, "use_cutlass_kernel": 90, "my_cutlass_linear": 90, "elif": 90, "use_triton_kernel": 90, "my_triton_linear": 90, "disappear": 90, "extrem": 90, "sole": 90, "think": 90, "littl": 90, "world": 90, "explicitli": 90, "spooki": 90, "action": 90, "distanc": 90, "statu": 90, "due": 90, "hub": 90, "team": 90, "2338": 90, "creation": 90, "detect": 90, "illustr": 90, "tutorials_python": 91, "zip": [91, 93], "jupyt": [91, 93], "notebook": [91, 93], "tutorials_jupyt": 91, "sphinx": [91, 93], "firstnam": 93, "lastnam": 93, "prerequisit": 93, "v2": 93, "topic": 93, "rand": 93, "8696": 93, "9275": 93, "7441": 93, "2209": 93, "3237": 93, "3872": 93, "5021": 93, "4005": 93, "3543": 93, "5580": 93, "0051": 93, "7050": 93, "0707": 93, "2842": 93, "8377": 93, "practic": 93, "test": 93, "summar": 93, "takeawai": 93, "link1": 93, "link2": 93, "minut": 93, "ipynb": 93}, "objects": {"torchao.dtypes": [[8, 0, 1, "", "AffineQuantizedTensor"], [9, 0, 1, "", "BlockSparseLayout"], [10, 0, 1, "", "CutlassInt4PackedLayout"], [11, 0, 1, "", "CutlassSemiSparseLayout"], [12, 0, 1, "", "Float8Layout"], [13, 0, 1, "", "Int4CPULayout"], [14, 0, 1, "", "Layout"], [15, 0, 1, "", "MarlinQQQLayout"], [16, 0, 1, "", "MarlinQQQTensor"], [17, 0, 1, "", "MarlinSparseLayout"], [18, 0, 1, "", "NF4Tensor"], [19, 0, 1, "", "PlainLayout"], [20, 0, 1, "", "SemiSparseLayout"], [21, 0, 1, "", "TensorCoreTiledLayout"], [22, 0, 1, "", "UintxLayout"], [23, 2, 1, "", "to_affine_quantized_floatx"], [24, 2, 1, "", "to_affine_quantized_floatx_static"], [25, 2, 1, "", "to_affine_quantized_fpx"], [26, 2, 1, "", "to_affine_quantized_intx"], [27, 2, 1, "", "to_affine_quantized_intx_static"], [28, 2, 1, "", "to_marlinqqq_quantized_intx"], [29, 2, 1, "", "to_nf4"]], "torchao.dtypes.AffineQuantizedTensor": [[8, 1, 1, "", "dequantize"], [8, 1, 1, "", "from_hp_to_floatx"], [8, 1, 1, "", "from_hp_to_floatx_static"], [8, 1, 1, "", "from_hp_to_fpx"], [8, 1, 1, "", "from_hp_to_intx"], [8, 1, 1, "", "from_hp_to_intx_static"], [8, 1, 1, "", "to"]], "torchao.dtypes.MarlinQQQTensor": [[16, 1, 1, "", "dequantize"], [16, 1, 1, "", "from_hp_to_intx"]], "torchao.dtypes.MarlinSparseLayout": [[17, 1, 1, "", "pre_process"]], "torchao.dtypes.NF4Tensor": [[18, 1, 1, "", "convert_to_norm_float_weight"], [18, 1, 1, "", "dequantize"], [18, 1, 1, "", "dequantize_scalers"], [18, 1, 1, "", "double_quantize_scalers"], [18, 1, 1, "", "get_original_weight"], [18, 1, 1, "", "quantize_tensor_nearest"]], "torchao.float8": [[30, 0, 1, "", "CastConfig"], [31, 0, 1, "", "Float8LinearConfig"], [32, 0, 1, "", "ScalingGranularity"], [33, 0, 1, "", "ScalingType"], [34, 2, 1, "", "convert_to_float8_training"], [35, 2, 1, "", "precompute_float8_dynamic_scale_for_fsdp"]], "torchao.float8.Float8LinearConfig": [[31, 1, 1, "", "from_recipe_name"]], "torchao.quantization": [[36, 0, 1, "", "FPXWeightOnlyConfig"], [37, 0, 1, "", "Float8DynamicActivationFloat8WeightConfig"], [38, 0, 1, "", "Float8StaticActivationFloat8WeightConfig"], [39, 0, 1, "", "Float8WeightOnlyConfig"], [40, 0, 1, "", "GemliteUIntXWeightOnlyConfig"], [41, 0, 1, "", "Int4WeightOnlyConfig"], [42, 0, 1, "", "Int8DynamicActivationInt4WeightConfig"], [43, 0, 1, "", "Int8DynamicActivationInt8WeightConfig"], [44, 0, 1, "", "Int8WeightOnlyConfig"], [45, 0, 1, "", "MappingType"], [46, 0, 1, "", "TorchAODType"], [47, 0, 1, "", "UIntXWeightOnlyConfig"], [48, 0, 1, "", "ZeroPointDomain"], [49, 2, 1, "", "autoquant"], [50, 2, 1, "", "choose_qparams_affine"], [51, 2, 1, "", "choose_qparams_affine_floatx"], [52, 2, 1, "", "choose_qparams_affine_with_min_max"], [53, 2, 1, "", "choose_qparams_and_quantize_affine_hqq"], [54, 2, 1, "", "dequantize_affine"], [55, 2, 1, "", "dequantize_affine_floatx"], [56, 2, 1, "", "fake_quantize_affine"], [57, 2, 1, "", "fake_quantize_affine_cachemask"], [58, 2, 1, "", "int_scaled_matmul"], [67, 2, 1, "", "quantize_"], [68, 2, 1, "", "quantize_affine"], [69, 2, 1, "", "quantize_affine_floatx"], [70, 2, 1, "", "safe_int_mm"], [71, 2, 1, "", "smooth_fq_linear_to_inference"], [72, 2, 1, "", "swap_linear_with_smooth_fq_linear"], [73, 2, 1, "", "to_linear_activation_quantized"]], "torchao.quantization.qat": [[59, 0, 1, "", "ComposableQATQuantizer"], [60, 0, 1, "", "FakeQuantizeConfig"], [61, 0, 1, "", "FromIntXQuantizationAwareTrainingConfig"], [62, 0, 1, "", "Int4WeightOnlyEmbeddingQATQuantizer"], [63, 0, 1, "", "Int4WeightOnlyQATQuantizer"], [64, 0, 1, "", "Int8DynActInt4WeightQATQuantizer"], [65, 0, 1, "", "IntXQuantizationAwareTrainingConfig"], [66, 2, 1, "", "initialize_fake_quantizers"]], "torchao.quantization.qat.FakeQuantizeConfig": [[60, 3, 1, "", "group_size"], [60, 3, 1, "", "is_symmetric"]], "torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer": [[62, 1, 1, "", "convert"], [62, 1, 1, "", "prepare"]], "torchao": [[5, 4, 0, "-", "sparsity"]], "torchao.sparsity": [[74, 0, 1, "", "PerChannelNormObserver"], [75, 0, 1, "", "WandaSparsifier"], [76, 2, 1, "", "apply_fake_sparsity"], [77, 5, 1, "", "semi_sparse_weight"], [78, 2, 1, "", "sparsify_"]], "torchao.sparsity.PerChannelNormObserver": [[74, 1, 1, "", "forward"]], "torchao.sparsity.WandaSparsifier": [[75, 1, 1, "", "prepare"], [75, 1, 1, "", "squash_mask"], [75, 1, 1, "", "update_mask"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:module", "5": "py:attribute"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "module", "Python module"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchao": [0, 1, 2, 3, 4, 5, 6, 79, 81, 82, 90], "dtype": [0, 7, 82], "layout": [0, 6, 14, 82], "tensor": [0, 6, 82, 88, 89, 90], "subclass": [0, 6, 82, 89, 90], "quantiz": [0, 4, 67, 82, 83, 87, 88, 89, 90], "techniqu": 0, "float8": [1, 81], "main": [1, 4], "train": [1, 82], "api": [1, 2, 4, 79, 81], "other": [1, 4, 6, 82], "type": 1, "refer": [2, 79], "python": 2, "kernel": [3, 6, 80, 82, 90], "infer": 4, "quantize_": 4, "qat": 4, "primit": [4, 82], "sparsiti": [5, 86], "contributor": 6, "guid": [6, 83, 90], "gener": 6, "extend": 6, "ad": [6, 82, 90], "effici": [6, 82], "custom": 6, "triton": 6, "hand": 6, "written": 6, "dispatch": [6, 90], "tensorimpl": [6, 82], "flow": [6, 82, 84, 90], "us": 6, "torch": 6, "compil": [6, 90], "perform": [6, 80], "serial": [6, 84, 90], "featur": 6, "support": [6, 82, 90], "function": [6, 82], "compos": 6, "test": 6, "microbenchmark": 6, "model": [6, 81, 82, 84, 90], "benchmark": 6, "eval": 6, "affinequantizedtensor": 8, "blocksparselayout": 9, "cutlassint4packedlayout": 10, "cutlasssemisparselayout": 11, "float8layout": 12, "int4cpulayout": 13, "marlinqqqlayout": 15, "marlinqqqtensor": 16, "marlinsparselayout": 17, "nf4tensor": 18, "plainlayout": 19, "semisparselayout": 20, "tensorcoretiledlayout": 21, "uintxlayout": 22, "to_affine_quantized_floatx": 23, "to_affine_quantized_floatx_stat": 24, "to_affine_quantized_fpx": 25, "to_affine_quantized_intx": 26, "to_affine_quantized_intx_stat": 27, "to_marlinqqq_quantized_intx": 28, "to_nf4": 29, "castconfig": 30, "float8linearconfig": 31, "scalinggranular": 32, "scalingtyp": 33, "convert_to_float8_train": 34, "precompute_float8_dynamic_scale_for_fsdp": 35, "fpxweightonlyconfig": 36, "float8dynamicactivationfloat8weightconfig": 37, "float8staticactivationfloat8weightconfig": 38, "float8weightonlyconfig": 39, "gemliteuintxweightonlyconfig": 40, "int4weightonlyconfig": 41, "int8dynamicactivationint4weightconfig": 42, "int8dynamicactivationint8weightconfig": 43, "int8weightonlyconfig": 44, "mappingtyp": 45, "torchaodtyp": 46, "uintxweightonlyconfig": 47, "zeropointdomain": 48, "autoqu": 49, "choose_qparams_affin": 50, "choose_qparams_affine_floatx": 51, "choose_qparams_affine_with_min_max": 52, "choose_qparams_and_quantize_affine_hqq": 53, "dequantize_affin": 54, "dequantize_affine_floatx": 55, "fake_quantize_affin": 56, "fake_quantize_affine_cachemask": 57, "int_scaled_matmul": 58, "composableqatquant": 59, "fakequantizeconfig": 60, "fromintxquantizationawaretrainingconfig": 61, "int4weightonlyembeddingqatquant": 62, "int4weightonlyqatquant": 63, "int8dynactint4weightqatquant": 64, "intxquantizationawaretrainingconfig": 65, "initialize_fake_quant": 66, "quantize_affin": 68, "quantize_affine_floatx": 69, "safe_int_mm": 70, "smooth_fq_linear_to_infer": 71, "swap_linear_with_smooth_fq_linear": 72, "to_linear_activation_quant": 73, "perchannelnormobserv": 74, "wandasparsifi": 75, "apply_fake_spars": 76, "semi_sparse_weight": 77, "sparsifi": 78, "welcom": 79, "document": 79, "get": 79, "start": [79, 83], "develop": 79, "note": [79, 81], "tutori": [79, 93], "pretrain": 81, "torchtitan": 81, "prerequisit": 81, "rowwis": 81, "scale": 81, "tensorwis": 81, "pick": 81, "recip": 81, "import": 81, "directli": 81, "convers": 81, "overview": [82, 86, 93], "basic": 82, "current": 82, "placehold": 82, "pytorch": 82, "implement": [82, 89, 90], "oper": [82, 89, 90], "integr": [82, 90], "nativ": 82, "factori": 82, "op": 82, "deriv": 82, "algorithm": 82, "weight": 82, "onli": 82, "dynam": 82, "activ": 82, "static": [82, 87], "insert": 82, "observ": 82, "how": 82, "defin": 82, "modul": [82, 89, 90], "add": [82, 90], "calibr": [82, 87], "awar": 82, "low": 82, "bit": 82, "optim": [82, 84], "case": 82, "studi": 82, "int4": 82, "work": 82, "dure": 82, "execut": 82, "save": 82, "load": 82, "quick": 83, "first": 83, "exampl": [83, 90], "next": [83, 89], "step": [83, 89, 90, 93], "deseri": 84, "what": [84, 89], "happen": 84, "when": 84, "an": 84, "comput": [85, 92], "time": [85, 92], "goal": 86, "design": 86, "context": 86, "prune": 86, "configur": [86, 90], "criteria": 86, "strategi": 86, "pattern": 86, "phase": 87, "write": [88, 89], "your": [88, 89, 90], "own": [88, 89], "advanc": 88, "ar": 89, "swap": 89, "which": 89, "should": 89, "we": 89, "compar": 89, "output": 89, "vllm": 90, "architectur": 90, "usag": 90, "system": 90, "1": 90, "huggingfac": 90, "2": 90, "class": 90, "3": 90, "level": 90, "serv": 90, "new": 90, "method": 90, "minim": 90, "requir": 90, "compat": 90, "why": 90, "creat": 90, "regist": 90, "s": 90, "kei": 90, "detail": 90, "hardwar": 90, "specif": 90, "linear": 90, "benefit": 90, "trade": 90, "off": 90, "share": 90, "safetensor": 90, "diagram": 90, "high": 90, "transform": 90, "point": 90, "bring": 90, "extern": 90, "templat": 93, "option": 93, "addit": 93, "exercis": 93, "conclus": 93, "further": 93, "read": 93}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})