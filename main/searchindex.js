Search.setIndex({"docnames": ["api_ref_dtypes", "api_ref_float8", "api_ref_intro", "api_ref_kernel", "api_ref_qat", "api_ref_quantization", "api_ref_sparsity", "benchmarking_api_guide", "benchmarking_user_guide", "contributor_guide", "dtypes", "finetuning", "generated/torchao.dtypes.AffineQuantizedTensor", "generated/torchao.dtypes.BlockSparseLayout", "generated/torchao.dtypes.CutlassInt4PackedLayout", "generated/torchao.dtypes.CutlassSemiSparseLayout", "generated/torchao.dtypes.Float8Layout", "generated/torchao.dtypes.Int4CPULayout", "generated/torchao.dtypes.Layout", "generated/torchao.dtypes.MarlinQQQLayout", "generated/torchao.dtypes.MarlinQQQTensor", "generated/torchao.dtypes.MarlinSparseLayout", "generated/torchao.dtypes.NF4Tensor", "generated/torchao.dtypes.PlainLayout", "generated/torchao.dtypes.SemiSparseLayout", "generated/torchao.dtypes.TensorCoreTiledLayout", "generated/torchao.dtypes.UintxLayout", "generated/torchao.dtypes.to_affine_quantized_floatx", "generated/torchao.dtypes.to_affine_quantized_floatx_static", "generated/torchao.dtypes.to_affine_quantized_fpx", "generated/torchao.dtypes.to_affine_quantized_intx", "generated/torchao.dtypes.to_affine_quantized_intx_static", "generated/torchao.dtypes.to_marlinqqq_quantized_intx", "generated/torchao.dtypes.to_nf4", "generated/torchao.float8.CastConfig", "generated/torchao.float8.Float8LinearConfig", "generated/torchao.float8.ScalingGranularity", "generated/torchao.float8.ScalingType", "generated/torchao.float8.convert_to_float8_training", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp", "generated/torchao.quantization.FPXWeightOnlyConfig", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig", "generated/torchao.quantization.Float8WeightOnlyConfig", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig", "generated/torchao.quantization.Int4WeightOnlyConfig", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig", "generated/torchao.quantization.Int8WeightOnlyConfig", "generated/torchao.quantization.MappingType", "generated/torchao.quantization.TorchAODType", "generated/torchao.quantization.UIntXWeightOnlyConfig", "generated/torchao.quantization.ZeroPointDomain", "generated/torchao.quantization.autoquant", "generated/torchao.quantization.choose_qparams_affine", "generated/torchao.quantization.choose_qparams_affine_with_min_max", "generated/torchao.quantization.dequantize_affine", "generated/torchao.quantization.int_scaled_matmul", "generated/torchao.quantization.qat.ComposableQATQuantizer", "generated/torchao.quantization.qat.FakeQuantizeConfig", "generated/torchao.quantization.qat.FakeQuantizedEmbedding", "generated/torchao.quantization.qat.FakeQuantizedLinear", "generated/torchao.quantization.qat.FakeQuantizer", "generated/torchao.quantization.qat.Float8ActInt4WeightQATQuantizer", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.embedding.Int4WeightOnlyEmbedding", "generated/torchao.quantization.qat.embedding.Int4WeightOnlyQATEmbedding", "generated/torchao.quantization.qat.initialize_fake_quantizers", "generated/torchao.quantization.qat.linear.Int4WeightOnlyQATLinear", "generated/torchao.quantization.qat.linear.Int8DynActInt4WeightQATLinear", "generated/torchao.quantization.qat.linear.disable_linear_fake_quant", "generated/torchao.quantization.qat.linear.enable_linear_fake_quant", "generated/torchao.quantization.quantize_", "generated/torchao.quantization.quantize_affine", "generated/torchao.quantization.safe_int_mm", "generated/torchao.quantization.smooth_fq_linear_to_inference", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear", "generated/torchao.quantization.to_linear_activation_quantized", "generated/torchao.sparsity.PerChannelNormObserver", "generated/torchao.sparsity.WandaSparsifier", "generated/torchao.sparsity.apply_fake_sparsity", "generated/torchao.sparsity.semi_sparse_weight", "generated/torchao.sparsity.sparsify_", "index", "performant_kernels", "pretraining", "quantization", "quick_start", "serialization", "serving", "sg_execution_times", "sparsity", "static_quantization", "subclass_advanced", "subclass_basic", "torchao_vllm_integration", "tutorials/index", "tutorials/sg_execution_times", "tutorials/template_tutorial", "tutorials_source/pt2e_quant_openvino_inductor", "tutorials_source/pt2e_quant_ptq", "tutorials_source/pt2e_quant_qat", "tutorials_source/pt2e_quant_x86_inductor", "tutorials_source/pt2e_quant_xpu_inductor", "tutorials_source/pt2e_quantizer"], "filenames": ["api_ref_dtypes.rst", "api_ref_float8.rst", "api_ref_intro.rst", "api_ref_kernel.rst", "api_ref_qat.rst", "api_ref_quantization.rst", "api_ref_sparsity.rst", "benchmarking_api_guide.md", "benchmarking_user_guide.md", "contributor_guide.rst", "dtypes.rst", "finetuning.rst", "generated/torchao.dtypes.AffineQuantizedTensor.rst", "generated/torchao.dtypes.BlockSparseLayout.rst", "generated/torchao.dtypes.CutlassInt4PackedLayout.rst", "generated/torchao.dtypes.CutlassSemiSparseLayout.rst", "generated/torchao.dtypes.Float8Layout.rst", "generated/torchao.dtypes.Int4CPULayout.rst", "generated/torchao.dtypes.Layout.rst", "generated/torchao.dtypes.MarlinQQQLayout.rst", "generated/torchao.dtypes.MarlinQQQTensor.rst", "generated/torchao.dtypes.MarlinSparseLayout.rst", "generated/torchao.dtypes.NF4Tensor.rst", "generated/torchao.dtypes.PlainLayout.rst", "generated/torchao.dtypes.SemiSparseLayout.rst", "generated/torchao.dtypes.TensorCoreTiledLayout.rst", "generated/torchao.dtypes.UintxLayout.rst", "generated/torchao.dtypes.to_affine_quantized_floatx.rst", "generated/torchao.dtypes.to_affine_quantized_floatx_static.rst", "generated/torchao.dtypes.to_affine_quantized_fpx.rst", "generated/torchao.dtypes.to_affine_quantized_intx.rst", "generated/torchao.dtypes.to_affine_quantized_intx_static.rst", "generated/torchao.dtypes.to_marlinqqq_quantized_intx.rst", "generated/torchao.dtypes.to_nf4.rst", "generated/torchao.float8.CastConfig.rst", "generated/torchao.float8.Float8LinearConfig.rst", "generated/torchao.float8.ScalingGranularity.rst", "generated/torchao.float8.ScalingType.rst", "generated/torchao.float8.convert_to_float8_training.rst", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp.rst", "generated/torchao.quantization.FPXWeightOnlyConfig.rst", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8WeightOnlyConfig.rst", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.rst", "generated/torchao.quantization.Int4WeightOnlyConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.rst", "generated/torchao.quantization.Int8WeightOnlyConfig.rst", "generated/torchao.quantization.MappingType.rst", "generated/torchao.quantization.TorchAODType.rst", "generated/torchao.quantization.UIntXWeightOnlyConfig.rst", "generated/torchao.quantization.ZeroPointDomain.rst", "generated/torchao.quantization.autoquant.rst", "generated/torchao.quantization.choose_qparams_affine.rst", "generated/torchao.quantization.choose_qparams_affine_with_min_max.rst", "generated/torchao.quantization.dequantize_affine.rst", "generated/torchao.quantization.int_scaled_matmul.rst", "generated/torchao.quantization.qat.ComposableQATQuantizer.rst", "generated/torchao.quantization.qat.FakeQuantizeConfig.rst", "generated/torchao.quantization.qat.FakeQuantizedEmbedding.rst", "generated/torchao.quantization.qat.FakeQuantizedLinear.rst", "generated/torchao.quantization.qat.FakeQuantizer.rst", "generated/torchao.quantization.qat.Float8ActInt4WeightQATQuantizer.rst", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer.rst", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.rst", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.rst", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.embedding.Int4WeightOnlyEmbedding.rst", "generated/torchao.quantization.qat.embedding.Int4WeightOnlyQATEmbedding.rst", "generated/torchao.quantization.qat.initialize_fake_quantizers.rst", "generated/torchao.quantization.qat.linear.Int4WeightOnlyQATLinear.rst", "generated/torchao.quantization.qat.linear.Int8DynActInt4WeightQATLinear.rst", "generated/torchao.quantization.qat.linear.disable_linear_fake_quant.rst", "generated/torchao.quantization.qat.linear.enable_linear_fake_quant.rst", "generated/torchao.quantization.quantize_.rst", "generated/torchao.quantization.quantize_affine.rst", "generated/torchao.quantization.safe_int_mm.rst", "generated/torchao.quantization.smooth_fq_linear_to_inference.rst", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear.rst", "generated/torchao.quantization.to_linear_activation_quantized.rst", "generated/torchao.sparsity.PerChannelNormObserver.rst", "generated/torchao.sparsity.WandaSparsifier.rst", "generated/torchao.sparsity.apply_fake_sparsity.rst", "generated/torchao.sparsity.semi_sparse_weight.rst", "generated/torchao.sparsity.sparsify_.rst", "index.rst", "performant_kernels.rst", "pretraining.rst", "quantization.rst", "quick_start.rst", "serialization.rst", "serving.rst", "sg_execution_times.rst", "sparsity.rst", "static_quantization.rst", "subclass_advanced.rst", "subclass_basic.rst", "torchao_vllm_integration.md", "tutorials/index.rst", "tutorials/sg_execution_times.rst", "tutorials/template_tutorial.rst", "tutorials_source/pt2e_quant_openvino_inductor.rst", "tutorials_source/pt2e_quant_ptq.rst", "tutorials_source/pt2e_quant_qat.rst", "tutorials_source/pt2e_quant_x86_inductor.rst", "tutorials_source/pt2e_quant_xpu_inductor.rst", "tutorials_source/pt2e_quantizer.rst"], "titles": ["torchao.dtypes", "torchao.float8", "<code class=\"docutils literal notranslate\"><span class=\"pre\">torchao</span></code> API Reference", "torchao.kernel", "torchao.quantization.qat", "torchao.quantization", "torchao.sparsity", "Benchmarking API Guide", "Benchmarking User Guide", "Contributor Guide", "Dtypes", "(Part 2) Fine-tuning with QAT, QLoRA, and float8", "AffineQuantizedTensor", "BlockSparseLayout", "CutlassInt4PackedLayout", "CutlassSemiSparseLayout", "Float8Layout", "Int4CPULayout", "Layout", "MarlinQQQLayout", "MarlinQQQTensor", "MarlinSparseLayout", "NF4Tensor", "PlainLayout", "SemiSparseLayout", "TensorCoreTiledLayout", "UintxLayout", "to_affine_quantized_floatx", "to_affine_quantized_floatx_static", "to_affine_quantized_fpx", "to_affine_quantized_intx", "to_affine_quantized_intx_static", "to_marlinqqq_quantized_intx", "to_nf4", "CastConfig", "Float8LinearConfig", "ScalingGranularity", "ScalingType", "convert_to_float8_training", "precompute_float8_dynamic_scale_for_fsdp", "FPXWeightOnlyConfig", "Float8DynamicActivationFloat8WeightConfig", "Float8StaticActivationFloat8WeightConfig", "Float8WeightOnlyConfig", "GemliteUIntXWeightOnlyConfig", "Int4WeightOnlyConfig", "Int8DynamicActivationInt4WeightConfig", "Int8DynamicActivationInt8WeightConfig", "Int8WeightOnlyConfig", "MappingType", "TorchAODType", "UIntXWeightOnlyConfig", "ZeroPointDomain", "autoquant", "choose_qparams_affine", "choose_qparams_affine_with_min_max", "dequantize_affine", "int_scaled_matmul", "ComposableQATQuantizer", "FakeQuantizeConfig", "FakeQuantizedEmbedding", "FakeQuantizedLinear", "FakeQuantizer", "Float8ActInt4WeightQATQuantizer", "FromIntXQuantizationAwareTrainingConfig", "Int4WeightOnlyEmbeddingQATQuantizer", "Int4WeightOnlyQATQuantizer", "Int8DynActInt4WeightQATQuantizer", "IntXQuantizationAwareTrainingConfig", "Int4WeightOnlyEmbedding", "Int4WeightOnlyQATEmbedding", "initialize_fake_quantizers", "Int4WeightOnlyQATLinear", "Int8DynActInt4WeightQATLinear", "disable_linear_fake_quant", "enable_linear_fake_quant", "quantize", "quantize_affine", "safe_int_mm", "smooth_fq_linear_to_inference", "swap_linear_with_smooth_fq_linear", "to_linear_activation_quantized", "PerChannelNormObserver", "WandaSparsifier", "apply_fake_sparsity", "semi_sparse_weight", "sparsify", "Welcome to the torchao Documentation", "Performant Kernels", "(Part 1) Pre-training with float8", "Quantization Overview", "Quick Start Guide", "Serialization", "(Part 3) Serving on vLLM, SGLang, ExecuTorch", "Computation times", "Sparsity Overview", "Static Quantization", "Writing Your Own Quantized Tensor (advanced)", "Writing Your Own Quantized Tensor", "Integration with VLLM: Architecture and Usage Guide", "&lt;no title&gt;", "Computation times", "Template Tutorial", "PyTorch 2 Export Quantization for OpenVINO torch.compile Backend", "PyTorch 2 Export Post Training Quantization", "PyTorch 2 Export Quantization-Aware Training (QAT)", "PyTorch 2 Export Quantization with X86 Backend through Inductor", "PyTorch 2 Export Quantization with Intel GPU Backend through Inductor", "How to Write a <code class=\"docutils literal notranslate\"><span class=\"pre\">Quantizer</span></code> for PyTorch 2 Export Quantization"], "terms": {"thi": [2, 7, 8, 9, 11, 12, 21, 22, 23, 24, 26, 39, 40, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 59, 60, 61, 64, 69, 70, 72, 73, 76, 77, 82, 83, 84, 86, 89, 90, 91, 92, 93, 95, 96, 98, 99, 102, 103, 104, 105, 106, 107, 108], "section": [2, 9, 90, 95, 99, 104, 105, 108], "introduc": [2, 11, 103, 104, 106, 107, 108], "dive": 2, "detail": [2, 7, 9, 11, 40, 53, 89, 90, 91, 93, 95, 96, 98, 103, 104, 105, 106], "how": [2, 4, 9, 11, 12, 18, 26, 45, 47, 49, 54, 59, 77, 87, 89, 91, 92, 93, 95, 96, 98, 99, 103, 106, 107], "integr": [2, 9, 87, 89, 92, 93, 95, 98, 106, 108], "pytorch": [2, 7, 9, 11, 12, 17, 20, 50, 59, 87, 89, 93, 95, 98, 99, 102], "optim": [2, 9, 11, 21, 39, 53, 76, 87, 89, 95, 98, 103, 105, 106, 107], "your": [2, 7, 9, 11, 87, 89, 90, 91, 93, 95, 104, 105, 106, 107, 108], "machin": [2, 105], "learn": [2, 45, 59, 91, 95, 102, 104, 106, 107, 108], "model": [2, 11, 39, 44, 46, 53, 58, 63, 64, 65, 66, 67, 68, 71, 76, 79, 80, 83, 84, 86, 91, 95, 96, 98, 106, 107, 108], "dtype": [2, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 40, 41, 42, 43, 50, 51, 53, 54, 55, 56, 59, 60, 61, 63, 65, 66, 67, 69, 70, 72, 73, 77, 86, 87, 89, 91, 92, 96, 98, 99, 104, 106, 107, 108], "quantiz": [2, 7, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 27, 30, 32, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 86, 89, 92, 95], "sparsiti": [2, 7, 11, 15, 21, 24, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93], "tba": [3, 10, 88], "For": [4, 7, 9, 11, 12, 40, 59, 90, 91, 92, 93, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "full": [4, 9, 11, 91, 96, 102, 103, 105], "exampl": [4, 7, 9, 11, 12, 39, 49, 53, 58, 59, 60, 61, 64, 68, 71, 76, 83, 86, 90, 92, 93, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107], "us": [4, 7, 8, 11, 12, 16, 17, 18, 21, 22, 23, 26, 28, 31, 41, 42, 45, 46, 47, 49, 51, 53, 54, 55, 56, 58, 59, 63, 64, 68, 72, 73, 77, 83, 87, 89, 90, 91, 92, 93, 95, 96, 98, 99, 103, 104, 105, 106, 107], "our": [4, 9, 11, 22, 89, 91, 93, 95, 96, 98, 104, 105], "main": [4, 9, 12, 20, 45, 90, 91, 93, 95, 96, 98, 104, 108], "pleas": [4, 8, 9, 11, 12, 20, 40, 45, 87, 90, 91, 93, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "refer": [4, 7, 9, 11, 12, 73, 89, 93, 95, 96, 98, 99, 103, 104, 105, 106], "readm": [4, 7, 11, 87, 91, 95], "tutori": [7, 9, 11, 12, 89, 90, 91, 93, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108], "you": [7, 8, 9, 11, 59, 83, 89, 90, 91, 92, 93, 95, 98, 99, 102, 103, 104, 105, 106, 107, 108], "through": [7, 9, 11, 55, 60, 61, 87, 90, 91, 93, 96, 98, 99, 102, 103, 104, 108], "torchao": [7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 95, 96, 98, 103, 104, 105, 106, 107], "framework": [7, 11, 89, 93, 103], "The": [7, 9, 11, 12, 13, 18, 21, 26, 38, 40, 41, 42, 43, 53, 57, 76, 78, 79, 80, 83, 89, 90, 91, 92, 93, 95, 98, 99, 103, 104, 105, 106, 107, 108], "contain": [7, 53, 79, 80, 95, 98, 105, 108], "new": [7, 9, 11, 12, 89, 90, 96, 98, 104, 105, 106, 108], "architectur": [7, 87, 93, 95, 103, 104, 106, 107], "micro": 7, "current": [7, 41, 46, 63, 76, 80, 83, 86, 89, 91, 95, 98, 99, 104, 105, 107], "support": [7, 11, 12, 29, 41, 46, 59, 63, 68, 86, 89, 91, 92, 93, 95, 98, 103, 104, 105, 106, 107, 108], "which": [7, 9, 11, 20, 26, 53, 72, 89, 90, 91, 92, 93, 95, 96, 99, 103, 104, 105, 106, 107, 108], "can": [7, 9, 11, 25, 41, 44, 49, 53, 58, 59, 76, 77, 89, 90, 91, 92, 93, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "quantize_": [7, 9, 11, 64, 68, 76, 86, 90, 91, 92, 93, 96], "sparsity_": 7, "function": [7, 11, 12, 25, 38, 53, 60, 61, 69, 74, 75, 76, 82, 83, 84, 86, 89, 91, 92, 95, 96, 98, 99, 103, 108], "To": [7, 9, 11, 12, 20, 53, 73, 89, 90, 91, 92, 93, 95, 96, 99, 104, 105, 106, 108], "correspond": [7, 11, 64, 76, 90, 92, 95, 98, 107, 108], "string": [7, 35, 59, 83], "string_to_config": 7, "microbenchmark": 7, "util": [7, 9, 44, 89, 90, 91, 92, 98, 99, 103, 104, 105, 106, 107, 108], "py": [7, 9, 12, 20, 93, 94, 101, 102, 106, 107], "def": [7, 11, 86, 89, 90, 91, 92, 96, 98, 99, 103, 104, 105, 106, 107, 108], "option": [7, 9, 12, 16, 20, 27, 30, 31, 32, 34, 35, 38, 41, 42, 44, 45, 47, 48, 53, 54, 55, 56, 59, 60, 61, 63, 66, 68, 69, 70, 76, 77, 79, 80, 81, 83, 86, 89, 91, 99, 104, 105, 106, 107, 108], "str": [7, 35, 38, 44, 59, 76, 80, 81, 83, 86, 89, 98, 99, 107], "kwarg": [7, 12, 59, 60, 61, 63, 65, 70, 82, 83, 84, 98, 99], "aobaseconfig": [7, 76, 86, 96, 99], "code": [7, 9, 45, 89, 90, 91, 93, 95, 96, 98, 100, 102, 104, 105, 106, 107, 108], "elif": [7, 99], "my_new_quant": 7, "If": [7, 8, 11, 12, 16, 38, 41, 47, 48, 53, 57, 59, 68, 78, 79, 83, 90, 91, 93, 95, 98, 104, 105], "addit": [7, 11, 18, 23, 53, 89, 95, 98, 103, 104, 107, 108], "inform": [7, 12, 93, 95, 99, 103, 104], "need": [7, 9, 11, 41, 60, 61, 69, 82, 83, 90, 91, 92, 93, 95, 98, 99, 104, 105, 106, 108], "pass": [7, 38, 47, 53, 55, 60, 61, 69, 82, 90, 96, 98, 99, 105, 108], "process": [7, 11, 18, 21, 23, 25, 26, 53, 80, 90, 95, 102, 103, 107], "here": [7, 8, 9, 12, 73, 77, 90, 91, 92, 93, 96, 98, 99, 103, 104, 105, 106, 107, 108], "return": [7, 11, 12, 20, 21, 22, 38, 53, 57, 59, 76, 78, 79, 80, 86, 89, 90, 91, 92, 96, 98, 99, 103, 104, 105, 106, 107, 108], "mynewquantizationconfig": 7, "my_new_spars": 7, "mynewsparsityconfig": 7, "rest": [7, 98, 105], "now": [7, 9, 11, 40, 46, 54, 89, 90, 91, 95, 96, 98, 103, 104, 106, 108], "we": [7, 9, 11, 12, 22, 49, 51, 53, 54, 55, 56, 59, 68, 73, 76, 77, 86, 89, 90, 91, 92, 93, 95, 96, 99, 103, 104, 105, 106, 107, 108], "throughout": 7, "note": [7, 9, 11, 58, 68, 73, 83, 90, 91, 93, 95, 98, 99, 105, 106, 107], "input": [7, 9, 12, 21, 22, 24, 35, 38, 39, 53, 54, 55, 56, 57, 71, 76, 77, 78, 83, 86, 89, 90, 91, 93, 96, 98, 103, 104, 105, 106, 107, 108], "paramet": [7, 11, 12, 18, 21, 22, 28, 31, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 54, 56, 57, 59, 63, 70, 72, 73, 76, 77, 78, 79, 80, 83, 86, 89, 90, 92, 93, 95, 98, 99, 103, 104], "like": [7, 9, 11, 18, 53, 89, 90, 91, 92, 95, 98, 99, 103, 104, 105, 106, 107, 108], "bit": [7, 11, 26, 33, 40, 44, 51, 62, 93, 98, 99, 104, 106, 107], "width": [7, 26, 44, 62], "group": [7, 11, 45, 46, 48, 51, 59, 63, 65, 66, 67, 69, 70, 72, 73, 90, 91], "size": [7, 9, 12, 13, 20, 22, 44, 45, 46, 48, 51, 54, 56, 59, 77, 89, 91, 92, 93, 95, 96, 98, 99, 105], "etc": [7, 9, 60, 61, 90, 103, 108], "them": [7, 11, 53, 60, 61, 69, 82, 90, 108], "append": [7, 95, 104, 105], "config": [7, 11, 35, 38, 53, 59, 60, 61, 62, 64, 68, 76, 83, 86, 91, 93, 95, 96, 99, 104, 106, 107], "gemliteuintxweightonlyconfig": 7, "gemlitewo": 7, "bit_width": [7, 44], "group_siz": [7, 11, 44, 45, 46, 48, 51, 59, 60, 61, 63, 65, 68, 69, 70, 76, 91, 99], "system": [7, 9, 93], "model_architectur": 7, "type": [7, 9, 11, 12, 21, 22, 26, 35, 36, 37, 38, 41, 42, 43, 45, 46, 47, 49, 50, 52, 53, 57, 59, 77, 78, 87, 90, 92, 93, 95, 98, 99, 103, 104, 106, 107, 108], "defin": [7, 9, 18, 26, 36, 40, 60, 61, 69, 82, 83, 91, 95, 96, 98, 99, 103, 106, 107, 108], "class": [7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 82, 83, 90, 91, 92, 96, 98, 104, 105, 106, 108], "mycustommodel": 7, "torch": [7, 11, 12, 21, 22, 26, 28, 35, 38, 41, 42, 43, 45, 51, 53, 54, 56, 57, 59, 60, 61, 63, 65, 66, 67, 68, 69, 70, 72, 73, 76, 77, 78, 79, 80, 86, 87, 89, 90, 91, 92, 93, 95, 96, 98, 99, 102, 106, 107, 108], "nn": [7, 11, 35, 38, 53, 58, 63, 65, 68, 76, 79, 80, 86, 89, 90, 91, 92, 93, 95, 96, 98, 99, 104, 105, 106, 108], "modul": [7, 9, 11, 35, 36, 37, 38, 39, 49, 50, 52, 53, 58, 60, 61, 62, 63, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 82, 83, 86, 89, 91, 92, 96, 103, 104, 105, 106, 107, 108], "__init__": [7, 11, 91, 92, 96, 98, 99, 104, 105, 106], "self": [7, 11, 12, 90, 91, 92, 96, 98, 99, 104, 105, 106], "input_dim": 7, "output_dim": 7, "bfloat16": [7, 9, 22, 63, 66, 72, 77, 89, 90, 91, 92, 93, 95, 96, 99, 106, 107], "super": [7, 11, 91, 92, 96, 98, 104, 105, 106], "layer1": 7, "linear": [7, 9, 11, 21, 35, 38, 41, 43, 45, 46, 47, 48, 51, 53, 58, 61, 63, 66, 67, 68, 72, 73, 74, 75, 76, 80, 84, 86, 89, 90, 91, 92, 93, 95, 96, 98, 103, 104, 105, 106, 108], "512": [7, 89], "bia": [7, 9, 11, 61, 72, 73, 90, 91, 92, 96, 98, 99, 105, 108], "fals": [7, 11, 12, 30, 35, 45, 47, 51, 53, 59, 60, 61, 67, 68, 69, 70, 72, 73, 79, 83, 89, 90, 91, 92, 93, 96, 98, 99, 103, 104, 105, 107, 108], "activ": [7, 9, 11, 41, 42, 44, 46, 47, 53, 59, 60, 61, 63, 67, 68, 73, 79, 83, 87, 91, 93, 95, 96, 99, 103, 106, 107, 108], "relu": [7, 91, 103, 108], "layer2": 7, "forward": [7, 47, 53, 60, 61, 62, 69, 72, 82, 90, 91, 92, 95, 96, 98, 99, 104, 105, 106], "x": [7, 51, 60, 61, 62, 69, 89, 91, 92, 93, 96, 98, 99, 102, 103, 104, 105, 106, 107], "updat": [7, 87, 91, 92, 95, 104, 105, 106, 108], "create_model_and_input_data": 7, "handl": [7, 21, 24, 25, 53, 90], "model_typ": [7, 11, 99, 103], "m": [7, 9, 11, 76, 86, 89, 91, 92, 93, 96, 98, 104, 105, 106], "int": [7, 11, 12, 13, 20, 22, 25, 26, 27, 28, 30, 31, 32, 33, 40, 44, 45, 46, 48, 51, 54, 55, 56, 59, 60, 61, 63, 65, 66, 67, 69, 70, 72, 73, 76, 77, 83, 91, 96, 98, 99], "k": [7, 9, 78, 91, 92, 96, 98, 104, 105], "n": [7, 9, 11, 91, 92, 96, 98, 104, 105, 108], "high_precision_dtyp": 7, "devic": [7, 9, 11, 12, 69, 72, 73, 76, 78, 89, 91, 92, 93, 96, 98, 99, 103, 104, 105, 106, 107], "cuda": [7, 9, 11, 12, 76, 89, 91, 92, 93, 95, 96, 98, 105], "my_custom_model": 7, "input_data": 7, "randn": [7, 11, 12, 61, 89, 91, 92, 96, 98, 103, 104, 105, 106, 107], "when": [7, 9, 11, 12, 23, 54, 56, 77, 89, 90, 93, 95, 96, 99, 103, 104, 105, 106, 107, 108], "ad": [7, 11, 12, 56, 83, 95, 96, 98, 105], "dimens": [7, 9, 12, 26, 51, 54, 56, 57, 77, 89, 98, 99, 104, 105], "ensur": [7, 21, 93, 105], "convent": 7, "where": [7, 24, 49, 51, 55, 65, 66, 67, 90, 95, 99, 108], "batch": [7, 93, 96, 105], "sequenc": 7, "length": 7, "featur": [7, 11, 12, 98, 103, 106, 107], "data": [7, 11, 12, 13, 18, 21, 26, 41, 42, 43, 45, 47, 55, 87, 90, 92, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "typic": [7, 11, 22, 23, 90, 91, 92, 96, 99, 108], "compat": [7, 9, 21, 59, 91], "work": [7, 9, 11, 24, 44, 89, 92, 95, 98, 99, 104, 105, 106], "cpu": [7, 9, 12, 17, 92, 95, 96, 99, 103, 104, 105, 106], "other": [7, 11, 12, 18, 62, 83, 89, 92, 93, 95, 98, 99, 102, 104, 105, 106, 108], "target": [7, 9, 11, 41, 42, 43, 45, 54, 60, 61, 83, 91, 95, 103, 104, 105, 106, 107, 108], "method": [7, 9, 18, 21, 24, 25, 53, 76, 83, 91, 95, 96, 98, 103, 104, 105, 107, 108], "come": [7, 8, 89, 90, 93, 95, 96, 97, 105, 106, 107], "soon": [7, 8, 93, 97, 105], "file": [7, 9, 89, 93, 94, 98, 99, 101, 104, 105], "microbenchmark_quantization_config": 7, "yml": 7, "benchmark_mod": 7, "infer": [7, 9, 11, 12, 79, 87, 90, 91, 92, 95, 96, 98, 103, 104, 105, 106, 107], "quantization_config_recipe_nam": 7, "int8wo": 7, "int8dq": 7, "float8dq": [7, 93], "tensor": [7, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 42, 43, 45, 46, 47, 53, 54, 55, 56, 57, 60, 61, 62, 77, 78, 81, 83, 87, 89, 91, 92, 95, 96, 102, 104, 106, 107], "row": [7, 57, 89, 95], "float8wo": 7, "output_dir": 7, "result": [7, 11, 53, 57, 78, 90, 95, 96, 104, 105, 106, 107, 108], "model_param": 7, "name": [7, 36, 37, 49, 50, 52, 76, 80, 83, 86, 93, 95, 98, 99, 103, 104, 105, 108], "small_bf16_linear": 7, "matrix_shap": 7, "small_sweep": 7, "min_pow": 7, "10": [7, 9, 11, 49, 60, 77, 89, 91, 93, 96, 104, 105], "max_pow": 7, "15": [7, 89, 91, 93], "use_torch_compil": 7, "true": [7, 9, 11, 12, 30, 35, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 55, 59, 60, 61, 68, 75, 76, 79, 86, 89, 91, 92, 93, 96, 98, 99, 103, 104, 105, 106, 108], "torch_compile_mod": 7, "max": [7, 9, 49, 90, 91, 96, 98, 104, 105, 108], "autotun": [7, 9, 91, 96], "runner": 7, "gener": [7, 12, 60, 61, 62, 90, 91, 93, 95, 96, 98, 99, 100, 102, 103, 105, 106, 107, 108], "oss": 7, "databas": 7, "python": [7, 9, 90, 91, 93, 95, 100, 102, 103, 104, 106, 107], "ci_microbenchmark_runn": 7, "benchmark_result": 7, "json": [7, 93, 99], "specif": [7, 9, 11, 18, 21, 23, 24, 60, 61, 73, 83, 89, 90, 91, 92, 93, 95, 103, 106, 107, 108], "requir": [7, 11, 23, 25, 89, 90, 91, 93, 95, 98, 103, 106, 108], "mode": [7, 9, 44, 45, 53, 91, 96, 103, 105, 106, 107, 108], "extra_info": 7, "arch": 7, "nvidia": [7, 95], "a100": [7, 11, 91], "sxm4": 7, "80gb": [7, 91], "1024": [7, 76, 86, 91, 92, 106], "custom": [7, 11, 18, 82, 87, 89, 90, 91, 95, 98, 99, 103, 104, 106, 108], "layer": [7, 21, 38, 41, 43, 45, 47, 48, 51, 53, 60, 61, 63, 65, 66, 67, 69, 70, 72, 73, 79, 80, 83, 84, 89, 93, 95, 96, 98, 99, 103, 108], "origin": [7, 11, 12, 22, 43, 47, 64, 77, 83, 90, 91, 92, 93, 95, 103, 104, 108], "metric": [7, 11, 83], "speedup": [7, 9, 11, 45, 89, 90, 91, 93, 95], "wrt": 7, "bf16": [7, 11, 54, 90, 91, 95, 106, 107], "benchmark_valu": 7, "25": [7, 91], "target_valu": 7, "0": [7, 9, 11, 12, 53, 59, 60, 69, 70, 77, 80, 83, 89, 91, 92, 93, 94, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108], "depend": [7, 12, 44, 53, 92, 95, 98, 104, 105, 107], "step": [7, 11, 23, 39, 53, 89, 90, 95, 103, 104, 105, 106, 107, 108], "workflow": [7, 76, 86, 89, 91, 95, 108], "github": [7, 9, 12, 20, 40, 91, 93], "action": [7, 99, 104, 105], "upload": 7, "verifi": [7, 91, 92, 98], "setup": [7, 93], "suit": [7, 9, 104, 106], "unittest": 7, "discov": 7, "out": [7, 9, 11, 24, 49, 53, 83, 89, 90, 91, 95, 98, 103, 104, 105, 106], "memori": [7, 9, 11, 12, 89, 91, 95, 98, 106, 107], "reduc": [7, 11, 39, 89, 93, 95, 106], "matrix": [7, 13, 16, 41, 42, 57, 78, 83, 91, 95, 106], "compil": [7, 11, 53, 76, 78, 87, 89, 90, 91, 96, 98, 106, 107], "error": [7, 49, 53, 59, 89, 98, 104], "set": [7, 11, 12, 16, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 55, 59, 76, 79, 83, 91, 95, 103, 105, 106, 107], "debug": [7, 79], "miss": [7, 95], "properli": [7, 92], "instal": [7, 89, 91, 93, 104, 107], "Not": [7, 95], "avail": [7, 90, 103, 104, 105, 106, 107], "check": [7, 9, 11, 12, 20, 90, 91, 92, 98, 103, 105, 108], "driver": 7, "basic": [7, 9, 23, 91, 96, 98], "shape": [7, 9, 12, 20, 53, 57, 78, 91, 96, 98, 99, 104, 107], "comprehens": [7, 99, 106], "analysi": [7, 95], "enabl": [7, 75, 89, 90, 93, 99, 106], "profil": [7, 9], "onli": [7, 9, 11, 17, 38, 41, 43, 44, 45, 46, 47, 48, 51, 63, 73, 86, 89, 91, 92, 93, 95, 98, 99, 103, 104, 106, 107, 108], "overhead": [7, 95, 99, 106], "multipl": [7, 11, 16, 41, 42, 53, 57, 58, 78, 91, 95, 96, 98, 99, 106, 108], "possibl": [7, 12, 95, 104, 105, 106, 108], "consist": [7, 93, 95, 98, 106, 107, 108], "reproduc": [7, 93], "differ": [7, 9, 11, 18, 45, 55, 58, 77, 78, 89, 90, 91, 92, 93, 95, 98, 99, 104, 105, 106, 108], "case": [7, 8, 9, 53, 78, 93, 95, 98, 99, 103, 104, 108], "user": [7, 11, 53, 58, 73, 87, 89, 90, 91, 93, 95, 96, 98, 102, 104, 105, 106, 107, 108], "more": [7, 9, 11, 12, 40, 44, 45, 46, 51, 53, 89, 90, 91, 93, 95, 96, 98, 99, 103, 104, 105, 106, 107], "about": [7, 9, 11, 45, 90, 91, 92, 93, 95, 104, 105, 106, 108], "compon": [7, 90, 98, 99], "see": [7, 9, 11, 12, 20, 40, 89, 90, 91, 92, 93, 95, 96, 98, 99, 103, 104, 108], "directori": [7, 89], "intend": [8, 90, 104], "provid": [8, 9, 11, 18, 21, 24, 25, 53, 54, 58, 71, 89, 90, 93, 95, 98, 99, 104, 105, 107, 108], "instruct": [8, 11, 91, 93, 104, 105, 106], "most": [8, 23, 90, 93, 95, 99, 104, 105, 108], "fequent": 8, "have": [8, 9, 11, 44, 45, 49, 53, 65, 66, 67, 77, 83, 90, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "ani": [8, 9, 23, 53, 63, 65, 71, 81, 83, 90, 95, 98, 103, 105, 107], "answer": [8, 95], "creat": [8, 9, 12, 28, 29, 31, 89, 90, 95, 98, 103, 104, 106, 107, 108], "an": [8, 9, 11, 12, 25, 30, 31, 53, 59, 68, 73, 83, 87, 89, 90, 91, 93, 95, 96, 98, 103, 104, 105, 106, 107, 108], "issu": [8, 9, 90, 91, 98, 106], "train": [9, 35, 58, 59, 87, 91, 95, 98, 108], "fp4": 9, "s": [9, 11, 12, 49, 53, 54, 56, 77, 89, 90, 91, 93, 95, 96, 98, 104, 105, 106, 107, 108], "fine": [9, 44, 45, 46, 51, 87, 89, 93, 95], "start": [9, 11, 36, 37, 49, 50, 52, 53, 89, 90, 93, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "prototyp": [9, 59, 71, 90, 108], "folder": [9, 93, 104, 105], "could": [9, 90, 98, 103, 104, 106, 107, 108], "also": [9, 11, 53, 59, 76, 90, 91, 92, 95, 96, 98, 99, 104, 107, 108], "take": [9, 22, 60, 61, 69, 76, 82, 86, 90, 95, 103, 104, 105, 106, 107, 108], "look": [9, 12, 89, 90, 95, 103, 104, 105, 106, 107], "affinequantizedtensor": [9, 20, 28, 29, 31, 90, 91, 92, 96, 98], "what": [9, 11, 12, 20, 53, 89, 90, 91, 93, 95, 96, 99, 102, 104, 108], "want": [9, 76, 86, 90, 91, 92, 95, 98, 99, 103, 104, 105, 108], "do": [9, 50, 53, 57, 76, 90, 93, 95, 96, 98, 99, 104, 105, 106, 108], "mostli": [9, 55, 91, 106], "e": [9, 11, 12, 40, 49, 53, 54, 56, 58, 59, 76, 77, 89, 90, 92, 96, 98, 103, 108], "g": [9, 11, 12, 40, 49, 53, 54, 56, 58, 59, 76, 77, 90, 92, 96, 98, 103, 108], "int3": 9, "exact": [9, 11, 73, 104, 105], "same": [9, 11, 12, 41, 54, 55, 56, 73, 77, 78, 86, 89, 90, 95, 96, 98, 105, 106, 107, 108], "affin": [9, 12, 14, 15, 16, 17, 21, 24, 25, 30, 56, 77, 90], "feel": [9, 90, 95, 98, 99], "free": [9, 90, 98], "open": [9, 90, 95], "question": [9, 90, 92, 95, 98, 108], "overview": [9, 87, 91, 99], "page": [9, 91, 106], "contribut": [9, 91, 95], "exist": [9, 50, 89, 90, 95, 96, 98, 104, 108], "base": [9, 18, 23, 49, 62, 71, 83, 90, 91, 95, 98, 99, 103, 104, 105, 106, 107, 108], "make": [9, 90, 91, 98, 99, 104, 108], "trainabl": [9, 11, 90, 98], "add": [9, 23, 98, 102, 106, 108], "parallel": [9, 89, 98, 99], "affine_quantized_tensor": [9, 92], "api": [9, 53, 90, 91, 95, 96, 98, 103, 104, 105, 106, 107], "quant_api": [9, 76, 92, 93, 96], "primit": [9, 12, 20, 98, 104], "op": [9, 11, 12, 20, 45, 53, 76, 91, 95, 98, 99, 104, 105, 106, 108], "slight": [9, 95], "variat": [9, 90], "quant_primit": [9, 12, 20, 96], "mp": 9, "csrc": 9, "mayb": [9, 34], "well": [9, 18, 53, 90, 91, 95, 104, 105, 108], "spars": [9, 13, 21, 24, 60, 69, 70, 83, 90, 95], "marlin": [9, 19, 20, 21, 32], "aqt": 9, "621": 9, "ar": [9, 11, 12, 16, 24, 26, 38, 40, 41, 44, 45, 53, 54, 56, 58, 60, 61, 68, 76, 77, 78, 83, 89, 90, 91, 92, 93, 95, 96, 99, 103, 104, 105, 106, 107, 108], "still": [9, 11, 90, 95, 104, 108], "decid": [9, 90, 95, 96], "split": [9, 93, 104, 105], "implement": [9, 11, 35, 69, 70, 72, 73, 92, 95, 96, 103, 104, 108], "regist": [9, 60, 61, 69, 82, 98], "mai": [9, 55, 59, 90, 92, 96, 104, 105, 106, 107, 108], "own": [9, 11, 87, 89, 91, 95, 96, 104, 105, 108], "int4": [9, 11, 14, 17, 46, 49, 59, 60, 61, 63, 65, 66, 67, 68, 69, 70, 72, 73, 76, 86, 91, 92, 93, 99], "access": [9, 47, 103], "my_custom_op": 9, "condit": [9, 90], "__torch_function__": [9, 90, 98], "__torch_dispatch__": [9, 98], "oper": [9, 11, 12, 16, 18, 21, 47, 55, 91, 93, 103, 104, 105, 106, 107], "uint4": [9, 45, 90, 91], "weight": [9, 11, 21, 22, 39, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 59, 60, 61, 63, 65, 66, 67, 69, 70, 72, 73, 76, 83, 86, 87, 89, 91, 92, 95, 96, 98, 99, 103, 104, 105, 106, 107, 108], "found": [9, 90, 91, 93, 95, 96, 98], "allow": [9, 73, 91, 95, 98, 103, 104, 105, 106, 108], "peopl": [9, 90, 92, 99, 108], "two": [9, 11, 20, 24, 41, 90, 91, 95, 98, 103, 104, 105, 106, 108], "dispatch_condit": [9, 90], "impl": [9, 12, 90], "actual": [9, 11, 43, 90, 96, 98, 99, 104, 105, 108], "run": [9, 11, 39, 53, 60, 61, 69, 76, 79, 82, 89, 90, 91, 93, 95, 98, 102, 103, 104, 105, 106, 107, 108], "both": [9, 12, 41, 73, 90, 91, 95, 96, 98, 104, 106, 107, 108], "input_tensor": [9, 22, 90, 99], "weight_tensor": [9, 90, 99], "argument": [9, 12, 25, 53, 56, 59, 76, 89, 90, 93, 106], "register_aqt_quantized_linear_dispatch": 9, "show": [9, 77, 89, 90, 91, 93, 95, 99, 104, 105], "sometim": [9, 95], "ha": [9, 11, 12, 90, 93, 95, 98, 99, 103, 104, 105, 107, 108], "pack": [9, 12, 14, 25, 26, 40, 44, 51, 90], "order": [9, 53, 58, 90, 95, 98, 108], "yield": [9, 11, 95], "And": [9, 22, 41, 90, 98, 106, 108], "abstract": [9, 90], "after": [9, 11, 39, 53, 89, 90, 92, 95, 103, 104, 105, 106, 107, 108], "wrap": [9, 53, 98, 106, 107], "factori": 9, "convert": [9, 11, 12, 20, 22, 27, 30, 32, 33, 35, 58, 64, 65, 76, 86, 89, 90, 93, 95, 103, 106, 107, 108], "from": [9, 11, 12, 22, 23, 28, 29, 31, 40, 46, 55, 64, 68, 76, 77, 86, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108], "float": [9, 11, 12, 20, 22, 30, 32, 33, 40, 45, 49, 52, 53, 54, 55, 56, 59, 60, 69, 70, 77, 80, 83, 90, 91, 92, 98, 104, 105, 108], "point": [9, 12, 20, 32, 40, 45, 49, 52, 56, 59, 70, 71, 72, 73, 89, 90, 91, 92, 95, 96, 98, 104, 108], "my": [9, 95, 105], "to_my_dtyp": 9, "mydtypetensor": 9, "from_float": [9, 96, 98], "level": [9, 83, 90, 95, 98, 103, 104, 106, 107], "reus": [9, 90, 98], "appli": [9, 11, 12, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 58, 62, 63, 68, 76, 86, 90, 91, 93, 95, 99, 105], "convers": [9, 11, 12, 38, 90], "filter": [9, 11, 38, 53, 89, 96], "choos": [9, 90, 95, 98, 104, 106], "should": [9, 11, 12, 39, 44, 56, 60, 61, 64, 69, 82, 83, 89, 90, 95, 99, 103, 104, 108], "algorithm": [9, 45, 51, 93, 95, 103], "dynam": [9, 11, 34, 35, 39, 41, 44, 46, 47, 59, 63, 67, 73, 86, 93, 96, 98, 104, 105, 106], "quant": [9, 12, 20, 40, 90, 93, 99, 104, 107, 108], "static": [9, 12, 18, 22, 28, 31, 35, 42, 55, 59, 87, 91, 104, 105, 106, 107, 108], "2": [9, 12, 15, 17, 21, 24, 45, 49, 53, 59, 60, 69, 70, 77, 84, 86, 87, 89, 90, 95, 96, 98, 102], "4": [9, 11, 15, 21, 24, 33, 44, 84, 86, 90, 91, 92, 93, 95, 98, 104, 105], "below": [9, 89, 90, 95, 98, 99, 102, 103], "follow": [9, 11, 45, 59, 89, 90, 91, 93, 95, 96, 98, 103, 104, 105, 106, 107, 108], "import": [9, 11, 64, 68, 76, 86, 91, 92, 93, 95, 96, 98, 99, 102, 103, 106, 107], "unwrap_tensor_subclass": [9, 91], "m_unwrap": 9, "In": [9, 11, 89, 90, 91, 95, 96, 98, 103, 104, 105, 106, 107, 108], "aim": [9, 90, 95, 107], "fullgraph": [9, 91], "first": [9, 22, 53, 57, 83, 90, 93, 96, 98, 99, 104, 105, 108], "remov": [9, 54, 83, 89, 95, 99, 104, 105], "unnecessari": 9, "graph": [9, 91, 104, 105, 108], "break": 9, "torch_log": 9, "output_cod": 9, "script": [9, 91, 93, 96, 98, 102, 105, 106, 107], "inductor": [9, 53, 87, 91, 103, 104], "checkout": [9, 12, 20, 87, 90], "doc": [9, 89, 90, 91, 93, 98], "huggingfac": 9, "transform": [9, 11, 12, 90, 96, 103, 104, 105, 106, 107], "deseri": [9, 90, 104, 105], "save_pretrain": [9, 93], "push_to_hub": [9, 93, 99], "from_pretrain": [9, 11, 93, 99], "http": [9, 12, 20, 40, 53, 83, 91, 93, 95, 107], "co": [9, 93], "en": [9, 53], "anoth": [9, 90, 95, 98, 104, 108], "diffus": 9, "com": [9, 12, 20, 40, 93], "sayakpaul": 9, "blob": [9, 12, 20], "serialization_and_load": 9, "md": 9, "abov": [9, 11, 49, 90, 92, 95, 96, 98, 104, 105, 108], "just": [9, 49, 59, 90, 92, 95, 98, 104, 105, 108], "talk": [9, 90, 93], "fsdp": [9, 90], "ll": [9, 49, 89, 90, 93, 98, 104, 105, 108], "put": [9, 86, 106, 108], "developer_api_guid": 9, "cover": [9, 90, 102, 104, 107, 108], "executorch": [9, 46, 76, 87, 91, 104, 105], "torchchat": 9, "todo": [9, 90], "qat": [9, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 87, 93, 106], "dtensor": [9, 98], "recommend": [9, 11, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 89, 103, 106, 107], "copi": [9, 12, 83, 91, 92, 95, 96, 98, 105, 106], "past": [9, 95], "adapt": [9, 89, 96], "befor": [9, 11, 76, 90, 92, 93, 95, 96, 98, 104, 105, 108], "some": [9, 53, 76, 83, 90, 91, 93, 95, 96, 98, 103, 104, 105, 106, 107, 108], "singl": [9, 11, 34, 39, 41, 53, 55, 89, 91, 95, 104, 108], "comput": [9, 21, 25, 39, 43, 60, 61, 69, 82, 83, 95, 96, 98, 104, 105, 106, 107], "intens": 9, "get": [9, 11, 22, 73, 89, 90, 91, 93, 95, 99, 103, 104, 105, 106, 108], "sens": [9, 90, 98], "d": [9, 90, 93, 105], "benchmark_aq": 9, "A": [9, 11, 12, 26, 53, 55, 82, 95, 98, 99, 104], "quick": [9, 87], "wai": [9, 12, 53, 89, 90, 93, 95, 96, 98, 104, 105, 108], "relev": [9, 45, 90, 102], "chang": [9, 76, 89, 90, 91, 92, 93, 95, 96, 98, 103, 104, 105, 107, 108], "interest": [9, 90, 95, 98], "print_op_and_shap": 9, "output": [9, 11, 35, 53, 54, 56, 77, 89, 90, 91, 93, 95, 102, 103, 104, 105, 106, 107, 108], "torch_func": 9, "built": [9, 89, 98], "_c": 9, "tensorbas": 9, "object": [9, 26, 76, 86, 98, 104, 105, 108], "arg": [9, 12, 60, 61, 63, 65, 70, 83, 98, 99, 105, 108], "all": [9, 39, 49, 53, 55, 60, 61, 63, 65, 69, 71, 82, 83, 84, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 103, 104, 106, 108], "under": [9, 11, 93], "benchmark_your_kernel": 9, "helper": [9, 74, 75], "right": [9, 90, 95, 104], "1": [9, 21, 26, 36, 37, 45, 49, 50, 51, 52, 53, 77, 83, 87, 90, 91, 92, 94, 95, 96, 98, 101, 102, 104, 105], "either": [9, 12, 41, 83, 93, 95, 105, 106, 107], "one": [9, 41, 53, 55, 60, 61, 69, 82, 89, 90, 95, 98, 99, 105, 108], "probabl": 9, "keep": [9, 21, 47, 83, 104], "futur": [9, 40, 96, 99, 104, 105, 106, 108], "llama": [9, 11, 93, 99, 103], "llama2": 9, "llama3": [9, 11, 89], "sam": 9, "alreadi": [9, 12, 53, 98, 108], "modifi": [9, 38, 76, 83, 89, 90, 95, 98], "friendli": [9, 90], "compar": [9, 11, 45, 83, 89, 90, 93, 104, 106, 108], "techniqu": [9, 11, 89, 92, 93, 95, 96, 98, 99], "repres": [9, 12, 13, 16, 18, 29, 35, 59, 77, 83, 90, 92, 98, 104, 105], "bound": [9, 93, 95, 99], "help": [9, 11, 89, 90, 93, 99, 103, 104], "each": [9, 22, 53, 59, 63, 70, 72, 73, 79, 82, 90, 95, 96, 98, 99, 104, 105, 108], "understand": [9, 89, 106, 108], "profile_path": 9, "chrome": 9, "trace": [9, 90], "let": [9, 49, 77, 90, 91, 95, 96, 98, 108], "know": [9, 53, 98], "end": [11, 89, 90, 93, 95, 98, 99, 102, 105, 108], "pre": [11, 18, 21, 25, 87, 91, 93, 95, 108], "serv": [11, 12, 18, 87, 89, 98, 107], "flow": [11, 46, 89, 93, 95, 96, 103, 104, 105, 106, 107], "leverag": [11, 89, 91, 93, 98, 106, 107], "partner": [11, 89, 93], "showcas": [11, 89, 93], "focus": [11, 89, 90, 93, 95], "domain": [11, 12, 45, 52, 54, 56, 59, 89], "demonstr": [11, 89, 90, 91, 93, 98, 103, 105], "dure": [11, 12, 20, 47, 53, 56, 59, 80, 89, 91, 93, 95, 96, 98, 103, 105], "numer": [11, 53, 72, 73, 89, 95, 104, 105, 106], "goal": 11, "mitig": [11, 95], "degrad": [11, 95], "eventu": [11, 89], "blog": 11, "resourc": [11, 98], "small": 11, "matric": [11, 24, 95], "freez": [11, 105, 106, 107], "checkpoint": [11, 89, 93, 99], "effici": [11, 25, 72, 91, 95, 96, 107], "paper": [11, 40, 95, 102], "speed": [11, 76, 93, 95, 103], "up": [11, 22, 76, 89, 90, 91, 95, 103, 104, 105, 108], "high": [11, 12, 27, 28, 29, 30, 31, 89, 90, 93, 95, 96, 98, 103, 104, 106, 107], "precis": [11, 12, 27, 28, 29, 30, 31, 43, 47, 63, 66, 67, 70, 72, 73, 90, 96, 98, 103, 106, 107], "similar": [11, 90, 95, 96, 105, 106], "so": [11, 53, 89, 90, 91, 92, 95, 98, 104, 105, 108], "inevit": 11, "presum": 11, "been": [11, 53, 98, 105, 106, 107, 108], "successfulli": [11, 95], "recent": [11, 87], "releas": [11, 91, 106], "1b": [11, 99], "3b": 11, "llamaguard": 11, "8b": [11, 89], "improv": [11, 89, 93, 95, 104, 107, 108], "qualiti": [11, 95], "involv": [11, 16, 95], "separ": [11, 59, 60, 61, 95, 99, 104, 108], "prepar": [11, 53, 58, 63, 65, 79, 83, 90, 95, 103, 106, 107, 108], "fake": [11, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 89, 104, 105, 108], "mean": [11, 12, 22, 49, 54, 56, 77, 89, 90, 91, 95, 104, 105, 108], "valu": [11, 12, 22, 35, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 77, 79, 83, 90, 95, 96, 98, 103, 104, 105, 108], "map": [11, 47, 49, 59, 90, 98, 104, 108], "without": [11, 64, 90, 95, 99, 106, 108], "cast": [11, 34, 36], "lower": [11, 46, 90, 91, 93, 95, 96, 105], "replac": [11, 80, 95, 99], "real": [11, 91, 104, 108], "doe": [11, 23, 45, 90, 95, 98, 104, 106, 107], "perform": [11, 12, 25, 39, 44, 47, 48, 53, 57, 60, 61, 65, 66, 67, 69, 78, 79, 82, 89, 91, 95, 96, 98, 99, 103, 105, 106, 107], "There": [11, 90, 96, 98, 104, 108], "directli": [11, 49, 55, 90, 95, 96, 98], "loop": [11, 89, 95], "distribut": [11, 89, 96, 98, 99, 103], "recip": [11, 35, 60, 61, 69, 82], "instead": [11, 45, 55, 59, 60, 61, 69, 82, 89, 90, 91, 95, 98, 105, 106, 107, 108], "command": [11, 89, 91], "regular": [11, 103, 106, 107], "nnode": 11, "nproc_per_nod": 11, "full_finetune_distribut": 11, "llama3_2": 11, "3b_full": 11, "batch_siz": [11, 92, 93, 96, 104, 105], "16": [11, 61, 89], "equival": [11, 59, 80, 95, 105, 106, 108], "specifi": [11, 12, 35, 38, 48, 51, 58, 60, 61, 62, 73, 76, 77, 83, 86, 89, 95, 103, 104, 105, 108], "default": [11, 12, 13, 16, 23, 25, 26, 41, 42, 43, 44, 45, 51, 53, 54, 56, 59, 63, 73, 76, 79, 80, 89, 91, 98, 99, 103, 104, 105, 106, 107, 108], "asymmetr": [11, 44, 45, 46, 49, 51, 54, 59, 90, 91, 96, 103, 107, 108], "per": [11, 12, 43, 45, 46, 47, 48, 51, 54, 56, 59, 63, 65, 66, 67, 69, 70, 72, 73, 77, 83, 89, 90, 91, 95, 96, 107], "token": [11, 46, 47, 59, 67, 73, 89, 93], "int8": [11, 22, 46, 47, 48, 59, 61, 67, 68, 73, 76, 86, 90, 93, 98, 104, 106, 107, 108], "symmetr": [11, 41, 42, 43, 44, 46, 47, 48, 49, 54, 59, 60, 63, 98, 103, 104, 107, 108], "configur": [11, 16, 34, 35, 38, 41, 42, 43, 45, 46, 47, 48, 51, 76, 86, 89, 90, 91, 93, 106, 107, 108], "_component_": 11, "qat_distribut": 11, "3b_qat_ful": 11, "evalu": [11, 105], "whether": [11, 45, 51, 52, 53, 54, 59, 98], "wa": [11, 98, 105], "llama3_2_3b": 11, "fullmodelhfcheckpoint": 11, "checkpoint_fil": 11, "00001": 11, "00002": 11, "safetensor": 11, "int8dynactint4weightquant": 11, "groupsiz": [11, 66, 67, 72, 73, 77], "32": [11, 44, 45, 46, 59, 61, 68, 69, 70, 76, 86, 89, 91, 92, 93, 96, 98, 105], "hellaswag": [11, 93], "wikitext": 11, "eleuther_ev": 11, "eleuther_evalu": 11, "task": [11, 93], "fullmodeltorchtunecheckpoint": 11, "8da4w": [11, 93], "ckpt": 11, "llama3_token": 11, "path": [11, 76, 78, 91, 93, 103, 104, 105, 106, 108], "tmp": [11, 91], "meta": [11, 92, 99, 108], "print": [11, 83, 91, 92, 93, 98, 102, 104, 105], "version": [11, 17, 59, 89, 91, 98, 99, 104, 105, 108], "shot": [11, 95], "stderr": 11, "none": [11, 12, 16, 20, 27, 30, 31, 32, 34, 35, 36, 37, 38, 39, 41, 42, 44, 45, 48, 49, 50, 52, 53, 54, 55, 56, 59, 60, 61, 63, 68, 69, 70, 71, 72, 73, 76, 77, 79, 80, 81, 83, 86, 96, 98, 99, 103, 104, 105, 107], "acc": [11, 104, 105], "5021": 11, "0050": 11, "acc_norm": 11, "6797": 11, "0047": 11, "bits_per_byt": 11, "6965": 11, "byte_perplex": 11, "6206": 11, "word_perplex": 11, "13": 11, "2199": 11, "much": [11, 91, 95, 108], "openassist": 11, "oasst1": 11, "dataset": [11, 89, 90, 93, 103, 106, 107], "find": [11, 22, 95, 104, 108], "achiev": [11, 22, 89, 95, 96, 98, 105, 106], "higher": [11, 89, 90, 98, 103, 104, 106, 107], "accuraci": [11, 89, 93, 95, 96, 103, 105, 106], "than": [11, 26, 59, 89, 90, 95, 98, 104], "recov": [11, 95, 105], "69": [11, 96], "8": [11, 25, 26, 44, 45, 49, 60, 61, 66, 72, 89, 90, 91, 93, 99, 106, 107], "overal": [11, 87, 91, 104, 108], "vanilla": 11, "compos": [11, 58, 90, 95, 98, 104, 105, 108], "lora": 11, "89x": 11, "usag": [11, 12, 39, 53, 58, 59, 60, 61, 64, 68, 87, 89, 93, 106, 107], "36": [11, 89, 93], "qat_lora_finetune_distribut": 11, "3b_qat_lora": 11, "try": [11, 90, 95, 98, 104], "fsdp2": [11, 89], "yaml": 11, "onc": [11, 53, 95], "complet": [11, 53, 93, 103, 107], "save": [11, 83, 89, 91, 92, 93, 99], "qat_out": 11, "quatiz": 11, "document": [11, 98, 99, 103, 104, 106], "prefer": [11, 90, 91, 98], "call": [11, 12, 53, 60, 61, 69, 82, 90, 91, 92, 95, 96, 98, 99, 105, 107], "These": [11, 95, 98, 103, 104, 105, 108], "hood": 11, "mini": [11, 93], "gpu": [11, 87, 89, 91, 99, 102, 103], "smaller": [11, 26, 44, 45, 46, 51, 91, 92], "fit": [11, 25, 90, 92], "adjust": [11, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53], "attribut": [11, 98, 99, 106, 107], "accordingli": 11, "get_model": 11, "vocab_s": 11, "4096": [11, 89], "num_lay": 11, "num_head": 11, "num_kv_head": 11, "embed_dim": 11, "2048": [11, 89], "max_seq_len": 11, "train_loop": 11, "sgd": 11, "lr": [11, 89], "001": 11, "momentum": [11, 105], "9": [11, 89], "weight_decai": 11, "1e": [11, 89], "5": [11, 49, 60, 80, 83, 89, 91, 93, 95, 99, 102, 104, 105], "loss_fn": 11, "crossentropyloss": [11, 104, 105], "i": [11, 78, 89, 93, 95, 103, 104, 105], "rang": [11, 49, 89, 95, 96, 104, 105], "randint": 11, "loss": [11, 89, 95, 104, 105], "backward": [11, 39, 89, 95, 105], "zero_grad": [11, 89, 105], "next": [11, 89, 90, 96, 104, 105, 106, 107], "scheme": [11, 47, 48, 60, 61, 93, 103], "although": [11, 60, 61, 69, 82, 98], "integ": [11, 12, 30, 31, 44, 45, 49, 52, 54, 56, 57, 59, 78, 96, 104, 105, 106], "arithmet": 11, "float32": [11, 12, 28, 56, 59, 65, 67, 69, 70, 73, 77, 92, 93, 95, 96, 98, 106, 107, 108], "becaus": [11, 21, 89, 90, 92, 95, 98, 105, 108], "fakequantizeconfig": [11, 60, 61, 62, 68], "intxquantizationawaretrainingconfig": 11, "insert": [11, 91, 96, 103, 104, 105, 106, 107, 108], "swap": [11, 38, 63, 65, 89, 90, 95, 96, 105], "fakequantizedlinear": [11, 63, 64, 74, 75], "activation_config": [11, 61, 68], "per_token": [11, 59, 61, 68], "is_symmetr": [11, 59, 61, 68], "weight_config": [11, 60, 61, 68], "qat_config": 11, "structur": [11, 24, 86, 91, 92, 95, 98, 104], "attun": 11, "benefici": 11, "later": [11, 90, 98, 104, 105, 107], "int8dynamicactivationint4weightconfig": [11, 73], "fromintxquantizationawaretrainingconfig": 11, "back": [11, 64, 98], "subclass": [11, 12, 20, 38, 53, 60, 61, 69, 82, 86, 91, 92, 95], "readi": [11, 89, 91, 93, 96, 98, 105], "did": [11, 46], "altern": [11, 59, 90, 96, 98, 106, 107], "legaci": 11, "offer": [11, 98, 104], "customiz": [11, 76], "unlik": [11, 96], "int8dynactint4weightqatquant": 11, "qat_quant": 11, "int8dynactint4weightqatlinear": 11, "int8dynactint4weightlinear": 11, "fraction": 11, "therebi": 11, "significantli": [11, 103, 104, 106, 107], "footprint": 11, "extens": [11, 98, 104, 106], "addition": [11, 106, 107], "frozen": 11, "further": [11, 90, 98, 103, 104, 105, 106], "nf4": [11, 22], "propos": [11, 83], "express": [11, 91, 98, 103, 104, 105, 108], "nf4tensor": 11, "cleanli": 11, "simpli": [11, 53, 95, 96, 98], "to_nf4": 11, "frozennf4linear": 11, "in_dim": 11, "out_dim": 11, "bool": [11, 12, 30, 35, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 55, 59, 60, 61, 67, 69, 70, 72, 73, 75, 76, 79, 86, 96], "quantization_kwarg": 11, "No": [11, 90, 92, 95], "requires_grad_": 11, "nf4_weight": 11, "requires_grad": [11, 90, 96, 98, 99], "though": [11, 98], "shown": [11, 93, 95, 105, 108], "competit": [11, 89], "baselin": [11, 89, 93, 104], "while": [11, 60, 61, 69, 82, 83, 93, 95, 98, 103, 104, 108], "even": [11, 12, 89, 95, 108], "newer": 11, "mxfp4": 11, "nvfp4": 11, "blackwel": 11, "reap": 11, "benefit": [11, 95, 98, 104, 107], "vari": [11, 104, 105, 106, 107], "tradeoff": [11, 95], "incorpor": 11, "its": [11, 44, 95, 98, 99, 104, 108], "loralinear": 11, "lora_finetune_single_devic": 11, "3b_qlora_single_devic": 11, "limit": [11, 89, 98, 99, 104], "yet": [11, 46, 50, 98, 99, 105, 106, 107], "invok": [11, 106], "loraconfig": 11, "get_peft_model": 11, "automodelforcausallm": [11, 93, 99], "torchaoconfig": [11, 93, 99], "int8weightonlyconfig": [11, 99], "base_model": 11, "quantization_config": [11, 93, 99, 107], "peft_config": 11, "throughput": [11, 89, 93], "increas": [11, 95, 104], "torchtitan": 11, "enable_fp8_train": 11, "fp8_recipe_nam": 11, "tensorwis": [11, 34, 35], "initi": [11, 12, 71, 90, 91, 92, 105], "experi": [11, 89, 107], "saw": 11, "experiment_nam": 11, "tok": 11, "peak_mem_reserv": 11, "6502": 11, "143": 11, "000": 11, "30": [11, 89, 91, 104], "090": 11, "fp8_nonam": 11, "7205": 11, "386": 11, "816": 11, "010": 11, "266": 11, "fp8_tensorwis": 11, "7222": 11, "198": 11, "11": [11, 89], "074": [11, 89], "fp8_rowwis": 11, "6387": 11, "968": 11, "756": 11, "29": [11, 89], "158": 11, "096": 11, "fp8_rowwise_with_gw_hp": 11, "7573": 11, "698": 11, "480": 11, "516": 11, "908": 11, "hellaswag_acc": 11, "wikitext_word_perplex": 11, "533": 11, "12": [11, 89, 107, 108], "407": [11, 89], "414": 11, "007": 11, "412": 11, "005": 11, "420": 11, "013": [11, 89], "534": 11, "416": 11, "009": 11, "tensor_impl": [12, 20, 90, 96], "aqttensorimpl": [12, 20], "block_siz": [12, 18, 20, 22, 27, 28, 30, 31, 32, 33, 54, 55, 56, 77, 91, 96], "tupl": [12, 20, 22, 27, 28, 30, 31, 32, 41, 42, 54, 55, 56, 71, 77, 83, 98, 99, 104, 105, 108], "quant_min": [12, 20, 30, 31, 32, 49, 54, 55, 56, 77, 90, 91, 98, 107, 108], "union": [12, 20, 35, 41, 42, 54, 56, 59, 76, 77], "quant_max": [12, 20, 30, 31, 32, 49, 54, 55, 56, 77, 90, 91, 98, 107, 108], "zero_point_domain": [12, 20, 30, 31, 32, 45, 54, 55, 59], "zeropointdomain": [12, 20, 30, 31, 32, 45, 54, 55, 59], "stride": [12, 20, 90, 98], "sourc": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 93, 100, 102], "quantized_tensor": 12, "float_tensor": [12, 98], "scale": [12, 18, 21, 28, 31, 36, 39, 42, 49, 52, 54, 55, 56, 57, 59, 63, 70, 71, 72, 73, 77, 79, 80, 90, 95, 96, 98, 99, 108], "zero_point": [12, 18, 31, 45, 52, 54, 55, 56, 77, 90, 95, 96, 98, 108], "happen": [12, 20, 53, 90, 98, 104, 106], "choose_qparam": [12, 90], "dequant": [12, 20, 22, 45, 56, 90, 91, 98, 99, 104, 106, 107, 108], "ao": [12, 20, 95, 99], "three": [12, 53, 83, 86, 90, 106, 107], "choose_qparams_affin": [12, 45, 55, 90], "quantize_affin": [12, 45, 90], "qand": 12, "dequantize_affin": [12, 45], "extern": [12, 106], "regardless": 12, "intern": [12, 25], "represent": [12, 18, 29, 45, 90, 95, 99, 104, 108], "orient": 12, "field": [12, 59, 108], "storag": [12, 21, 90, 95], "store": [12, 21, 22, 26, 47, 82, 90, 95, 99, 104, 105], "plain": [12, 99], "int_data": [12, 98], "format": [12, 21, 22, 40, 44, 90, 93, 95, 104, 105, 108], "kernel": [12, 14, 15, 17, 21, 25, 40, 44, 45, 72, 76, 91, 93, 95, 103, 106, 107], "granular": [12, 36, 41, 42, 44, 45, 46, 48, 51, 54, 56, 59, 60, 61, 62, 63, 77, 89, 90, 93, 96, 99], "element": [12, 24, 26, 53, 54, 56, 63, 70, 72, 73, 77, 95], "share": [12, 54, 56, 77, 95], "qparam": [12, 54, 56, 77], "minimum": [12, 53, 54, 56, 77], "deriv": [12, 55, 77], "maximum": [12, 54, 56, 77, 79], "zero": [12, 24, 45, 47, 54, 56, 59, 70, 71, 72, 73, 83, 95, 96, 108], "subtract": [12, 22], "unquant": [12, 108], "given": [12, 20, 33, 89, 95, 99, 108], "classmethod": [12, 20, 96, 98, 99], "from_hp_to_floatx": 12, "input_float": [12, 20, 27, 28, 29, 30, 31, 32, 81], "target_dtyp": [12, 27, 28, 30, 31, 34, 35, 54, 55, 90, 96], "_layout": [12, 20, 27, 28, 29, 30, 31, 32, 90, 91, 96], "layout": [12, 13, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 45, 46, 47, 86, 95], "scale_dtyp": [12, 27, 28, 30, 54, 55, 96], "float8": [12, 15, 16, 27, 28, 34, 35, 36, 37, 38, 39, 41, 42, 43, 63, 87, 90, 93, 96], "from_hp_to_floatx_stat": 12, "from_hp_to_fpx": 12, "floatx": [12, 29, 90], "ebit": [12, 29, 40], "mbit": [12, 29, 40], "float1": [12, 29], "float7": [12, 29], "from_hp_to_intx": [12, 20], "mapping_typ": [12, 30, 46, 54, 55, 59], "mappingtyp": [12, 30, 46, 47, 54, 55, 59, 96], "ep": [12, 30, 54, 55, 59, 96, 105, 107, 108], "zero_point_dtyp": [12, 30, 54, 55, 96], "preserve_zero": [12, 30, 45, 54, 55], "plainlayout": [12, 30, 31, 46, 47, 96], "use_hqq": [12, 30, 45, 51, 99], "from_hp_to_intx_stat": 12, "correct": [12, 21, 104, 105], "otherwis": [12, 48, 58, 59, 90, 105], "desir": [12, 53, 96], "non_block": 12, "memory_format": [12, 106, 107], "preserve_format": 12, "attempt": 12, "asynchron": 12, "respect": [12, 95, 105], "host": [12, 99], "behavior": [12, 18, 58, 99, 104, 105], "pin": 12, "pageabl": 12, "howev": [12, 95, 99, 105, 108], "caution": 12, "advis": [12, 90], "good": [12, 91, 98, 108], "pin_memori": 12, "match": [12, 56, 57, 72, 73, 95, 104], "float64": 12, "5044": 12, "0005": 12, "3310": 12, "0584": 12, "cuda0": 12, "blocksiz": 13, "64": [13, 33, 45, 51, 63, 92, 93, 96, 98, 99], "block": [13, 22, 83, 95], "variabl": [13, 16, 25, 26, 83, 95], "cutlass": [14, 15], "mm_config": [16, 41, 42], "float8mmconfig": [16, 41, 42], "tinygemm": [17, 45, 72, 76, 90, 91], "_weight_int4pack_mm_for_cpu": [17, 45], "least": 17, "6": [17, 59, 89, 90, 91, 93, 95, 104, 105, 106], "It": [18, 21, 23, 25, 39, 91, 95, 98, 108], "post": [18, 25, 87, 91, 98, 105, 108], "design": [18, 21, 24, 93, 99, 103, 104, 108], "extend": [18, 90, 95, 106], "conjunct": 18, "tensorimpl": 18, "interact": [18, 90, 104], "qqq": [19, 20, 32], "marlinqqq": 20, "inherit": [20, 23, 98, 99, 106, 107], "_choose_qparams_and_quantize_affine_qqq": 20, "_dequantize_affine_qqq": 20, "pattern": [21, 24, 90, 91, 99, 103, 104], "preprocess": [21, 24], "manag": 21, "pre_process": 21, "1\u00ba": 21, "transpos": [21, 90, 98], "sinc": [21, 60, 61, 69, 82, 90, 92, 93, 95, 96, 98, 104, 105, 106, 107, 108], "2\u00ba": 21, "inject": 21, "3\u00ba": 21, "again": [21, 22, 95, 104, 108], "dim": [21, 96, 98, 99, 104, 105], "tensor_meta": 22, "subclasstensorarg": 22, "n_block": 22, "scaler_block_s": [22, 33], "quantized_scal": 22, "quantization_factor": 22, "scaler_mean": 22, "quantized_data": [22, 99], "qlora": [22, 87, 93], "convert_to_norm_float_weight": 22, "normal": [22, 33, 53, 95, 104, 105], "dequantize_scal": 22, "unpack": [22, 90], "doubl": 22, "scaler": 22, "per_scaler_block": 22, "factor": [22, 57, 80, 89, 95], "inpt_weight": 22, "double_quantize_scal": 22, "calcul": [22, 39, 49, 54, 55, 79, 90, 95, 104, 108], "absmax": 22, "posit": 22, "per_block": 22, "int16": [22, 104], "n_scaler_block": 22, "get_original_weight": 22, "quantize_tensor_nearest": 22, "float16": [22, 77, 95], "nearest": 22, "round": [22, 49, 98], "metadata": [23, 90, 93, 98, 99], "semi": [24, 86, 95], "everi": [24, 60, 61, 69, 82, 95, 98, 104, 105], "four": [24, 103], "prune": [24, 83], "conform": 24, "inner_k_til": [25, 45, 66, 72, 91], "core": [25, 50, 90, 96, 99, 104], "tile": [25, 90], "affect": [25, 95], "matmul": [25, 43, 90, 95, 98], "pack_dim": [26, 51], "uintx": [26, 51, 90], "standard": [26, 90, 99], "byte": [26, 40, 51], "uintxtensor": 26, "determin": [26, 54, 89, 95, 99], "along": [26, 95, 99, 103], "indic": [26, 52, 95, 108], "last": [26, 89, 103], "256": [33, 45, 65, 66, 67, 72, 73, 93, 104, 105, 108], "scaling_typ": [34, 35], "scalingtyp": [34, 35], "scaling_granular": [34, 35], "scalinggranular": [34, 35], "cast_config_input": 35, "castconfig": 35, "cast_config_input_for_grad_weight": 35, "cast_config_weight": 35, "cast_config_weight_for_grad_input": 35, "cast_config_grad_output": 35, "cast_config_grad_output_for_grad_weight": 35, "gemm_config_output": 35, "float8gemmconfig": 35, "use_fast_accum": 35, "gemm_config_grad_input": 35, "gemm_config_grad_weight": 35, "enable_fsdp_float8_all_gath": 35, "pad_inner_dim": 35, "emul": 35, "force_recompute_fp8_weight_in_bwd": 35, "round_scales_to_power_of_2": 35, "from_recipe_nam": 35, "recipe_nam": [35, 89], "float8linearrecipenam": 35, "qualnam": [36, 37, 49, 50, 52], "boundari": [36, 37, 49, 50, 52], "strategi": 36, "module_filter_fn": [38, 89], "callabl": [38, 53, 76, 81, 86, 99], "float8linearconfig": 38, "float8linear": [38, 89], "instanc": [38, 60, 61, 69, 76, 82, 86, 92, 98, 104, 106, 107, 108], "fqn": [38, 83, 86, 89, 96], "sum": [39, 104, 105], "set_inductor_config": [40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53], "sub": [40, 51, 95], "expon": 40, "mantissa": 40, "fp6_e3_m2": 40, "fp6_e2_m3": 40, "fp6": 40, "llm": 40, "arxiv": [40, 83, 95], "org": [40, 53, 83, 90, 91, 93, 95, 107], "ab": [40, 83, 95], "2401": 40, "14112": 40, "repo": 40, "usyd": 40, "fsalab": 40, "fp6_llm": 40, "renam": [40, 104, 105], "fpxtensorcoreaqttensorimpl": 40, "experiment": [40, 103], "merg": 40, "to_affine_quantized_floatx": 40, "activation_dtyp": [41, 42], "float8_e4m3fn": [41, 42, 43, 90], "weight_dtyp": [41, 42, 43, 93], "pertensor": [41, 42, 96], "perrow": [41, 42, 93], "list": [41, 53, 56, 58, 80, 83, 90, 91, 98, 99, 103, 105, 108], "fast": [41, 42, 95], "accumul": [41, 42], "torchinductor": [41, 42, 43, 44, 45, 46, 47, 48, 51, 106, 107], "float8_e4m": 42, "channel": [43, 47, 48, 59, 63, 65, 66, 67, 69, 70, 72, 73, 82, 96, 107], "128": [44, 45, 89, 93, 96, 98, 99, 107, 108], "packing_bitwidth": 44, "weight_onli": 44, "gemlit": 44, "triton": [44, 90, 106, 107], "associ": [44, 96], "fp16": [44, 54], "control": [44, 45, 46, 47, 48, 51, 83, 95, 99, 104], "grain": [44, 45, 46, 51, 98], "impact": [44, 53, 89, 93, 99], "hardwar": [44, 90, 91, 93, 95], "runtim": [44, 90, 104], "tensorcoretiledlayout": [45, 90, 91], "tensor_core_til": [45, 90], "int4mm": [45, 91], "aten": [45, 90, 91, 98, 99, 103, 104, 105, 106, 107], "_weight_int4pack_mm": [45, 90], "tradit": 45, "exactli": [45, 98], "chosen": [45, 95], "choic": 45, "hqq": [45, 51, 90], "preserv": [45, 54, 83, 93, 95, 103], "Will": 45, "act_mapping_typ": [46, 47], "produc": [46, 91, 103, 104, 105, 106, 107], "backend": [46, 87, 91, 93, 95, 108], "marlinqqqlayout": 46, "cutlassint4packedlayout": 46, "weight_only_decod": 47, "around": [47, 89, 91, 92, 104], "decod": [47, 93], "better": [47, 48, 89, 98, 104, 105, 106, 107, 108], "number": [49, 51, 53, 63, 70, 72, 73, 83, 93, 95, 98, 105, 106], "sai": [49, 77, 90, 99, 108], "3": [49, 53, 60, 77, 87, 89, 90, 91, 95, 102, 104, 105], "7": [49, 89, 93, 106, 107], "symmetric_no_clipping_err": 49, "variant": [49, 55, 98], "smin": 49, "smax": 49, "min_val_neg": [49, 98], "max_val_po": [49, 98], "By": [49, 95], "individu": [49, 95], "less": [49, 95, 98, 104], "neg": 49, "placehold": [50, 107], "uint1": [51, 90], "uint7": [51, 90], "enum": 52, "quantized_v": 52, "float_val": 52, "mid_point": 52, "example_input": [53, 71, 91, 92, 96, 103, 104, 105, 106, 107, 108], "qtensor_class_list": 53, "aqdefaultlinearweight": 53, "aqint8weightonlyquantizedlinearweight": 53, "aqint8weightonlyquantizedlinearweight2": 53, "aqint8dynamicallyquantizedlinearweight": 53, "filter_fn": [53, 76, 86], "interpol": 53, "85": 53, "manual": [53, 105], "supress_autoquant_error": 53, "min_sqnr": 53, "aq_kwarg": 53, "autoquant": 53, "identifi": [53, 96, 108], "fastest": 53, "over": [53, 89, 95, 104, 105], "potenti": [53, 95, 96, 103, 104, 106, 107], "qtensor": 53, "search": [53, 95], "whose": [53, 108], "exchang": 53, "autoquantizablelinearweight": 53, "calibr": [53, 55, 91, 103, 105, 106, 107], "seen": 53, "record": [53, 90, 96], "final": [53, 76, 90, 91, 95, 103, 104, 105, 106, 107, 108], "benchmark": [53, 79, 87, 89, 91, 103, 106, 107], "member": 53, "pick": 53, "highli": 53, "had": [53, 98, 104], "proce": 53, "combin": [53, 59, 93, 95, 98, 104, 106], "finalize_autoqu": 53, "log": [53, 98], "fulli": [53, 76, 80, 86, 93, 95, 104], "unless": [53, 99], "default_autoquant_class_list": 53, "second": [53, 57, 89, 90, 102, 108], "stop": 53, "wait": [53, 90], "sever": [53, 89, 99, 103, 108], "automat": [53, 89, 93, 98, 99, 102], "suppress": 53, "accept": [53, 93, 108], "signal": 53, "nois": 53, "ration": 53, "wikipedia": 53, "wiki": 53, "noise_ratio": 53, "v": [53, 108], "non": [53, 90, 95, 98, 103, 106, 107], "caus": [53, 89], "too": 53, "larg": [53, 93, 98, 106], "resaon": 53, "40": [53, 89], "keyword": [53, 59], "example_input1": 53, "example_input2": 53, "int32": [54, 59, 65, 69, 70, 90, 91, 104, 108], "fp32": [54, 56, 59, 73, 96, 98, 104, 106], "optioanl": 54, "param": [54, 55, 83, 93], "request": [54, 56, 77], "min_val": [55, 90, 98], "max_val": [55, 90, 98], "observ": [55, 82, 95, 96, 103, 104, 105, 106, 107, 108], "obtain": 55, "track": [55, 90, 99], "input_dtyp": 56, "output_dtyp": [56, 69, 77], "uint8": [56, 77, 90, 96, 108], "b": 57, "scales1": 57, "multipli": [57, 78, 95], "rais": [57, 68, 78, 89, 98, 99], "assertionerror": [57, 78, 89, 98], "expect": [57, 89, 95, 98, 103, 104, 106, 107, 108], "twostepquant": 58, "easili": [58, 103], "thei": [58, 89, 90, 91, 95, 98, 104, 105, 108], "constructor": [58, 98], "must": [58, 59, 73, 89, 95, 99, 105, 107, 108], "embed": [58, 60, 65, 68, 69, 70], "undefin": [58, 83], "my_quant": 58, "qatquantizer1": 58, "qatquantizer2": 58, "qatquantizer3": 58, "torchaodtyp": 59, "scale_precis": [59, 63, 65, 69, 70], "zero_point_precis": [59, 65, 69, 70], "is_dynam": [59, 106, 107, 108], "range_learn": 59, "simul": [59, 84, 90, 95], "older": 59, "int1": [59, 90], "int7": 59, "pergroup": [59, 93], "pertoken": 59, "per_channel": 59, "peraxi": [59, 93, 96], "per_group": [59, 77], "leav": 59, "empti": [59, 90], "properti": [59, 62, 90], "throw": 59, "els": [59, 93, 99, 104, 105], "num_embed": [60, 69, 70], "embedding_dim": [60, 69, 70], "padding_idx": [60, 69, 70], "max_norm": [60, 69, 70], "norm_typ": [60, 69, 70], "scale_grad_by_freq": [60, 69, 70], "fq_embed": 60, "longtensor": 60, "overridden": [60, 61, 69, 82], "within": [60, 61, 69, 82, 93, 95, 99, 106, 107], "afterward": [60, 61, 69, 82], "former": [60, 61, 69, 82], "care": [60, 61, 69, 82, 92, 95, 104], "hook": [60, 61, 69, 82, 90], "latter": [60, 61, 69, 82, 105], "silent": [60, 61, 69, 82, 106], "ignor": [60, 61, 69, 82, 89, 104, 105], "in_featur": [61, 72, 73, 89, 91, 92, 96, 98], "out_featur": [61, 72, 73, 89, 91, 96, 98], "fq_linear": 61, "symmetri": 62, "rowwis": 63, "fakequantizedembed": 64, "model_with_fake_quantized_linear": 64, "int4weightonlyqatembed": 65, "int4weightonlyembed": 65, "scales_precis": [66, 67, 72, 73], "padding_allow": 67, "valueerror": 68, "fakequant": 71, "weightonlyint4linear": 72, "hardcod": [73, 90, 108], "mod": [74, 75, 89, 95, 98], "disabl": [74, 98, 105], "inplac": [76, 83, 91], "qualifi": [76, 80, 86, 95], "move": [76, 90, 96, 99, 105, 106], "predefin": [76, 108], "execut": [76, 94, 98, 101], "int8_dynamic_activation_int4_weight": 76, "int8_dynamic_activation_int8_weight": [76, 86], "mm": [76, 98, 104], "int4_weight_onli": 76, "int8_weight_onli": 76, "sequenti": [76, 86, 89], "tabl": [77, 89, 90, 95], "per_tensor": 77, "per_axi": 77, "axi": [77, 96], "mat2": 78, "safe": 78, "consid": [78, 90, 95], "cubla": 78, "fallback": [78, 99], "j": 78, "debug_skip_calibr": 79, "smoothquant": [79, 80, 103], "smoothfakedynamicallyquantizedlinear": [79, 80], "skip_fqn_list": 80, "cur_fqn": 80, "alpha": 80, "skip": [80, 83, 95], "being": [80, 89, 90, 95, 99, 106, 107], "input_quant_func": [81, 90], "quant_kwarg": 81, "dict": [81, 83, 98, 99, 107, 108], "l2": [82, 95], "norm": [82, 83, 95], "buffer": 82, "x_orig": 82, "sparsity_level": [83, 95], "semi_structured_block_s": 83, "wanda": 83, "sparsifi": [83, 87, 92, 95], "2306": 83, "11695": 83, "awar": [83, 87, 91, 95, 98], "product": [83, 93, 99, 106, 108], "magnitud": [83, 95], "parametr": 83, "deepcopi": [83, 91, 96, 98, 105], "squash_mask": [83, 95], "params_to_keep": 83, "params_to_keep_per_lay": 83, "squash": 83, "mask": [83, 95], "appropri": [83, 90, 103, 104, 105, 106, 107], "sparse_param": 83, "attach": [83, 95, 108], "kei": [83, 95, 102], "xdoctest": 83, "local": [83, 93, 95], "don": [83, 89, 91, 95, 99, 108], "t": [83, 89, 90, 91, 95, 96, 98, 99, 104, 105, 108], "hasattr": [83, 99], "submodule1": 83, "linear1": [83, 91, 92, 96, 98], "foo": [83, 104], "bar": [83, 104], "submodule2": 83, "linear42": 83, "baz": 83, "42": [83, 96], "24": 83, "ones": [83, 90, 105], "update_mask": 83, "tensor_nam": [83, 99], "statist": [83, 90, 95, 96, 104, 105], "retriev": 83, "act_per_input": 83, "Then": [83, 98, 107, 108], "across": [83, 93, 95, 98, 99], "whole": [83, 108], "alia": [85, 99], "semisparseweightconfig": 85, "sparsify_": 86, "apply_tensor_subclass": [86, 90], "essenti": [86, 99, 103], "semi_sparse_weight": 86, "semisparselayout": 86, "sparsemarlinlayout": 86, "isinst": [86, 89, 95, 96, 98, 99, 105, 108], "sparse_api": 86, "librari": [87, 92], "gradient": [87, 95], "nativ": [87, 89, 98, 104], "introduct": [87, 90, 93], "highlight": [87, 98, 102], "guid": [87, 90, 93, 103], "contributor": [87, 91], "part": [87, 90, 95, 98, 105], "tune": [87, 89, 93, 95, 103], "vllm": 87, "sglang": 87, "serial": [87, 90, 104, 105], "write": [87, 91, 103, 104, 105], "advanc": [87, 96, 98, 103, 106, 107], "export": 87, "x86": [87, 91], "intel": [87, 103, 106], "openvino": [87, 91], "5x": 89, "cluster": 89, "34": 89, "43x": 89, "2k": 89, "h200": 89, "latest": [89, 91], "offic": 89, "offici": 89, "popular": [89, 90], "flagship": 89, "common": [89, 90, 95], "form": [89, 90, 95], "quickli": [89, 98], "batteri": 89, "includ": [89, 90, 98, 103, 106, 107, 108], "commonli": [89, 95], "fork": 89, "build": [89, 90, 95, 98, 99, 104], "top": [89, 90, 98, 103, 104, 105, 106, 107], "re": [89, 92, 93, 98, 104, 105], "virtual": 89, "environ": [89, 93], "conda": 89, "venv": 89, "download": [89, 91, 93, 100, 102, 104, 105, 107], "job": 89, "root": [89, 93], "launch": 89, "ngpu": 89, "config_fil": 89, "train_config": 89, "llama3_8b": 89, "toml": 89, "run_train": 89, "sh": [89, 93], "hyperparamet": 89, "edit": [89, 93], "line": [89, 95], "flag": [89, 105], "termin": 89, "rank0": 89, "titan": 89, "2025": 89, "06": 89, "04": 89, "08": 89, "51": 89, "48": 89, "info": 89, "2254": 89, "27": 89, "34gib": 89, "28": 89, "78": 89, "tp": [89, 99], "375": 89, "tflop": 89, "21": 89, "73": [89, 96], "mfu": 89, "20": [89, 93, 105], "58": 89, "557": 89, "7069": 89, "99gib": 89, "62": 89, "034": 89, "35": [89, 93, 96], "41": [89, 93], "19": 89, "52": 89, "224": [89, 96, 103, 104, 105, 106, 107], "9196": 89, "022": 89, "406": [89, 104, 105], "65": 89, "904": 89, "1423": 89, "014": 89, "23": [89, 96], "As": [89, 90, 104, 108], "warmup": 89, "7k": 89, "99gb": 89, "peak": [89, 93], "against": 89, "02": 89, "37": 89, "404": 89, "2611": 89, "22gib": 89, "595": 89, "47": 89, "49": [89, 96], "027": 89, "4260": 89, "89gib": 89, "344": 89, "367": 89, "39": 89, "03": 89, "01": 89, "988": 89, "9482": 89, "321": 89, "366": 89, "14": 89, "991": 89, "1183": 89, "300": 89, "364": 89, "89": 89, "4659": 89, "291": 89, "84": 89, "769": 89, "gc": 89, "peform": 89, "period": 89, "collect": [89, 90, 95], "3k": 89, "89gb": 89, "11x": 89, "nearli": 89, "ident": [89, 95], "performan": 89, "vs": [89, 95, 104, 108], "curv": [89, 95], "omit": [89, 104, 105, 106], "648": 89, "2648": 89, "28gib": 89, "71": 89, "26": 89, "475": 89, "9106": 89, "91gib": 89, "53": [89, 93], "503": 89, "434": 89, "43": 89, "94": [89, 104], "166": 89, "0774": 89, "663": 89, "443": 89, "44": [89, 96], "87": 89, "50": [89, 95, 96, 103, 104, 106, 107], "885": 89, "3233": 89, "643": 89, "442": 89, "66": [89, 93, 96], "76": 89, "613": 89, "6150": 89, "637": 89, "72": [89, 93], "6k": 89, "91gb": 89, "21x": [89, 93], "tl": 89, "dr": 89, "priorit": 89, "accur": [89, 95, 103], "stabil": 89, "cost": [89, 96], "slightli": [89, 98], "outlier": 89, "underflow": 89, "8xh100": 89, "box": [89, 95, 106], "toi": [89, 91, 96, 98, 106], "convert_to_float8_train": 89, "recurs": 89, "kind": [89, 104], "gemm": [89, 106, 107], "snippet": [89, 104, 105], "f": [89, 90, 92, 93, 95, 96, 98, 99, 104, 105], "float8_linear_util": 89, "float8_linear": 89, "torch_version_at_least_2_5": [89, 91], "greater": 89, "sampl": [89, 90, 104, 106, 107], "adamw": 89, "elig": 89, "divis": 89, "_": [89, 96, 99, 103, 104, 105, 106], "label": 89, "purpos": [89, 90, 98, 104], "fake_label": 89, "ones_lik": 89, "mse_loss": 89, "model_state_dict": 89, "state_dict": [89, 92, 104, 105], "optimizer_state_dict": 89, "pth": [89, 104, 105], "explor": [89, 91, 107], "few": [89, 98, 104, 105], "lai": 90, "stack": [90, 93], "awq": 90, "gptq": 90, "codebookquantizedtensor": 90, "float3": 90, "overload": [90, 95], "term": [90, 95, 104, 108], "extra": [90, 93], "dev": 90, "discuss": [90, 98], "1833": 90, "matter": [90, 95], "float3_e2_m0": 90, "float4_e2_m1": 90, "float4_e3_m0": 90, "float5_e2_m2": 90, "float5_e3_m1": 90, "float6_e2_m3": 90, "float6_e3_m2": 90, "float8_e5m2": 90, "float8_e4m3fnuz": 90, "float8_e5m2fnuz": 90, "plan": [90, 105], "float4": 90, "float6": 90, "becom": [90, 104], "uint2": 90, "117208": 90, "outsid": 90, "mention": [90, 104], "criteria": 90, "wide": 90, "adopt": 90, "fundament": [90, 95, 105], "until": 90, "evid": 90, "hopefulli": 90, "amen": 90, "haven": 90, "enough": 90, "ont": 90, "revisit": 90, "intx": 90, "connect": [90, 108], "int4tensor": 90, "previou": [90, 93, 104, 105, 106, 107], "between": [90, 95, 98, 99, 103, 105, 106, 108], "preicison": 90, "mainli": [90, 103, 106, 108], "accommod": 90, "choose_qparams_affine_with_min_max": 90, "min": [90, 96, 98, 104, 108], "int_matmul": 90, "int_scaled_matmul": 90, "reli": [90, 91, 95, 96, 98], "On": [90, 91], "glue": 90, "everyth": 90, "togeth": [90, 93, 104, 106, 108], "construct": [90, 104, 108], "low_precision_v": 90, "high_precision_v": 90, "procedur": 90, "veri": [90, 95, 99, 105], "straightforward": [90, 108], "high_preicsion_v": 90, "especi": [90, 92, 95, 106, 107], "bitwidth": [90, 108], "codebook": 90, "select": [90, 104], "multi": 90, "dimension": [90, 95], "view": [90, 98, 104, 105], "mkldnn": 90, "coo": [90, 95], "sparse_coo": [90, 95], "sparsetensorimpl": 90, "idea": [90, 95], "nice": [90, 95], "concept": [90, 102, 104, 106, 107, 108], "why": [90, 98, 102], "c": [90, 91, 98, 106, 107], "conflict": 90, "quantized_linear": [90, 96, 104], "semant": 90, "stai": [90, 91, 98, 106], "develop": [90, 91, 104, 105, 108], "tradition": 90, "to_affine_quant": 90, "simplic": 90, "explain": [90, 103, 106], "simplest": [90, 95], "easi": [90, 93], "linear_modul": 90, "to_affine_quantized_intx": 90, "to_linear_activation_quant": 90, "quantized_weight": [90, 99], "activation_and_weight_quant": 90, "encount": 90, "input_qunat_func": 90, "redispatch": 90, "fx": [90, 104, 108], "symbolic_trac": 90, "But": [90, 98, 99, 108], "easier": [90, 108], "modif": 90, "figur": [90, 95, 104], "At": [90, 95, 104], "thing": [90, 92, 95, 98, 104], "address": [90, 104], "stat": [90, 105], "averag": [90, 96, 104, 105], "calculate_qparam": [90, 96, 108], "affinequantizedminmaxobserv": [90, 96], "insert_observer_": 90, "observedlinear": [90, 96], "complic": [90, 95, 104], "done": [90, 98], "manner": 90, "autoround": 90, "multitensor": 90, "sure": [90, 93, 108], "describ": [90, 92, 95, 102, 104, 105], "todai": [90, 93], "low_bit_optim": 90, "quantized_train": 90, "progress": [90, 99], "lot": [90, 95], "walk": [90, 96, 98, 102, 103, 106], "int4weightonlyconfig": [90, 91, 92, 99], "_convert_weight_to_int4pack": 90, "tensorcoretiledaqttensorimpl": 90, "_quantized_linear_op": 90, "goe": 90, "_aqt_qlinear_dispatch_t": 90, "dispatch": 90, "explan": 90, "wint4": 90, "stabl": 91, "pip": [91, 93, 103, 104], "nightli": [91, 93], "index": [91, 93, 95, 107], "url": [91, 93, 107], "whl": [91, 93, 107], "cu121": 91, "major": 91, "entri": 91, "mutat": 91, "logic": [91, 98, 99], "toylinearmodel": [91, 92, 96], "linear2": [91, 92, 96, 98], "eval": [91, 92, 93, 96, 103, 105, 106, 107], "faster": [91, 95], "model_bf16": 91, "mix": [91, 93, 103, 106, 107], "tensor_impl_dtyp": 91, "roughli": [91, 95], "quarter": 91, "os": [91, 104, 105], "int4_model": 91, "pt": [91, 93], "bfloat16_model": 91, "int4_model_size_mb": 91, "getsiz": [91, 104, 105], "bfloat16_model_size_mb": 91, "2f": [91, 104, 105], "mb": [91, 92, 94, 101, 104, 105], "00": [91, 94, 101], "benchmark_model": 91, "temporari": 91, "workaround": [91, 99], "num_run": 91, "100": [91, 98, 104, 105], "_dynamo": [91, 98], "reset": [91, 104, 105], "bf16_time": 91, "int4_tim": 91, "time": [91, 95, 98, 102, 103, 104, 105], "3f": [91, 105], "ms": 91, "1fx": 91, "393": 91, "410": 91, "9x": 91, "recogn": [91, 108], "decis": 91, "pt2e": [91, 103, 104, 105, 106, 107], "fuse": [91, 95, 98, 105], "deleg": [91, 104], "x86inductorquant": [91, 106, 107], "quantize_pt2": [91, 103, 104, 105, 106, 107], "prepare_pt2": [91, 103, 104, 106, 107], "x86_inductor_quant": [91, 106], "get_default_x86_inductor_quantization_config": [91, 106], "float_model": [91, 98, 103, 104, 105, 106, 107], "data_load": [91, 104, 105, 106, 107], "no_grad": [91, 98, 103, 104, 105, 106, 107], "imag": [91, 103, 104, 105, 106, 107], "program": [91, 104, 105, 106, 108], "captur": [91, 104, 105, 108], "expos": [91, 104, 105], "set_glob": [91, 104, 105, 106, 107], "xiq": [91, 106], "prepare_qat_pt2": [91, 105, 106], "sample_inference_data": 91, "convert_pt2": [91, 103, 104, 105, 106, 107], "wrapper": [91, 98, 106], "_inductor": [91, 106], "cpp_wrapper": [91, 106], "optimized_model": [91, 103, 106, 107], "converted_model": [91, 106, 107], "xpu": [91, 107], "simpl": [91, 95, 96, 98, 103, 106, 107], "visit": 91, "would": [91, 95, 98, 105, 107], "forget": 91, "tempfil": 92, "get_model_size_in_byt": 92, "ref": [92, 104], "namedtemporaryfil": 92, "seek": [92, 95], "load": [92, 93, 99], "m_load": 92, "load_state_dict": [92, 104, 105], "assign": 92, "assert": [92, 96, 98, 99, 108], "equal": [92, 95], "float_weight1": 92, "float_weight2": 92, "quantized_weight1": 92, "quantized_weight2": 92, "go": [92, 98, 102, 108], "techinqu": 92, "reduct": [92, 93, 95, 98], "4x": [92, 93], "0625": 92, "reason": [92, 95], "avoid": [92, 95], "deploi": 93, "underli": [93, 98], "engin": 93, "seamlessli": [93, 98, 106, 107], "seamless": [93, 106], "git": 93, "cu126": 93, "acceler": [93, 95], "float8dynamicactivationfloat8weightconfig": 93, "phi": 93, "autotoken": 93, "model_id": 93, "microsoft": 93, "quant_config": 93, "quant_typ": [93, 99], "quantized_model": [93, 98, 103, 104, 105], "device_map": [93, 99], "auto": [93, 99], "torch_dtyp": [93, 99], "push": [93, 95, 99], "hub": [93, 99], "user_id": 93, "your_user_id": 93, "model_nam": [93, 103, 106, 107], "save_to": 93, "safe_seri": [93, 99], "hf": 93, "signific": [93, 95], "wheel": 93, "ai": 93, "hug": 93, "face": [93, 95, 104], "server": [93, 99], "o3": 93, "client": 93, "curl": 93, "localhost": 93, "8000": 93, "v1": 93, "chat": 93, "h": 93, "content": 93, "applic": 93, "messag": 93, "role": 93, "give": [93, 95, 98], "me": 93, "short": 93, "languag": 93, "temperatur": 93, "top_p": 93, "95": 93, "top_k": 93, "max_token": 93, "32768": 93, "vram": 93, "15x": 93, "2x": [93, 95], "littl": [93, 99], "packag": 93, "pipelin": 93, "random": [93, 95, 104, 105], "manual_se": [93, 104, 105], "model_path": 93, "trust_remote_cod": 93, "assist": 93, "eat": 93, "banana": 93, "dragonfruit": 93, "smoothi": 93, "blend": 93, "milk": 93, "honei": 93, "salad": 93, "slice": [93, 99], "lemon": 93, "juic": 93, "solv": [93, 95, 98], "equat": 93, "pipe": 93, "text": 93, "generation_arg": 93, "max_new_token": 93, "500": 93, "return_full_text": 93, "do_sampl": 93, "generated_text": 93, "finetun": 93, "lm_head": 93, "those": [93, 95, 96, 98], "ti": 93, "autoprocessor": 93, "modeling_util": 93, "find_tied_paramet": 93, "untied_model": 93, "getattr": [93, 99], "get_text_config": 93, "tie_word_embed": 93, "setattr": [93, 98], "_tied_weights_kei": 93, "clone": [93, 99], "save_to_local_path": 93, "int8dynamicactivationintxweightconfig": 93, "ve": [93, 95], "intxweightonlyconfig": 93, "modulefqntoconfig": [93, 99], "untied_model_id": 93, "untied_model_local_path": 93, "embedding_config": 93, "linear_config": 93, "weight_granular": 93, "weight_scale_dtyp": 93, "_default": [93, 99], "embed_token": 93, "include_embed": 93, "untie_embedding_weight": 93, "modules_to_not_convert": 93, "pte": 93, "cd": 93, "install_requir": 93, "phi_4_mini": 93, "convert_weight": 93, "pytorch_model": 93, "bin": 93, "pytorch_model_convert": 93, "export_llama": 93, "kv": 93, "use_sdpa_with_kv_cach": 93, "get_bos_id": 93, "199999": 93, "get_eos_id": 93, "200020": 93, "max_seq_length": 93, "max_context_length": 93, "output_nam": 93, "phi4": 93, "phone": 93, "io": 93, "2gb": 93, "iphon": 93, "pro": [93, 95], "17": 93, "sec": 93, "maintain": [93, 95], "test": [93, 102, 104, 106], "lm": 93, "har": 93, "eleutherai": 93, "lm_eval": 93, "model_arg": 93, "pretrain": [93, 95, 103, 104, 105, 106], "reset_peak_memory_stat": 93, "prompt": 93, "hei": 93, "consciou": 93, "templated_prompt": 93, "apply_chat_templ": 93, "add_generation_prompt": 93, "templat": [93, 94, 100, 101], "return_tensor": 93, "generated_id": 93, "output_text": 93, "batch_decod": 93, "skip_special_token": 93, "clean_up_tokenization_spac": 93, "respons": 93, "len": [93, 99, 104, 105, 108], "mem": [93, 94, 101], "max_memory_reserv": 93, "1e9": 93, "02f": 93, "gb": 93, "hello": 93, "ye": 93, "am": 93, "digit": 93, "70": [93, 96], "91": 93, "benchmark_lat": 93, "vllm_disable_compile_cach": 93, "project": 93, "vllm_use_precompil": 93, "sharegpt": 93, "wget": 93, "anon8231489123": 93, "sharegpt_vicuna_unfilt": 93, "resolv": 93, "sharegpt_v3_unfiltered_cleaned_split": 93, "tree": 93, "num": 93, "benchmark_serv": 93, "16x": 93, "1s": 93, "14x": 93, "num_prompt": 93, "req": 93, "57": [93, 96], "1000": [93, 106], "68": 93, "80": 93, "entir": [93, 104, 105], "ml": 93, "gain": [93, 95, 107], "eas": 93, "alwai": [93, 98], "valid": [93, 99, 108], "trade": [93, 95], "off": [93, 95], "003": [94, 101, 102], "total": [94, 101, 102], "galleri": [94, 100, 102], "tutorials_sourc": 94, "template_tutori": [94, 101, 102], "neural": [95, 103, 106], "network": [95, 98, 103, 106], "latenc": 95, "carefulli": 95, "pai": 95, "low": [95, 98, 103], "price": 95, "f1": 95, "problem": [95, 98], "research": [95, 102], "fragment": 95, "rightfulli": 95, "spent": 95, "compress": [95, 103], "place": [95, 103, 104, 105, 106, 107], "dens": 95, "focu": [95, 98], "realli": 95, "concret": [95, 108], "hope": 95, "modular": 95, "scratch": [95, 102], "minim": [95, 103, 106, 107], "algorthim": 95, "realiz": 95, "theoret": 95, "analog": 95, "fix": [95, 96], "unstructur": 95, "One": [95, 98, 99, 108], "close": 95, "relat": 95, "retrain": 95, "neglig": 95, "area": 95, "agre": 95, "upon": 95, "consensu": 95, "mind": 95, "thought": 95, "subproblem": 95, "satisfi": 95, "independ": 95, "frontend": [95, 106], "arbitrari": 95, "handoff": 95, "piec": 95, "natur": [95, 98, 104, 108], "present": 95, "clear": 95, "contract": 95, "7x": 95, "advantag": 95, "anticip": 95, "mani": [95, 98], "solut": 95, "third": 95, "parti": 95, "to_sparse_semi_structur": 95, "sparsesemistructuredtensor": 95, "weightnormsparsifi": 95, "half": 95, "subnetwork": 95, "sparse_config": 95, "named_modul": 95, "tensor_fqn": 95, "sparse_block_shap": 95, "zeros_per_block": 95, "fakespars": 95, "manipul": 95, "dictionari": 95, "paramer": 95, "parameter": 95, "necessari": [95, 96, 98, 103, 104, 105, 106, 107], "suitabl": [95, 106], "0s": 95, "spot": 95, "definit": [95, 99], "academia": 95, "industri": 95, "often": [95, 98], "interchang": 95, "confus": [95, 104], "distinct": 95, "behind": 95, "doesn": [95, 105, 108], "itself": [95, 98], "loos": 95, "speak": 95, "tightli": 95, "coupl": [95, 98], "csc": 95, "fbgemm": 95, "qnnpack": 95, "descript": [95, 103], "coordin": 95, "vector": [95, 106], "locat": 95, "bsr": 95, "sparse_bsr": 95, "except": [95, 98, 108], "scalar": [95, 104], "csr": 95, "sparse_csr": 95, "sparse_csc": 95, "column": 95, "compact": 95, "sparse_matrix": 95, "1d": 95, "indexptr": 95, "\u00bd": 95, "bitmask": 95, "2bit": 95, "unprun": 95, "quit": [95, 98], "broken": 95, "down": 95, "sensit": 95, "effect": [95, 96, 98, 106, 107, 108], "best": [95, 106], "subsequ": [95, 98, 106, 107], "infinit": 95, "lost": 95, "degre": 95, "drop": 95, "proxi": 95, "aforement": 95, "smallest": 95, "absolut": 95, "global": [95, 98], "scope": 95, "impli": 95, "con": 95, "span": 95, "threshold": 95, "complex": 95, "constant": [95, 98, 104], "ctr_mobile_fe": 95, "score": 95, "w": [95, 99], "tenosr": 95, "udpat": 95, "cannot": [95, 96, 99], "histori": 95, "regrow": 95, "dw": 95, "via": [95, 103], "backprop": 95, "pat": 95, "unmask": 95, "resid": 95, "salienc": 95, "lowest": 95, "l1": 95, "abl": [95, 98, 99, 104, 108], "repeat": [95, 104, 105], "movement": 95, "2005": 95, "07683": 95, "rank": [95, 98], "wx": 95, "sqx": 95, "q": [95, 104], "usual": 95, "sort": 95, "wise": 95, "reconstruct": [95, 99], "randomli": 95, "tri": 95, "remedi": 95, "item": [95, 102], "ultim": [95, 96], "literatur": 95, "vision": 95, "nlp": [95, 102, 106], "iter": [95, 104, 105], "ctr_feed": 95, "na": 95, "multimask": 95, "pyspeech": 95, "fastna": 95, "approach": [95, 98, 103, 106, 107], "knowledg": [95, 102], "distil": 95, "pdf": 95, "2204": 95, "09656": 95, "arrang": 95, "recal": 95, "counterpart": 95, "slower": 95, "suffici": 95, "flexibl": [95, 98, 103, 106], "98": 95, "special": [95, 103, 104], "exhibit": 95, "penalti": 95, "expens": [95, 98], "dictat": 95, "characterist": 95, "highest": 95, "wouldn": [95, 98], "visual": 95, "fig": 95, "4x4": 95, "benchmak": 95, "fly": 96, "welcom": 96, "histogram": [96, 104], "act_ob": 96, "finfo": 96, "weight_ob": 96, "observed_input": 96, "observed_weight": 96, "cl": [96, 98, 99], "float_linear": 96, "observed_linear": 96, "_replace_with_custom_fn_if_matches_filt": 96, "insert_observers_": 96, "_is_linear": 96, "lambda": [96, 99], "replacement_fn": 96, "copied_act_ob": 96, "copied_weight_ob": 96, "popul": 96, "feed": 96, "simpler": [96, 104], "quantizedlinear": [96, 98], "isn": 96, "strictli": 96, "to_affine_quantized_intx_stat": 96, "act_scal": [96, 108], "act_zero_point": 96, "weight_scal": [96, 104, 108], "weight_zero_point": [96, 104], "qweight": 96, "qinput": 96, "from_observ": 96, "begin": [96, 98], "dataclass": [96, 99, 108], "transform_modul": [96, 99], "register_quantize_module_handl": [96, 99], "staticquantconfig": 96, "_apply_static_qu": 96, "is_observed_linear": 96, "optimizedmodul": 96, "_orig_mod": 96, "0237": 96, "plainaqttensorimpl": 96, "142": 96, "31": [96, 108], "113": 96, "157": 96, "59": 96, "160": 96, "150": 96, "67": 96, "241": 96, "238": 96, "235": 96, "228": 96, "255": [96, 108], "201": 96, "114": 96, "236": 96, "88": [96, 104], "83": 96, "109": 96, "209": 96, "92": 96, "184": 96, "141": 96, "110": 96, "0009": 96, "0010": 96, "130": 96, "122": 96, "132": 96, "125": 96, "126": 96, "129": 96, "127": [96, 98, 107, 108], "133": 96, "124": 96, "131": 96, "135": 96, "136": 96, "foundat": 98, "autograd": [98, 108], "interpos": 98, "namespac": 98, "continu": [98, 105, 106, 107, 108], "obviou": 98, "int8quantizedlinear": 98, "finer": 98, "intercept": 98, "contrast": 98, "long": [98, 104], "clunki": 98, "distributedlinear": 98, "duplic": 98, "bypass": 98, "outer": 98, "inner": 98, "allgath": 98, "bandwidth": 98, "read": 98, "zoo": 98, "podcast": 98, "edward": 98, "yang": 98, "int8_symmetric_quant": 98, "fp32_tensor": 98, "amin": 98, "keepdim": [98, 104, 105], "amax": 98, "zeros_lik": 98, "clamp": [98, 104], "w_int8": 98, "new_linear": 98, "left": [98, 108], "toymodel": 98, "child": 98, "named_children": 98, "drawback": 98, "won": 98, "suppos": 98, "clean": 98, "eleg": 98, "pretti": 98, "power": [98, 99], "overrid": 98, "almost": 98, "shard": [98, 99], "ragged": 98, "rag": 98, "nestedtensor": 98, "who": 98, "link": [98, 102], "googl": 98, "collab": 98, "flopcount": 98, "memorytrack": 98, "With": [98, 104, 106, 108], "bare": 98, "bone": 98, "int8symmetrictensor": 98, "hold": 98, "staticmethod": 98, "__new__": [98, 99], "_make_wrapper_subclass": [98, 99], "storage_offset": 98, "ndim": 98, "__tensor_flatten__": [98, 99], "pt2": [98, 106], "__tensor_unflatten__": [98, 99], "tensor_data_dict": [98, 99], "extra_metadata": 98, "outer_s": [98, 99], "outer_strid": [98, 99], "undo": 98, "__repr__": 98, "repr": 98, "ahead": 98, "insid": 98, "int8_tensor": 98, "func": [98, 99], "op_implementations_dict": 98, "conveni": 98, "register_op": 98, "_op": 98, "opoverload": 98, "impl_decor": 98, "op_impl": 98, "particular": 98, "largest": 98, "tell": 98, "desugar": 98, "decor": [98, 99], "surfac": 98, "coverag": [98, 103, 104, 106, 107], "brute": 98, "forc": 98, "repeatedli": 98, "loggingtensor": 98, "_python_dispatch": [98, 99], "return_and_correct_alias": [98, 99], "int8_mm": 98, "detach": [98, 99], "int8_view_op": 98, "out_data": 98, "out_scal": [98, 104], "notic": 98, "hit": 98, "background": 98, "decomposit": 98, "live": 98, "decomp": 98, "shrink": 98, "author": [98, 102, 103, 104, 105, 106, 107, 108], "pain": 98, "rather": 98, "worth": 98, "written": 98, "differenti": 98, "nuanc": 98, "longer": [98, 104, 105], "That": 98, "transposit": 98, "got": [98, 104, 108], "propag": [98, 104, 106, 107], "fact": 98, "themselv": [98, 104], "pointwis": [98, 106, 107], "were": 98, "might": [98, 99, 104, 108], "unwrap": 98, "dim0": 98, "dim1": 98, "confirm": 98, "quantized_model_module_swap": 98, "quantized_model_subclass": 98, "subclass_param": 98, "out_module_swap": 98, "allclos": 98, "out_compil": 98, "seri": 98, "e2": 99, "_type": 99, "_data": 99, "capabl": [99, 104, 106], "self_attn": 99, "q_proj": 99, "k_proj": 99, "mlp": 99, "gate_proj": 99, "usernam": 99, "narrow": 99, "copy_": 99, "state": 99, "chunk": 99, "_apply_fn_to_data": 99, "heavi": 99, "codebas": 99, "fn": 99, "ctx": 99, "new_tensor": 99, "__class__": 99, "principl": 99, "torchaobasetensor": 99, "mynewquantconfig": 99, "classvar": 99, "myquantizedtensor": 99, "fbgemmfp8tensor": 99, "tensor_data_attr": 99, "tensor_attribut": 99, "attr": 99, "_to_copi": 99, "fill_default": 99, "notimplementederror": 99, "_my_quant_transform": 99, "my_quantization_funct": 99, "use_cutlass_kernel": 99, "my_cutlass_linear": 99, "use_triton_kernel": 99, "my_triton_linear": 99, "disappear": 99, "extrem": 99, "sole": 99, "think": 99, "world": 99, "explicitli": [99, 108], "spooki": 99, "distanc": 99, "statu": 99, "due": [99, 103, 108], "team": 99, "2338": 99, "creation": 99, "detect": 99, "illustr": 99, "tutorials_python": 100, "zip": [100, 102], "jupyt": [100, 102], "notebook": [100, 102], "tutorials_jupyt": 100, "sphinx": [100, 102], "firstnam": 102, "lastnam": 102, "prerequisit": [102, 104], "v2": 102, "topic": 102, "rand": [102, 104, 105], "6404": 102, "7387": 102, "2921": 102, "6287": 102, "8887": 102, "8385": 102, "1354": 102, "2416": 102, "4004": 102, "4453": 102, "4993": 102, "9889": 102, "9407": 102, "5986": 102, "3524": 102, "practic": 102, "summar": 102, "takeawai": 102, "link1": 102, "link2": 102, "minut": 102, "ipynb": 102, "daniil": 103, "lyakhov": 103, "aamir": 103, "nazir": 103, "alexand": 103, "suslov": 103, "yamini": 103, "nimmagadda": 103, "kozlov": 103, "subject": [103, 105], "openvinoquant": 103, "unlock": 103, "placement": 103, "simplifi": [103, 104, 106, 107], "ux": [103, 104, 106], "torchdynamo": [103, 106, 107, 108], "eager": [103, 104, 105, 106, 107, 108], "mechan": [103, 106, 107], "torchvis": [103, 104, 105, 106, 107, 108], "resnet18": [103, 104, 105, 106, 107], "u": 103, "__dict__": [103, 104, 105, 106, 107], "dummi": [103, 106, 107], "traced_b": [103, 106, 107], "exported_model": [103, 104, 105, 106, 107], "preset": 103, "elu": 103, "prelu": 103, "gelu": 103, "quantizationpreset": 103, "bert": [103, 106], "modeltyp": 103, "ignored_scop": 103, "exclud": 103, "layer_1": 103, "layer_2": 103, "layer_3": 103, "ignoredscop": 103, "conv2d": [103, 104, 105, 106, 107, 108], "regex": 103, "layer_": 103, "subgraph": [103, 105], "node": [103, 105, 106, 107, 108], "target_devic": 103, "taken": 103, "account": 103, "cpu_spr": 103, "npu": 103, "targetdevic": 103, "fold": [103, 104, 106, 107], "batchnorm": [103, 104, 105, 106, 107], "preced": [103, 104, 106, 107], "prepared_model": [103, 104, 105, 106, 107], "fold_quant": 103, "finish": [103, 106], "comparison": 103, "biascorrect": 103, "discrep": 103, "calibration_load": 103, "dataload": [103, 104, 105], "transform_fn": 103, "data_item": 103, "calibration_dataset": 103, "smooth_quant": 103, "fast_bias_correct": 103, "deploy": [103, 106], "jerri": [104, 106, 108], "zhang": [104, 106, 107, 108], "_export": [104, 105, 106], "14k": 104, "programm": [104, 106, 107], "db": 104, "xnnpack": [104, 105, 108], "xnnpack_quant": [104, 105], "get_symmetric_quantization_config": [104, 105], "xnnpackquant": [104, 105, 108], "prior": 104, "qconfigmap": [104, 108], "backendconfig": [104, 108], "rel": 104, "intent": [104, 108], "qconfig": [104, 108], "3d": [104, 108], "incompat": 104, "great": 104, "ideal": 104, "fake_qu": 104, "hidden": 104, "summari": 104, "thu": 104, "queri": [104, 108], "previous": 104, "embedding_byt": 104, "executorchquant": 104, "concaten": 104, "prone": 104, "cleaner": 104, "composed_quant": 104, "quantization_cap": 104, "concern": 104, "decoupl": 104, "minmax": 104, "freed": 104, "identitc": 104, "imagenet": [104, 105], "unzip": [104, 105], "data_path": [104, 105], "resnet18_pretrained_float": [104, 105], "sy": [104, 105], "numpi": [104, 105], "np": [104, 105], "resnet": [104, 105, 106], "warn": [104, 105], "filterwarn": [104, 105], "categori": [104, 105], "deprecationwarn": [104, 105], "r": [104, 105], "seed": [104, 105], "191009": [104, 105], "averagemet": [104, 105], "fmt": [104, 105], "val": [104, 105], "avg": [104, 105], "count": [104, 105], "__str__": [104, 105], "fmtstr": [104, 105], "topk": [104, 105], "predict": [104, 105], "maxk": [104, 105], "pred": [104, 105], "eq": [104, 105], "expand_a": [104, 105], "correct_k": [104, 105], "reshap": [104, 105], "mul_": [104, 105], "criterion": [104, 105], "top1": [104, 105], "top5": [104, 105], "cnt": [104, 105], "acc1": [104, 105], "acc5": [104, 105], "load_model": [104, 105], "model_fil": [104, 105], "weights_onli": [104, 105], "print_size_of_model": [104, 105], "temp": [104, 105], "p": [104, 105], "1e6": [104, 105], "prepare_data_load": [104, 105], "485": [104, 105], "456": [104, 105], "std": [104, 105], "229": [104, 105], "225": [104, 105], "randomresizedcrop": [104, 105], "randomhorizontalflip": [104, 105], "totensor": [104, 105], "dataset_test": [104, 105], "resiz": [104, 105], "centercrop": [104, 105], "train_sampl": [104, 105], "randomsampl": [104, 105], "test_sampl": [104, 105], "sequentialsampl": [104, 105], "train_batch_s": [104, 105], "sampler": [104, 105], "data_loader_test": [104, 105, 106, 107], "eval_batch_s": [104, 105], "saved_model_dir": [104, 105], "float_model_fil": [104, 105], "model_to_quant": [104, 105], "capture_pre_autograd_graph": [104, 105, 106], "dynamic_shap": [104, 105], "export_for_train": 104, "dynamic_dim": [104, 105], "constraint": [104, 105, 108], "qconfig_opt": 104, "set_object_typ": 104, "set_module_nam": 104, "workload": 104, "themodel": 104, "feedback": 104, "dq": 104, "fp32_op": 104, "qauntiz": 104, "x_int8": 104, "x_scale": 104, "x_zero_point": 104, "weight_int8": 104, "bias_fp32": 104, "output_scal": 104, "output_zero_point": 104, "x_fp32": 104, "quantized_decompos": 104, "dequantize_per_tensor": 104, "x_i8": 104, "x_quant_min": 104, "x_quant_max": 104, "weight_fp32": 104, "weight_i8": 104, "weight_quant_min": 104, "weight_quant_max": 104, "weight_permut": 104, "permute_copi": 104, "out_fp32": 104, "addmm": 104, "out_i8": 104, "quantize_per_tensor": 104, "out_zero_point": 104, "out_quant_min": 104, "out_quant_max": 104, "float32_op": 104, "decompos": 104, "use_reference_represent": 104, "x_int16": 104, "weight_int16": 104, "acc_int32": 104, "out_dtyp": 104, "bias_scal": 104, "bias_int32": 104, "div": 104, "mul": 104, "out_int8": 104, "qmin": 104, "qmax": 104, "date": 104, "unus": 104, "serila": 104, "consult": 104, "exportedprogram": 104, "pt2e_quantized_model_file_path": 104, "resnet18_pt2e_quant": 104, "quantized_ep": 104, "loaded_quantized_ep": 104, "loaded_quantized_model": 104, "diff": 104, "79": 104, "82": 104, "55": 104, "edg": [104, 108], "went": 104, "andrew": 105, "Or": 105, "ptq": [105, 106], "move_exported_model_to_ev": [105, 106], "correctli": 105, "certain": 105, "dropout": 105, "move_exported_model_to_train": 105, "jit": 105, "recursivescriptmodul": 105, "train_one_epoch": 105, "ntrain_batch": 105, "avgloss": 105, "5f": 105, "start_tim": 105, "global_avg": 105, "is_qat": [105, 106], "fusion": 105, "batchnorm2d": 105, "_native_batch_norm_legit": 105, "cudnn_batch_norm": 105, "mobilenetv2": 105, "recompil": 105, "consolid": 105, "epoch": 105, "far": 105, "num_epoch": 105, "num_train_batch": 105, "num_eval_batch": 105, "num_observer_update_epoch": 105, "num_batch_norm_update_epoch": 105, "num_epochs_between_ev": 105, "nepoch": 105, "subseq": 105, "disable_observ": 105, "bn": 105, "running_mean": 105, "running_var": 105, "new_arg": 105, "wish": 105, "prepared_model_copi": 105, "neval_batch": 105, "paus": 105, "resum": 105, "fail": [105, 108], "checkpoint_path": 105, "checkpoint_": 105, "behav": 105, "incorrectli": 105, "lesli": [106, 108], "fang": [106, 108], "weiwen": [106, 108], "xia": [106, 108], "jiong": [106, 108], "gong": [106, 108], "cnn": 106, "rnn": 106, "outstand": 106, "fourth": 106, "spr": 106, "xeon": 106, "processor": 106, "boost": 106, "contigu": [106, 107], "channels_last": [106, 107], "onednn": [106, 107], "assum": [106, 108], "word": 106, "satur": 106, "pure": 106, "dedic": 106, "scenario": [106, 107], "plai": [106, 107], "convolut": [106, 107, 108], "absenc": [106, 107], "enhanc": [106, 107], "mirror": [106, 107], "autocast": [106, 107], "context": [106, 107], "device_typ": [106, 107], "turn": [106, 107], "cpp": 106, "qconvolut": [106, 107], "qlinear": [106, 107], "presenc": [106, 107], "pair": [106, 107], "remain": [106, 107], "conting": [106, 107], "qmaxpool2d": [106, 107], "torchinductor_freez": [106, 107], "example_x86inductorquantizer_pytorch_2_1": 106, "torchbench": 106, "measur": 106, "proven": 106, "depth": 106, "shoud": 106, "example_x86inductorquantizer_qat": 106, "yan": 107, "zhiwei": 107, "wang": 107, "eikan": 107, "liangang": 107, "liu": 107, "river": 107, "cui": 107, "yifeng": 107, "xpuinductorquant": 107, "pip3": 107, "torchaudio": 107, "xpu_inductor_quantizer_exampl": 107, "xpu_inductor_quant": 107, "xpuiq": 107, "resnet18_weight": 107, "get_default_xpu_inductor_quantization_config": 107, "sign": 107, "wherea": 107, "histogramobserv": [107, 108], "perchannelminmaxobserv": 107, "quantizationspec": [107, 108], "quantizationconfig": [107, 108], "type_check": 107, "observerorfakequantizeconstructor": 107, "get_xpu_inductor_symm_quantization_config": 107, "extra_arg": 107, "act_observer_or_fake_quant_ctr": 107, "act_quantization_spec": [107, 108], "qscheme": [107, 108], "per_tensor_symmetr": [107, 108], "observer_or_fake_quant_ctr": [107, 108], "with_arg": [107, 108], "weight_observer_or_fake_quant_ctr": 107, "weight_quantization_spec": [107, 108], "per_channel_symmetr": 107, "ch_axi": 107, "oc": 107, "ic": 107, "kh": 107, "kw": 107, "conv": [107, 108], "bias_quantization_spec": 107, "amp": 107, "indcutor": 107, "kimish": 108, "patel": 108, "made": 108, "explicit": 108, "quantiat": 108, "encod": 108, "convei": 108, "quantizationannot": 108, "furthermor": 108, "minmaxobserv": 108, "input_qspec_map": 108, "output_qspec": 108, "_annot": 108, "conclud": 108, "matcher": 108, "get_source_partit": 108, "add_partit": 108, "gm": 108, "itertool": 108, "chain": 108, "add_nod": 108, "output_nod": 108, "per_tensor_affin": 108, "input_act_qspec": 108, "output_act_qspec": 108, "input_act0": 108, "input_act1": 108, "quantization_annot": 108, "phase": 108, "substitut": 108, "among": 108, "sharedquantizationspec": 108, "maxpool": 108, "average_pool": 108, "concat": 108, "edgeornod": 108, "transit": 108, "spec": 108, "conv1": 108, "conv2": 108, "fed": 108, "cat": 108, "conv1_out": 108, "conv2_out": 108, "qspec1": 108, "cat_input0": 108, "cat_input1": 108, "implicitli": 108, "therefor": 108, "ob": 108, "consum": 108, "rewrit": 108, "share_qparams_with_input_act0_qspec": 108, "known": 108, "beforehand": 108, "sigmoid": 108, "fixedqparamsquantizationspec": 108, "act_qspec": 108, "sigmoid_nod": 108, "input_act": 108, "derivedquantizationspec": 108, "derive_qparams_fn": 108, "observerorfakequant": 108, "observerbas": 108, "fakequantizebas": 108, "heurist": 108, "obejct": 108, "obs_or_fq": 108, "fq": 108, "act_obs_or_fq": 108, "weight_obs_or_fq": 108, "act_zp": 108, "weight_zp": 108, "bias_qspec": 108, "derived_from": 108, "backendquant": 108, "get_input_act_qspec": 108, "get_output_act_qspec": 108, "get_weight_qspec": 108, "get_bias_qspec": 108, "intermedi": 108, "call_funct": 108, "relu_": 108, "relu_nod": 108, "maybe_conv_nod": 108, "conv1d": 108, "unexpect": 108, "recognz": 108, "subgraphmatch": 108, "conv_relu_pattern": 108, "name_node_map": 108, "input_nod": 108, "weight_nod": 108, "bias_nod": 108, "caveat": 108, "exhaust": 108, "2d": 108, "4d": 108, "symbol": 108, "outcom": 108}, "objects": {"torchao.dtypes": [[12, 0, 1, "", "AffineQuantizedTensor"], [13, 0, 1, "", "BlockSparseLayout"], [14, 0, 1, "", "CutlassInt4PackedLayout"], [15, 0, 1, "", "CutlassSemiSparseLayout"], [16, 0, 1, "", "Float8Layout"], [17, 0, 1, "", "Int4CPULayout"], [18, 0, 1, "", "Layout"], [19, 0, 1, "", "MarlinQQQLayout"], [20, 0, 1, "", "MarlinQQQTensor"], [21, 0, 1, "", "MarlinSparseLayout"], [22, 0, 1, "", "NF4Tensor"], [23, 0, 1, "", "PlainLayout"], [24, 0, 1, "", "SemiSparseLayout"], [25, 0, 1, "", "TensorCoreTiledLayout"], [26, 0, 1, "", "UintxLayout"], [27, 2, 1, "", "to_affine_quantized_floatx"], [28, 2, 1, "", "to_affine_quantized_floatx_static"], [29, 2, 1, "", "to_affine_quantized_fpx"], [30, 2, 1, "", "to_affine_quantized_intx"], [31, 2, 1, "", "to_affine_quantized_intx_static"], [32, 2, 1, "", "to_marlinqqq_quantized_intx"], [33, 2, 1, "", "to_nf4"]], "torchao.dtypes.AffineQuantizedTensor": [[12, 1, 1, "", "dequantize"], [12, 1, 1, "", "from_hp_to_floatx"], [12, 1, 1, "", "from_hp_to_floatx_static"], [12, 1, 1, "", "from_hp_to_fpx"], [12, 1, 1, "", "from_hp_to_intx"], [12, 1, 1, "", "from_hp_to_intx_static"], [12, 1, 1, "", "to"]], "torchao.dtypes.MarlinQQQTensor": [[20, 1, 1, "", "dequantize"], [20, 1, 1, "", "from_hp_to_intx"]], "torchao.dtypes.MarlinSparseLayout": [[21, 1, 1, "", "pre_process"]], "torchao.dtypes.NF4Tensor": [[22, 1, 1, "", "convert_to_norm_float_weight"], [22, 1, 1, "", "dequantize"], [22, 1, 1, "", "dequantize_scalers"], [22, 1, 1, "", "double_quantize_scalers"], [22, 1, 1, "", "get_original_weight"], [22, 1, 1, "", "quantize_tensor_nearest"]], "torchao.float8": [[34, 0, 1, "", "CastConfig"], [35, 0, 1, "", "Float8LinearConfig"], [36, 0, 1, "", "ScalingGranularity"], [37, 0, 1, "", "ScalingType"], [38, 2, 1, "", "convert_to_float8_training"], [39, 2, 1, "", "precompute_float8_dynamic_scale_for_fsdp"]], "torchao.float8.Float8LinearConfig": [[35, 1, 1, "", "from_recipe_name"]], "torchao.quantization": [[40, 0, 1, "", "FPXWeightOnlyConfig"], [41, 0, 1, "", "Float8DynamicActivationFloat8WeightConfig"], [42, 0, 1, "", "Float8StaticActivationFloat8WeightConfig"], [43, 0, 1, "", "Float8WeightOnlyConfig"], [44, 0, 1, "", "GemliteUIntXWeightOnlyConfig"], [45, 0, 1, "", "Int4WeightOnlyConfig"], [46, 0, 1, "", "Int8DynamicActivationInt4WeightConfig"], [47, 0, 1, "", "Int8DynamicActivationInt8WeightConfig"], [48, 0, 1, "", "Int8WeightOnlyConfig"], [49, 0, 1, "", "MappingType"], [50, 0, 1, "", "TorchAODType"], [51, 0, 1, "", "UIntXWeightOnlyConfig"], [52, 0, 1, "", "ZeroPointDomain"], [53, 2, 1, "", "autoquant"], [54, 2, 1, "", "choose_qparams_affine"], [55, 2, 1, "", "choose_qparams_affine_with_min_max"], [56, 2, 1, "", "dequantize_affine"], [57, 2, 1, "", "int_scaled_matmul"], [76, 2, 1, "", "quantize_"], [77, 2, 1, "", "quantize_affine"], [78, 2, 1, "", "safe_int_mm"], [79, 2, 1, "", "smooth_fq_linear_to_inference"], [80, 2, 1, "", "swap_linear_with_smooth_fq_linear"], [81, 2, 1, "", "to_linear_activation_quantized"]], "torchao.quantization.qat": [[58, 0, 1, "", "ComposableQATQuantizer"], [59, 0, 1, "", "FakeQuantizeConfig"], [60, 0, 1, "", "FakeQuantizedEmbedding"], [61, 0, 1, "", "FakeQuantizedLinear"], [62, 0, 1, "", "FakeQuantizer"], [63, 0, 1, "", "Float8ActInt4WeightQATQuantizer"], [64, 0, 1, "", "FromIntXQuantizationAwareTrainingConfig"], [65, 0, 1, "", "Int4WeightOnlyEmbeddingQATQuantizer"], [66, 0, 1, "", "Int4WeightOnlyQATQuantizer"], [67, 0, 1, "", "Int8DynActInt4WeightQATQuantizer"], [68, 0, 1, "", "IntXQuantizationAwareTrainingConfig"], [71, 2, 1, "", "initialize_fake_quantizers"]], "torchao.quantization.qat.FakeQuantizeConfig": [[59, 3, 1, "", "group_size"], [59, 3, 1, "", "is_symmetric"]], "torchao.quantization.qat.FakeQuantizedEmbedding": [[60, 1, 1, "", "forward"]], "torchao.quantization.qat.FakeQuantizedLinear": [[61, 1, 1, "", "forward"]], "torchao.quantization.qat.FakeQuantizer": [[62, 1, 1, "", "forward"]], "torchao.quantization.qat.Float8ActInt4WeightQATQuantizer": [[63, 1, 1, "", "prepare"]], "torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer": [[65, 1, 1, "", "convert"], [65, 1, 1, "", "prepare"]], "torchao.quantization.qat.embedding": [[69, 0, 1, "", "Int4WeightOnlyEmbedding"], [70, 0, 1, "", "Int4WeightOnlyQATEmbedding"]], "torchao.quantization.qat.embedding.Int4WeightOnlyEmbedding": [[69, 1, 1, "", "forward"]], "torchao.quantization.qat.linear": [[72, 0, 1, "", "Int4WeightOnlyQATLinear"], [73, 0, 1, "", "Int8DynActInt4WeightQATLinear"], [74, 2, 1, "", "disable_linear_fake_quant"], [75, 2, 1, "", "enable_linear_fake_quant"]], "torchao": [[6, 4, 0, "-", "sparsity"]], "torchao.sparsity": [[82, 0, 1, "", "PerChannelNormObserver"], [83, 0, 1, "", "WandaSparsifier"], [84, 2, 1, "", "apply_fake_sparsity"], [85, 5, 1, "", "semi_sparse_weight"], [86, 2, 1, "", "sparsify_"]], "torchao.sparsity.PerChannelNormObserver": [[82, 1, 1, "", "forward"]], "torchao.sparsity.WandaSparsifier": [[83, 1, 1, "", "prepare"], [83, 1, 1, "", "squash_mask"], [83, 1, 1, "", "update_mask"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:module", "5": "py:attribute"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "module", "Python module"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchao": [0, 1, 2, 3, 4, 5, 6, 9, 11, 87, 89, 90, 99], "dtype": [0, 10, 90], "layout": [0, 9, 18, 90], "tensor": [0, 9, 90, 97, 98, 99, 108], "subclass": [0, 9, 90, 98, 99], "quantiz": [0, 4, 5, 11, 76, 87, 90, 91, 93, 96, 97, 98, 99, 103, 104, 105, 106, 107, 108], "techniqu": 0, "float8": [1, 11, 89], "main": [1, 5], "train": [1, 11, 89, 90, 93, 103, 104, 105, 106, 107], "api": [1, 2, 4, 5, 7, 11, 87, 89, 108], "other": [1, 5, 9, 90], "type": 1, "refer": [2, 87], "python": 2, "kernel": [3, 9, 88, 90, 99], "qat": [4, 11, 105], "config": 4, "quantize_": [4, 5], "custom": [4, 9], "legaci": 4, "prototyp": 4, "infer": [5, 93], "primit": [5, 90], "sparsiti": [6, 95], "benchmark": [7, 8, 9, 93], "guid": [7, 8, 9, 91, 99], "add": [7, 90, 99], "an": [7, 92], "recip": [7, 89], "model": [7, 9, 89, 90, 92, 93, 99, 103, 104, 105], "design": [7, 95], "consider": 7, "hf": 7, "ci": 7, "dashboard": 7, "1": [7, 11, 89, 93, 99, 103, 106, 107, 108], "modifi": 7, "exist": 7, "configur": [7, 95, 99, 104, 105], "2": [7, 11, 91, 93, 99, 103, 104, 105, 106, 107, 108], "run": 7, "3": [7, 11, 93, 99, 103, 106, 107, 108], "output": [7, 98], "format": 7, "4": [7, 103, 108], "integr": [7, 11, 90, 99], "pipelin": 7, "troubleshoot": 7, "test": [7, 9], "common": [7, 108], "issu": 7, "best": 7, "practic": 7, "user": 8, "contributor": 9, "gener": 9, "extend": 9, "ad": [9, 90, 99], "effici": [9, 90], "triton": 9, "hand": 9, "written": 9, "dispatch": [9, 99], "tensorimpl": [9, 90], "flow": [9, 90, 92, 99, 108], "us": [9, 108], "torch": [9, 103, 104, 105], "compil": [9, 99, 103], "perform": [9, 88, 93, 104], "serial": [9, 92, 99], "featur": 9, "support": [9, 90, 99], "function": [9, 90, 104, 105], "compos": 9, "microbenchmark": 9, "eval": [9, 104], "part": [11, 89, 93], "fine": 11, "tune": 11, "qlora": 11, "awar": [11, 90, 105, 106], "option": [11, 93, 102, 103], "torchtun": 11, "axolotl": 11, "low": [11, 90], "rank": 11, "adapt": 11, "huggingfac": [11, 93, 99], "peft": 11, "affinequantizedtensor": 12, "blocksparselayout": 13, "cutlassint4packedlayout": 14, "cutlasssemisparselayout": 15, "float8layout": 16, "int4cpulayout": 17, "marlinqqqlayout": 19, "marlinqqqtensor": 20, "marlinsparselayout": 21, "nf4tensor": 22, "plainlayout": 23, "semisparselayout": 24, "tensorcoretiledlayout": 25, "uintxlayout": 26, "to_affine_quantized_floatx": 27, "to_affine_quantized_floatx_stat": 28, "to_affine_quantized_fpx": 29, "to_affine_quantized_intx": 30, "to_affine_quantized_intx_stat": 31, "to_marlinqqq_quantized_intx": 32, "to_nf4": 33, "castconfig": 34, "float8linearconfig": 35, "scalinggranular": 36, "scalingtyp": 37, "convert_to_float8_train": 38, "precompute_float8_dynamic_scale_for_fsdp": 39, "fpxweightonlyconfig": 40, "float8dynamicactivationfloat8weightconfig": 41, "float8staticactivationfloat8weightconfig": 42, "float8weightonlyconfig": 43, "gemliteuintxweightonlyconfig": 44, "int4weightonlyconfig": 45, "int8dynamicactivationint4weightconfig": 46, "int8dynamicactivationint8weightconfig": 47, "int8weightonlyconfig": 48, "mappingtyp": 49, "torchaodtyp": 50, "uintxweightonlyconfig": 51, "zeropointdomain": 52, "autoqu": 53, "choose_qparams_affin": 54, "choose_qparams_affine_with_min_max": 55, "dequantize_affin": 56, "int_scaled_matmul": 57, "composableqatquant": 58, "fakequantizeconfig": 59, "fakequantizedembed": 60, "fakequantizedlinear": 61, "fakequant": 62, "float8actint4weightqatquant": 63, "fromintxquantizationawaretrainingconfig": 64, "int4weightonlyembeddingqatquant": 65, "int4weightonlyqatquant": 66, "int8dynactint4weightqatquant": 67, "intxquantizationawaretrainingconfig": 68, "int4weightonlyembed": 69, "int4weightonlyqatembed": 70, "initialize_fake_quant": 71, "int4weightonlyqatlinear": 72, "int8dynactint4weightqatlinear": 73, "disable_linear_fake_qu": 74, "enable_linear_fake_qu": 75, "quantize_affin": 77, "safe_int_mm": 78, "smooth_fq_linear_to_infer": 79, "swap_linear_with_smooth_fq_linear": 80, "to_linear_activation_quant": 81, "perchannelnormobserv": 82, "wandasparsifi": 83, "apply_fake_spars": 84, "semi_sparse_weight": 85, "sparsifi": 86, "welcom": 87, "document": 87, "get": 87, "start": [87, 91], "develop": 87, "note": [87, 89, 108], "eager": 87, "tutori": [87, 102], "pt2e": [87, 108], "pre": 89, "torchtitan": 89, "prerequisit": [89, 103, 106, 107, 108], "rowwis": 89, "scale": 89, "tensorwis": 89, "pick": 89, "import": [89, 104, 105], "directli": [89, 108], "convers": 89, "overview": [90, 95, 102], "basic": 90, "current": 90, "placehold": 90, "pytorch": [90, 91, 103, 104, 105, 106, 107, 108], "implement": [90, 98, 99], "oper": [90, 98, 99, 108], "nativ": 90, "factori": 90, "op": 90, "deriv": [90, 108], "algorithm": 90, "weight": [90, 93], "onli": 90, "dynam": 90, "activ": 90, "static": [90, 96], "insert": 90, "observ": 90, "how": [90, 104, 105, 108], "defin": [90, 104, 105], "modul": [90, 98, 99], "calibr": [90, 96, 104], "bit": 90, "optim": [90, 92, 93], "case": 90, "studi": 90, "int4": 90, "work": 90, "dure": 90, "execut": 90, "save": [90, 104, 105], "load": [90, 104, 105], "quick": 91, "first": 91, "exampl": [91, 99, 108], "export": [91, 93, 103, 104, 105, 106, 107, 108], "next": [91, 98], "step": [91, 93, 98, 99, 102], "deseri": 92, "what": [92, 98], "happen": 92, "when": 92, "serv": [93, 99], "vllm": [93, 99], "sglang": 93, "executorch": 93, "post": [93, 103, 104, 106, 107], "transform": [93, 99], "mobil": 93, "deploy": 93, "unti": 93, "embed": 93, "creat": [93, 99], "characterist": 93, "evalu": [93, 104], "qualiti": 93, "assess": 93, "memori": 93, "latenc": 93, "result": 93, "h100": 93, "machin": 93, "conclus": [93, 102, 103, 104, 105, 106, 107, 108], "comput": [94, 101], "time": [94, 101], "goal": 95, "context": 95, "prune": 95, "criteria": 95, "strategi": 95, "pattern": [95, 108], "phase": 96, "write": [97, 98, 108], "your": [97, 98, 99], "own": [97, 98], "advanc": 97, "ar": 98, "swap": 98, "which": 98, "should": 98, "we": 98, "compar": 98, "architectur": 99, "usag": 99, "system": 99, "class": 99, "level": 99, "new": 99, "method": 99, "minim": 99, "requir": 99, "compat": 99, "why": 99, "regist": 99, "s": 99, "kei": 99, "detail": 99, "hardwar": 99, "specif": [99, 104, 105], "linear": 99, "benefit": 99, "trade": 99, "off": 99, "share": [99, 108], "safetensor": 99, "diagram": 99, "high": 99, "point": 99, "bring": 99, "extern": 99, "templat": 102, "addit": 102, "exercis": 102, "further": 102, "read": 102, "openvino": 103, "backend": [103, 104, 105, 106, 107], "introduct": [103, 106, 107, 108], "nncf": 103, "instal": 103, "captur": [103, 106, 107], "fx": [103, 106, 107], "graph": [103, 106, 107], "appli": [103, 106, 107], "lower": [103, 104, 106, 107], "represent": 103, "improv": 103, "metric": 103, "motiv": [104, 108], "helper": [104, 105], "prepar": [104, 105], "dataset": [104, 105], "set": 104, "mode": 104, "convert": [104, 105], "check": 104, "size": 104, "accuraci": 104, "debug": 104, "loop": 105, "checkpoint": 105, "x86": 106, "through": [106, 107], "inductor": [106, 107], "intel": 107, "gpu": 107, "annot": 108, "param": 108, "fix": 108, "paramet": 108, "5": 108, "A": 108, "toi": 108, "resnet18": 108, "ir": 108, "problem": 108, "match": 108, "aten": 108, "recommend": 108, "subgraphmatcherwithnamenodemap": 108}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})