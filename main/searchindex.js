Search.setIndex({"docnames": ["api_ref_dtypes", "api_ref_float8", "api_ref_intro", "api_ref_kernel", "api_ref_quantization", "api_ref_sparsity", "contributor_guide", "dtypes", "generated/torchao.dtypes.AffineQuantizedTensor", "generated/torchao.dtypes.BlockSparseLayout", "generated/torchao.dtypes.CutlassInt4PackedLayout", "generated/torchao.dtypes.CutlassSemiSparseLayout", "generated/torchao.dtypes.Float8Layout", "generated/torchao.dtypes.Int4CPULayout", "generated/torchao.dtypes.Layout", "generated/torchao.dtypes.MarlinQQQLayout", "generated/torchao.dtypes.MarlinQQQTensor", "generated/torchao.dtypes.MarlinSparseLayout", "generated/torchao.dtypes.NF4Tensor", "generated/torchao.dtypes.PlainLayout", "generated/torchao.dtypes.SemiSparseLayout", "generated/torchao.dtypes.TensorCoreTiledLayout", "generated/torchao.dtypes.UintxLayout", "generated/torchao.dtypes.to_affine_quantized_floatx", "generated/torchao.dtypes.to_affine_quantized_floatx_static", "generated/torchao.dtypes.to_affine_quantized_fpx", "generated/torchao.dtypes.to_affine_quantized_intx", "generated/torchao.dtypes.to_affine_quantized_intx_static", "generated/torchao.dtypes.to_marlinqqq_quantized_intx", "generated/torchao.dtypes.to_nf4", "generated/torchao.float8.CastConfig", "generated/torchao.float8.Float8LinearConfig", "generated/torchao.float8.ScalingType", "generated/torchao.float8.convert_to_float8_training", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp", "generated/torchao.quantization.MappingType", "generated/torchao.quantization.TorchAODType", "generated/torchao.quantization.ZeroPointDomain", "generated/torchao.quantization.autoquant", "generated/torchao.quantization.choose_qparams_affine", "generated/torchao.quantization.choose_qparams_affine_floatx", "generated/torchao.quantization.choose_qparams_affine_with_min_max", "generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq", "generated/torchao.quantization.dequantize_affine", "generated/torchao.quantization.dequantize_affine_floatx", "generated/torchao.quantization.fake_quantize_affine", "generated/torchao.quantization.fake_quantize_affine_cachemask", "generated/torchao.quantization.float8_dynamic_activation_float8_weight", "generated/torchao.quantization.float8_static_activation_float8_weight", "generated/torchao.quantization.float8_weight_only", "generated/torchao.quantization.fpx_weight_only", "generated/torchao.quantization.gemlite_uintx_weight_only", "generated/torchao.quantization.int4_weight_only", "generated/torchao.quantization.int8_dynamic_activation_int4_weight", "generated/torchao.quantization.int8_dynamic_activation_int8_weight", "generated/torchao.quantization.int8_weight_only", "generated/torchao.quantization.int_scaled_matmul", "generated/torchao.quantization.intx_quantization_aware_training", "generated/torchao.quantization.quantize_", "generated/torchao.quantization.quantize_affine", "generated/torchao.quantization.quantize_affine_floatx", "generated/torchao.quantization.safe_int_mm", "generated/torchao.quantization.smooth_fq_linear_to_inference", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear", "generated/torchao.quantization.to_linear_activation_quantized", "generated/torchao.quantization.uintx_weight_only", "generated/torchao.sparsity.PerChannelNormObserver", "generated/torchao.sparsity.WandaSparsifier", "generated/torchao.sparsity.apply_fake_sparsity", "generated/torchao.sparsity.int8_dynamic_activation_int8_semi_sparse_weight", "generated/torchao.sparsity.semi_sparse_weight", "generated/torchao.sparsity.sparsify_", "index", "performant_kernels", "quantization", "quick_start", "serialization", "sg_execution_times", "sparsity", "subclass_advanced", "subclass_basic", "tutorials/index", "tutorials/sg_execution_times", "tutorials/template_tutorial"], "filenames": ["api_ref_dtypes.rst", "api_ref_float8.rst", "api_ref_intro.rst", "api_ref_kernel.rst", "api_ref_quantization.rst", "api_ref_sparsity.rst", "contributor_guide.rst", "dtypes.rst", "generated/torchao.dtypes.AffineQuantizedTensor.rst", "generated/torchao.dtypes.BlockSparseLayout.rst", "generated/torchao.dtypes.CutlassInt4PackedLayout.rst", "generated/torchao.dtypes.CutlassSemiSparseLayout.rst", "generated/torchao.dtypes.Float8Layout.rst", "generated/torchao.dtypes.Int4CPULayout.rst", "generated/torchao.dtypes.Layout.rst", "generated/torchao.dtypes.MarlinQQQLayout.rst", "generated/torchao.dtypes.MarlinQQQTensor.rst", "generated/torchao.dtypes.MarlinSparseLayout.rst", "generated/torchao.dtypes.NF4Tensor.rst", "generated/torchao.dtypes.PlainLayout.rst", "generated/torchao.dtypes.SemiSparseLayout.rst", "generated/torchao.dtypes.TensorCoreTiledLayout.rst", "generated/torchao.dtypes.UintxLayout.rst", "generated/torchao.dtypes.to_affine_quantized_floatx.rst", "generated/torchao.dtypes.to_affine_quantized_floatx_static.rst", "generated/torchao.dtypes.to_affine_quantized_fpx.rst", "generated/torchao.dtypes.to_affine_quantized_intx.rst", "generated/torchao.dtypes.to_affine_quantized_intx_static.rst", "generated/torchao.dtypes.to_marlinqqq_quantized_intx.rst", "generated/torchao.dtypes.to_nf4.rst", "generated/torchao.float8.CastConfig.rst", "generated/torchao.float8.Float8LinearConfig.rst", "generated/torchao.float8.ScalingType.rst", "generated/torchao.float8.convert_to_float8_training.rst", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp.rst", "generated/torchao.quantization.MappingType.rst", "generated/torchao.quantization.TorchAODType.rst", "generated/torchao.quantization.ZeroPointDomain.rst", "generated/torchao.quantization.autoquant.rst", "generated/torchao.quantization.choose_qparams_affine.rst", "generated/torchao.quantization.choose_qparams_affine_floatx.rst", "generated/torchao.quantization.choose_qparams_affine_with_min_max.rst", "generated/torchao.quantization.choose_qparams_and_quantize_affine_hqq.rst", "generated/torchao.quantization.dequantize_affine.rst", "generated/torchao.quantization.dequantize_affine_floatx.rst", "generated/torchao.quantization.fake_quantize_affine.rst", "generated/torchao.quantization.fake_quantize_affine_cachemask.rst", "generated/torchao.quantization.float8_dynamic_activation_float8_weight.rst", "generated/torchao.quantization.float8_static_activation_float8_weight.rst", "generated/torchao.quantization.float8_weight_only.rst", "generated/torchao.quantization.fpx_weight_only.rst", "generated/torchao.quantization.gemlite_uintx_weight_only.rst", "generated/torchao.quantization.int4_weight_only.rst", "generated/torchao.quantization.int8_dynamic_activation_int4_weight.rst", "generated/torchao.quantization.int8_dynamic_activation_int8_weight.rst", "generated/torchao.quantization.int8_weight_only.rst", "generated/torchao.quantization.int_scaled_matmul.rst", "generated/torchao.quantization.intx_quantization_aware_training.rst", "generated/torchao.quantization.quantize_.rst", "generated/torchao.quantization.quantize_affine.rst", "generated/torchao.quantization.quantize_affine_floatx.rst", "generated/torchao.quantization.safe_int_mm.rst", "generated/torchao.quantization.smooth_fq_linear_to_inference.rst", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear.rst", "generated/torchao.quantization.to_linear_activation_quantized.rst", "generated/torchao.quantization.uintx_weight_only.rst", "generated/torchao.sparsity.PerChannelNormObserver.rst", "generated/torchao.sparsity.WandaSparsifier.rst", "generated/torchao.sparsity.apply_fake_sparsity.rst", "generated/torchao.sparsity.int8_dynamic_activation_int8_semi_sparse_weight.rst", "generated/torchao.sparsity.semi_sparse_weight.rst", "generated/torchao.sparsity.sparsify_.rst", "index.rst", "performant_kernels.rst", "quantization.rst", "quick_start.rst", "serialization.rst", "sg_execution_times.rst", "sparsity.rst", "subclass_advanced.rst", "subclass_basic.rst", "tutorials/index.rst", "tutorials/sg_execution_times.rst", "tutorials/template_tutorial.rst"], "titles": ["torchao.dtypes", "torchao.float8", "<code class=\"docutils literal notranslate\"><span class=\"pre\">torchao</span></code> API Reference", "torchao.kernel", "torchao.quantization", "torchao.sparsity", "Contributor Guide", "Dtypes", "AffineQuantizedTensor", "BlockSparseLayout", "CutlassInt4PackedLayout", "CutlassSemiSparseLayout", "Float8Layout", "Int4CPULayout", "Layout", "MarlinQQQLayout", "MarlinQQQTensor", "MarlinSparseLayout", "NF4Tensor", "PlainLayout", "SemiSparseLayout", "TensorCoreTiledLayout", "UintxLayout", "to_affine_quantized_floatx", "to_affine_quantized_floatx_static", "to_affine_quantized_fpx", "to_affine_quantized_intx", "to_affine_quantized_intx_static", "to_marlinqqq_quantized_intx", "to_nf4", "CastConfig", "Float8LinearConfig", "ScalingType", "convert_to_float8_training", "precompute_float8_dynamic_scale_for_fsdp", "MappingType", "TorchAODType", "ZeroPointDomain", "autoquant", "choose_qparams_affine", "choose_qparams_affine_floatx", "choose_qparams_affine_with_min_max", "choose_qparams_and_quantize_affine_hqq", "dequantize_affine", "dequantize_affine_floatx", "fake_quantize_affine", "fake_quantize_affine_cachemask", "float8_dynamic_activation_float8_weight", "float8_static_activation_float8_weight", "float8_weight_only", "fpx_weight_only", "gemlite_uintx_weight_only", "int4_weight_only", "int8_dynamic_activation_int4_weight", "int8_dynamic_activation_int8_weight", "int8_weight_only", "int_scaled_matmul", "intx_quantization_aware_training", "quantize", "quantize_affine", "quantize_affine_floatx", "safe_int_mm", "smooth_fq_linear_to_inference", "swap_linear_with_smooth_fq_linear", "to_linear_activation_quantized", "uintx_weight_only", "PerChannelNormObserver", "WandaSparsifier", "apply_fake_sparsity", "int8_dynamic_activation_int8_semi_sparse_weight", "semi_sparse_weight", "sparsify", "Welcome to the torchao Documentation", "Performant Kernels", "Quantization Overview", "Quick Start Guide", "Serialization", "Computation times", "Sparsity Overview", "Writing Your Own Quantized Tensor (advanced)", "Writing Your Own Quantized Tensor", "&lt;no title&gt;", "Computation times", "Template Tutorial"], "terms": {"thi": [2, 6, 8, 17, 18, 19, 20, 22, 34, 35, 38, 39, 41, 43, 45, 46, 58, 59, 66, 67, 68, 71, 74, 75, 76, 78, 80, 83], "section": [2, 6, 74, 78], "introduc": 2, "dive": 2, "detail": [2, 6, 38, 74, 75, 78, 80], "how": [2, 6, 8, 14, 22, 35, 39, 59, 75, 76, 78, 80], "integr": [2, 6, 76, 78, 80], "pytorch": [2, 6, 8, 13, 16, 36, 72, 75, 78, 80, 83], "optim": [2, 6, 17, 34, 38, 42, 58, 72, 78, 80], "your": [2, 6, 72, 74, 75, 78], "machin": 2, "learn": [2, 75, 78, 83], "model": [2, 34, 38, 58, 62, 63, 67, 68, 71, 75, 78, 80], "dtype": [2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 38, 39, 41, 42, 43, 44, 45, 46, 59, 71, 72, 75, 76, 80], "quantiz": [2, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 20, 21, 23, 26, 28, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 69, 71, 72, 76, 78], "sparsiti": [2, 11, 17, 20, 66, 67, 68, 69, 70, 71, 72, 74, 76], "tba": [3, 7, 73], "For": [6, 8, 74, 75, 76, 78, 80], "new": [6, 8, 74, 80], "case": [6, 38, 61, 78, 80], "exampl": [6, 8, 34, 35, 38, 58, 67, 71, 74, 76, 77, 78, 80, 81, 82, 83], "train": [6, 31, 45, 46, 72, 78, 80], "like": [6, 14, 38, 74, 75, 76, 78, 80], "fp4": 6, "s": [6, 8, 35, 38, 39, 43, 45, 59, 60, 74, 75, 78, 80], "fine": [6, 78], "start": [6, 32, 35, 36, 37, 38, 74, 78, 80], "prototyp": [6, 74], "folder": 6, "you": [6, 67, 74, 75, 76, 78, 80, 83], "could": [6, 74, 80], "also": [6, 38, 58, 74, 75, 76, 78, 80], "take": [6, 18, 58, 66, 71, 74, 78], "look": [6, 8, 74, 78], "affinequantizedtensor": [6, 16, 24, 25, 27, 74, 75, 76, 80], "what": [6, 8, 16, 38, 74, 75, 78, 83], "want": [6, 58, 71, 74, 76, 78, 80], "do": [6, 36, 38, 56, 58, 74, 78, 80], "mostli": [6, 41], "e": [6, 8, 35, 38, 39, 43, 45, 58, 59, 60, 74, 76, 80], "g": [6, 8, 35, 38, 39, 43, 45, 58, 59, 74, 76, 80], "int3": 6, "exact": 6, "same": [6, 8, 39, 41, 43, 45, 46, 59, 61, 71, 74, 78, 80], "affin": [6, 8, 10, 11, 12, 13, 17, 20, 21, 26, 43, 45, 59, 74], "pleas": [6, 8, 16, 72, 74, 78, 80], "feel": [6, 74, 78, 80], "free": [6, 74, 80], "open": [6, 74, 78], "an": [6, 8, 21, 26, 27, 38, 46, 67, 72, 74, 78, 80], "issu": [6, 74, 75, 80], "have": [6, 35, 38, 59, 67, 74, 78, 80], "question": [6, 74, 76, 78, 80], "specif": [6, 14, 17, 19, 20, 67, 74, 75, 76, 78], "more": [6, 8, 38, 46, 74, 75, 78, 80], "refer": [6, 8, 78, 80], "our": [6, 18, 75, 78, 80], "overview": [6, 72, 75], "page": [6, 75], "To": [6, 8, 16, 38, 74, 75, 76, 78], "contribut": [6, 75, 78], "exist": [6, 36, 74, 78, 80], "code": [6, 74, 75, 78, 80, 81, 83], "base": [6, 14, 19, 35, 67, 74, 75, 78, 80], "make": [6, 74, 80], "trainabl": [6, 74, 80], "add": [6, 19, 80, 83], "parallel": [6, 80], "etc": [6, 74], "affine_quantized_tensor": [6, 76], "py": [6, 8, 16, 77, 82, 83], "api": [6, 38, 74, 75, 78, 80], "quant_api": [6, 58, 76], "primit": [6, 8, 16, 80], "op": [6, 8, 16, 38, 45, 46, 58, 78, 80], "slight": [6, 78], "variat": [6, 74], "quant_primit": [6, 8, 16], "autotun": [6, 75], "cpu": [6, 8, 13, 76, 78], "cuda": [6, 8, 42, 58, 75, 76, 78, 80], "mp": 6, "csrc": 6, "mayb": [6, 30], "well": [6, 14, 38, 74, 78], "spars": [6, 9, 17, 20, 67, 74, 78], "marlin": [6, 15, 16, 17, 28], "aqt": 6, "621": 6, "we": [6, 8, 18, 35, 38, 39, 41, 43, 45, 58, 59, 71, 74, 75, 76, 78], "ar": [6, 8, 12, 20, 22, 33, 38, 39, 43, 45, 58, 59, 61, 67, 74, 75, 76, 78], "still": [6, 74, 78], "decid": [6, 74, 78], "split": 6, "can": [6, 21, 35, 38, 58, 59, 74, 75, 76, 78, 80], "implement": [6, 31, 76, 78], "regist": [6, 66, 80], "mai": [6, 41, 74, 76], "need": [6, 66, 67, 74, 75, 76, 78, 80], "defin": [6, 14, 22, 66, 67, 78, 80], "own": [6, 72, 78], "through": [6, 41, 74, 75, 80, 83], "int4": [6, 10, 13, 35, 58, 71, 75, 76], "access": 6, "my_custom_op": 6, "devic": [6, 8, 42, 58, 61, 75, 76, 80], "check": [6, 8, 16, 74, 75, 76, 80], "condit": [6, 74], "__torch_function__": [6, 74, 80], "__torch_dispatch__": [6, 80], "target": [6, 39, 67, 78], "oper": [6, 8, 12, 14, 17, 41], "bfloat16": [6, 18, 45, 59, 74, 75, 76, 78], "activ": [6, 38, 62, 67, 69, 72, 78], "uint4": [6, 74, 75], "weight": [6, 17, 18, 34, 38, 58, 67, 69, 71, 72, 75, 76, 78, 80], "found": [6, 74, 75, 78, 80], "here": [6, 8, 59, 74, 76, 80], "allow": [6, 78, 80], "peopl": [6, 74, 76], "linear": [6, 17, 31, 33, 38, 58, 63, 68, 69, 71, 74, 75, 76, 78, 80], "two": [6, 16, 20, 74, 78, 80], "dispatch_condit": [6, 74], "impl": [6, 8, 74], "actual": [6, 74, 80], "bia": [6, 74, 75, 76, 80], "run": [6, 34, 38, 58, 62, 66, 74, 78, 80, 83], "both": [6, 8, 74, 78, 80], "input_tensor": [6, 18, 74], "weight_tensor": [6, 74], "argument": [6, 8, 21, 38, 43, 58, 74], "register_aqt_quantized_linear_dispatch": 6, "show": [6, 59, 74, 78], "work": [6, 20, 76, 78, 80], "sometim": [6, 78], "ha": [6, 8, 74, 78, 80], "pack": [6, 8, 10, 21, 22, 74], "order": [6, 38, 74, 78, 80], "yield": [6, 78], "And": [6, 18, 74, 80], "abstract": [6, 74], "see": [6, 8, 16, 74, 75, 76, 78, 80], "full": [6, 83], "after": [6, 34, 38, 74, 76, 78], "wrap": [6, 38, 80], "factori": 6, "convert": [6, 8, 16, 18, 23, 26, 28, 29, 31, 58, 60, 71, 74, 78], "from": [6, 8, 18, 19, 24, 25, 27, 41, 45, 58, 59, 71, 74, 75, 76, 77, 78, 80, 82, 83], "float": [6, 8, 16, 18, 26, 28, 29, 35, 37, 38, 39, 41, 42, 43, 45, 46, 59, 60, 63, 67, 74, 76, 80], "point": [6, 8, 16, 28, 35, 37, 43, 45, 60, 74, 75, 76, 78, 80], "my": [6, 78], "to_my_dtyp": 6, "mydtypetensor": 6, "from_float": [6, 80], "level": [6, 67, 74, 78, 80], "reus": [6, 74, 80], "quantize_": [6, 58, 71, 74, 75, 76], "appli": [6, 8, 38, 58, 69, 71, 74, 75, 78], "convers": [6, 8, 33, 74], "filter": [6, 33, 38], "choos": [6, 74, 78, 80], "which": [6, 16, 22, 38, 74, 75, 76, 78], "modul": [6, 31, 32, 33, 34, 35, 36, 37, 38, 58, 62, 63, 66, 67, 71, 75, 76], "should": [6, 8, 34, 43, 45, 66, 67, 74, 78], "algorithm": [6, 78], "onli": [6, 13, 33, 71, 75, 76, 78, 80], "dynam": [6, 30, 31, 34, 71, 80], "quant": [6, 8, 16, 74], "static": [6, 8, 14, 18, 24, 27, 31, 41], "type": [6, 8, 17, 18, 22, 31, 32, 33, 35, 36, 37, 38, 42, 56, 59, 61, 72, 74, 76, 78, 80], "note": [6, 46, 67, 74, 75, 78, 80], "2": [6, 8, 11, 13, 17, 20, 35, 38, 46, 59, 68, 69, 71, 74, 75, 78, 80, 83], "4": [6, 11, 17, 20, 29, 42, 68, 69, 71, 74, 75, 76, 78, 80], "below": [6, 74, 78, 80, 83], "follow": [6, 74, 75, 78, 80], "util": [6, 74, 75, 76, 80], "import": [6, 58, 71, 75, 76, 78, 80, 83], "unwrap_tensor_subclass": [6, 75], "m_unwrap": 6, "m": [6, 58, 60, 71, 75, 76, 80], "In": [6, 74, 75, 78, 80], "compat": [6, 17, 75], "aim": [6, 74, 78], "fullgraph": [6, 75], "true": [6, 8, 26, 31, 38, 39, 41, 42, 58, 62, 71, 75, 76, 80], "first": [6, 18, 38, 56, 67, 74, 80], "remov": [6, 39, 67, 78], "ani": [6, 19, 38, 64, 67, 74, 78, 80], "unnecessari": 6, "graph": 6, "break": 6, "torch_log": 6, "output_cod": 6, "when": [6, 8, 19, 39, 43, 45, 59, 74, 78], "script": [6, 75, 80, 83], "inductor": [6, 38], "python": [6, 74, 78, 81, 83], "mode": [6, 38, 75], "max": [6, 35, 74, 75, 80], "checkout": [6, 8, 16, 72, 74], "doc": [6, 74, 80], "huggingfac": 6, "transform": [6, 8, 74], "deseri": [6, 74], "save_pretrain": 6, "push_to_hub": 6, "from_pretrain": 6, "http": [6, 8, 16, 38, 67, 75, 78], "co": 6, "main": [6, 8, 16, 74, 75, 78, 80], "en": [6, 38], "anoth": [6, 74, 78, 80], "diffus": 6, "github": [6, 8, 16, 75], "com": [6, 8, 16], "sayakpaul": 6, "blob": [6, 8, 16], "infer": [6, 8, 62, 72, 74, 75, 76, 78, 80], "serialization_and_load": 6, "md": 6, "The": [6, 8, 9, 14, 17, 22, 33, 38, 56, 58, 61, 62, 63, 67, 74, 75, 76, 78, 80], "abov": [6, 35, 74, 76, 78, 80], "just": [6, 35, 74, 76, 78, 80], "talk": [6, 74], "about": [6, 74, 75, 76, 78], "basic": [6, 19, 75, 80], "provid": [6, 14, 17, 20, 21, 38, 39, 74, 78, 80], "fsdp": [6, 74], "ll": [6, 35, 74, 80], "put": [6, 71], "developer_api_guid": 6, "cover": [6, 74, 83], "executorch": [6, 58], "torchchat": 6, "todo": [6, 74], "qat": [6, 45, 46], "suit": 6, "out": [6, 20, 35, 38, 67, 74, 75, 78, 80], "differ": [6, 14, 41, 59, 61, 74, 75, 76, 78, 80], "system": 6, "dtensor": [6, 80], "recommend": [6, 38], "copi": [6, 8, 67, 75, 76, 78, 80], "past": [6, 78], "adapt": 6, "now": [6, 39, 74, 75, 78, 80], "befor": [6, 58, 74, 76, 78, 80], "some": [6, 38, 58, 67, 74, 78, 80], "singl": [6, 30, 34, 38, 41, 75, 78], "comput": [6, 17, 21, 34, 66, 67, 78, 80], "intens": 6, "memori": [6, 8, 46, 75, 78, 80], "input": [6, 8, 17, 18, 20, 31, 33, 34, 38, 39, 41, 43, 45, 46, 56, 58, 59, 61, 67, 71, 74, 80], "dimens": [6, 8, 22, 39, 43, 45, 56, 59, 80], "get": [6, 18, 74, 78], "sens": [6, 74, 80], "speedup": [6, 74, 75, 78], "d": [6, 74], "creat": [6, 8, 24, 25, 27, 74, 78, 80], "file": [6, 77, 80, 82], "benchmark_aq": 6, "shape": [6, 8, 16, 38, 56, 61, 75, 80], "A": [6, 8, 22, 38, 41, 46, 66, 78, 80], "quick": [6, 72], "wai": [6, 8, 38, 74, 78, 80], "relev": [6, 74, 83], "chang": [6, 58, 74, 75, 76, 78, 80], "interest": [6, 74, 78, 80], "tutori": [6, 8, 74, 77, 78, 80, 81, 82], "print_op_and_shap": 6, "output": [6, 31, 38, 39, 43, 45, 59, 74, 78, 83], "torch_func": 6, "built": [6, 80], "k": [6, 61, 75, 76, 80], "n": [6, 75, 76, 80], "10": [6, 35, 59], "method": [6, 14, 17, 20, 21, 38, 58, 67, 78, 80], "_c": 6, "tensorbas": 6, "object": [6, 22, 58, 71, 80], "arg": [6, 8, 67, 80], "0": [6, 8, 38, 59, 63, 67, 75, 76, 77, 78, 80, 82, 83], "size": [6, 8, 9, 16, 18, 39, 43, 45, 59, 75, 76, 78, 80], "all": [6, 34, 35, 38, 41, 66, 67, 68, 74, 75, 76, 77, 78, 80, 81], "under": 6, "benchmark_your_kernel": 6, "helper": 6, "right": [6, 74, 78], "1": [6, 17, 22, 32, 35, 36, 37, 38, 42, 59, 67, 74, 75, 76, 77, 78, 80, 82, 83], "either": [6, 8, 45, 67, 78], "one": [6, 38, 41, 66, 74, 78, 80], "probabl": 6, "keep": [6, 17, 67], "futur": 6, "llama": 6, "llama2": 6, "llama3": 6, "sam": 6, "alreadi": [6, 8, 38, 80], "modifi": [6, 33, 58, 67, 74, 78, 80], "friendli": [6, 74], "compar": [6, 46, 67, 74], "techniqu": [6, 76, 78, 80], "repres": [6, 8, 9, 12, 14, 25, 31, 59, 67, 74, 76, 80], "bound": [6, 78], "help": [6, 74], "option": [6, 8, 12, 16, 23, 26, 27, 28, 30, 31, 33, 38, 39, 41, 43, 45, 46, 58, 59, 62, 63, 64, 67, 71, 75], "each": [6, 18, 38, 62, 66, 74, 78, 80], "understand": 6, "profil": 6, "profile_path": 6, "chrome": 6, "trace": [6, 74], "let": [6, 35, 59, 74, 75, 78, 80], "know": [6, 38, 80], "class": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 30, 31, 32, 35, 36, 37, 38, 66, 67, 74, 75, 76, 80], "torchao": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 78, 80], "tensor_impl": [8, 16, 74], "aqttensorimpl": [8, 16], "block_siz": [8, 14, 16, 18, 23, 24, 26, 27, 28, 29, 39, 41, 43, 45, 46, 59, 75], "tupl": [8, 16, 18, 23, 24, 26, 27, 28, 39, 41, 42, 43, 45, 46, 59, 67, 80], "int": [8, 9, 16, 18, 21, 22, 23, 24, 26, 27, 28, 29, 39, 40, 41, 42, 43, 44, 45, 46, 58, 59, 60, 67, 75, 80], "quant_min": [8, 16, 26, 27, 28, 35, 39, 41, 43, 45, 46, 59, 74, 75, 80], "union": [8, 16, 31, 39, 43, 45, 46, 58, 59], "none": [8, 12, 16, 23, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 43, 45, 46, 58, 59, 62, 63, 64, 67, 71, 80], "quant_max": [8, 16, 26, 27, 28, 35, 39, 41, 43, 45, 46, 59, 74, 75, 80], "zero_point_domain": [8, 16, 26, 27, 28, 39, 41, 45, 46], "zeropointdomain": [8, 16, 26, 27, 28, 39, 41, 45, 46], "stride": [8, 16, 74, 80], "sourc": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 56, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 71, 81, 83], "tensor": [8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 38, 39, 40, 41, 42, 43, 44, 45, 46, 56, 59, 60, 61, 64, 67, 72, 75, 76, 78, 83], "subclass": [8, 16, 33, 38, 66, 71, 75, 76, 78], "mean": [8, 18, 35, 39, 43, 45, 59, 60, 74, 75, 78], "quantized_tensor": 8, "float_tensor": [8, 80], "scale": [8, 14, 17, 24, 27, 34, 35, 37, 39, 41, 43, 44, 45, 46, 56, 59, 60, 62, 63, 74, 78, 80], "zero_point": [8, 14, 27, 37, 39, 41, 43, 45, 46, 59, 74, 78, 80], "happen": [8, 16, 38, 74, 80], "dure": [8, 16, 38, 43, 45, 63, 78, 80], "choose_qparam": [8, 74], "dequant": [8, 16, 18, 43, 74, 80], "ao": [8, 16, 78], "three": [8, 38, 67, 71, 74], "choose_qparams_affin": [8, 41, 74], "quantize_affin": [8, 45, 46, 74], "qand": 8, "dequantize_affin": [8, 45, 46], "extern": 8, "regardless": 8, "intern": [8, 21], "represent": [8, 14, 25, 74, 78], "orient": 8, "field": 8, "serv": [8, 14, 80], "gener": [8, 45, 46, 74, 75, 78, 80, 81, 83], "storag": [8, 17, 74, 78], "data": [8, 9, 14, 17, 22, 41, 72, 74, 76, 78, 80], "store": [8, 17, 18, 22, 66, 74, 78], "plain": 8, "int_data": [8, 80], "format": [8, 17, 18, 60, 74, 78], "depend": [8, 38, 76, 78, 80], "kernel": [8, 10, 11, 13, 17, 21, 58, 75, 78], "granular": [8, 39, 43, 45, 59, 74], "element": [8, 20, 22, 38, 39, 43, 45, 59, 78], "share": [8, 39, 43, 45, 59, 78], "qparam": [8, 39, 43, 45, 59], "us": [8, 12, 13, 14, 17, 18, 19, 22, 24, 27, 35, 38, 39, 41, 43, 45, 59, 67, 72, 74, 75, 76, 78, 80], "per": [8, 39, 43, 45, 59, 67, 69, 74, 75, 78], "torch": [8, 17, 18, 22, 24, 31, 33, 38, 39, 42, 43, 44, 45, 56, 58, 59, 61, 62, 63, 71, 74, 75, 76, 78, 80, 83], "origin": [8, 18, 45, 59, 67, 74, 75, 76, 78], "high": [8, 23, 24, 25, 26, 27, 60, 74, 78, 80], "precis": [8, 23, 24, 25, 26, 27, 60, 74, 80], "minimum": [8, 38, 39, 43, 45, 59], "valu": [8, 18, 31, 32, 35, 36, 37, 38, 39, 43, 45, 46, 59, 62, 67, 74, 78, 80], "specifi": [8, 31, 33, 45, 58, 59, 67, 71, 78], "deriv": [8, 41, 45, 59], "maximum": [8, 39, 43, 45, 59, 62], "domain": [8, 37, 39, 43, 45], "integ": [8, 26, 27, 35, 37, 39, 43, 45, 56, 61], "zero": [8, 20, 39, 43, 45, 67, 78], "ad": [8, 43, 45, 67, 78, 80], "subtract": [8, 18, 45], "unquant": [8, 45], "default": [8, 9, 12, 19, 21, 22, 38, 39, 43, 45, 58, 62, 63, 80], "float32": [8, 24, 43, 44, 45, 59, 60, 76, 78, 80], "given": [8, 16, 29, 78], "return": [8, 16, 17, 18, 33, 38, 46, 56, 58, 61, 62, 63, 71, 74, 75, 76, 80], "classmethod": [8, 16, 80], "from_hp_to_floatx": 8, "input_float": [8, 16, 23, 24, 25, 26, 27, 28, 64], "target_dtyp": [8, 23, 24, 26, 27, 30, 31, 39, 41, 74], "_layout": [8, 16, 23, 24, 25, 26, 27, 28, 74, 75], "layout": [8, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 71, 78], "scale_dtyp": [8, 23, 24, 26, 39, 41], "float8": [8, 11, 12, 23, 24, 30, 31, 32, 33, 34, 72, 74], "from_hp_to_floatx_stat": 8, "paramet": [8, 14, 17, 18, 24, 27, 33, 34, 35, 38, 39, 43, 45, 56, 58, 59, 61, 62, 63, 67, 71, 74, 76, 78, 80], "from_hp_to_fpx": 8, "floatx": [8, 25, 74], "ebit": [8, 25, 40, 44, 60], "mbit": [8, 25, 40, 44, 60], "support": [8, 25, 71, 75, 76, 78, 80], "float1": [8, 25], "float7": [8, 25], "from_hp_to_intx": [8, 16], "mapping_typ": [8, 26, 39, 41], "mappingtyp": [8, 26, 39, 41], "ep": [8, 26, 39, 41], "zero_point_dtyp": [8, 26, 39, 41], "preserve_zero": [8, 26, 39, 41], "bool": [8, 26, 31, 33, 38, 39, 41, 42, 58, 62, 71], "plainlayout": [8, 26, 27], "use_hqq": [8, 26], "fals": [8, 26, 31, 38, 42, 62, 67, 74, 75, 76, 80], "from_hp_to_intx_stat": 8, "kwarg": [8, 66, 67, 68, 80], "perform": [8, 21, 34, 38, 56, 61, 62, 66, 75, 78, 80], "self": [8, 74, 75, 76, 80], "If": [8, 12, 33, 38, 56, 61, 62, 67, 74, 75, 78, 80], "correct": [8, 17], "otherwis": [8, 74], "desir": [8, 38, 45], "call": [8, 38, 45, 46, 66, 74, 75, 76, 78, 80], "non_block": 8, "memory_format": 8, "preserve_format": 8, "set": [8, 12, 38, 41, 58, 62, 67, 75, 78], "function": [8, 21, 33, 38, 42, 58, 66, 67, 68, 71, 75, 76, 78, 80], "attempt": 8, "asynchron": 8, "respect": [8, 78], "host": 8, "possibl": [8, 78], "behavior": [8, 14], "pin": 8, "pageabl": 8, "howev": [8, 78], "caution": 8, "advis": [8, 74], "featur": [8, 80], "inform": [8, 78], "good": [8, 75, 80], "usag": [8, 34, 38], "pin_memori": 8, "even": [8, 78], "match": [8, 43, 56, 78], "other": [8, 14, 67, 76, 78, 80, 83], "randn": [8, 75, 76, 80], "initi": [8, 74, 76], "float64": 8, "5044": 8, "0005": 8, "3310": 8, "0584": 8, "cuda0": 8, "blocksiz": 9, "64": [9, 29, 42, 76, 80], "block": [9, 18, 67, 78], "matrix": [9, 12, 56, 61, 67, 75, 78], "variabl": [9, 12, 21, 22, 67, 78], "cutlass": [10, 11], "mm_config": 12, "float8mmconfig": 12, "configur": [12, 30, 31, 33, 58, 71, 74, 75], "multipl": [12, 38, 56, 61, 75, 78, 80], "involv": [12, 78], "tinygemm": [13, 58, 74, 75], "_weight_int4pack_mm_for_cpu": 13, "version": [13, 75, 80], "least": 13, "6": [13, 74, 75, 78], "It": [14, 17, 19, 21, 34, 78, 80], "pre": [14, 17, 21, 75, 78], "process": [14, 17, 19, 21, 22, 38, 63, 74, 78, 83], "post": [14, 21, 80], "addit": [14, 19, 38, 46, 78, 80], "design": [14, 17, 20], "extend": [14, 74, 78], "conjunct": 14, "tensorimpl": 14, "custom": [14, 66, 72, 74, 75, 78, 80], "interact": [14, 74], "qqq": [15, 16, 28], "marlinqqq": 16, "inherit": [16, 19, 80], "choose_qparams_and_quantize_affine_qqq": 16, "dequantize_affine_qqq": 16, "handl": [17, 20, 21, 38, 74], "pattern": [17, 20, 74], "ensur": 17, "preprocess": [17, 20], "manag": 17, "pre_process": 17, "1\u00ba": 17, "transpos": [17, 74, 80], "sinc": [17, 66, 74, 76, 78, 80], "layer": [17, 33, 38, 62, 63, 67, 68, 69, 78, 80], "2\u00ba": 17, "inject": 17, "3\u00ba": 17, "again": [17, 18, 78], "becaus": [17, 74, 76, 78, 80], "dim": [17, 80], "tensor_meta": 18, "subclasstensorarg": 18, "n_block": 18, "scaler_block_s": [18, 29], "quantized_scal": 18, "quantization_factor": 18, "scaler_mean": 18, "quantized_data": 18, "nf4": 18, "qlora": 18, "convert_to_norm_float_weight": 18, "normal": [18, 29, 38, 78], "dequantize_scal": 18, "unpack": [18, 60, 74], "doubl": 18, "scaler": 18, "int8": [18, 58, 69, 71, 74, 80], "per_scaler_block": 18, "factor": [18, 56, 63, 78], "inpt_weight": 18, "double_quantize_scal": 18, "achiev": [18, 78, 80], "calcul": [18, 34, 35, 39, 41, 62, 74, 78], "absmax": 18, "find": [18, 78], "posit": 18, "typic": [18, 19, 74, 76], "per_block": 18, "int16": 18, "n_scaler_block": 18, "get_original_weight": 18, "quantize_tensor_nearest": 18, "float16": [18, 42, 45, 59, 78], "nearest": 18, "round": [18, 35, 80], "up": [18, 58, 74, 75, 78], "most": [19, 74, 78], "doe": [19, 74, 78, 80], "metadata": [19, 74, 80], "step": [19, 34, 38, 74, 78], "requir": [19, 21, 74, 78, 80], "semi": [20, 71, 78], "structur": [20, 71, 75, 76, 78, 80], "matric": [20, 78], "where": [20, 35, 41, 60, 74, 78], "everi": [20, 66, 78, 80], "four": 20, "prune": [20, 67], "conform": 20, "inner_k_til": [21, 75], "8": [21, 22, 35, 74, 75], "core": [21, 36, 74], "tile": [21, 74], "fit": [21, 74, 76], "effici": [21, 75, 78], "affect": [21, 78], "matmul": [21, 74, 78, 80], "pack_dim": 22, "uintx": [22, 74], "smaller": [22, 75, 76], "bit": [22, 29, 60, 80], "width": 22, "than": [22, 74, 78, 80], "standard": [22, 74], "byte": 22, "uintxtensor": 22, "determin": [22, 39, 45, 78], "along": [22, 78], "indic": [22, 37, 78], "last": 22, "256": 29, "scaling_typ": [30, 31], "scalingtyp": [30, 31], "scaling_granular": [30, 31], "scalinggranular": [30, 31], "tensorwis": [30, 31], "cast": [30, 45, 46], "cast_config_input": 31, "config": [31, 33, 38, 58, 67, 71, 78], "castconfig": 31, "cast_config_input_for_grad_weight": 31, "cast_config_weight": 31, "cast_config_weight_for_grad_input": 31, "cast_config_grad_output": 31, "cast_config_grad_output_for_grad_weight": 31, "gemm_config_output": 31, "float8gemmconfig": 31, "use_fast_accum": 31, "gemm_config_grad_input": 31, "gemm_config_grad_weight": 31, "enable_fsdp_float8_all_gath": 31, "pad_inner_dim": 31, "emul": 31, "force_recompute_fp8_weight_in_bwd": 31, "round_scales_to_power_of_2": 31, "nn": [31, 33, 38, 58, 62, 63, 71, 74, 75, 76, 78, 80], "from_recipe_nam": 31, "recipe_nam": 31, "float8linearrecipenam": 31, "str": [31, 33, 42, 58, 63, 64, 67, 71, 80], "string": [31, 67], "recip": [31, 66], "name": [32, 35, 36, 37, 58, 63, 67, 71, 78, 80], "qualnam": [32, 35, 36, 37], "boundari": [32, 35, 36, 37], "module_filter_fn": 33, "callabl": [33, 38, 42, 58, 64, 71], "float8linearconfig": 33, "swap": [33, 74, 78], "float8linear": 33, "pass": [33, 38, 41, 66, 74, 80], "instanc": [33, 58, 66, 71, 76, 80], "fqn": [33, 67, 71], "reduc": [34, 78], "sum": 34, "backward": [34, 78], "number": [35, 38, 60, 67, 78, 80], "map": [35, 74, 80], "symmetr": [35, 39, 69, 80], "rang": [35, 78], "sai": [35, 59, 74], "3": [35, 38, 59, 74, 75, 78, 83], "5": [35, 63, 67, 75, 78, 83], "7": 35, "symmetric_no_clipping_err": 35, "variant": [35, 41, 80], "smin": 35, "smax": 35, "min_val_neg": [35, 80], "max_val_po": [35, 80], "By": [35, 78], "individu": [35, 78], "less": [35, 78, 80], "error": [35, 38, 80], "neg": 35, "asymmetr": [35, 39, 74, 75], "directli": [35, 41, 74, 78, 80], "placehold": 36, "yet": [36, 80], "enum": 37, "whether": [37, 38, 39, 80], "quantized_v": 37, "float_val": 37, "mid_point": 37, "example_input": [38, 75, 76], "qtensor_class_list": 38, "aqdefaultlinearweight": 38, "aqint8weightonlyquantizedlinearweight": 38, "aqint8weightonlyquantizedlinearweight2": 38, "aqint8dynamicallyquantizedlinearweight": 38, "filter_fn": [38, 58, 71], "interpol": 38, "85": 38, "manual": 38, "set_inductor_config": 38, "supress_autoquant_error": 38, "min_sqnr": 38, "aq_kwarg": 38, "autoquant": 38, "identifi": 38, "fastest": 38, "over": [38, 78], "potenti": [38, 78], "qtensor": 38, "prepar": [38, 62, 67, 74, 78], "search": [38, 78], "whose": 38, "exchang": 38, "autoquantizablelinearweight": 38, "calibr": [38, 41], "user": [38, 74, 75, 78, 80, 83], "seen": 38, "record": [38, 74], "so": [38, 74, 75, 76, 78, 80], "final": [38, 46, 58, 74, 75, 78], "benchmark": [38, 62], "member": 38, "pick": 38, "result": [38, 56, 60, 61, 74, 78], "highli": 38, "complet": 38, "simpli": [38, 78, 80], "had": [38, 80], "compil": [38, 58, 61, 74, 75, 80], "them": [38, 66, 74], "onc": [38, 78], "proce": 38, "combin": [38, 78, 80], "finalize_autoqu": 38, "been": [38, 80], "log": [38, 80], "forward": [38, 66, 74, 75, 76, 78, 80], "fulli": [38, 58, 63, 71, 78], "unless": 38, "list": [38, 43, 63, 67, 74, 75, 80], "default_autoquant_class_list": 38, "contain": [38, 62, 63, 78, 80], "second": [38, 56, 74, 83], "stop": 38, "wait": [38, 74], "sever": 38, "automat": [38, 80, 83], "suppress": 38, "accept": 38, "signal": 38, "nois": 38, "ration": 38, "wikipedia": 38, "org": [38, 67, 74, 75, 78], "wiki": 38, "noise_ratio": 38, "v": 38, "non": [38, 74, 78, 80], "impact": 38, "caus": 38, "too": 38, "larg": [38, 80], "numer": [38, 78], "resaon": 38, "40": 38, "adjust": 38, "keyword": 38, "example_input1": 38, "example_input2": 38, "int32": [39, 74, 75], "fp32": [39, 43, 80], "bf16": [39, 74, 75, 78], "fp16": 39, "optioanl": 39, "param": [39, 41, 46, 67], "preserv": [39, 67, 78], "request": [39, 43, 59], "min_val": [41, 74, 80], "max_val": [41, 74, 80], "instead": [41, 66, 74, 75, 78, 80], "observ": [41, 66, 78], "obtain": 41, "track": [41, 74], "nbit": 42, "group_siz": [42, 58, 75], "axi": [42, 59], "compute_dtyp": 42, "verbos": 42, "raw_output": 42, "optimize_weight": 42, "optimize_weights_proximal_legaci": 42, "input_dtyp": 43, "output_dtyp": [43, 44, 59], "uint8": [43, 59, 74], "quant_dtyp": [45, 46], "fake": [45, 46], "awar": [45, 46, 67, 78, 80], "equival": [45, 46, 63, 78], "without": [45, 46, 74, 78], "valid": 45, "fake_quantize_affin": 46, "consum": 46, "outlier": 46, "mask": [46, 67, 78], "intermedi": 46, "alia": [47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 65, 70], "float8dynamicactivationfloat8weightconfig": 47, "float8staticactivationfloat8weightconfig": 48, "float8weightonlyconfig": 49, "fpxweightonlyconfig": 50, "gemliteuintxweightonlyconfig": 51, "int4weightonlyconfig": 52, "int8dynamicactivationint4weightconfig": 53, "int8dynamicactivationint8weightconfig": 54, "int8weightonlyconfig": 55, "b": 56, "scales1": 56, "multipli": [56, 61, 78], "row": [56, 78], "rais": [56, 61, 80], "assertionerror": [56, 61, 80], "expect": [56, 78, 80], "intxquantizationawaretrainingconfig": 57, "aobaseconfig": [58, 71], "inplac": [58, 67, 75], "workflow": [58, 71, 75, 78], "qualifi": [58, 63, 71, 78], "move": [58, 74], "speed": [58, 78], "predefin": 58, "correspond": [58, 74, 76, 78, 80], "execut": [58, 77, 80, 82], "path": [58, 61, 75], "customiz": 58, "current": [58, 63, 67, 71, 78, 80], "int8_dynamic_activation_int4_weight": 58, "int8_dynamic_activation_int8_weight": [58, 71], "mm": [58, 80], "int4_weight_onli": [58, 74, 75, 76], "int8_weight_onli": 58, "sequenti": [58, 71], "32": [58, 71, 75, 76, 80], "1024": [58, 71, 75, 76], "tabl": [59, 74, 78], "per_tensor": 59, "per_axi": 59, "per_group": 59, "groupsiz": 59, "low": [60, 78, 80], "00seeemm": 60, "fp6_e3m2": 60, "sign": 60, "expon": 60, "mantissa": 60, "mat2": 61, "safe": 61, "consid": [61, 74, 78], "cubla": 61, "fallback": 61, "i": [61, 78], "j": 61, "debug_skip_calibr": 62, "smoothquant": [62, 63], "smoothfakedynamicallyquantizedlinear": [62, 63], "debug": 62, "skip_fqn_list": 63, "cur_fqn": 63, "alpha": 63, "replac": [63, 78], "skip": [63, 67, 78], "being": [63, 74, 78], "input_quant_func": [64, 74], "quant_kwarg": 64, "dict": [64, 67, 80], "uintxweightonlyconfig": 65, "l2": [66, 78], "norm": [66, 67, 78], "channel": [66, 69], "buffer": 66, "x_orig": 66, "overridden": 66, "although": [66, 80], "within": [66, 78], "afterward": 66, "former": 66, "care": [66, 76, 78], "hook": [66, 74], "while": [66, 67, 78, 80], "latter": 66, "silent": 66, "ignor": 66, "sparsity_level": [67, 78], "semi_structured_block_s": 67, "wanda": 67, "sparsifi": [67, 72, 76, 78], "propos": 67, "arxiv": [67, 78], "ab": [67, 78], "2306": 67, "11695": 67, "product": 67, "magnitud": [67, 78], "control": [67, 78], "parametr": 67, "deepcopi": [67, 75, 80], "squash_mask": [67, 78], "params_to_keep": 67, "params_to_keep_per_lay": 67, "squash": 67, "appropri": [67, 74], "sparse_param": 67, "attach": [67, 78], "kei": [67, 78, 83], "save": [67, 75, 76], "xdoctest": 67, "local": [67, 78], "undefin": 67, "don": [67, 75, 78], "t": [67, 74, 75, 78, 80], "hasattr": 67, "submodule1": 67, "linear1": [67, 75, 76, 80], "foo": 67, "bar": 67, "submodule2": 67, "linear42": 67, "baz": 67, "print": [67, 75, 76, 80, 83], "42": 67, "24": 67, "ones": [67, 74], "update_mask": 67, "tensor_nam": 67, "statist": [67, 74, 78], "retriev": 67, "act_per_input": 67, "Then": [67, 80], "metric": 67, "across": [67, 78, 80], "whole": 67, "simul": [68, 74, 78], "dnynam": 69, "token": 69, "semisparseweightconfig": 70, "sparsify_": 71, "apply_tensor_subclass": [71, 74], "essenti": 71, "semi_sparse_weight": 71, "semisparselayout": 71, "sparsemarlinlayout": 71, "def": [71, 74, 75, 76, 80], "isinst": [71, 78, 80], "sparse_api": 71, "librari": [72, 76], "gradient": [72, 78], "nativ": [72, 80], "readm": [72, 75, 78], "overal": [72, 75], "introduct": [72, 74], "recent": 72, "highlight": [72, 80, 83], "updat": [72, 75, 76, 78], "guid": [72, 74], "contributor": [72, 75], "serial": [72, 74], "write": 72, "advanc": [72, 80], "lai": 74, "stack": 74, "hqq": 74, "awq": 74, "gptq": 74, "codebookquantizedtensor": 74, "uint1": 74, "uint7": 74, "int1": 74, "float3": 74, "compon": [74, 80], "tensorcoretiledlayout": [74, 75], "compos": [74, 78, 80], "overload": [74, 78], "term": [74, 78], "extra": 74, "empti": 74, "dev": 74, "discuss": [74, 80], "1833": 74, "No": [74, 76, 78], "matter": [74, 78], "end": [74, 78, 80, 83], "avail": 74, "later": [74, 80], "float3_e2_m0": 74, "float4_e2_m1": 74, "float4_e3_m0": 74, "float5_e2_m2": 74, "float5_e3_m1": 74, "float6_e2_m3": 74, "float6_e3_m2": 74, "float8_e4m3fn": 74, "float8_e5m2": 74, "float8_e4m3fnuz": 74, "float8_e5m2fnuz": 74, "plan": 74, "float4": 74, "float6": 74, "thei": [74, 78, 80], "becom": 74, "popular": 74, "hardwar": [74, 78], "part": [74, 78, 80], "uint2": 74, "117208": 74, "outsid": 74, "As": 74, "mention": 74, "criteria": 74, "wide": 74, "adopt": 74, "fundament": [74, 78], "until": 74, "evid": 74, "hopefulli": 74, "amen": 74, "haven": 74, "enough": 74, "ont": 74, "revisit": 74, "intx": 74, "connect": 74, "int4tensor": 74, "previou": 74, "between": [74, 78, 80], "preicison": 74, "mainli": 74, "There": [74, 80], "accommod": 74, "choose_qparams_affine_with_min_max": 74, "min": [74, 80], "_weight_int4pack_mm": 74, "int_matmul": 74, "int_scaled_matmul": 74, "reli": [74, 78, 80], "triton": 74, "On": [74, 75], "top": [74, 80], "glue": 74, "everyth": 74, "togeth": 74, "build": [74, 78, 80], "construct": 74, "low_precision_v": 74, "high_precision_v": 74, "procedur": 74, "veri": [74, 78], "common": [74, 78], "straightforward": 74, "try": [74, 78, 80], "higher": [74, 80], "lower": [74, 78], "high_preicsion_v": 74, "especi": [74, 76, 78], "bitwidth": 74, "codebook": 74, "hardcod": 74, "select": 74, "multi": 74, "dimension": [74, 78], "view": [74, 80], "mkldnn": 74, "coo": [74, 78], "sparse_coo": [74, 78], "sparsetensorimpl": 74, "idea": [74, 78], "nice": [74, 78], "concept": [74, 83], "why": [74, 80, 83], "c": [74, 80], "conflict": 74, "properti": 74, "quantized_linear": 74, "semant": 74, "stai": [74, 75, 80], "develop": 74, "tradition": 74, "come": [74, 78, 79], "demonstr": [74, 75, 80], "purpos": [74, 80], "to_affine_quant": 74, "simplic": 74, "explain": 74, "simplest": [74, 78], "form": [74, 78], "easi": 74, "linear_modul": 74, "to_affine_quantized_intx": 74, "requires_grad": [74, 80], "runtim": 74, "to_linear_activation_quant": 74, "quantized_weight": 74, "activation_and_weight_quant": 74, "encount": 74, "f": [74, 76, 78, 80], "input_qunat_func": 74, "redispatch": 74, "fx": 74, "symbolic_trac": 74, "But": [74, 80], "prefer": [74, 75, 80], "easier": 74, "further": [74, 80], "modif": 74, "sampl": 74, "figur": [74, 78], "At": [74, 78], "collect": [74, 78], "thing": [74, 76, 78, 80], "address": 74, "stat": 74, "averag": 74, "calculate_qparam": 74, "affinequantizedminmaxobserv": 74, "insert_observer_": 74, "altern": [74, 80], "observedlinear": 74, "dataset": 74, "complic": [74, 78], "next": 74, "done": [74, 80], "manner": 74, "intend": 74, "autoround": 74, "multitensor": 74, "sure": 74, "describ": [74, 76, 78, 83], "focus": [74, 78], "todai": 74, "low_bit_optim": 74, "similar": [74, 78], "quantized_train": 74, "enabl": 74, "progress": 74, "lot": [74, 78], "includ": [74, 80], "walk": [74, 80, 83], "_convert_weight_to_int4pack": 74, "aten": [74, 80], "group": [74, 75], "tensor_core_til": 74, "tensorcoretiledaqttensorimpl": 74, "_quantized_linear_op": 74, "goe": 74, "_aqt_qlinear_dispatch_t": 74, "dispatch": 74, "explan": 74, "wint4": 74, "explor": 75, "instal": 75, "latest": 75, "stabl": 75, "releas": 75, "pip": 75, "nightli": 75, "command": 75, "index": [75, 78], "url": 75, "download": [75, 81, 83], "whl": 75, "cu121": 75, "major": 75, "instruct": 75, "entri": 75, "mutat": 75, "insert": 75, "logic": [75, 80], "toi": [75, 80], "toylinearmodel": [75, 76], "__init__": [75, 76, 80], "super": [75, 76, 80], "linear2": [75, 76, 80], "x": [75, 76, 80, 83], "eval": [75, 76], "faster": [75, 78], "model_bf16": 75, "leverag": [75, 80], "int4mm": 75, "mix": 75, "readi": [75, 80], "in_featur": [75, 76, 80], "out_featur": [75, 80], "tensor_impl_dtyp": 75, "15": 75, "verifi": [75, 76, 80], "roughli": [75, 78], "quarter": 75, "os": 75, "tmp": 75, "int4_model": 75, "pt": 75, "bfloat16_model": 75, "int4_model_size_mb": 75, "getsiz": 75, "bfloat16_model_size_mb": 75, "2f": 75, "mb": [75, 76, 77, 82], "25": 75, "00": [75, 77, 82], "much": [75, 78], "torch_version_at_least_2_5": 75, "benchmark_model": 75, "temporari": 75, "workaround": 75, "num_run": 75, "100": [75, 80], "_dynamo": [75, 80], "reset": 75, "bf16_time": 75, "int4_tim": 75, "time": [75, 78, 80, 83], "3f": 75, "ms": 75, "1fx": 75, "a100": 75, "gpu": [75, 83], "80gb": 75, "30": 75, "393": 75, "410": 75, "9x": 75, "simpl": [75, 78, 80], "visit": 75, "would": [75, 78, 80], "forget": 75, "tempfil": 76, "get_model_size_in_byt": 76, "batch_siz": 76, "ref": 76, "namedtemporaryfil": 76, "state_dict": 76, "seek": [76, 78], "load": 76, "meta": 76, "m_load": 76, "load_state_dict": 76, "assign": 76, "re": [76, 80], "assert": [76, 80], "equal": [76, 78], "float_weight1": 76, "float_weight2": 76, "quantized_weight1": 76, "quantized_weight2": 76, "go": [76, 80, 83], "techinqu": 76, "reduct": [76, 78, 80], "around": 76, "4x": 76, "0625": 76, "reason": [76, 78], "avoid": [76, 78], "properli": 76, "003": [77, 82, 83], "total": [77, 82, 83], "galleri": [77, 81, 83], "mem": [77, 82], "templat": [77, 81, 82], "tutorials_sourc": 77, "template_tutori": [77, 82, 83], "neural": 78, "network": [78, 80], "its": [78, 80], "overhead": 78, "latenc": 78, "carefulli": 78, "signific": 78, "pai": 78, "price": 78, "qualiti": 78, "accuraci": 78, "f1": 78, "problem": [78, 80], "research": [78, 83], "face": 78, "fragment": 78, "rightfulli": 78, "spent": 78, "compress": 78, "place": 78, "dens": 78, "solv": [78, 80], "focu": [78, 80], "realli": 78, "push": 78, "accur": 78, "concret": 78, "hope": 78, "modular": 78, "acceler": 78, "scratch": [78, 83], "minim": 78, "loss": 78, "recov": 78, "algorthim": 78, "realiz": 78, "improv": 78, "trade": 78, "off": 78, "degrad": 78, "architectur": 78, "theoret": 78, "gain": 78, "2x": 78, "analog": 78, "fix": 78, "50": 78, "unstructur": 78, "One": [78, 80], "chosen": 78, "close": 78, "relat": 78, "mitig": 78, "retrain": 78, "neglig": 78, "area": 78, "agre": 78, "upon": 78, "consensu": 78, "mind": 78, "thought": 78, "separ": 78, "subproblem": 78, "satisfi": 78, "consist": [78, 80], "answer": 78, "independ": 78, "frontend": 78, "arbitrari": 78, "backend": 78, "handoff": 78, "piec": 78, "miss": 78, "natur": [78, 80], "present": 78, "clear": 78, "contract": 78, "7x": 78, "advantag": 78, "fast": 78, "anticip": 78, "mani": [78, 80], "solut": 78, "third": 78, "parti": 78, "to_sparse_semi_structur": 78, "sparsesemistructuredtensor": 78, "weightnormsparsifi": 78, "half": 78, "subnetwork": 78, "sparse_config": 78, "mod": [78, 80], "named_modul": 78, "append": 78, "tensor_fqn": 78, "sparse_block_shap": 78, "zeros_per_block": 78, "fakespars": 78, "flow": 78, "manipul": 78, "dictionari": 78, "paramer": 78, "parameter": 78, "necessari": [78, 80], "ve": 78, "suitabl": 78, "fuse": [78, 80], "0s": 78, "spot": 78, "definit": 78, "academia": 78, "industri": 78, "often": [78, 80], "interchang": 78, "confus": 78, "distinct": 78, "pretrain": 78, "behind": 78, "doesn": 78, "box": 78, "itself": [78, 80], "those": [78, 80], "loos": 78, "speak": 78, "tightli": 78, "coupl": [78, 80], "nvidia": 78, "csc": 78, "fbgemm": 78, "qnnpack": 78, "descript": 78, "coordin": 78, "vector": 78, "locat": 78, "bsr": 78, "sparse_bsr": 78, "except": [78, 80], "scalar": 78, "csr": 78, "sparse_csr": 78, "sparse_csc": 78, "column": 78, "compact": 78, "sparse_matrix": 78, "1d": 78, "indexptr": 78, "\u00bd": 78, "bitmask": 78, "2bit": 78, "unprun": 78, "quit": [78, 80], "must": 78, "successfulli": 78, "These": [78, 80], "broken": 78, "down": 78, "Not": 78, "sensit": 78, "effect": [78, 80], "best": 78, "subsequ": [78, 80], "infinit": 78, "lost": 78, "degre": 78, "analysi": 78, "drop": 78, "give": [78, 80], "curv": 78, "proxi": 78, "aforement": 78, "smallest": 78, "absolut": 78, "vs": 78, "global": [78, 80], "scope": 78, "impli": 78, "pro": 78, "con": 78, "sub": 78, "tradeoff": 78, "span": 78, "threshold": 78, "increas": 78, "complex": 78, "constant": [78, 80], "ctr_mobile_fe": 78, "paper": [78, 83], "score": 78, "w": 78, "tenosr": 78, "udpat": 78, "cannot": 78, "histori": 78, "regrow": 78, "dw": 78, "via": 78, "backprop": 78, "pat": 78, "unmask": 78, "resid": 78, "salienc": 78, "lowest": 78, "l1": 78, "commonli": 78, "shown": 78, "abl": [78, 80], "ident": 78, "repeat": 78, "loop": 78, "shot": 78, "movement": 78, "tune": 78, "2005": 78, "07683": 78, "rank": [78, 80], "wx": 78, "sqx": 78, "q": 78, "usual": 78, "sort": 78, "wise": 78, "reconstruct": 78, "random": 78, "randomli": 78, "tri": 78, "remedi": 78, "line": 78, "item": [78, 83], "ultim": 78, "literatur": 78, "vision": 78, "nlp": [78, 83], "iter": 78, "ctr_feed": 78, "na": 78, "multimask": 78, "pyspeech": 78, "fastna": 78, "approach": [78, 80], "knowledg": [78, 83], "distil": 78, "pdf": 78, "2204": 78, "09656": 78, "arrang": 78, "recal": 78, "counterpart": 78, "slower": 78, "suffici": 78, "flexibl": [78, 80], "98": 78, "benefit": [78, 80], "special": 78, "exhibit": 78, "maintain": 78, "penalti": 78, "expens": [78, 80], "dictat": 78, "characterist": 78, "highest": 78, "wouldn": [78, 80], "visual": 78, "fig": 78, "4x4": 78, "benchmak": 78, "soon": 79, "foundat": 80, "extens": 80, "autograd": 80, "distribut": 80, "express": 80, "interpos": 80, "namespac": 80, "continu": 80, "seamlessli": 80, "obviou": 80, "int8quantizedlinear": 80, "few": 80, "finer": 80, "grain": 80, "intercept": 80, "slightli": 80, "contrast": 80, "long": 80, "better": 80, "clunki": 80, "distributedlinear": 80, "duplic": 80, "bypass": 80, "offer": 80, "outer": 80, "inner": 80, "allgath": 80, "bandwidth": 80, "exactli": 80, "rest": 80, "read": 80, "document": 80, "zoo": 80, "podcast": 80, "edward": 80, "yang": 80, "begin": 80, "int8_symmetric_quant": 80, "fp32_tensor": 80, "128": 80, "127": 80, "amin": 80, "keepdim": 80, "amax": 80, "zeros_lik": 80, "clamp": 80, "quantizedlinear": 80, "w_int8": 80, "cl": 80, "new_linear": 80, "left": 80, "toymodel": 80, "float_model": 80, "quantized_model": 80, "child": 80, "named_children": 80, "setattr": 80, "drawback": 80, "won": 80, "suppos": 80, "clean": 80, "limit": 80, "eleg": 80, "pretti": 80, "power": 80, "overrid": 80, "almost": 80, "shard": 80, "ragged": 80, "rag": 80, "nestedtensor": 80, "resourc": 80, "who": 80, "link": [80, 83], "googl": 80, "collab": 80, "flopcount": 80, "memorytrack": 80, "With": 80, "bare": 80, "bone": 80, "int8symmetrictensor": 80, "hold": 80, "staticmethod": 80, "disabl": 80, "__new__": 80, "_make_wrapper_subclass": 80, "storage_offset": 80, "ndim": 80, "__tensor_flatten__": 80, "attribut": 80, "pt2": 80, "__tensor_unflatten__": 80, "tensor_data_dict": 80, "extra_metadata": 80, "outer_s": 80, "outer_strid": 80, "undo": 80, "back": 80, "__repr__": 80, "repr": 80, "ahead": 80, "insid": 80, "int8_tensor": 80, "func": 80, "op_implementations_dict": 80, "conveni": 80, "register_op": 80, "_op": 80, "opoverload": 80, "impl_decor": 80, "op_impl": 80, "wrapper": 80, "particular": 80, "largest": 80, "tell": 80, "desugar": 80, "decor": 80, "constructor": 80, "surfac": 80, "coverag": 80, "though": 80, "brute": 80, "forc": 80, "repeatedli": 80, "loggingtensor": 80, "_python_dispatch": 80, "return_and_correct_alias": 80, "int8_mm": 80, "detach": 80, "int8_view_op": 80, "out_data": 80, "out_scal": 80, "notic": 80, "quickli": 80, "hit": 80, "background": 80, "decomposit": 80, "live": 80, "decomp": 80, "shrink": 80, "author": [80, 83], "pain": 80, "rather": 80, "underli": 80, "worth": 80, "written": 80, "differenti": 80, "nuanc": 80, "longer": 80, "That": 80, "transposit": 80, "got": 80, "propag": 80, "fact": 80, "themselv": 80, "pointwis": 80, "alwai": 80, "were": 80, "might": 80, "unwrap": 80, "dim0": 80, "dim1": 80, "confirm": 80, "quantized_model_module_swap": 80, "quantized_model_subclass": 80, "subclass_param": 80, "no_grad": 80, "out_module_swap": 80, "allclos": 80, "out_compil": 80, "seri": 80, "wa": 80, "tutorials_python": 81, "zip": [81, 83], "jupyt": [81, 83], "notebook": [81, 83], "tutorials_jupyt": 81, "sphinx": [81, 83], "firstnam": 83, "lastnam": 83, "prerequisit": 83, "v2": 83, "topic": 83, "rand": 83, "0317": 83, "5894": 83, "4580": 83, "4062": 83, "9416": 83, "0184": 83, "0532": 83, "4349": 83, "6331": 83, "5711": 83, "4372": 83, "9014": 83, "6030": 83, "6495": 83, "6387": 83, "practic": 83, "test": 83, "summar": 83, "takeawai": 83, "link1": 83, "link2": 83, "minut": 83, "ipynb": 83}, "objects": {"torchao.dtypes": [[8, 0, 1, "", "AffineQuantizedTensor"], [9, 0, 1, "", "BlockSparseLayout"], [10, 0, 1, "", "CutlassInt4PackedLayout"], [11, 0, 1, "", "CutlassSemiSparseLayout"], [12, 0, 1, "", "Float8Layout"], [13, 0, 1, "", "Int4CPULayout"], [14, 0, 1, "", "Layout"], [15, 0, 1, "", "MarlinQQQLayout"], [16, 0, 1, "", "MarlinQQQTensor"], [17, 0, 1, "", "MarlinSparseLayout"], [18, 0, 1, "", "NF4Tensor"], [19, 0, 1, "", "PlainLayout"], [20, 0, 1, "", "SemiSparseLayout"], [21, 0, 1, "", "TensorCoreTiledLayout"], [22, 0, 1, "", "UintxLayout"], [23, 2, 1, "", "to_affine_quantized_floatx"], [24, 2, 1, "", "to_affine_quantized_floatx_static"], [25, 2, 1, "", "to_affine_quantized_fpx"], [26, 2, 1, "", "to_affine_quantized_intx"], [27, 2, 1, "", "to_affine_quantized_intx_static"], [28, 2, 1, "", "to_marlinqqq_quantized_intx"], [29, 2, 1, "", "to_nf4"]], "torchao.dtypes.AffineQuantizedTensor": [[8, 1, 1, "", "dequantize"], [8, 1, 1, "", "from_hp_to_floatx"], [8, 1, 1, "", "from_hp_to_floatx_static"], [8, 1, 1, "", "from_hp_to_fpx"], [8, 1, 1, "", "from_hp_to_intx"], [8, 1, 1, "", "from_hp_to_intx_static"], [8, 1, 1, "", "to"]], "torchao.dtypes.MarlinQQQTensor": [[16, 1, 1, "", "dequantize"], [16, 1, 1, "", "from_hp_to_intx"]], "torchao.dtypes.MarlinSparseLayout": [[17, 1, 1, "", "pre_process"]], "torchao.dtypes.NF4Tensor": [[18, 1, 1, "", "convert_to_norm_float_weight"], [18, 1, 1, "", "dequantize"], [18, 1, 1, "", "dequantize_scalers"], [18, 1, 1, "", "double_quantize_scalers"], [18, 1, 1, "", "get_original_weight"], [18, 1, 1, "", "quantize_tensor_nearest"]], "torchao.float8": [[30, 0, 1, "", "CastConfig"], [31, 0, 1, "", "Float8LinearConfig"], [32, 0, 1, "", "ScalingType"], [33, 2, 1, "", "convert_to_float8_training"], [34, 2, 1, "", "precompute_float8_dynamic_scale_for_fsdp"]], "torchao.float8.Float8LinearConfig": [[31, 1, 1, "", "from_recipe_name"]], "torchao.quantization": [[35, 0, 1, "", "MappingType"], [36, 0, 1, "", "TorchAODType"], [37, 0, 1, "", "ZeroPointDomain"], [38, 2, 1, "", "autoquant"], [39, 2, 1, "", "choose_qparams_affine"], [40, 2, 1, "", "choose_qparams_affine_floatx"], [41, 2, 1, "", "choose_qparams_affine_with_min_max"], [42, 2, 1, "", "choose_qparams_and_quantize_affine_hqq"], [43, 2, 1, "", "dequantize_affine"], [44, 2, 1, "", "dequantize_affine_floatx"], [45, 2, 1, "", "fake_quantize_affine"], [46, 2, 1, "", "fake_quantize_affine_cachemask"], [47, 3, 1, "", "float8_dynamic_activation_float8_weight"], [48, 3, 1, "", "float8_static_activation_float8_weight"], [49, 3, 1, "", "float8_weight_only"], [50, 3, 1, "", "fpx_weight_only"], [51, 3, 1, "", "gemlite_uintx_weight_only"], [52, 3, 1, "", "int4_weight_only"], [53, 3, 1, "", "int8_dynamic_activation_int4_weight"], [54, 3, 1, "", "int8_dynamic_activation_int8_weight"], [55, 3, 1, "", "int8_weight_only"], [56, 2, 1, "", "int_scaled_matmul"], [57, 3, 1, "", "intx_quantization_aware_training"], [58, 2, 1, "", "quantize_"], [59, 2, 1, "", "quantize_affine"], [60, 2, 1, "", "quantize_affine_floatx"], [61, 2, 1, "", "safe_int_mm"], [62, 2, 1, "", "smooth_fq_linear_to_inference"], [63, 2, 1, "", "swap_linear_with_smooth_fq_linear"], [64, 2, 1, "", "to_linear_activation_quantized"], [65, 3, 1, "", "uintx_weight_only"]], "torchao": [[5, 4, 0, "-", "sparsity"]], "torchao.sparsity": [[66, 0, 1, "", "PerChannelNormObserver"], [67, 0, 1, "", "WandaSparsifier"], [68, 2, 1, "", "apply_fake_sparsity"], [69, 2, 1, "", "int8_dynamic_activation_int8_semi_sparse_weight"], [70, 3, 1, "", "semi_sparse_weight"], [71, 2, 1, "", "sparsify_"]], "torchao.sparsity.PerChannelNormObserver": [[66, 1, 1, "", "forward"]], "torchao.sparsity.WandaSparsifier": [[67, 1, 1, "", "prepare"], [67, 1, 1, "", "squash_mask"], [67, 1, 1, "", "update_mask"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:attribute", "4": "py:module"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "module", "Python module"]}, "titleterms": {"torchao": [0, 1, 2, 3, 4, 5, 6, 72, 74], "dtype": [0, 7, 74], "layout": [0, 6, 14, 74], "tensor": [0, 6, 74, 79, 80], "subclass": [0, 6, 74, 80], "quantiz": [0, 4, 58, 74, 75, 79, 80], "techniqu": 0, "float8": 1, "main": [1, 4], "train": [1, 74], "api": [1, 2, 4, 72], "other": [1, 4, 6, 74], "type": 1, "refer": [2, 72], "python": 2, "kernel": [3, 6, 73, 74], "quantize_": 4, "primit": [4, 74], "sparsiti": [5, 78], "contributor": 6, "guid": [6, 75], "gener": 6, "extend": 6, "ad": [6, 74], "effici": [6, 74], "custom": 6, "triton": 6, "hand": 6, "written": 6, "dispatch": 6, "tensorimpl": [6, 74], "flow": [6, 74, 76], "us": 6, "torch": 6, "compil": 6, "perform": [6, 73], "serial": [6, 76], "featur": 6, "support": [6, 74], "function": [6, 74], "compos": 6, "test": 6, "microbenchmark": 6, "model": [6, 74, 76], "benchmark": 6, "eval": 6, "affinequantizedtensor": 8, "blocksparselayout": 9, "cutlassint4packedlayout": 10, "cutlasssemisparselayout": 11, "float8layout": 12, "int4cpulayout": 13, "marlinqqqlayout": 15, "marlinqqqtensor": 16, "marlinsparselayout": 17, "nf4tensor": 18, "plainlayout": 19, "semisparselayout": 20, "tensorcoretiledlayout": 21, "uintxlayout": 22, "to_affine_quantized_floatx": 23, "to_affine_quantized_floatx_stat": 24, "to_affine_quantized_fpx": 25, "to_affine_quantized_intx": 26, "to_affine_quantized_intx_stat": 27, "to_marlinqqq_quantized_intx": 28, "to_nf4": 29, "castconfig": 30, "float8linearconfig": 31, "scalingtyp": 32, "convert_to_float8_train": 33, "precompute_float8_dynamic_scale_for_fsdp": 34, "mappingtyp": 35, "torchaodtyp": 36, "zeropointdomain": 37, "autoqu": 38, "choose_qparams_affin": 39, "choose_qparams_affine_floatx": 40, "choose_qparams_affine_with_min_max": 41, "choose_qparams_and_quantize_affine_hqq": 42, "dequantize_affin": 43, "dequantize_affine_floatx": 44, "fake_quantize_affin": 45, "fake_quantize_affine_cachemask": 46, "float8_dynamic_activation_float8_weight": 47, "float8_static_activation_float8_weight": 48, "float8_weight_onli": 49, "fpx_weight_onli": 50, "gemlite_uintx_weight_onli": 51, "int4_weight_onli": 52, "int8_dynamic_activation_int4_weight": 53, "int8_dynamic_activation_int8_weight": 54, "int8_weight_onli": 55, "int_scaled_matmul": 56, "intx_quantization_aware_train": 57, "quantize_affin": 59, "quantize_affine_floatx": 60, "safe_int_mm": 61, "smooth_fq_linear_to_infer": 62, "swap_linear_with_smooth_fq_linear": 63, "to_linear_activation_quant": 64, "uintx_weight_onli": 65, "perchannelnormobserv": 66, "wandasparsifi": 67, "apply_fake_spars": 68, "int8_dynamic_activation_int8_semi_sparse_weight": 69, "semi_sparse_weight": 70, "sparsifi": 71, "welcom": 72, "document": 72, "get": 72, "start": [72, 75], "develop": 72, "note": 72, "tutori": [72, 83], "overview": [74, 78, 83], "basic": 74, "current": 74, "placehold": 74, "pytorch": 74, "implement": [74, 80], "oper": [74, 80], "integr": 74, "nativ": 74, "factori": 74, "op": 74, "deriv": 74, "algorithm": 74, "weight": 74, "onli": 74, "dynam": 74, "activ": 74, "static": 74, "insert": 74, "observ": 74, "how": 74, "defin": 74, "modul": [74, 80], "add": 74, "calibr": 74, "awar": 74, "low": 74, "bit": 74, "optim": [74, 76], "case": 74, "studi": 74, "int4": 74, "work": 74, "dure": 74, "execut": 74, "save": 74, "load": 74, "quick": 75, "first": 75, "exampl": 75, "next": [75, 80], "step": [75, 80, 83], "deseri": 76, "what": [76, 80], "happen": 76, "when": 76, "an": 76, "comput": [77, 82], "time": [77, 82], "goal": 78, "design": 78, "context": 78, "prune": 78, "configur": 78, "criteria": 78, "strategi": 78, "pattern": 78, "write": [79, 80], "your": [79, 80], "own": [79, 80], "advanc": 79, "ar": 80, "swap": 80, "which": 80, "should": 80, "we": 80, "compar": 80, "output": 80, "templat": 83, "option": 83, "addit": 83, "exercis": 83, "conclus": 83, "further": 83, "read": 83}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})