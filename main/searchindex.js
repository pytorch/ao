Search.setIndex({"docnames": ["api_ref_dtypes", "api_ref_float8", "api_ref_intro", "api_ref_kernel", "api_ref_quantization", "api_ref_sparsity", "benchmarking_api_guide", "benchmarking_user_guide", "contributor_guide", "dtypes", "finetuning", "generated/torchao.dtypes.AffineQuantizedTensor", "generated/torchao.dtypes.BlockSparseLayout", "generated/torchao.dtypes.CutlassInt4PackedLayout", "generated/torchao.dtypes.CutlassSemiSparseLayout", "generated/torchao.dtypes.Float8Layout", "generated/torchao.dtypes.Int4CPULayout", "generated/torchao.dtypes.Layout", "generated/torchao.dtypes.MarlinQQQLayout", "generated/torchao.dtypes.MarlinQQQTensor", "generated/torchao.dtypes.MarlinSparseLayout", "generated/torchao.dtypes.NF4Tensor", "generated/torchao.dtypes.PlainLayout", "generated/torchao.dtypes.SemiSparseLayout", "generated/torchao.dtypes.TensorCoreTiledLayout", "generated/torchao.dtypes.UintxLayout", "generated/torchao.dtypes.to_affine_quantized_floatx", "generated/torchao.dtypes.to_affine_quantized_floatx_static", "generated/torchao.dtypes.to_affine_quantized_fpx", "generated/torchao.dtypes.to_affine_quantized_intx", "generated/torchao.dtypes.to_affine_quantized_intx_static", "generated/torchao.dtypes.to_marlinqqq_quantized_intx", "generated/torchao.dtypes.to_nf4", "generated/torchao.float8.CastConfig", "generated/torchao.float8.Float8LinearConfig", "generated/torchao.float8.ScalingGranularity", "generated/torchao.float8.ScalingType", "generated/torchao.float8.convert_to_float8_training", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp", "generated/torchao.quantization.FPXWeightOnlyConfig", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig", "generated/torchao.quantization.Float8WeightOnlyConfig", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig", "generated/torchao.quantization.Int4WeightOnlyConfig", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig", "generated/torchao.quantization.Int8WeightOnlyConfig", "generated/torchao.quantization.MappingType", "generated/torchao.quantization.TorchAODType", "generated/torchao.quantization.UIntXWeightOnlyConfig", "generated/torchao.quantization.ZeroPointDomain", "generated/torchao.quantization.autoquant", "generated/torchao.quantization.choose_qparams_affine", "generated/torchao.quantization.choose_qparams_affine_with_min_max", "generated/torchao.quantization.dequantize_affine", "generated/torchao.quantization.int_scaled_matmul", "generated/torchao.quantization.qat.ComposableQATQuantizer", "generated/torchao.quantization.qat.FakeQuantizeConfig", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig", "generated/torchao.quantization.qat.initialize_fake_quantizers", "generated/torchao.quantization.quantize_", "generated/torchao.quantization.quantize_affine", "generated/torchao.quantization.safe_int_mm", "generated/torchao.quantization.smooth_fq_linear_to_inference", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear", "generated/torchao.quantization.to_linear_activation_quantized", "generated/torchao.sparsity.PerChannelNormObserver", "generated/torchao.sparsity.WandaSparsifier", "generated/torchao.sparsity.apply_fake_sparsity", "generated/torchao.sparsity.semi_sparse_weight", "generated/torchao.sparsity.sparsify_", "index", "performant_kernels", "pretraining", "quantization", "quick_start", "serialization", "serving", "sg_execution_times", "sparsity", "static_quantization", "subclass_advanced", "subclass_basic", "torchao_vllm_integration", "tutorials/index", "tutorials/sg_execution_times", "tutorials/template_tutorial", "tutorials_source/pt2e_quant_openvino_inductor", "tutorials_source/pt2e_quant_ptq", "tutorials_source/pt2e_quant_qat", "tutorials_source/pt2e_quant_x86_inductor", "tutorials_source/pt2e_quant_xpu_inductor", "tutorials_source/pt2e_quantizer"], "filenames": ["api_ref_dtypes.rst", "api_ref_float8.rst", "api_ref_intro.rst", "api_ref_kernel.rst", "api_ref_quantization.rst", "api_ref_sparsity.rst", "benchmarking_api_guide.md", "benchmarking_user_guide.md", "contributor_guide.rst", "dtypes.rst", "finetuning.rst", "generated/torchao.dtypes.AffineQuantizedTensor.rst", "generated/torchao.dtypes.BlockSparseLayout.rst", "generated/torchao.dtypes.CutlassInt4PackedLayout.rst", "generated/torchao.dtypes.CutlassSemiSparseLayout.rst", "generated/torchao.dtypes.Float8Layout.rst", "generated/torchao.dtypes.Int4CPULayout.rst", "generated/torchao.dtypes.Layout.rst", "generated/torchao.dtypes.MarlinQQQLayout.rst", "generated/torchao.dtypes.MarlinQQQTensor.rst", "generated/torchao.dtypes.MarlinSparseLayout.rst", "generated/torchao.dtypes.NF4Tensor.rst", "generated/torchao.dtypes.PlainLayout.rst", "generated/torchao.dtypes.SemiSparseLayout.rst", "generated/torchao.dtypes.TensorCoreTiledLayout.rst", "generated/torchao.dtypes.UintxLayout.rst", "generated/torchao.dtypes.to_affine_quantized_floatx.rst", "generated/torchao.dtypes.to_affine_quantized_floatx_static.rst", "generated/torchao.dtypes.to_affine_quantized_fpx.rst", "generated/torchao.dtypes.to_affine_quantized_intx.rst", "generated/torchao.dtypes.to_affine_quantized_intx_static.rst", "generated/torchao.dtypes.to_marlinqqq_quantized_intx.rst", "generated/torchao.dtypes.to_nf4.rst", "generated/torchao.float8.CastConfig.rst", "generated/torchao.float8.Float8LinearConfig.rst", "generated/torchao.float8.ScalingGranularity.rst", "generated/torchao.float8.ScalingType.rst", "generated/torchao.float8.convert_to_float8_training.rst", "generated/torchao.float8.precompute_float8_dynamic_scale_for_fsdp.rst", "generated/torchao.quantization.FPXWeightOnlyConfig.rst", "generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8StaticActivationFloat8WeightConfig.rst", "generated/torchao.quantization.Float8WeightOnlyConfig.rst", "generated/torchao.quantization.GemliteUIntXWeightOnlyConfig.rst", "generated/torchao.quantization.Int4WeightOnlyConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.rst", "generated/torchao.quantization.Int8DynamicActivationInt8WeightConfig.rst", "generated/torchao.quantization.Int8WeightOnlyConfig.rst", "generated/torchao.quantization.MappingType.rst", "generated/torchao.quantization.TorchAODType.rst", "generated/torchao.quantization.UIntXWeightOnlyConfig.rst", "generated/torchao.quantization.ZeroPointDomain.rst", "generated/torchao.quantization.autoquant.rst", "generated/torchao.quantization.choose_qparams_affine.rst", "generated/torchao.quantization.choose_qparams_affine_with_min_max.rst", "generated/torchao.quantization.dequantize_affine.rst", "generated/torchao.quantization.int_scaled_matmul.rst", "generated/torchao.quantization.qat.ComposableQATQuantizer.rst", "generated/torchao.quantization.qat.FakeQuantizeConfig.rst", "generated/torchao.quantization.qat.FromIntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer.rst", "generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.rst", "generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.rst", "generated/torchao.quantization.qat.IntXQuantizationAwareTrainingConfig.rst", "generated/torchao.quantization.qat.initialize_fake_quantizers.rst", "generated/torchao.quantization.quantize_.rst", "generated/torchao.quantization.quantize_affine.rst", "generated/torchao.quantization.safe_int_mm.rst", "generated/torchao.quantization.smooth_fq_linear_to_inference.rst", "generated/torchao.quantization.swap_linear_with_smooth_fq_linear.rst", "generated/torchao.quantization.to_linear_activation_quantized.rst", "generated/torchao.sparsity.PerChannelNormObserver.rst", "generated/torchao.sparsity.WandaSparsifier.rst", "generated/torchao.sparsity.apply_fake_sparsity.rst", "generated/torchao.sparsity.semi_sparse_weight.rst", "generated/torchao.sparsity.sparsify_.rst", "index.rst", "performant_kernels.rst", "pretraining.rst", "quantization.rst", "quick_start.rst", "serialization.rst", "serving.rst", "sg_execution_times.rst", "sparsity.rst", "static_quantization.rst", "subclass_advanced.rst", "subclass_basic.rst", "torchao_vllm_integration.md", "tutorials/index.rst", "tutorials/sg_execution_times.rst", "tutorials/template_tutorial.rst", "tutorials_source/pt2e_quant_openvino_inductor.rst", "tutorials_source/pt2e_quant_ptq.rst", "tutorials_source/pt2e_quant_qat.rst", "tutorials_source/pt2e_quant_x86_inductor.rst", "tutorials_source/pt2e_quant_xpu_inductor.rst", "tutorials_source/pt2e_quantizer.rst"], "titles": ["torchao.dtypes", "torchao.float8", "<code class=\"docutils literal notranslate\"><span class=\"pre\">torchao</span></code> API Reference", "torchao.kernel", "torchao.quantization", "torchao.sparsity", "Benchmarking API Guide", "Benchmarking User Guide", "Contributor Guide", "Dtypes", "(Part 2) Fine-tuning with QAT, QLoRA, and float8", "AffineQuantizedTensor", "BlockSparseLayout", "CutlassInt4PackedLayout", "CutlassSemiSparseLayout", "Float8Layout", "Int4CPULayout", "Layout", "MarlinQQQLayout", "MarlinQQQTensor", "MarlinSparseLayout", "NF4Tensor", "PlainLayout", "SemiSparseLayout", "TensorCoreTiledLayout", "UintxLayout", "to_affine_quantized_floatx", "to_affine_quantized_floatx_static", "to_affine_quantized_fpx", "to_affine_quantized_intx", "to_affine_quantized_intx_static", "to_marlinqqq_quantized_intx", "to_nf4", "CastConfig", "Float8LinearConfig", "ScalingGranularity", "ScalingType", "convert_to_float8_training", "precompute_float8_dynamic_scale_for_fsdp", "FPXWeightOnlyConfig", "Float8DynamicActivationFloat8WeightConfig", "Float8StaticActivationFloat8WeightConfig", "Float8WeightOnlyConfig", "GemliteUIntXWeightOnlyConfig", "Int4WeightOnlyConfig", "Int8DynamicActivationInt4WeightConfig", "Int8DynamicActivationInt8WeightConfig", "Int8WeightOnlyConfig", "MappingType", "TorchAODType", "UIntXWeightOnlyConfig", "ZeroPointDomain", "autoquant", "choose_qparams_affine", "choose_qparams_affine_with_min_max", "dequantize_affine", "int_scaled_matmul", "ComposableQATQuantizer", "FakeQuantizeConfig", "FromIntXQuantizationAwareTrainingConfig", "Int4WeightOnlyEmbeddingQATQuantizer", "Int4WeightOnlyQATQuantizer", "Int8DynActInt4WeightQATQuantizer", "IntXQuantizationAwareTrainingConfig", "initialize_fake_quantizers", "quantize", "quantize_affine", "safe_int_mm", "smooth_fq_linear_to_inference", "swap_linear_with_smooth_fq_linear", "to_linear_activation_quantized", "PerChannelNormObserver", "WandaSparsifier", "apply_fake_sparsity", "semi_sparse_weight", "sparsify", "Welcome to the torchao Documentation", "Performant Kernels", "(Part 1) Pre-training with float8", "Quantization Overview", "Quick Start Guide", "Serialization", "(Part 3) Serving on vLLM, SGLang, ExecuTorch", "Computation times", "Sparsity Overview", "Static Quantization", "Writing Your Own Quantized Tensor (advanced)", "Writing Your Own Quantized Tensor", "Integration with VLLM: Architecture and Usage Guide", "&lt;no title&gt;", "Computation times", "Template Tutorial", "PyTorch 2 Export Quantization for OpenVINO torch.compile Backend", "PyTorch 2 Export Post Training Quantization", "PyTorch 2 Export Quantization-Aware Training (QAT)", "PyTorch 2 Export Quantization with X86 Backend through Inductor", "PyTorch 2 Export Quantization with Intel GPU Backend through Inductor", "How to Write a <code class=\"docutils literal notranslate\"><span class=\"pre\">Quantizer</span></code> for PyTorch 2 Export Quantization"], "terms": {"thi": [2, 6, 7, 8, 10, 11, 20, 21, 22, 23, 25, 38, 39, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 58, 59, 65, 66, 71, 72, 73, 75, 78, 79, 80, 81, 82, 84, 85, 87, 88, 91, 92, 93, 94, 95, 96, 97], "section": [2, 8, 79, 84, 88, 93, 94, 97], "introduc": [2, 10, 92, 93, 95, 96, 97], "dive": 2, "detail": [2, 6, 8, 10, 39, 52, 78, 79, 80, 82, 84, 85, 87, 92, 93, 94, 95], "how": [2, 8, 10, 11, 17, 25, 44, 46, 48, 53, 58, 66, 76, 78, 80, 81, 82, 84, 85, 87, 88, 92, 95, 96], "integr": [2, 8, 76, 78, 81, 82, 84, 87, 95, 97], "pytorch": [2, 6, 8, 10, 11, 16, 19, 49, 58, 76, 78, 82, 84, 87, 88, 91], "optim": [2, 8, 10, 20, 38, 52, 65, 76, 78, 84, 87, 92, 94, 95, 96], "your": [2, 6, 8, 10, 76, 78, 79, 80, 82, 84, 93, 94, 95, 96, 97], "machin": [2, 94], "learn": [2, 44, 58, 80, 84, 91, 93, 95, 96, 97], "model": [2, 10, 38, 43, 45, 52, 57, 59, 60, 61, 62, 63, 64, 65, 68, 69, 72, 73, 75, 80, 84, 85, 87, 95, 96, 97], "dtype": [2, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 49, 50, 52, 53, 54, 55, 58, 60, 61, 62, 66, 75, 76, 78, 80, 81, 85, 87, 88, 93, 95, 96, 97], "quantiz": [2, 6, 8, 11, 13, 14, 15, 16, 18, 19, 20, 21, 23, 24, 26, 29, 31, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 75, 78, 81, 84], "sparsiti": [2, 6, 10, 14, 20, 23, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82], "tba": [3, 9, 77], "tutori": [6, 8, 10, 11, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97], "you": [6, 7, 8, 10, 58, 72, 78, 79, 80, 81, 82, 84, 87, 88, 91, 92, 93, 94, 95, 96, 97], "through": [6, 8, 10, 54, 76, 79, 80, 82, 85, 87, 88, 91, 92, 93, 97], "us": [6, 7, 10, 11, 15, 16, 17, 20, 21, 22, 25, 27, 30, 40, 41, 44, 45, 46, 48, 50, 52, 53, 54, 55, 57, 58, 59, 63, 66, 72, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 92, 93, 94, 95, 96], "torchao": [6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 80, 81, 82, 84, 85, 87, 92, 93, 94, 95, 96], "framework": [6, 10, 78, 82, 92], "The": [6, 8, 10, 11, 12, 17, 20, 25, 37, 39, 40, 41, 42, 52, 56, 65, 67, 68, 69, 72, 78, 79, 80, 81, 82, 84, 87, 88, 92, 93, 94, 95, 96, 97], "contain": [6, 52, 68, 69, 84, 87, 94, 97], "new": [6, 8, 10, 11, 78, 79, 85, 87, 93, 94, 95, 97], "architectur": [6, 76, 82, 84, 92, 93, 95, 96], "micro": 6, "current": [6, 40, 45, 65, 69, 72, 75, 78, 80, 84, 87, 88, 93, 94, 96], "support": [6, 10, 11, 28, 40, 45, 58, 63, 75, 78, 80, 81, 82, 84, 87, 92, 93, 94, 95, 96, 97], "which": [6, 8, 10, 19, 25, 52, 78, 79, 80, 81, 82, 84, 85, 88, 92, 93, 94, 95, 96, 97], "can": [6, 8, 10, 24, 40, 43, 48, 52, 57, 58, 65, 66, 78, 79, 80, 81, 82, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "quantize_": [6, 8, 10, 59, 63, 65, 75, 79, 80, 81, 82, 85], "sparsity_": 6, "function": [6, 10, 11, 24, 37, 52, 65, 71, 72, 73, 75, 78, 80, 81, 84, 85, 87, 88, 92, 97], "To": [6, 8, 10, 11, 19, 52, 78, 79, 80, 81, 82, 84, 85, 88, 93, 94, 95, 97], "correspond": [6, 10, 59, 65, 79, 81, 84, 87, 96, 97], "string": [6, 34, 58, 72], "string_to_config": 6, "microbenchmark": 6, "util": [6, 8, 43, 78, 79, 80, 81, 87, 88, 92, 93, 94, 95, 96, 97], "py": [6, 8, 11, 19, 82, 83, 90, 91, 95, 96], "def": [6, 10, 75, 78, 79, 80, 81, 85, 87, 88, 92, 93, 94, 95, 96, 97], "option": [6, 8, 11, 15, 19, 26, 29, 30, 31, 33, 34, 37, 40, 41, 43, 44, 46, 47, 52, 53, 54, 55, 58, 61, 63, 65, 66, 68, 69, 70, 72, 75, 78, 80, 88, 93, 94, 95, 96, 97], "str": [6, 34, 37, 43, 58, 65, 69, 70, 72, 75, 78, 87, 88, 96], "kwarg": [6, 11, 58, 60, 71, 72, 73, 87, 88], "aobaseconfig": [6, 65, 75, 85, 88], "code": [6, 8, 44, 78, 79, 80, 82, 84, 85, 87, 89, 91, 93, 94, 95, 96, 97], "elif": [6, 88], "my_new_quant": 6, "If": [6, 7, 10, 11, 15, 37, 40, 46, 47, 52, 56, 58, 63, 67, 68, 72, 79, 80, 82, 84, 87, 93, 94], "addit": [6, 10, 17, 22, 52, 78, 84, 87, 92, 93, 96, 97], "inform": [6, 11, 82, 84, 88, 92, 93], "need": [6, 8, 10, 40, 71, 72, 79, 80, 81, 82, 84, 87, 88, 93, 94, 95, 97], "pass": [6, 37, 46, 52, 54, 71, 79, 85, 87, 88, 94, 97], "process": [6, 10, 17, 20, 22, 24, 25, 52, 69, 79, 84, 91, 92, 96], "here": [6, 7, 8, 11, 66, 79, 80, 81, 82, 85, 87, 88, 92, 93, 94, 95, 96, 97], "return": [6, 10, 11, 19, 20, 21, 37, 52, 56, 58, 65, 67, 68, 69, 75, 78, 79, 80, 81, 85, 87, 88, 92, 93, 94, 95, 96, 97], "mynewquantizationconfig": 6, "my_new_spars": 6, "mynewsparsityconfig": 6, "rest": [6, 87, 94], "now": [6, 8, 10, 39, 45, 53, 78, 79, 80, 84, 85, 87, 92, 93, 95, 97], "we": [6, 8, 10, 11, 21, 48, 50, 52, 53, 54, 55, 58, 63, 65, 66, 75, 78, 79, 80, 81, 82, 84, 85, 88, 92, 93, 94, 95, 96, 97], "throughout": 6, "note": [6, 8, 10, 57, 63, 72, 79, 80, 82, 84, 87, 88, 94, 95, 96], "input": [6, 8, 11, 20, 21, 23, 34, 37, 38, 52, 53, 54, 55, 56, 64, 65, 66, 67, 72, 75, 78, 79, 80, 82, 85, 87, 92, 93, 94, 95, 96, 97], "paramet": [6, 10, 11, 17, 20, 21, 27, 30, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 52, 53, 55, 56, 58, 65, 66, 67, 68, 69, 72, 75, 78, 79, 81, 82, 84, 87, 88, 92, 93], "like": [6, 8, 10, 17, 52, 78, 79, 80, 81, 84, 87, 88, 92, 93, 94, 95, 96, 97], "bit": [6, 10, 25, 32, 39, 43, 50, 82, 87, 88, 93, 95, 96], "width": [6, 25, 43], "group": [6, 10, 44, 45, 47, 50, 58, 60, 61, 62, 79, 80], "size": [6, 8, 11, 12, 19, 21, 43, 44, 45, 47, 50, 53, 55, 58, 66, 78, 80, 81, 82, 84, 85, 87, 88, 94], "etc": [6, 8, 79, 92, 97], "them": [6, 10, 52, 71, 79, 97], "append": [6, 84, 93, 94], "config": [6, 10, 34, 37, 52, 58, 59, 63, 65, 72, 75, 80, 82, 84, 85, 88, 93, 95, 96], "For": [6, 8, 10, 11, 39, 58, 79, 80, 81, 82, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "exampl": [6, 8, 10, 11, 38, 48, 52, 57, 58, 59, 63, 64, 65, 72, 75, 79, 81, 82, 83, 84, 85, 87, 89, 90, 91, 92, 93, 94, 95, 96], "gemliteuintxweightonlyconfig": 6, "gemlitewo": 6, "bit_width": [6, 43], "group_siz": [6, 10, 43, 44, 45, 47, 50, 58, 60, 63, 65, 80, 88], "system": [6, 8, 82], "model_architectur": 6, "type": [6, 8, 10, 11, 20, 21, 25, 34, 35, 36, 37, 40, 41, 42, 44, 45, 46, 48, 49, 51, 52, 56, 58, 66, 67, 76, 79, 81, 82, 84, 87, 88, 92, 93, 95, 96, 97], "defin": [6, 8, 17, 25, 35, 39, 71, 72, 80, 84, 85, 87, 88, 92, 95, 96, 97], "class": [6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 57, 58, 59, 60, 61, 62, 63, 71, 72, 79, 80, 81, 85, 87, 93, 94, 95, 97], "mycustommodel": 6, "torch": [6, 10, 11, 20, 21, 25, 27, 34, 37, 40, 41, 42, 44, 50, 52, 53, 55, 56, 58, 60, 61, 62, 63, 65, 66, 67, 68, 69, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 91, 95, 96, 97], "nn": [6, 10, 34, 37, 52, 57, 60, 63, 65, 68, 69, 75, 78, 79, 80, 81, 82, 84, 85, 87, 88, 93, 94, 95, 97], "modul": [6, 8, 10, 34, 35, 36, 37, 38, 48, 49, 51, 52, 57, 59, 60, 63, 64, 65, 68, 69, 71, 72, 75, 78, 80, 81, 85, 92, 93, 94, 95, 96, 97], "__init__": [6, 10, 80, 81, 85, 87, 88, 93, 94, 95], "self": [6, 10, 11, 79, 80, 81, 85, 87, 88, 93, 94, 95], "input_dim": 6, "output_dim": 6, "bfloat16": [6, 8, 21, 61, 66, 78, 79, 80, 81, 82, 84, 85, 88, 95, 96], "super": [6, 10, 80, 81, 85, 87, 93, 94, 95], "layer1": 6, "linear": [6, 8, 10, 20, 34, 37, 40, 42, 44, 45, 46, 47, 50, 52, 57, 61, 62, 63, 65, 69, 73, 75, 78, 79, 80, 81, 82, 84, 85, 87, 92, 93, 94, 95, 97], "512": [6, 78], "bia": [6, 8, 10, 79, 80, 81, 85, 87, 88, 94, 97], "fals": [6, 10, 11, 29, 34, 44, 46, 50, 52, 58, 62, 63, 68, 72, 78, 79, 80, 81, 82, 85, 87, 88, 92, 93, 94, 96, 97], "activ": [6, 8, 10, 40, 41, 43, 45, 46, 52, 58, 62, 63, 68, 72, 76, 80, 82, 84, 85, 88, 92, 95, 96, 97], "relu": [6, 80, 92, 97], "layer2": 6, "forward": [6, 46, 52, 71, 79, 80, 81, 84, 85, 87, 88, 93, 94, 95], "x": [6, 50, 78, 80, 81, 82, 85, 87, 88, 91, 92, 93, 94, 95, 96], "updat": [6, 76, 80, 81, 84, 93, 94, 95, 97], "create_model_and_input_data": 6, "handl": [6, 20, 23, 24, 52, 79], "model_typ": [6, 10, 88, 92], "m": [6, 8, 10, 65, 75, 78, 80, 81, 82, 85, 87, 93, 94, 95], "int": [6, 10, 11, 12, 19, 21, 24, 25, 26, 27, 29, 30, 31, 32, 39, 43, 44, 45, 47, 50, 53, 54, 55, 58, 60, 61, 62, 65, 66, 72, 80, 85, 87, 88], "k": [6, 8, 67, 80, 81, 85, 87, 93, 94], "n": [6, 8, 10, 80, 81, 85, 87, 93, 94, 97], "high_precision_dtyp": 6, "devic": [6, 8, 10, 11, 65, 67, 78, 80, 81, 82, 85, 87, 88, 92, 93, 94, 95, 96], "cuda": [6, 8, 10, 11, 65, 78, 80, 81, 82, 84, 85, 87, 94], "my_custom_model": 6, "input_data": 6, "randn": [6, 10, 11, 78, 80, 81, 85, 87, 92, 93, 94, 95, 96], "when": [6, 8, 10, 11, 22, 53, 55, 66, 78, 79, 82, 84, 85, 88, 92, 93, 94, 95, 96, 97], "ad": [6, 10, 11, 55, 72, 84, 85, 87, 94], "dimens": [6, 8, 11, 25, 50, 53, 55, 56, 66, 78, 87, 88, 93, 94], "ensur": [6, 20, 82, 94], "convent": 6, "where": [6, 23, 48, 50, 54, 60, 61, 62, 79, 84, 88, 97], "batch": [6, 82, 85, 94], "sequenc": 6, "length": 6, "featur": [6, 10, 11, 87, 92, 95, 96], "data": [6, 10, 11, 12, 17, 20, 25, 40, 41, 42, 44, 46, 54, 76, 79, 81, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "typic": [6, 10, 21, 22, 79, 80, 81, 85, 88, 97], "compat": [6, 8, 20, 58, 80], "work": [6, 8, 10, 23, 43, 78, 81, 84, 87, 88, 93, 94, 95], "cpu": [6, 8, 11, 16, 81, 84, 85, 88, 92, 93, 94, 95], "other": [6, 10, 11, 17, 72, 78, 81, 82, 84, 87, 88, 91, 93, 94, 95, 97], "target": [6, 8, 10, 40, 41, 42, 44, 53, 72, 80, 84, 92, 93, 94, 95, 96, 97], "method": [6, 8, 17, 20, 23, 24, 52, 65, 72, 80, 84, 85, 87, 92, 93, 94, 96, 97], "come": [6, 7, 78, 79, 82, 84, 85, 86, 94, 95, 96], "soon": [6, 7, 82, 86, 94], "file": [6, 8, 78, 82, 83, 87, 88, 90, 93, 94], "microbenchmark_quantization_config": 6, "yml": 6, "benchmark_mod": 6, "infer": [6, 8, 10, 11, 68, 76, 79, 80, 81, 84, 85, 87, 92, 93, 94, 95, 96], "quantization_config_recipe_nam": 6, "int8wo": 6, "int8dq": 6, "float8dq": [6, 82], "tensor": [6, 10, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 41, 42, 44, 45, 46, 52, 53, 54, 55, 56, 66, 67, 70, 72, 76, 78, 80, 81, 84, 85, 91, 93, 95, 96], "row": [6, 56, 78, 84], "float8wo": 6, "output_dir": 6, "result": [6, 10, 52, 56, 67, 79, 84, 85, 93, 94, 95, 96, 97], "model_param": 6, "name": [6, 35, 36, 48, 49, 51, 65, 69, 72, 75, 82, 84, 87, 88, 92, 93, 94, 97], "small_bf16_linear": 6, "matrix_shap": 6, "small_sweep": 6, "min_pow": 6, "10": [6, 8, 10, 48, 66, 78, 80, 82, 85, 93, 94], "max_pow": 6, "15": [6, 78, 80, 82], "use_torch_compil": 6, "true": [6, 8, 10, 11, 29, 34, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 53, 54, 58, 63, 65, 68, 75, 78, 80, 81, 82, 85, 87, 88, 92, 93, 94, 95, 97], "torch_compile_mod": 6, "max": [6, 8, 48, 79, 80, 85, 87, 93, 94, 97], "autotun": [6, 8, 80, 85], "runner": 6, "gener": [6, 11, 79, 80, 82, 84, 85, 87, 88, 89, 91, 92, 94, 95, 96, 97], "oss": 6, "databas": 6, "python": [6, 8, 79, 80, 82, 84, 89, 91, 92, 93, 95, 96], "ci_microbenchmark_runn": 6, "benchmark_result": 6, "json": [6, 82, 88], "specif": [6, 8, 10, 17, 20, 22, 23, 72, 78, 79, 80, 81, 82, 84, 92, 95, 96, 97], "requir": [6, 10, 22, 24, 78, 79, 80, 82, 84, 87, 92, 95, 97], "mode": [6, 8, 43, 44, 52, 80, 85, 92, 94, 95, 96, 97], "extra_info": 6, "arch": 6, "nvidia": [6, 84], "a100": [6, 10, 80], "sxm4": 6, "80gb": [6, 80], "1024": [6, 65, 75, 80, 81, 95], "custom": [6, 10, 17, 71, 76, 78, 79, 80, 84, 87, 88, 92, 93, 95, 97], "layer": [6, 20, 37, 40, 42, 44, 46, 47, 50, 52, 60, 61, 62, 68, 69, 72, 73, 78, 82, 84, 85, 87, 88, 92, 97], "origin": [6, 10, 11, 21, 42, 46, 59, 66, 72, 79, 80, 81, 82, 84, 92, 93, 97], "metric": [6, 10, 72], "speedup": [6, 8, 10, 44, 78, 79, 80, 82, 84], "wrt": 6, "bf16": [6, 10, 53, 79, 80, 84, 95, 96], "benchmark_valu": 6, "25": [6, 80], "target_valu": 6, "0": [6, 8, 10, 11, 52, 58, 66, 69, 72, 78, 80, 81, 82, 83, 84, 85, 87, 88, 90, 91, 93, 94, 96, 97], "depend": [6, 11, 43, 52, 81, 84, 87, 93, 94, 96], "step": [6, 10, 22, 38, 52, 78, 79, 84, 92, 93, 94, 95, 96, 97], "workflow": [6, 65, 75, 78, 80, 84, 97], "github": [6, 8, 11, 19, 39, 80, 82], "action": [6, 88, 93, 94], "upload": 6, "verifi": [6, 80, 81, 87], "setup": [6, 82], "suit": [6, 8, 93, 95], "unittest": 6, "discov": 6, "out": [6, 8, 10, 23, 48, 52, 72, 78, 79, 80, 84, 87, 92, 93, 94, 95], "memori": [6, 8, 10, 11, 78, 80, 84, 87, 95, 96], "reduc": [6, 10, 38, 78, 82, 84, 95], "matrix": [6, 12, 15, 40, 41, 56, 67, 72, 80, 84, 95], "compil": [6, 10, 52, 65, 67, 76, 78, 79, 80, 85, 87, 95, 96], "error": [6, 48, 52, 58, 78, 87, 93], "set": [6, 10, 11, 15, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 54, 58, 65, 68, 72, 80, 84, 92, 94, 95, 96], "debug": [6, 68], "miss": [6, 84], "properli": [6, 81], "instal": [6, 78, 80, 82, 93, 96], "Not": [6, 84], "avail": [6, 79, 92, 93, 94, 95, 96], "check": [6, 8, 10, 11, 19, 79, 80, 81, 87, 92, 94, 97], "driver": 6, "basic": [6, 8, 22, 80, 85, 87], "shape": [6, 8, 11, 19, 52, 56, 67, 80, 85, 87, 88, 93, 96], "comprehens": [6, 88, 95], "analysi": [6, 84], "enabl": [6, 78, 79, 82, 88, 95], "profil": [6, 8], "onli": [6, 8, 10, 16, 37, 40, 42, 43, 44, 45, 46, 47, 50, 75, 78, 80, 81, 82, 84, 87, 88, 92, 93, 95, 96, 97], "overhead": [6, 84, 88, 95], "multipl": [6, 10, 15, 40, 41, 52, 56, 57, 67, 80, 84, 85, 87, 88, 95, 97], "possibl": [6, 11, 84, 93, 94, 95, 97], "consist": [6, 82, 84, 87, 95, 96, 97], "reproduc": [6, 82], "differ": [6, 8, 10, 17, 44, 54, 57, 66, 67, 78, 79, 80, 81, 82, 84, 87, 88, 93, 94, 95, 97], "case": [6, 7, 8, 52, 67, 82, 84, 87, 88, 92, 93, 97], "refer": [6, 8, 10, 11, 78, 82, 84, 85, 87, 88, 92, 93, 94, 95], "user": [6, 10, 52, 57, 76, 78, 79, 80, 82, 84, 85, 87, 91, 93, 94, 95, 96, 97], "more": [6, 8, 10, 11, 39, 43, 44, 45, 50, 52, 78, 79, 80, 82, 84, 85, 87, 88, 92, 93, 94, 95, 96], "about": [6, 8, 10, 44, 79, 80, 81, 82, 84, 93, 94, 95, 97], "compon": [6, 79, 87, 88], "see": [6, 8, 10, 11, 19, 39, 78, 79, 80, 81, 82, 84, 85, 87, 88, 92, 93, 97], "readm": [6, 10, 76, 80, 84], "directori": [6, 78], "intend": [7, 79, 93], "provid": [7, 8, 10, 17, 20, 23, 24, 52, 53, 57, 64, 78, 79, 82, 84, 87, 88, 93, 94, 96, 97], "instruct": [7, 10, 80, 82, 93, 94, 95], "most": [7, 22, 79, 82, 84, 88, 93, 94, 97], "fequent": 7, "have": [7, 8, 10, 43, 44, 48, 52, 60, 61, 62, 66, 72, 79, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "ani": [7, 8, 22, 52, 60, 64, 70, 72, 79, 84, 87, 92, 94, 96], "answer": [7, 84], "pleas": [7, 8, 10, 11, 19, 39, 44, 76, 79, 80, 82, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "creat": [7, 8, 11, 27, 28, 30, 78, 79, 84, 87, 92, 93, 95, 96, 97], "an": [7, 8, 10, 11, 24, 29, 30, 52, 58, 63, 72, 76, 78, 79, 80, 82, 84, 85, 87, 92, 93, 94, 95, 96, 97], "issu": [7, 8, 79, 80, 87, 95], "train": [8, 34, 57, 58, 76, 80, 84, 87, 97], "fp4": 8, "s": [8, 10, 11, 48, 52, 53, 55, 66, 78, 79, 80, 82, 84, 85, 87, 93, 94, 95, 96, 97], "fine": [8, 43, 44, 45, 50, 76, 78, 82, 84], "start": [8, 10, 35, 36, 48, 49, 51, 52, 78, 79, 82, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "prototyp": [8, 58, 64, 79, 97], "folder": [8, 82, 93, 94], "could": [8, 79, 87, 92, 93, 95, 96, 97], "also": [8, 10, 52, 58, 65, 79, 80, 81, 84, 85, 87, 88, 93, 96, 97], "take": [8, 21, 65, 71, 75, 79, 84, 92, 93, 94, 95, 96, 97], "look": [8, 11, 78, 79, 84, 92, 93, 94, 95, 96], "affinequantizedtensor": [8, 19, 27, 28, 30, 79, 80, 81, 85, 87], "what": [8, 10, 11, 19, 52, 78, 79, 80, 82, 84, 85, 88, 91, 93, 97], "want": [8, 65, 75, 79, 80, 81, 84, 87, 88, 92, 93, 94, 97], "do": [8, 49, 52, 56, 65, 79, 82, 84, 85, 87, 88, 93, 94, 95, 97], "mostli": [8, 54, 80, 95], "e": [8, 10, 11, 39, 48, 52, 53, 55, 57, 58, 65, 66, 78, 79, 81, 85, 87, 92, 97], "g": [8, 10, 11, 39, 48, 52, 53, 55, 57, 58, 65, 66, 79, 81, 85, 87, 92, 97], "int3": 8, "exact": [8, 10, 93, 94], "same": [8, 10, 11, 40, 53, 54, 55, 66, 67, 75, 78, 79, 84, 85, 87, 94, 95, 96, 97], "affin": [8, 11, 13, 14, 15, 16, 20, 23, 24, 29, 55, 66, 79], "feel": [8, 79, 84, 87, 88], "free": [8, 79, 87], "open": [8, 79, 84], "question": [8, 79, 81, 84, 87, 97], "our": [8, 10, 21, 78, 80, 82, 84, 85, 87, 93, 94], "overview": [8, 76, 80, 88], "page": [8, 80, 95], "contribut": [8, 80, 84], "exist": [8, 49, 78, 79, 84, 85, 87, 93, 97], "base": [8, 17, 22, 48, 64, 72, 79, 80, 84, 87, 88, 92, 93, 94, 95, 96, 97], "make": [8, 79, 80, 87, 88, 93, 97], "trainabl": [8, 10, 79, 87], "add": [8, 22, 87, 91, 95, 97], "parallel": [8, 78, 87, 88], "affine_quantized_tensor": [8, 81], "api": [8, 52, 79, 80, 84, 85, 87, 92, 93, 94, 95, 96], "quant_api": [8, 65, 81, 82, 85], "primit": [8, 11, 19, 87, 93], "op": [8, 10, 11, 19, 44, 52, 65, 80, 84, 87, 88, 93, 94, 95, 97], "slight": [8, 84], "variat": [8, 79], "quant_primit": [8, 11, 19, 85], "mp": 8, "csrc": 8, "mayb": [8, 33], "well": [8, 17, 52, 79, 80, 84, 93, 94, 97], "spars": [8, 12, 20, 23, 72, 79, 84], "marlin": [8, 18, 19, 20, 31], "aqt": 8, "621": 8, "ar": [8, 10, 11, 15, 23, 25, 37, 39, 40, 43, 44, 52, 53, 55, 57, 63, 65, 66, 67, 72, 78, 79, 80, 81, 82, 84, 85, 88, 92, 93, 94, 95, 96, 97], "still": [8, 10, 79, 84, 93, 97], "decid": [8, 79, 84, 85], "split": [8, 82, 93, 94], "implement": [8, 10, 34, 81, 84, 85, 92, 93, 97], "regist": [8, 71, 87], "mai": [8, 54, 58, 79, 81, 85, 93, 94, 95, 96, 97], "own": [8, 10, 76, 78, 80, 84, 85, 93, 94, 97], "int4": [8, 10, 13, 16, 45, 48, 58, 60, 61, 62, 63, 65, 75, 80, 81, 82, 88], "access": [8, 46, 92], "my_custom_op": 8, "condit": [8, 79], "__torch_function__": [8, 79, 87], "__torch_dispatch__": [8, 87], "oper": [8, 10, 11, 15, 17, 20, 46, 54, 80, 82, 92, 93, 94, 95, 96], "uint4": [8, 44, 79, 80], "weight": [8, 10, 20, 21, 38, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 58, 60, 61, 62, 65, 72, 75, 76, 78, 80, 81, 84, 85, 87, 88, 92, 93, 94, 95, 96, 97], "found": [8, 79, 80, 82, 84, 85, 87], "allow": [8, 80, 84, 87, 92, 93, 94, 95, 97], "peopl": [8, 79, 81, 88, 97], "two": [8, 10, 19, 23, 40, 79, 80, 84, 87, 92, 93, 94, 95, 97], "dispatch_condit": [8, 79], "impl": [8, 11, 79], "actual": [8, 10, 42, 79, 85, 87, 88, 93, 94, 97], "run": [8, 10, 38, 52, 65, 68, 71, 78, 79, 80, 82, 84, 87, 91, 92, 93, 94, 95, 96, 97], "both": [8, 11, 40, 79, 80, 84, 85, 87, 93, 95, 96, 97], "input_tensor": [8, 21, 79, 88], "weight_tensor": [8, 79, 88], "argument": [8, 11, 24, 52, 55, 65, 78, 79, 82, 95], "register_aqt_quantized_linear_dispatch": 8, "show": [8, 66, 78, 79, 80, 82, 84, 88, 93, 94], "sometim": [8, 84], "ha": [8, 10, 11, 79, 82, 84, 87, 88, 92, 93, 94, 96, 97], "pack": [8, 11, 13, 24, 25, 39, 43, 50, 79], "order": [8, 52, 57, 79, 84, 87, 97], "yield": [8, 10, 84], "And": [8, 21, 40, 79, 87, 95, 97], "abstract": [8, 79], "full": [8, 10, 80, 85, 91, 92, 94], "after": [8, 10, 38, 52, 78, 79, 81, 84, 92, 93, 94, 95, 96, 97], "wrap": [8, 52, 87, 95, 96], "factori": 8, "convert": [8, 10, 11, 19, 21, 26, 29, 31, 32, 34, 57, 59, 60, 65, 75, 78, 79, 82, 84, 92, 95, 96, 97], "from": [8, 10, 11, 21, 22, 27, 28, 30, 39, 45, 54, 59, 63, 65, 66, 75, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97], "float": [8, 10, 11, 19, 21, 29, 31, 32, 39, 44, 48, 51, 52, 53, 54, 55, 58, 66, 69, 72, 79, 80, 81, 87, 93, 94, 97], "point": [8, 11, 19, 31, 39, 44, 48, 51, 55, 58, 64, 78, 79, 80, 81, 84, 85, 87, 93, 97], "my": [8, 84, 94], "to_my_dtyp": 8, "mydtypetensor": 8, "from_float": [8, 85, 87], "level": [8, 72, 79, 84, 87, 92, 93, 95, 96], "reus": [8, 79, 87], "appli": [8, 10, 11, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 57, 63, 65, 75, 79, 80, 82, 84, 88, 94], "convers": [8, 10, 11, 37, 79], "filter": [8, 10, 37, 52, 78, 85], "choos": [8, 79, 84, 87, 93, 95], "should": [8, 10, 11, 38, 43, 55, 59, 71, 72, 78, 79, 84, 88, 92, 93, 97], "algorithm": [8, 44, 50, 82, 84, 92], "dynam": [8, 10, 33, 34, 38, 40, 43, 45, 46, 58, 62, 75, 82, 85, 87, 93, 94, 95], "quant": [8, 11, 19, 39, 79, 82, 88, 93, 96, 97], "static": [8, 11, 17, 21, 27, 30, 34, 41, 54, 58, 76, 80, 93, 94, 95, 96, 97], "2": [8, 11, 14, 16, 20, 23, 44, 48, 52, 58, 66, 73, 75, 76, 78, 79, 84, 85, 87, 91], "4": [8, 10, 14, 20, 23, 32, 43, 73, 75, 79, 80, 81, 82, 84, 87, 93, 94], "below": [8, 78, 79, 84, 87, 88, 91, 92], "follow": [8, 10, 44, 58, 78, 79, 80, 82, 84, 85, 87, 92, 93, 94, 95, 96, 97], "import": [8, 10, 59, 63, 65, 75, 80, 81, 82, 84, 85, 87, 88, 91, 92, 95, 96], "unwrap_tensor_subclass": [8, 80], "m_unwrap": 8, "In": [8, 10, 78, 79, 80, 84, 85, 87, 92, 93, 94, 95, 96, 97], "aim": [8, 79, 84, 96], "fullgraph": [8, 80], "first": [8, 21, 52, 56, 72, 79, 82, 85, 87, 88, 93, 94, 97], "remov": [8, 53, 72, 78, 84, 88, 93, 94], "unnecessari": 8, "graph": [8, 80, 93, 94, 97], "break": 8, "torch_log": 8, "output_cod": 8, "script": [8, 80, 82, 85, 87, 91, 94, 95, 96], "inductor": [8, 52, 76, 80, 92, 93], "checkout": [8, 11, 19, 76, 79], "doc": [8, 78, 79, 80, 82, 87], "huggingfac": 8, "transform": [8, 10, 11, 79, 85, 92, 93, 94, 95, 96], "deseri": [8, 79, 93, 94], "save_pretrain": [8, 82], "push_to_hub": [8, 82, 88], "from_pretrain": [8, 10, 82, 88], "http": [8, 11, 19, 39, 52, 72, 80, 82, 84, 96], "co": [8, 82], "main": [8, 11, 19, 44, 79, 80, 82, 84, 85, 87, 93, 97], "en": [8, 52], "anoth": [8, 79, 84, 87, 93, 97], "diffus": 8, "com": [8, 11, 19, 39, 82], "sayakpaul": 8, "blob": [8, 11, 19], "serialization_and_load": 8, "md": 8, "abov": [8, 10, 48, 79, 81, 84, 85, 87, 93, 94, 97], "just": [8, 48, 58, 79, 81, 84, 87, 93, 94, 97], "talk": [8, 79, 82], "fsdp": [8, 79], "ll": [8, 48, 78, 79, 82, 87, 93, 94, 97], "put": [8, 75, 95, 97], "developer_api_guid": 8, "cover": [8, 79, 91, 93, 96, 97], "executorch": [8, 45, 65, 76, 80, 93, 94], "torchchat": 8, "todo": [8, 79], "qat": [8, 57, 58, 59, 60, 61, 62, 63, 64, 76, 82, 95], "dtensor": [8, 87], "recommend": [8, 10, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 78, 92, 95, 96], "copi": [8, 11, 72, 80, 81, 84, 85, 87, 92, 94, 95], "past": [8, 84], "adapt": [8, 78, 85], "befor": [8, 10, 65, 79, 81, 82, 84, 85, 87, 93, 94, 97], "some": [8, 52, 65, 72, 79, 80, 82, 84, 85, 87, 92, 93, 94, 95, 96, 97], "singl": [8, 10, 33, 38, 40, 52, 54, 78, 80, 84, 93, 97], "comput": [8, 20, 24, 38, 42, 71, 72, 84, 85, 87, 93, 94, 95, 96], "intens": 8, "get": [8, 10, 21, 78, 79, 80, 82, 84, 88, 92, 93, 94, 95, 97], "sens": [8, 79, 87], "d": [8, 79, 82, 94], "benchmark_aq": 8, "A": [8, 10, 11, 25, 52, 54, 71, 84, 87, 88, 93], "quick": [8, 76], "wai": [8, 11, 52, 78, 79, 82, 84, 85, 87, 93, 94, 97], "relev": [8, 44, 79, 91], "chang": [8, 65, 78, 79, 80, 81, 82, 84, 85, 87, 92, 93, 94, 96, 97], "interest": [8, 79, 84, 87], "print_op_and_shap": 8, "output": [8, 10, 34, 52, 53, 55, 66, 78, 79, 80, 82, 84, 91, 92, 93, 94, 95, 96, 97], "torch_func": 8, "built": [8, 78, 87], "_c": 8, "tensorbas": 8, "object": [8, 25, 65, 75, 87, 93, 94, 97], "arg": [8, 11, 60, 72, 87, 88, 94, 97], "all": [8, 38, 48, 52, 54, 60, 64, 71, 72, 73, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 92, 93, 95, 97], "under": [8, 10, 82], "benchmark_your_kernel": 8, "helper": 8, "right": [8, 79, 84, 93], "1": [8, 20, 25, 35, 36, 44, 48, 49, 50, 51, 52, 66, 72, 76, 79, 80, 81, 83, 84, 85, 87, 90, 91, 93, 94], "either": [8, 11, 40, 72, 82, 84, 94, 95, 96], "one": [8, 40, 52, 54, 71, 78, 79, 84, 87, 88, 94, 97], "probabl": 8, "keep": [8, 20, 46, 72, 93], "futur": [8, 39, 85, 88, 93, 94, 95, 97], "llama": [8, 10, 82, 88, 92], "llama2": 8, "llama3": [8, 10, 78], "sam": 8, "alreadi": [8, 11, 52, 87, 97], "modifi": [8, 37, 65, 72, 78, 79, 84, 87], "friendli": [8, 79], "compar": [8, 10, 44, 72, 78, 79, 82, 93, 95, 97], "techniqu": [8, 10, 78, 81, 82, 84, 85, 87, 88], "repres": [8, 11, 12, 15, 17, 28, 34, 58, 66, 72, 79, 81, 87, 93, 94], "bound": [8, 82, 84, 88], "help": [8, 10, 78, 79, 82, 88, 92, 93], "each": [8, 21, 52, 58, 68, 71, 79, 84, 85, 87, 88, 93, 94, 97], "understand": [8, 78, 95, 97], "profile_path": 8, "chrome": 8, "trace": [8, 79], "let": [8, 48, 66, 79, 80, 84, 85, 87, 97], "know": [8, 52, 87], "end": [10, 78, 79, 82, 84, 87, 88, 91, 94, 97], "pre": [10, 17, 20, 24, 76, 80, 82, 84, 97], "serv": [10, 11, 17, 76, 78, 87, 96], "flow": [10, 45, 78, 82, 84, 85, 92, 93, 94, 95, 96], "leverag": [10, 78, 80, 82, 87, 95, 96], "partner": [10, 78, 82], "showcas": [10, 78, 82], "focus": [10, 78, 79, 82, 84], "domain": [10, 11, 44, 51, 53, 55, 58, 78], "demonstr": [10, 78, 79, 80, 82, 87, 92, 94], "dure": [10, 11, 19, 46, 52, 55, 58, 69, 78, 80, 82, 84, 85, 87, 92, 94], "numer": [10, 52, 78, 84, 93, 94, 95], "goal": 10, "mitig": [10, 84], "degrad": [10, 84], "eventu": [10, 78], "blog": 10, "resourc": [10, 87], "small": 10, "matric": [10, 23, 84], "freez": [10, 94, 95, 96], "checkpoint": [10, 78, 82, 88], "effici": [10, 24, 80, 84, 85, 96], "paper": [10, 39, 84, 91], "speed": [10, 65, 82, 84, 92], "up": [10, 21, 65, 78, 79, 80, 84, 92, 93, 94, 97], "high": [10, 11, 26, 27, 28, 29, 30, 78, 79, 82, 84, 85, 87, 92, 93, 95, 96], "precis": [10, 11, 26, 27, 28, 29, 30, 42, 46, 61, 62, 79, 85, 87, 92, 95, 96], "similar": [10, 79, 84, 85, 94, 95], "so": [10, 52, 78, 79, 80, 81, 84, 87, 93, 94, 97], "inevit": 10, "presum": 10, "been": [10, 52, 87, 94, 95, 96, 97], "successfulli": [10, 84], "recent": [10, 76], "releas": [10, 80, 95], "1b": [10, 88], "3b": 10, "llamaguard": 10, "8b": [10, 78], "improv": [10, 78, 82, 84, 93, 96, 97], "qualiti": [10, 84], "involv": [10, 15, 84], "separ": [10, 58, 84, 88, 93, 97], "prepar": [10, 52, 57, 60, 68, 72, 79, 84, 92, 95, 96, 97], "fake": [10, 58, 59, 60, 61, 62, 63, 78, 93, 94, 97], "mean": [10, 11, 21, 48, 53, 55, 66, 78, 79, 80, 84, 93, 94, 97], "valu": [10, 11, 21, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 66, 68, 72, 79, 84, 85, 87, 92, 93, 94, 97], "map": [10, 46, 48, 58, 79, 87, 93, 97], "without": [10, 59, 79, 84, 88, 95, 97], "cast": [10, 33, 35], "lower": [10, 45, 79, 80, 82, 84, 85, 94], "replac": [10, 69, 84, 88], "real": [10, 80, 93, 97], "doe": [10, 22, 44, 79, 84, 87, 93, 95, 96], "perform": [10, 11, 24, 38, 43, 46, 47, 52, 56, 60, 61, 62, 67, 68, 71, 78, 80, 84, 85, 87, 88, 92, 94, 95, 96], "There": [10, 79, 85, 87, 93, 97], "directli": [10, 48, 54, 79, 84, 85, 87], "loop": [10, 78, 84], "distribut": [10, 78, 85, 87, 88, 92], "recip": [10, 34, 71], "instead": [10, 44, 54, 58, 71, 78, 79, 80, 84, 87, 94, 95, 96, 97], "command": [10, 78, 80], "regular": [10, 92, 95, 96], "nnode": 10, "nproc_per_nod": 10, "full_finetune_distribut": 10, "llama3_2": 10, "3b_full": 10, "batch_siz": [10, 81, 82, 85, 93, 94], "16": [10, 78], "equival": [10, 58, 69, 84, 94, 95, 97], "specifi": [10, 11, 34, 37, 47, 50, 57, 65, 66, 72, 75, 78, 84, 92, 93, 94, 97], "default": [10, 11, 12, 15, 22, 24, 25, 40, 41, 42, 43, 44, 50, 52, 53, 55, 58, 65, 68, 69, 78, 80, 87, 88, 92, 93, 94, 95, 96, 97], "asymmetr": [10, 43, 44, 45, 48, 50, 53, 58, 79, 80, 85, 92, 96, 97], "per": [10, 11, 42, 44, 45, 46, 47, 50, 53, 55, 58, 60, 61, 62, 66, 72, 78, 79, 80, 84, 85, 96], "token": [10, 45, 46, 58, 62, 78, 82], "int8": [10, 21, 45, 46, 47, 58, 62, 63, 65, 75, 79, 82, 87, 93, 95, 96, 97], "symmetr": [10, 40, 41, 42, 43, 45, 46, 47, 48, 53, 58, 87, 92, 93, 96, 97], "configur": [10, 15, 33, 34, 37, 40, 41, 42, 44, 45, 46, 47, 50, 65, 75, 78, 79, 80, 82, 95, 96, 97], "_component_": 10, "qat_distribut": 10, "3b_qat_ful": 10, "evalu": [10, 94], "whether": [10, 44, 50, 51, 52, 53, 58, 87], "wa": [10, 87, 94], "llama3_2_3b": 10, "fullmodelhfcheckpoint": 10, "checkpoint_fil": 10, "00001": 10, "00002": 10, "safetensor": 10, "int8dynactint4weightquant": 10, "groupsiz": [10, 61, 62, 66], "32": [10, 43, 44, 45, 58, 63, 65, 75, 78, 80, 81, 82, 85, 87, 94], "hellaswag": [10, 82], "wikitext": 10, "eleuther_ev": 10, "eleuther_evalu": 10, "task": [10, 82], "fullmodeltorchtunecheckpoint": 10, "8da4w": [10, 82], "ckpt": 10, "llama3_token": 10, "path": [10, 65, 67, 80, 82, 92, 93, 94, 95, 97], "tmp": [10, 80], "meta": [10, 81, 88, 97], "print": [10, 72, 80, 81, 82, 87, 91, 93, 94], "version": [10, 16, 58, 78, 80, 87, 88, 93, 94, 97], "shot": [10, 84], "stderr": 10, "none": [10, 11, 15, 19, 26, 29, 30, 31, 33, 34, 35, 36, 37, 38, 40, 41, 43, 44, 47, 48, 49, 51, 52, 53, 54, 55, 58, 63, 64, 65, 66, 68, 69, 70, 72, 75, 85, 87, 88, 92, 93, 94, 96], "acc": [10, 93, 94], "5021": 10, "0050": 10, "acc_norm": 10, "6797": 10, "0047": 10, "bits_per_byt": 10, "6965": 10, "byte_perplex": 10, "6206": 10, "word_perplex": 10, "13": 10, "2199": 10, "much": [10, 80, 84, 97], "openassist": 10, "oasst1": 10, "dataset": [10, 78, 79, 82, 92, 95, 96], "find": [10, 21, 84, 93, 97], "achiev": [10, 21, 78, 84, 85, 87, 94, 95], "higher": [10, 78, 79, 87, 92, 93, 95, 96], "accuraci": [10, 78, 82, 84, 85, 92, 94, 95], "than": [10, 25, 58, 78, 79, 84, 87, 93], "recov": [10, 84, 94], "69": [10, 85], "8": [10, 24, 25, 43, 44, 48, 61, 78, 79, 80, 82, 88, 95, 96], "overal": [10, 76, 80, 93, 97], "vanilla": 10, "compos": [10, 57, 79, 84, 87, 93, 94, 97], "lora": 10, "89x": 10, "usag": [10, 11, 38, 52, 57, 58, 59, 63, 76, 78, 82, 95, 96], "36": [10, 78, 82], "qat_lora_finetune_distribut": 10, "3b_qat_lora": 10, "try": [10, 79, 84, 87, 93], "fsdp2": [10, 78], "yaml": 10, "onc": [10, 52, 84], "complet": [10, 52, 82, 92, 96], "save": [10, 72, 78, 80, 81, 82, 88], "qat_out": 10, "quatiz": 10, "document": [10, 87, 88, 92, 93, 95], "prefer": [10, 79, 80, 87], "call": [10, 11, 52, 71, 79, 80, 81, 84, 85, 87, 88, 94, 96], "These": [10, 84, 87, 92, 93, 94, 97], "hood": 10, "mini": [10, 82], "gpu": [10, 76, 78, 80, 88, 91, 92], "smaller": [10, 25, 43, 44, 45, 50, 80, 81], "fit": [10, 24, 79, 81], "adjust": [10, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52], "attribut": [10, 87, 88, 95, 96], "accordingli": 10, "get_model": 10, "vocab_s": 10, "4096": [10, 78], "num_lay": 10, "num_head": 10, "num_kv_head": 10, "embed_dim": 10, "2048": [10, 78], "max_seq_len": 10, "train_loop": 10, "sgd": 10, "lr": [10, 78], "001": 10, "momentum": [10, 94], "9": [10, 78], "weight_decai": 10, "1e": [10, 78], "5": [10, 48, 69, 72, 78, 80, 82, 84, 88, 91, 93, 94], "loss_fn": 10, "crossentropyloss": [10, 93, 94], "i": [10, 67, 78, 82, 84, 92, 93, 94], "rang": [10, 48, 78, 84, 85, 93, 94], "randint": 10, "loss": [10, 78, 84, 93, 94], "backward": [10, 38, 78, 84, 94], "zero_grad": [10, 78, 94], "next": [10, 78, 79, 85, 93, 94, 95, 96], "scheme": [10, 46, 47, 82, 92], "although": [10, 71, 87], "integ": [10, 11, 29, 30, 43, 44, 48, 51, 53, 55, 56, 58, 67, 85, 93, 94, 95], "arithmet": 10, "float32": [10, 11, 27, 55, 58, 60, 62, 66, 81, 82, 84, 85, 87, 95, 96, 97], "becaus": [10, 20, 78, 79, 81, 84, 87, 94, 97], "fakequantizeconfig": [10, 63], "intxquantizationawaretrainingconfig": 10, "insert": [10, 80, 85, 92, 93, 94, 95, 96, 97], "swap": [10, 37, 60, 78, 79, 84, 85, 94], "fakequantizedlinear": [10, 59], "activation_config": [10, 63], "per_token": [10, 58, 63], "is_symmetr": [10, 58, 63], "weight_config": [10, 63], "qat_config": 10, "structur": [10, 23, 75, 80, 81, 84, 87, 93], "attun": 10, "benefici": 10, "later": [10, 79, 87, 93, 94, 96], "int8dynamicactivationint4weightconfig": 10, "fromintxquantizationawaretrainingconfig": 10, "back": [10, 59, 87], "subclass": [10, 11, 19, 37, 52, 71, 75, 80, 81, 84], "readi": [10, 78, 80, 82, 85, 87, 94], "did": [10, 45], "altern": [10, 58, 79, 85, 87, 95, 96], "legaci": 10, "offer": [10, 87, 93], "customiz": [10, 65], "unlik": [10, 85], "int8dynactint4weightqatquant": 10, "qat_quant": 10, "int8dynactint4weightqatlinear": 10, "int8dynactint4weightlinear": 10, "fraction": 10, "therebi": 10, "significantli": [10, 92, 93, 95, 96], "footprint": 10, "extens": [10, 87, 93, 95], "addition": [10, 95, 96], "frozen": 10, "further": [10, 79, 87, 92, 93, 94, 95], "nf4": [10, 21], "propos": [10, 72], "express": [10, 80, 87, 92, 93, 94, 97], "nf4tensor": 10, "cleanli": 10, "simpli": [10, 52, 84, 85, 87], "to_nf4": 10, "frozennf4linear": 10, "in_dim": 10, "out_dim": 10, "bool": [10, 11, 29, 34, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52, 53, 54, 58, 62, 65, 68, 75, 85], "quantization_kwarg": 10, "No": [10, 79, 81, 84], "requires_grad_": 10, "nf4_weight": 10, "requires_grad": [10, 79, 85, 87, 88], "though": [10, 87], "shown": [10, 82, 84, 94, 97], "competit": [10, 78], "baselin": [10, 78, 82, 93], "while": [10, 71, 72, 82, 84, 87, 92, 93, 97], "even": [10, 11, 78, 84, 97], "newer": 10, "mxfp4": 10, "nvfp4": 10, "blackwel": 10, "reap": 10, "benefit": [10, 84, 87, 93, 96], "vari": [10, 93, 94, 95, 96], "tradeoff": [10, 84], "incorpor": 10, "its": [10, 43, 84, 87, 88, 93, 97], "loralinear": 10, "lora_finetune_single_devic": 10, "3b_qlora_single_devic": 10, "limit": [10, 78, 87, 88, 93], "yet": [10, 45, 49, 87, 88, 94, 95, 96], "invok": [10, 95], "loraconfig": 10, "get_peft_model": 10, "automodelforcausallm": [10, 82, 88], "torchaoconfig": [10, 82, 88], "int8weightonlyconfig": [10, 88], "base_model": 10, "quantization_config": [10, 82, 88, 96], "peft_config": 10, "throughput": [10, 78, 82], "increas": [10, 84, 93], "torchtitan": 10, "enable_fp8_train": 10, "fp8_recipe_nam": 10, "tensorwis": [10, 33, 34], "initi": [10, 11, 64, 79, 80, 81, 94], "experi": [10, 78, 96], "saw": 10, "experiment_nam": 10, "tok": 10, "peak_mem_reserv": 10, "6502": 10, "143": 10, "000": 10, "30": [10, 78, 80, 93], "090": 10, "fp8_nonam": 10, "7205": 10, "386": 10, "816": 10, "010": 10, "266": 10, "fp8_tensorwis": 10, "7222": 10, "198": 10, "11": [10, 78], "074": [10, 78], "fp8_rowwis": 10, "6387": 10, "968": 10, "756": 10, "29": [10, 78], "158": 10, "096": 10, "fp8_rowwise_with_gw_hp": 10, "7573": 10, "698": 10, "480": 10, "516": 10, "908": 10, "hellaswag_acc": 10, "wikitext_word_perplex": 10, "533": 10, "12": [10, 78, 96, 97], "407": [10, 78], "414": 10, "007": 10, "412": 10, "005": 10, "420": 10, "013": [10, 78], "534": 10, "416": 10, "009": 10, "tensor_impl": [11, 19, 79, 85], "aqttensorimpl": [11, 19], "block_siz": [11, 17, 19, 21, 26, 27, 29, 30, 31, 32, 53, 54, 55, 66, 80, 85], "tupl": [11, 19, 21, 26, 27, 29, 30, 31, 40, 41, 53, 54, 55, 64, 66, 72, 87, 88, 93, 94, 97], "quant_min": [11, 19, 29, 30, 31, 48, 53, 54, 55, 66, 79, 80, 87, 96, 97], "union": [11, 19, 34, 40, 41, 53, 55, 58, 65, 66], "quant_max": [11, 19, 29, 30, 31, 48, 53, 54, 55, 66, 79, 80, 87, 96, 97], "zero_point_domain": [11, 19, 29, 30, 31, 44, 53, 54, 58], "zeropointdomain": [11, 19, 29, 30, 31, 44, 53, 54, 58], "stride": [11, 19, 79, 87], "sourc": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 82, 89, 91], "quantized_tensor": 11, "float_tensor": [11, 87], "scale": [11, 17, 20, 27, 30, 35, 38, 41, 48, 51, 53, 54, 55, 56, 58, 64, 66, 68, 69, 79, 84, 85, 87, 88, 97], "zero_point": [11, 17, 30, 44, 51, 53, 54, 55, 66, 79, 84, 85, 87, 97], "happen": [11, 19, 52, 79, 87, 93, 95], "choose_qparam": [11, 79], "dequant": [11, 19, 21, 44, 55, 79, 80, 87, 88, 93, 95, 96, 97], "ao": [11, 19, 84, 88], "three": [11, 52, 72, 75, 79, 95, 96], "choose_qparams_affin": [11, 44, 54, 79], "quantize_affin": [11, 44, 79], "qand": 11, "dequantize_affin": [11, 44], "extern": [11, 95], "regardless": 11, "intern": [11, 24], "represent": [11, 17, 28, 44, 79, 84, 88, 93, 97], "orient": 11, "field": [11, 58, 97], "storag": [11, 20, 79, 84], "store": [11, 20, 21, 25, 46, 71, 79, 84, 88, 93, 94], "plain": [11, 88], "int_data": [11, 87], "format": [11, 20, 21, 39, 43, 79, 82, 84, 93, 94, 97], "kernel": [11, 13, 14, 16, 20, 24, 39, 43, 44, 65, 80, 82, 84, 92, 95, 96], "granular": [11, 35, 40, 41, 43, 44, 45, 47, 50, 53, 55, 58, 66, 78, 79, 82, 85, 88], "element": [11, 23, 25, 52, 53, 55, 66, 84], "share": [11, 53, 55, 66, 84], "qparam": [11, 53, 55, 66], "minimum": [11, 52, 53, 55, 66], "deriv": [11, 54, 66], "maximum": [11, 53, 55, 66, 68], "zero": [11, 23, 44, 46, 53, 55, 58, 64, 72, 84, 85, 97], "subtract": [11, 21], "unquant": [11, 97], "given": [11, 19, 32, 78, 84, 88, 97], "classmethod": [11, 19, 85, 87, 88], "from_hp_to_floatx": 11, "input_float": [11, 19, 26, 27, 28, 29, 30, 31, 70], "target_dtyp": [11, 26, 27, 29, 30, 33, 34, 53, 54, 79, 85], "_layout": [11, 19, 26, 27, 28, 29, 30, 31, 79, 80, 85], "layout": [11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 44, 45, 46, 75, 84], "scale_dtyp": [11, 26, 27, 29, 53, 54, 85], "float8": [11, 14, 15, 26, 27, 33, 34, 35, 36, 37, 38, 40, 41, 42, 76, 79, 82, 85], "from_hp_to_floatx_stat": 11, "from_hp_to_fpx": 11, "floatx": [11, 28, 79], "ebit": [11, 28, 39], "mbit": [11, 28, 39], "float1": [11, 28], "float7": [11, 28], "from_hp_to_intx": [11, 19], "mapping_typ": [11, 29, 45, 53, 54, 58], "mappingtyp": [11, 29, 45, 46, 53, 54, 58, 85], "ep": [11, 29, 53, 54, 58, 85, 94, 96, 97], "zero_point_dtyp": [11, 29, 53, 54, 85], "preserve_zero": [11, 29, 44, 53, 54], "plainlayout": [11, 29, 30, 45, 46, 85], "use_hqq": [11, 29, 44, 50, 88], "from_hp_to_intx_stat": 11, "correct": [11, 20, 93, 94], "otherwis": [11, 47, 57, 58, 79, 94], "desir": [11, 52, 85], "non_block": 11, "memory_format": [11, 95, 96], "preserve_format": 11, "attempt": 11, "asynchron": 11, "respect": [11, 84, 94], "host": [11, 88], "behavior": [11, 17, 57, 88, 93, 94], "pin": 11, "pageabl": 11, "howev": [11, 84, 88, 94, 97], "caution": 11, "advis": [11, 79], "good": [11, 80, 87, 97], "pin_memori": 11, "match": [11, 55, 56, 84, 93], "float64": 11, "5044": 11, "0005": 11, "3310": 11, "0584": 11, "cuda0": 11, "blocksiz": 12, "64": [12, 32, 44, 50, 81, 82, 85, 87, 88], "block": [12, 21, 72, 84], "variabl": [12, 15, 24, 25, 72, 84], "cutlass": [13, 14], "mm_config": [15, 40, 41], "float8mmconfig": [15, 40, 41], "tinygemm": [16, 44, 65, 79, 80], "_weight_int4pack_mm_for_cpu": [16, 44], "least": 16, "6": [16, 58, 78, 79, 80, 82, 84, 93, 94, 95], "It": [17, 20, 22, 24, 38, 80, 84, 87, 97], "post": [17, 24, 76, 80, 87, 94, 97], "design": [17, 20, 23, 82, 88, 92, 93, 97], "extend": [17, 79, 84, 95], "conjunct": 17, "tensorimpl": 17, "interact": [17, 79, 93], "qqq": [18, 19, 31], "marlinqqq": 19, "inherit": [19, 22, 87, 88, 95, 96], "_choose_qparams_and_quantize_affine_qqq": 19, "_dequantize_affine_qqq": 19, "pattern": [20, 23, 79, 80, 88, 92, 93], "preprocess": [20, 23], "manag": 20, "pre_process": 20, "1\u00ba": 20, "transpos": [20, 79, 87], "sinc": [20, 71, 79, 81, 82, 84, 85, 87, 93, 94, 95, 96, 97], "2\u00ba": 20, "inject": 20, "3\u00ba": 20, "again": [20, 21, 84, 93, 97], "dim": [20, 85, 87, 88, 93, 94], "tensor_meta": 21, "subclasstensorarg": 21, "n_block": 21, "scaler_block_s": [21, 32], "quantized_scal": 21, "quantization_factor": 21, "scaler_mean": 21, "quantized_data": [21, 88], "qlora": [21, 76, 82], "convert_to_norm_float_weight": 21, "normal": [21, 32, 52, 84, 93, 94], "dequantize_scal": 21, "unpack": [21, 79], "doubl": 21, "scaler": 21, "per_scaler_block": 21, "factor": [21, 56, 69, 78, 84], "inpt_weight": 21, "double_quantize_scal": 21, "calcul": [21, 38, 48, 53, 54, 68, 79, 84, 93, 97], "absmax": 21, "posit": 21, "per_block": 21, "int16": [21, 93], "n_scaler_block": 21, "get_original_weight": 21, "quantize_tensor_nearest": 21, "float16": [21, 66, 84], "nearest": 21, "round": [21, 48, 87], "metadata": [22, 79, 82, 87, 88], "semi": [23, 75, 84], "everi": [23, 71, 84, 87, 93, 94], "four": [23, 92], "prune": [23, 72], "conform": 23, "inner_k_til": [24, 44, 61, 80], "core": [24, 49, 79, 85, 88, 93], "tile": [24, 79], "affect": [24, 84], "matmul": [24, 42, 79, 84, 87], "pack_dim": [25, 50], "uintx": [25, 50, 79], "standard": [25, 79, 88], "byte": [25, 39, 50], "uintxtensor": 25, "determin": [25, 53, 78, 84, 88], "along": [25, 84, 88, 92], "indic": [25, 51, 84, 97], "last": [25, 78, 92], "256": [32, 44, 60, 61, 62, 82, 93, 94, 97], "scaling_typ": [33, 34], "scalingtyp": [33, 34], "scaling_granular": [33, 34], "scalinggranular": [33, 34], "cast_config_input": 34, "castconfig": 34, "cast_config_input_for_grad_weight": 34, "cast_config_weight": 34, "cast_config_weight_for_grad_input": 34, "cast_config_grad_output": 34, "cast_config_grad_output_for_grad_weight": 34, "gemm_config_output": 34, "float8gemmconfig": 34, "use_fast_accum": 34, "gemm_config_grad_input": 34, "gemm_config_grad_weight": 34, "enable_fsdp_float8_all_gath": 34, "pad_inner_dim": 34, "emul": 34, "force_recompute_fp8_weight_in_bwd": 34, "round_scales_to_power_of_2": 34, "from_recipe_nam": 34, "recipe_nam": [34, 78], "float8linearrecipenam": 34, "qualnam": [35, 36, 48, 49, 51], "boundari": [35, 36, 48, 49, 51], "strategi": 35, "module_filter_fn": [37, 78], "callabl": [37, 52, 65, 70, 75, 88], "float8linearconfig": 37, "float8linear": [37, 78], "instanc": [37, 65, 71, 75, 81, 87, 93, 95, 96, 97], "fqn": [37, 72, 75, 78, 85], "sum": [38, 93, 94], "set_inductor_config": [39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 52], "sub": [39, 50, 84], "expon": 39, "mantissa": 39, "fp6_e3_m2": 39, "fp6_e2_m3": 39, "fp6": 39, "llm": 39, "arxiv": [39, 72, 84], "org": [39, 52, 72, 79, 80, 82, 84, 96], "ab": [39, 72, 84], "2401": 39, "14112": 39, "repo": 39, "usyd": 39, "fsalab": 39, "fp6_llm": 39, "renam": [39, 93, 94], "fpxtensorcoreaqttensorimpl": 39, "experiment": [39, 92], "merg": 39, "to_affine_quantized_floatx": 39, "activation_dtyp": [40, 41], "float8_e4m3fn": [40, 41, 42, 79], "weight_dtyp": [40, 41, 42, 82], "pertensor": [40, 41, 85], "perrow": [40, 41, 82], "list": [40, 52, 55, 57, 69, 72, 79, 80, 87, 88, 92, 94, 97], "fast": [40, 41, 84], "accumul": [40, 41], "torchinductor": [40, 41, 42, 43, 44, 45, 46, 47, 50, 95, 96], "float8_e4m": 41, "channel": [42, 46, 47, 58, 60, 61, 62, 71, 85, 96], "128": [43, 44, 78, 82, 85, 87, 88, 96, 97], "packing_bitwidth": 43, "weight_onli": 43, "gemlit": 43, "triton": [43, 79, 95, 96], "associ": [43, 85], "fp16": [43, 53], "control": [43, 44, 45, 46, 47, 50, 72, 84, 88, 93], "grain": [43, 44, 45, 50, 87], "impact": [43, 52, 78, 82, 88], "hardwar": [43, 79, 80, 82, 84], "runtim": [43, 79, 93], "tensorcoretiledlayout": [44, 79, 80], "tensor_core_til": [44, 79], "int4mm": [44, 80], "aten": [44, 79, 80, 87, 88, 92, 93, 94, 95, 96], "_weight_int4pack_mm": [44, 79], "tradit": 44, "exactli": [44, 87], "chosen": [44, 84], "choic": 44, "hqq": [44, 50, 79], "preserv": [44, 53, 72, 82, 84, 92], "Will": 44, "act_mapping_typ": [45, 46], "produc": [45, 80, 92, 93, 94, 95, 96], "backend": [45, 76, 80, 82, 84, 97], "marlinqqqlayout": 45, "cutlassint4packedlayout": 45, "weight_only_decod": 46, "around": [46, 78, 80, 81, 93], "decod": [46, 82], "better": [46, 47, 78, 87, 93, 94, 95, 96, 97], "number": [48, 50, 52, 72, 82, 84, 87, 94, 95], "sai": [48, 66, 79, 88, 97], "3": [48, 52, 66, 76, 78, 79, 80, 84, 91, 93, 94], "7": [48, 78, 82, 95, 96], "symmetric_no_clipping_err": 48, "variant": [48, 54, 87], "smin": 48, "smax": 48, "min_val_neg": [48, 87], "max_val_po": [48, 87], "By": [48, 84], "individu": [48, 84], "less": [48, 84, 87, 93], "neg": 48, "placehold": [49, 96], "uint1": [50, 79], "uint7": [50, 79], "enum": 51, "quantized_v": 51, "float_val": 51, "mid_point": 51, "example_input": [52, 64, 80, 81, 85, 92, 93, 94, 95, 96, 97], "qtensor_class_list": 52, "aqdefaultlinearweight": 52, "aqint8weightonlyquantizedlinearweight": 52, "aqint8weightonlyquantizedlinearweight2": 52, "aqint8dynamicallyquantizedlinearweight": 52, "filter_fn": [52, 65, 75], "interpol": 52, "85": 52, "manual": [52, 94], "supress_autoquant_error": 52, "min_sqnr": 52, "aq_kwarg": 52, "autoquant": 52, "identifi": [52, 85, 97], "fastest": 52, "over": [52, 78, 84, 93, 94], "potenti": [52, 84, 85, 92, 93, 95, 96], "qtensor": 52, "search": [52, 84], "whose": [52, 97], "exchang": 52, "autoquantizablelinearweight": 52, "calibr": [52, 54, 80, 92, 94, 95, 96], "seen": 52, "record": [52, 79, 85], "final": [52, 65, 79, 80, 84, 92, 93, 94, 95, 96, 97], "benchmark": [52, 68, 76, 78, 80, 92, 95, 96], "member": 52, "pick": 52, "highli": 52, "had": [52, 87, 93], "proce": 52, "combin": [52, 58, 82, 84, 87, 93, 95], "finalize_autoqu": 52, "log": [52, 87], "fulli": [52, 65, 69, 75, 82, 84, 93], "unless": [52, 88], "default_autoquant_class_list": 52, "second": [52, 56, 78, 79, 91, 97], "stop": 52, "wait": [52, 79], "sever": [52, 78, 88, 92, 97], "automat": [52, 78, 82, 87, 88, 91], "suppress": 52, "accept": [52, 82, 97], "signal": 52, "nois": 52, "ration": 52, "wikipedia": 52, "wiki": 52, "noise_ratio": 52, "v": [52, 97], "non": [52, 79, 84, 87, 92, 95, 96], "caus": [52, 78], "too": 52, "larg": [52, 82, 87, 95], "resaon": 52, "40": [52, 78], "keyword": 52, "example_input1": 52, "example_input2": 52, "int32": [53, 58, 60, 79, 80, 93, 97], "fp32": [53, 55, 58, 85, 87, 93, 95], "optioanl": 53, "param": [53, 54, 72, 82], "request": [53, 55, 66], "min_val": [54, 79, 87], "max_val": [54, 79, 87], "observ": [54, 71, 84, 85, 92, 93, 94, 95, 96, 97], "obtain": 54, "track": [54, 79, 88], "input_dtyp": 55, "output_dtyp": [55, 66], "uint8": [55, 66, 79, 85, 97], "b": 56, "scales1": 56, "multipli": [56, 67, 84], "rais": [56, 63, 67, 78, 87, 88], "assertionerror": [56, 67, 78, 87], "expect": [56, 78, 84, 87, 92, 93, 95, 96, 97], "twostepquant": 57, "easili": [57, 92], "thei": [57, 78, 79, 80, 84, 87, 93, 94, 97], "constructor": [57, 87], "must": [57, 58, 78, 84, 88, 94, 96, 97], "embed": [57, 60, 63], "undefin": [57, 72], "my_quant": 57, "qatquantizer1": 57, "qatquantizer2": 57, "qatquantizer3": 57, "torchaodtyp": 58, "scale_precis": [58, 60], "zero_point_precis": [58, 60], "is_dynam": [58, 95, 96, 97], "range_learn": 58, "simul": [58, 73, 79, 84], "older": 58, "int1": [58, 79], "int7": 58, "pergroup": [58, 82], "pertoken": 58, "per_channel": 58, "peraxi": [58, 82, 85], "per_group": [58, 66], "leav": 58, "empti": [58, 79], "properti": [58, 79], "throw": 58, "els": [58, 82, 88, 93, 94], "fakequantizedembed": 59, "model_with_fake_quantized_linear": 59, "int4weightonlyqatembed": 60, "int4weightonlyembed": 60, "scales_precis": [61, 62], "padding_allow": 62, "valueerror": 63, "fakequant": 64, "inplac": [65, 72, 80], "qualifi": [65, 69, 75, 84], "move": [65, 79, 85, 88, 94, 95], "predefin": [65, 97], "execut": [65, 83, 87, 90], "int8_dynamic_activation_int4_weight": 65, "int8_dynamic_activation_int8_weight": [65, 75], "mm": [65, 87, 93], "int4_weight_onli": 65, "int8_weight_onli": 65, "sequenti": [65, 75, 78], "tabl": [66, 78, 79, 84], "per_tensor": 66, "per_axi": 66, "axi": [66, 85], "mat2": 67, "safe": 67, "consid": [67, 79, 84], "cubla": 67, "fallback": [67, 88], "j": 67, "debug_skip_calibr": 68, "smoothquant": [68, 69, 92], "smoothfakedynamicallyquantizedlinear": [68, 69], "skip_fqn_list": 69, "cur_fqn": 69, "alpha": 69, "skip": [69, 72, 84], "being": [69, 78, 79, 84, 88, 95, 96], "input_quant_func": [70, 79], "quant_kwarg": 70, "dict": [70, 72, 87, 88, 96, 97], "l2": [71, 84], "norm": [71, 72, 84], "buffer": 71, "x_orig": 71, "overridden": 71, "within": [71, 82, 84, 88, 95, 96], "afterward": 71, "former": 71, "care": [71, 81, 84, 93], "hook": [71, 79], "latter": [71, 94], "silent": [71, 95], "ignor": [71, 78, 93, 94], "sparsity_level": [72, 84], "semi_structured_block_s": 72, "wanda": 72, "sparsifi": [72, 76, 81, 84], "2306": 72, "11695": 72, "awar": [72, 76, 80, 84, 87], "product": [72, 82, 88, 95, 97], "magnitud": [72, 84], "parametr": 72, "deepcopi": [72, 80, 85, 87, 94], "squash_mask": [72, 84], "params_to_keep": 72, "params_to_keep_per_lay": 72, "squash": 72, "mask": [72, 84], "appropri": [72, 79, 92, 93, 94, 95, 96], "sparse_param": 72, "attach": [72, 84, 97], "kei": [72, 84, 91], "xdoctest": 72, "local": [72, 82, 84], "don": [72, 78, 80, 84, 88, 97], "t": [72, 78, 79, 80, 84, 85, 87, 88, 93, 94, 97], "hasattr": [72, 88], "submodule1": 72, "linear1": [72, 80, 81, 85, 87], "foo": [72, 93], "bar": [72, 93], "submodule2": 72, "linear42": 72, "baz": 72, "42": [72, 85], "24": 72, "ones": [72, 79, 94], "update_mask": 72, "tensor_nam": [72, 88], "statist": [72, 79, 84, 85, 93, 94], "retriev": 72, "act_per_input": 72, "Then": [72, 87, 96, 97], "across": [72, 82, 84, 87, 88], "whole": [72, 97], "alia": [74, 88], "semisparseweightconfig": 74, "sparsify_": 75, "apply_tensor_subclass": [75, 79], "essenti": [75, 88, 92], "semi_sparse_weight": 75, "semisparselayout": 75, "sparsemarlinlayout": 75, "isinst": [75, 78, 84, 85, 87, 88, 94, 97], "sparse_api": 75, "librari": [76, 81], "gradient": [76, 84], "nativ": [76, 78, 87, 93], "introduct": [76, 79, 82], "highlight": [76, 87, 91], "guid": [76, 79, 82, 92], "contributor": [76, 80], "part": [76, 79, 84, 87, 94], "tune": [76, 78, 82, 84, 92], "vllm": 76, "sglang": 76, "serial": [76, 79, 93, 94], "write": [76, 80, 92, 93, 94], "advanc": [76, 85, 87, 92, 95, 96], "export": 76, "x86": [76, 80], "intel": [76, 92, 95], "openvino": [76, 80], "5x": 78, "cluster": 78, "34": 78, "43x": 78, "2k": 78, "h200": 78, "latest": [78, 80], "offic": 78, "offici": 78, "popular": [78, 79], "flagship": 78, "common": [78, 79, 84], "form": [78, 79, 84], "quickli": [78, 87], "batteri": 78, "includ": [78, 79, 87, 92, 95, 96, 97], "commonli": [78, 84], "fork": 78, "build": [78, 79, 84, 87, 88, 93], "top": [78, 79, 87, 92, 93, 94, 95, 96], "re": [78, 81, 82, 87, 93, 94], "virtual": 78, "environ": [78, 82], "conda": 78, "venv": 78, "download": [78, 80, 82, 89, 91, 93, 94, 96], "job": 78, "root": [78, 82], "launch": 78, "ngpu": 78, "config_fil": 78, "train_config": 78, "llama3_8b": 78, "toml": 78, "run_train": 78, "sh": [78, 82], "hyperparamet": 78, "edit": [78, 82], "line": [78, 84], "flag": [78, 94], "termin": 78, "rank0": 78, "titan": 78, "2025": 78, "06": 78, "04": 78, "08": 78, "51": 78, "48": 78, "info": 78, "2254": 78, "27": 78, "34gib": 78, "28": 78, "78": 78, "tp": [78, 88], "375": 78, "tflop": 78, "21": 78, "73": [78, 85], "mfu": 78, "20": [78, 82, 94], "58": 78, "557": 78, "7069": 78, "99gib": 78, "62": 78, "034": 78, "35": [78, 82, 85], "41": [78, 82], "19": 78, "52": 78, "224": [78, 85, 92, 93, 94, 95, 96], "9196": 78, "022": 78, "406": [78, 93, 94], "65": 78, "904": 78, "1423": 78, "014": 78, "23": [78, 85], "As": [78, 79, 93, 97], "warmup": 78, "7k": 78, "99gb": 78, "peak": [78, 82], "against": 78, "02": 78, "37": 78, "404": 78, "2611": 78, "22gib": 78, "595": 78, "47": 78, "49": [78, 85], "027": 78, "4260": 78, "89gib": 78, "344": 78, "367": 78, "39": 78, "03": 78, "01": 78, "988": 78, "9482": 78, "321": 78, "366": 78, "14": 78, "991": 78, "1183": 78, "300": 78, "364": 78, "89": 78, "4659": 78, "291": 78, "84": 78, "769": 78, "gc": 78, "peform": 78, "period": 78, "collect": [78, 79, 84], "3k": 78, "89gb": 78, "11x": 78, "nearli": 78, "ident": [78, 84], "performan": 78, "vs": [78, 84, 93, 97], "curv": [78, 84], "omit": [78, 93, 94, 95], "648": 78, "2648": 78, "28gib": 78, "71": 78, "26": 78, "475": 78, "9106": 78, "91gib": 78, "53": [78, 82], "503": 78, "434": 78, "43": 78, "94": [78, 93], "166": 78, "0774": 78, "663": 78, "443": 78, "44": [78, 85], "87": 78, "50": [78, 84, 85, 92, 93, 95, 96], "885": 78, "3233": 78, "643": 78, "442": 78, "66": [78, 82, 85], "76": 78, "613": 78, "6150": 78, "637": 78, "72": [78, 82], "6k": 78, "91gb": 78, "21x": [78, 82], "tl": 78, "dr": 78, "priorit": 78, "accur": [78, 84, 92], "stabil": 78, "cost": [78, 85], "slightli": [78, 87], "outlier": 78, "underflow": 78, "8xh100": 78, "box": [78, 84, 95], "toi": [78, 80, 85, 87, 95], "convert_to_float8_train": 78, "recurs": 78, "kind": [78, 93], "gemm": [78, 95, 96], "snippet": [78, 93, 94], "f": [78, 79, 81, 82, 84, 85, 87, 88, 93, 94], "float8_linear_util": 78, "float8_linear": 78, "torch_version_at_least_2_5": [78, 80], "greater": 78, "sampl": [78, 79, 93, 95, 96], "adamw": 78, "elig": 78, "mod": [78, 84, 87], "divis": 78, "in_featur": [78, 80, 81, 85, 87], "out_featur": [78, 80, 85, 87], "_": [78, 85, 88, 92, 93, 94, 95], "label": 78, "purpos": [78, 79, 87, 93], "fake_label": 78, "ones_lik": 78, "mse_loss": 78, "model_state_dict": 78, "state_dict": [78, 81, 93, 94], "optimizer_state_dict": 78, "pth": [78, 93, 94], "explor": [78, 80, 96], "few": [78, 87, 93, 94], "lai": 79, "stack": [79, 82], "awq": 79, "gptq": 79, "codebookquantizedtensor": 79, "float3": 79, "overload": [79, 84], "term": [79, 84, 93, 97], "extra": [79, 82], "dev": 79, "discuss": [79, 87], "1833": 79, "matter": [79, 84], "float3_e2_m0": 79, "float4_e2_m1": 79, "float4_e3_m0": 79, "float5_e2_m2": 79, "float5_e3_m1": 79, "float6_e2_m3": 79, "float6_e3_m2": 79, "float8_e5m2": 79, "float8_e4m3fnuz": 79, "float8_e5m2fnuz": 79, "plan": [79, 94], "float4": 79, "float6": 79, "becom": [79, 93], "uint2": 79, "117208": 79, "outsid": 79, "mention": [79, 93], "criteria": 79, "wide": 79, "adopt": 79, "fundament": [79, 84, 94], "until": 79, "evid": 79, "hopefulli": 79, "amen": 79, "haven": 79, "enough": 79, "ont": 79, "revisit": 79, "intx": 79, "connect": [79, 97], "int4tensor": 79, "previou": [79, 82, 93, 94, 95, 96], "between": [79, 84, 87, 88, 92, 94, 95, 97], "preicison": 79, "mainli": [79, 92, 95, 97], "accommod": 79, "choose_qparams_affine_with_min_max": 79, "min": [79, 85, 87, 93, 97], "int_matmul": 79, "int_scaled_matmul": 79, "reli": [79, 80, 84, 85, 87], "On": [79, 80], "glue": 79, "everyth": 79, "togeth": [79, 82, 93, 95, 97], "construct": [79, 93, 97], "low_precision_v": 79, "high_precision_v": 79, "procedur": 79, "veri": [79, 84, 88, 94], "straightforward": [79, 97], "high_preicsion_v": 79, "especi": [79, 81, 84, 95, 96], "bitwidth": [79, 97], "codebook": 79, "hardcod": [79, 97], "select": [79, 93], "multi": 79, "dimension": [79, 84], "view": [79, 87, 93, 94], "mkldnn": 79, "coo": [79, 84], "sparse_coo": [79, 84], "sparsetensorimpl": 79, "idea": [79, 84], "nice": [79, 84], "concept": [79, 91, 93, 95, 96, 97], "why": [79, 87, 91], "c": [79, 80, 87, 95, 96], "conflict": 79, "quantized_linear": [79, 85, 93], "semant": 79, "stai": [79, 80, 87, 95], "develop": [79, 80, 93, 94, 97], "tradition": 79, "to_affine_quant": 79, "simplic": 79, "explain": [79, 92, 95], "simplest": [79, 84], "easi": [79, 82], "linear_modul": 79, "to_affine_quantized_intx": 79, "to_linear_activation_quant": 79, "quantized_weight": [79, 88], "activation_and_weight_quant": 79, "encount": 79, "input_qunat_func": 79, "redispatch": 79, "fx": [79, 93, 97], "symbolic_trac": 79, "But": [79, 87, 88, 97], "easier": [79, 97], "modif": 79, "figur": [79, 84, 93], "At": [79, 84, 93], "thing": [79, 81, 84, 87, 93], "address": [79, 93], "stat": [79, 94], "averag": [79, 85, 93, 94], "calculate_qparam": [79, 85, 97], "affinequantizedminmaxobserv": [79, 85], "insert_observer_": 79, "observedlinear": [79, 85], "complic": [79, 84, 93], "done": [79, 87], "manner": 79, "autoround": 79, "multitensor": 79, "sure": [79, 82, 97], "describ": [79, 81, 84, 91, 93, 94], "todai": [79, 82], "low_bit_optim": 79, "quantized_train": 79, "progress": [79, 88], "lot": [79, 84], "walk": [79, 85, 87, 91, 92, 95], "int4weightonlyconfig": [79, 80, 81, 88], "_convert_weight_to_int4pack": 79, "tensorcoretiledaqttensorimpl": 79, "_quantized_linear_op": 79, "goe": 79, "_aqt_qlinear_dispatch_t": 79, "dispatch": 79, "explan": 79, "wint4": 79, "stabl": 80, "pip": [80, 82, 92, 93], "nightli": [80, 82], "index": [80, 82, 84, 96], "url": [80, 82, 96], "whl": [80, 82, 96], "cu121": 80, "major": 80, "entri": 80, "mutat": 80, "logic": [80, 87, 88], "toylinearmodel": [80, 81, 85], "linear2": [80, 81, 85, 87], "eval": [80, 81, 82, 85, 92, 94, 95, 96], "faster": [80, 84], "model_bf16": 80, "mix": [80, 82, 92, 95, 96], "tensor_impl_dtyp": 80, "roughli": [80, 84], "quarter": 80, "os": [80, 93, 94], "int4_model": 80, "pt": [80, 82], "bfloat16_model": 80, "int4_model_size_mb": 80, "getsiz": [80, 93, 94], "bfloat16_model_size_mb": 80, "2f": [80, 93, 94], "mb": [80, 81, 83, 90, 93, 94], "00": [80, 83, 90], "benchmark_model": 80, "temporari": 80, "workaround": [80, 88], "num_run": 80, "100": [80, 87, 93, 94], "_dynamo": [80, 87], "reset": [80, 93, 94], "bf16_time": 80, "int4_tim": 80, "time": [80, 84, 87, 91, 92, 93, 94], "3f": [80, 94], "ms": 80, "1fx": 80, "393": 80, "410": 80, "9x": 80, "recogn": [80, 97], "decis": 80, "pt2e": [80, 92, 93, 94, 95, 96], "fuse": [80, 84, 87, 94], "deleg": [80, 93], "x86inductorquant": [80, 95, 96], "quantize_pt2": [80, 92, 93, 94, 95, 96], "prepare_pt2": [80, 92, 93, 95, 96], "x86_inductor_quant": [80, 95], "get_default_x86_inductor_quantization_config": [80, 95], "float_model": [80, 87, 92, 93, 94, 95, 96], "data_load": [80, 93, 94, 95, 96], "no_grad": [80, 87, 92, 93, 94, 95, 96], "imag": [80, 92, 93, 94, 95, 96], "program": [80, 93, 94, 95, 97], "captur": [80, 93, 94, 97], "expos": [80, 93, 94], "set_glob": [80, 93, 94, 95, 96], "xiq": [80, 95], "prepare_qat_pt2": [80, 94, 95], "sample_inference_data": 80, "convert_pt2": [80, 92, 93, 94, 95, 96], "wrapper": [80, 87, 95], "_inductor": [80, 95], "cpp_wrapper": [80, 95], "optimized_model": [80, 92, 95, 96], "converted_model": [80, 95, 96], "xpu": [80, 96], "simpl": [80, 84, 85, 87, 92, 95, 96], "visit": 80, "would": [80, 84, 87, 94, 96], "forget": 80, "tempfil": 81, "get_model_size_in_byt": 81, "ref": [81, 93], "namedtemporaryfil": 81, "seek": [81, 84], "load": [81, 82, 88], "m_load": 81, "load_state_dict": [81, 93, 94], "assign": 81, "assert": [81, 85, 87, 88, 97], "equal": [81, 84], "float_weight1": 81, "float_weight2": 81, "quantized_weight1": 81, "quantized_weight2": 81, "go": [81, 87, 91, 97], "techinqu": 81, "reduct": [81, 82, 84, 87], "4x": [81, 82], "0625": 81, "reason": [81, 84], "avoid": [81, 84], "deploi": 82, "underli": [82, 87], "engin": 82, "seamlessli": [82, 87, 95, 96], "seamless": [82, 95], "git": 82, "cu126": 82, "acceler": [82, 84], "float8dynamicactivationfloat8weightconfig": 82, "phi": 82, "autotoken": 82, "model_id": 82, "microsoft": 82, "quant_config": 82, "quant_typ": [82, 88], "quantized_model": [82, 87, 92, 93, 94], "device_map": [82, 88], "auto": [82, 88], "torch_dtyp": [82, 88], "push": [82, 84, 88], "hub": [82, 88], "user_id": 82, "your_user_id": 82, "model_nam": [82, 92, 95, 96], "save_to": 82, "safe_seri": [82, 88], "hf": 82, "signific": [82, 84], "wheel": 82, "ai": 82, "hug": 82, "face": [82, 84, 93], "server": [82, 88], "o3": 82, "client": 82, "curl": 82, "localhost": 82, "8000": 82, "v1": 82, "chat": 82, "h": 82, "content": 82, "applic": 82, "messag": 82, "role": 82, "give": [82, 84, 87], "me": 82, "short": 82, "languag": 82, "temperatur": 82, "top_p": 82, "95": 82, "top_k": 82, "max_token": 82, "32768": 82, "vram": 82, "15x": 82, "2x": [82, 84], "littl": [82, 88], "packag": 82, "pipelin": 82, "random": [82, 84, 93, 94], "manual_se": [82, 93, 94], "model_path": 82, "trust_remote_cod": 82, "assist": 82, "eat": 82, "banana": 82, "dragonfruit": 82, "smoothi": 82, "blend": 82, "milk": 82, "honei": 82, "salad": 82, "slice": [82, 88], "lemon": 82, "juic": 82, "solv": [82, 84, 87], "equat": 82, "pipe": 82, "text": 82, "generation_arg": 82, "max_new_token": 82, "500": 82, "return_full_text": 82, "do_sampl": 82, "generated_text": 82, "finetun": 82, "lm_head": 82, "those": [82, 84, 85, 87], "ti": 82, "autoprocessor": 82, "modeling_util": 82, "find_tied_paramet": 82, "untied_model": 82, "getattr": [82, 88], "get_text_config": 82, "tie_word_embed": 82, "setattr": [82, 87], "_tied_weights_kei": 82, "clone": [82, 88], "save_to_local_path": 82, "int8dynamicactivationintxweightconfig": 82, "ve": [82, 84], "intxweightonlyconfig": 82, "modulefqntoconfig": [82, 88], "untied_model_id": 82, "untied_model_local_path": 82, "embedding_config": 82, "linear_config": 82, "weight_granular": 82, "weight_scale_dtyp": 82, "_default": [82, 88], "embed_token": 82, "include_embed": 82, "untie_embedding_weight": 82, "modules_to_not_convert": 82, "pte": 82, "cd": 82, "install_requir": 82, "phi_4_mini": 82, "convert_weight": 82, "pytorch_model": 82, "bin": 82, "pytorch_model_convert": 82, "export_llama": 82, "kv": 82, "use_sdpa_with_kv_cach": 82, "get_bos_id": 82, "199999": 82, "get_eos_id": 82, "200020": 82, "max_seq_length": 82, "max_context_length": 82, "output_nam": 82, "phi4": 82, "phone": 82, "io": 82, "2gb": 82, "iphon": 82, "pro": [82, 84], "17": 82, "sec": 82, "maintain": [82, 84], "test": [82, 91, 93, 95], "lm": 82, "har": 82, "eleutherai": 82, "lm_eval": 82, "model_arg": 82, "pretrain": [82, 84, 92, 93, 94, 95], "reset_peak_memory_stat": 82, "prompt": 82, "hei": 82, "consciou": 82, "templated_prompt": 82, "apply_chat_templ": 82, "add_generation_prompt": 82, "templat": [82, 83, 89, 90], "return_tensor": 82, "generated_id": 82, "output_text": 82, "batch_decod": 82, "skip_special_token": 82, "clean_up_tokenization_spac": 82, "respons": 82, "len": [82, 88, 93, 94, 97], "mem": [82, 83, 90], "max_memory_reserv": 82, "1e9": 82, "02f": 82, "gb": 82, "hello": 82, "ye": 82, "am": 82, "digit": 82, "70": [82, 85], "91": 82, "benchmark_lat": 82, "vllm_disable_compile_cach": 82, "project": 82, "vllm_use_precompil": 82, "sharegpt": 82, "wget": 82, "anon8231489123": 82, "sharegpt_vicuna_unfilt": 82, "resolv": 82, "sharegpt_v3_unfiltered_cleaned_split": 82, "tree": 82, "num": 82, "benchmark_serv": 82, "16x": 82, "1s": 82, "14x": 82, "num_prompt": 82, "req": 82, "57": [82, 85], "1000": [82, 95], "68": 82, "80": 82, "entir": [82, 93, 94], "ml": 82, "gain": [82, 84, 96], "eas": 82, "alwai": [82, 87], "valid": [82, 88, 97], "trade": [82, 84], "off": [82, 84], "003": [83, 90, 91], "total": [83, 90, 91], "galleri": [83, 89, 91], "tutorials_sourc": 83, "template_tutori": [83, 90, 91], "neural": [84, 92, 95], "network": [84, 87, 92, 95], "latenc": 84, "carefulli": 84, "pai": 84, "low": [84, 87, 92], "price": 84, "f1": 84, "problem": [84, 87], "research": [84, 91], "fragment": 84, "rightfulli": 84, "spent": 84, "compress": [84, 92], "place": [84, 92, 93, 94, 95, 96], "dens": 84, "focu": [84, 87], "realli": 84, "concret": [84, 97], "hope": 84, "modular": 84, "scratch": [84, 91], "minim": [84, 92, 95, 96], "algorthim": 84, "realiz": 84, "theoret": 84, "analog": 84, "fix": [84, 85], "unstructur": 84, "One": [84, 87, 88, 97], "close": 84, "relat": 84, "retrain": 84, "neglig": 84, "area": 84, "agre": 84, "upon": 84, "consensu": 84, "mind": 84, "thought": 84, "subproblem": 84, "satisfi": 84, "independ": 84, "frontend": [84, 95], "arbitrari": 84, "handoff": 84, "piec": 84, "natur": [84, 87, 93, 97], "present": 84, "clear": 84, "contract": 84, "7x": 84, "advantag": 84, "anticip": 84, "mani": [84, 87], "solut": 84, "third": 84, "parti": 84, "to_sparse_semi_structur": 84, "sparsesemistructuredtensor": 84, "weightnormsparsifi": 84, "half": 84, "subnetwork": 84, "sparse_config": 84, "named_modul": 84, "tensor_fqn": 84, "sparse_block_shap": 84, "zeros_per_block": 84, "fakespars": 84, "manipul": 84, "dictionari": 84, "paramer": 84, "parameter": 84, "necessari": [84, 85, 87, 92, 93, 94, 95, 96], "suitabl": [84, 95], "0s": 84, "spot": 84, "definit": [84, 88], "academia": 84, "industri": 84, "often": [84, 87], "interchang": 84, "confus": [84, 93], "distinct": 84, "behind": 84, "doesn": [84, 94, 97], "itself": [84, 87], "loos": 84, "speak": 84, "tightli": 84, "coupl": [84, 87], "csc": 84, "fbgemm": 84, "qnnpack": 84, "descript": [84, 92], "coordin": 84, "vector": [84, 95], "locat": 84, "bsr": 84, "sparse_bsr": 84, "except": [84, 87, 97], "scalar": [84, 93], "csr": 84, "sparse_csr": 84, "sparse_csc": 84, "column": 84, "compact": 84, "sparse_matrix": 84, "1d": 84, "indexptr": 84, "\u00bd": 84, "bitmask": 84, "2bit": 84, "unprun": 84, "quit": [84, 87], "broken": 84, "down": 84, "sensit": 84, "effect": [84, 85, 87, 95, 96, 97], "best": [84, 95], "subsequ": [84, 87, 95, 96], "infinit": 84, "lost": 84, "degre": 84, "drop": 84, "proxi": 84, "aforement": 84, "smallest": 84, "absolut": 84, "global": [84, 87], "scope": 84, "impli": 84, "con": 84, "span": 84, "threshold": 84, "complex": 84, "constant": [84, 87, 93], "ctr_mobile_fe": 84, "score": 84, "w": [84, 88], "tenosr": 84, "udpat": 84, "cannot": [84, 85, 88], "histori": 84, "regrow": 84, "dw": 84, "via": [84, 92], "backprop": 84, "pat": 84, "unmask": 84, "resid": 84, "salienc": 84, "lowest": 84, "l1": 84, "abl": [84, 87, 88, 93, 97], "repeat": [84, 93, 94], "movement": 84, "2005": 84, "07683": 84, "rank": [84, 87], "wx": 84, "sqx": 84, "q": [84, 93], "usual": 84, "sort": 84, "wise": 84, "reconstruct": [84, 88], "randomli": 84, "tri": 84, "remedi": 84, "item": [84, 91], "ultim": [84, 85], "literatur": 84, "vision": 84, "nlp": [84, 91, 95], "iter": [84, 93, 94], "ctr_feed": 84, "na": 84, "multimask": 84, "pyspeech": 84, "fastna": 84, "approach": [84, 87, 92, 95, 96], "knowledg": [84, 91], "distil": 84, "pdf": 84, "2204": 84, "09656": 84, "arrang": 84, "recal": 84, "counterpart": 84, "slower": 84, "suffici": 84, "flexibl": [84, 87, 92, 95], "98": 84, "special": [84, 92, 93], "exhibit": 84, "penalti": 84, "expens": [84, 87], "dictat": 84, "characterist": 84, "highest": 84, "wouldn": [84, 87], "visual": 84, "fig": 84, "4x4": 84, "benchmak": 84, "fly": 85, "welcom": 85, "histogram": [85, 93], "act_ob": 85, "finfo": 85, "weight_ob": 85, "observed_input": 85, "observed_weight": 85, "cl": [85, 87, 88], "float_linear": 85, "observed_linear": 85, "_replace_with_custom_fn_if_matches_filt": 85, "insert_observers_": 85, "_is_linear": 85, "lambda": [85, 88], "replacement_fn": 85, "copied_act_ob": 85, "copied_weight_ob": 85, "popul": 85, "feed": 85, "simpler": [85, 93], "quantizedlinear": [85, 87], "isn": 85, "strictli": 85, "to_affine_quantized_intx_stat": 85, "act_scal": [85, 97], "act_zero_point": 85, "weight_scal": [85, 93, 97], "weight_zero_point": [85, 93], "qweight": 85, "qinput": 85, "from_observ": 85, "begin": [85, 87], "dataclass": [85, 88, 97], "transform_modul": [85, 88], "register_quantize_module_handl": [85, 88], "staticquantconfig": 85, "_apply_static_qu": 85, "is_observed_linear": 85, "optimizedmodul": 85, "_orig_mod": 85, "0237": 85, "plainaqttensorimpl": 85, "142": 85, "31": [85, 97], "113": 85, "157": 85, "59": 85, "160": 85, "150": 85, "67": 85, "241": 85, "238": 85, "235": 85, "228": 85, "255": [85, 97], "201": 85, "114": 85, "236": 85, "88": [85, 93], "83": 85, "109": 85, "209": 85, "92": 85, "184": 85, "141": 85, "110": 85, "0009": 85, "0010": 85, "130": 85, "122": 85, "132": 85, "125": 85, "126": 85, "129": 85, "127": [85, 87, 96, 97], "133": 85, "124": 85, "131": 85, "135": 85, "136": 85, "foundat": 87, "autograd": [87, 97], "interpos": 87, "namespac": 87, "continu": [87, 94, 95, 96, 97], "obviou": 87, "int8quantizedlinear": 87, "finer": 87, "intercept": 87, "contrast": 87, "long": [87, 93], "clunki": 87, "distributedlinear": 87, "duplic": 87, "bypass": 87, "outer": 87, "inner": 87, "allgath": 87, "bandwidth": 87, "read": 87, "zoo": 87, "podcast": 87, "edward": 87, "yang": 87, "int8_symmetric_quant": 87, "fp32_tensor": 87, "amin": 87, "keepdim": [87, 93, 94], "amax": 87, "zeros_lik": 87, "clamp": [87, 93], "w_int8": 87, "new_linear": 87, "left": [87, 97], "toymodel": 87, "child": 87, "named_children": 87, "drawback": 87, "won": 87, "suppos": 87, "clean": 87, "eleg": 87, "pretti": 87, "power": [87, 88], "overrid": 87, "almost": 87, "shard": [87, 88], "ragged": 87, "rag": 87, "nestedtensor": 87, "who": 87, "link": [87, 91], "googl": 87, "collab": 87, "flopcount": 87, "memorytrack": 87, "With": [87, 93, 95, 97], "bare": 87, "bone": 87, "int8symmetrictensor": 87, "hold": 87, "staticmethod": 87, "disabl": [87, 94], "__new__": [87, 88], "_make_wrapper_subclass": [87, 88], "storage_offset": 87, "ndim": 87, "__tensor_flatten__": [87, 88], "pt2": [87, 95], "__tensor_unflatten__": [87, 88], "tensor_data_dict": [87, 88], "extra_metadata": 87, "outer_s": [87, 88], "outer_strid": [87, 88], "undo": 87, "__repr__": 87, "repr": 87, "ahead": 87, "insid": 87, "int8_tensor": 87, "func": [87, 88], "op_implementations_dict": 87, "conveni": 87, "register_op": 87, "_op": 87, "opoverload": 87, "impl_decor": 87, "op_impl": 87, "particular": 87, "largest": 87, "tell": 87, "desugar": 87, "decor": [87, 88], "surfac": 87, "coverag": [87, 92, 93, 95, 96], "brute": 87, "forc": 87, "repeatedli": 87, "loggingtensor": 87, "_python_dispatch": [87, 88], "return_and_correct_alias": [87, 88], "int8_mm": 87, "detach": [87, 88], "int8_view_op": 87, "out_data": 87, "out_scal": [87, 93], "notic": 87, "hit": 87, "background": 87, "decomposit": 87, "live": 87, "decomp": 87, "shrink": 87, "author": [87, 91, 92, 93, 94, 95, 96, 97], "pain": 87, "rather": 87, "worth": 87, "written": 87, "differenti": 87, "nuanc": 87, "longer": [87, 93, 94], "That": 87, "transposit": 87, "got": [87, 93, 97], "propag": [87, 93, 95, 96], "fact": 87, "themselv": [87, 93], "pointwis": [87, 95, 96], "were": 87, "might": [87, 88, 93, 97], "unwrap": 87, "dim0": 87, "dim1": 87, "confirm": 87, "quantized_model_module_swap": 87, "quantized_model_subclass": 87, "subclass_param": 87, "out_module_swap": 87, "allclos": 87, "out_compil": 87, "seri": 87, "e2": 88, "_type": 88, "_data": 88, "capabl": [88, 93, 95], "self_attn": 88, "q_proj": 88, "k_proj": 88, "mlp": 88, "gate_proj": 88, "usernam": 88, "narrow": 88, "copy_": 88, "state": 88, "chunk": 88, "_apply_fn_to_data": 88, "heavi": 88, "codebas": 88, "fn": 88, "ctx": 88, "new_tensor": 88, "__class__": 88, "principl": 88, "torchaobasetensor": 88, "mynewquantconfig": 88, "classvar": 88, "myquantizedtensor": 88, "fbgemmfp8tensor": 88, "tensor_data_attr": 88, "tensor_attribut": 88, "attr": 88, "_to_copi": 88, "fill_default": 88, "notimplementederror": 88, "_my_quant_transform": 88, "my_quantization_funct": 88, "use_cutlass_kernel": 88, "my_cutlass_linear": 88, "use_triton_kernel": 88, "my_triton_linear": 88, "disappear": 88, "extrem": 88, "sole": 88, "think": 88, "world": 88, "explicitli": [88, 97], "spooki": 88, "distanc": 88, "statu": 88, "due": [88, 92, 97], "team": 88, "2338": 88, "creation": 88, "detect": 88, "illustr": 88, "tutorials_python": 89, "zip": [89, 91], "jupyt": [89, 91], "notebook": [89, 91], "tutorials_jupyt": 89, "sphinx": [89, 91], "firstnam": 91, "lastnam": 91, "prerequisit": [91, 93], "v2": 91, "topic": 91, "rand": [91, 93, 94], "9458": 91, "0836": 91, "5143": 91, "8223": 91, "2964": 91, "5763": 91, "5938": 91, "7810": 91, "9763": 91, "8172": 91, "2258": 91, "5191": 91, "0389": 91, "4166": 91, "2947": 91, "practic": 91, "summar": 91, "takeawai": 91, "link1": 91, "link2": 91, "minut": 91, "ipynb": 91, "daniil": 92, "lyakhov": 92, "aamir": 92, "nazir": 92, "alexand": 92, "suslov": 92, "yamini": 92, "nimmagadda": 92, "kozlov": 92, "subject": [92, 94], "openvinoquant": 92, "unlock": 92, "placement": 92, "simplifi": [92, 93, 95, 96], "ux": [92, 93, 95], "torchdynamo": [92, 95, 96, 97], "eager": [92, 93, 94, 95, 96, 97], "mechan": [92, 95, 96], "torchvis": [92, 93, 94, 95, 96, 97], "resnet18": [92, 93, 94, 95, 96], "u": 92, "__dict__": [92, 93, 94, 95, 96], "dummi": [92, 95, 96], "traced_b": [92, 95, 96], "disable_patch": 92, "exported_model": [92, 93, 94, 95, 96], "preset": 92, "elu": 92, "prelu": 92, "gelu": 92, "quantizationpreset": 92, "bert": [92, 95], "modeltyp": 92, "ignored_scop": 92, "exclud": 92, "layer_1": 92, "layer_2": 92, "layer_3": 92, "ignoredscop": 92, "conv2d": [92, 93, 94, 95, 96, 97], "regex": 92, "layer_": 92, "subgraph": [92, 94], "node": [92, 94, 95, 96, 97], "target_devic": 92, "taken": 92, "account": 92, "cpu_spr": 92, "npu": 92, "targetdevic": 92, "fold": [92, 93, 95, 96], "batchnorm": [92, 93, 94, 95, 96], "preced": [92, 93, 95, 96], "prepared_model": [92, 93, 94, 95, 96], "fold_quant": 92, "finish": [92, 95], "comparison": 92, "biascorrect": 92, "discrep": 92, "calibration_load": 92, "dataload": [92, 93, 94], "transform_fn": 92, "data_item": 92, "calibration_dataset": 92, "smooth_quant": 92, "fast_bias_correct": 92, "deploy": [92, 95], "jerri": [93, 95, 97], "zhang": [93, 95, 96, 97], "_export": [93, 94, 95], "14k": 93, "programm": [93, 95, 96], "db": 93, "xnnpack": [93, 94, 97], "xnnpack_quant": [93, 94], "get_symmetric_quantization_config": [93, 94], "xnnpackquant": [93, 94, 97], "prior": 93, "qconfigmap": [93, 97], "backendconfig": [93, 97], "rel": 93, "intent": [93, 97], "qconfig": [93, 97], "3d": [93, 97], "incompat": 93, "great": 93, "ideal": 93, "fake_qu": 93, "hidden": 93, "summari": 93, "thu": 93, "queri": [93, 97], "previous": 93, "embedding_byt": 93, "executorchquant": 93, "concaten": 93, "prone": 93, "cleaner": 93, "composed_quant": 93, "quantization_cap": 93, "concern": 93, "decoupl": 93, "minmax": 93, "freed": 93, "identitc": 93, "imagenet": [93, 94], "unzip": [93, 94], "data_path": [93, 94], "resnet18_pretrained_float": [93, 94], "sy": [93, 94], "numpi": [93, 94], "np": [93, 94], "resnet": [93, 94, 95], "warn": [93, 94], "filterwarn": [93, 94], "categori": [93, 94], "deprecationwarn": [93, 94], "r": [93, 94], "seed": [93, 94], "191009": [93, 94], "averagemet": [93, 94], "fmt": [93, 94], "val": [93, 94], "avg": [93, 94], "count": [93, 94], "__str__": [93, 94], "fmtstr": [93, 94], "topk": [93, 94], "predict": [93, 94], "maxk": [93, 94], "pred": [93, 94], "eq": [93, 94], "expand_a": [93, 94], "correct_k": [93, 94], "reshap": [93, 94], "mul_": [93, 94], "criterion": [93, 94], "top1": [93, 94], "top5": [93, 94], "cnt": [93, 94], "acc1": [93, 94], "acc5": [93, 94], "load_model": [93, 94], "model_fil": [93, 94], "weights_onli": [93, 94], "print_size_of_model": [93, 94], "temp": [93, 94], "p": [93, 94], "1e6": [93, 94], "prepare_data_load": [93, 94], "485": [93, 94], "456": [93, 94], "std": [93, 94], "229": [93, 94], "225": [93, 94], "randomresizedcrop": [93, 94], "randomhorizontalflip": [93, 94], "totensor": [93, 94], "dataset_test": [93, 94], "resiz": [93, 94], "centercrop": [93, 94], "train_sampl": [93, 94], "randomsampl": [93, 94], "test_sampl": [93, 94], "sequentialsampl": [93, 94], "train_batch_s": [93, 94], "sampler": [93, 94], "data_loader_test": [93, 94, 95, 96], "eval_batch_s": [93, 94], "saved_model_dir": [93, 94], "float_model_fil": [93, 94], "model_to_quant": [93, 94], "capture_pre_autograd_graph": [93, 94, 95], "dynamic_shap": [93, 94], "export_for_train": 93, "dynamic_dim": [93, 94], "constraint": [93, 94, 97], "qconfig_opt": 93, "set_object_typ": 93, "set_module_nam": 93, "workload": 93, "themodel": 93, "feedback": 93, "dq": 93, "fp32_op": 93, "qauntiz": 93, "x_int8": 93, "x_scale": 93, "x_zero_point": 93, "weight_int8": 93, "bias_fp32": 93, "output_scal": 93, "output_zero_point": 93, "x_fp32": 93, "quantized_decompos": 93, "dequantize_per_tensor": 93, "x_i8": 93, "x_quant_min": 93, "x_quant_max": 93, "weight_fp32": 93, "weight_i8": 93, "weight_quant_min": 93, "weight_quant_max": 93, "weight_permut": 93, "permute_copi": 93, "out_fp32": 93, "addmm": 93, "out_i8": 93, "quantize_per_tensor": 93, "out_zero_point": 93, "out_quant_min": 93, "out_quant_max": 93, "float32_op": 93, "decompos": 93, "use_reference_represent": 93, "x_int16": 93, "weight_int16": 93, "acc_int32": 93, "out_dtyp": 93, "bias_scal": 93, "bias_int32": 93, "div": 93, "mul": 93, "out_int8": 93, "qmin": 93, "qmax": 93, "date": 93, "unus": 93, "serila": 93, "consult": 93, "exportedprogram": 93, "pt2e_quantized_model_file_path": 93, "resnet18_pt2e_quant": 93, "quantized_ep": 93, "loaded_quantized_ep": 93, "loaded_quantized_model": 93, "diff": 93, "79": 93, "82": 93, "55": 93, "edg": [93, 97], "went": 93, "andrew": 94, "Or": 94, "ptq": [94, 95], "move_exported_model_to_ev": [94, 95], "correctli": 94, "certain": 94, "dropout": 94, "move_exported_model_to_train": 94, "jit": 94, "recursivescriptmodul": 94, "train_one_epoch": 94, "ntrain_batch": 94, "avgloss": 94, "5f": 94, "start_tim": 94, "global_avg": 94, "is_qat": [94, 95], "fusion": 94, "batchnorm2d": 94, "_native_batch_norm_legit": 94, "cudnn_batch_norm": 94, "mobilenetv2": 94, "recompil": 94, "consolid": 94, "epoch": 94, "far": 94, "num_epoch": 94, "num_train_batch": 94, "num_eval_batch": 94, "num_observer_update_epoch": 94, "num_batch_norm_update_epoch": 94, "num_epochs_between_ev": 94, "nepoch": 94, "subseq": 94, "disable_observ": 94, "bn": 94, "running_mean": 94, "running_var": 94, "new_arg": 94, "wish": 94, "prepared_model_copi": 94, "neval_batch": 94, "paus": 94, "resum": 94, "fail": [94, 97], "checkpoint_path": 94, "checkpoint_": 94, "behav": 94, "incorrectli": 94, "lesli": [95, 97], "fang": [95, 97], "weiwen": [95, 97], "xia": [95, 97], "jiong": [95, 97], "gong": [95, 97], "cnn": 95, "rnn": 95, "outstand": 95, "fourth": 95, "spr": 95, "xeon": 95, "processor": 95, "boost": 95, "contigu": [95, 96], "channels_last": [95, 96], "onednn": [95, 96], "assum": [95, 97], "word": 95, "satur": 95, "pure": 95, "dedic": 95, "scenario": [95, 96], "plai": [95, 96], "convolut": [95, 96, 97], "absenc": [95, 96], "enhanc": [95, 96], "mirror": [95, 96], "autocast": [95, 96], "context": [95, 96], "device_typ": [95, 96], "turn": [95, 96], "cpp": 95, "qconvolut": [95, 96], "qlinear": [95, 96], "presenc": [95, 96], "pair": [95, 96], "remain": [95, 96], "conting": [95, 96], "qmaxpool2d": [95, 96], "torchinductor_freez": [95, 96], "example_x86inductorquantizer_pytorch_2_1": 95, "torchbench": 95, "measur": 95, "proven": 95, "depth": 95, "shoud": 95, "example_x86inductorquantizer_qat": 95, "yan": 96, "zhiwei": 96, "wang": 96, "eikan": 96, "liangang": 96, "liu": 96, "river": 96, "cui": 96, "yifeng": 96, "xpuinductorquant": 96, "pip3": 96, "torchaudio": 96, "xpu_inductor_quantizer_exampl": 96, "xpu_inductor_quant": 96, "xpuiq": 96, "resnet18_weight": 96, "get_default_xpu_inductor_quantization_config": 96, "sign": 96, "wherea": 96, "histogramobserv": [96, 97], "perchannelminmaxobserv": 96, "quantizationspec": [96, 97], "quantizationconfig": [96, 97], "type_check": 96, "observerorfakequantizeconstructor": 96, "get_xpu_inductor_symm_quantization_config": 96, "extra_arg": 96, "act_observer_or_fake_quant_ctr": 96, "act_quantization_spec": [96, 97], "qscheme": [96, 97], "per_tensor_symmetr": [96, 97], "observer_or_fake_quant_ctr": [96, 97], "with_arg": [96, 97], "weight_observer_or_fake_quant_ctr": 96, "weight_quantization_spec": [96, 97], "per_channel_symmetr": 96, "ch_axi": 96, "oc": 96, "ic": 96, "kh": 96, "kw": 96, "conv": [96, 97], "bias_quantization_spec": 96, "amp": 96, "indcutor": 96, "kimish": 97, "patel": 97, "made": 97, "explicit": 97, "quantiat": 97, "encod": 97, "convei": 97, "quantizationannot": 97, "furthermor": 97, "minmaxobserv": 97, "input_qspec_map": 97, "output_qspec": 97, "_annot": 97, "conclud": 97, "matcher": 97, "get_source_partit": 97, "add_partit": 97, "gm": 97, "itertool": 97, "chain": 97, "add_nod": 97, "output_nod": 97, "per_tensor_affin": 97, "input_act_qspec": 97, "output_act_qspec": 97, "input_act0": 97, "input_act1": 97, "quantization_annot": 97, "phase": 97, "substitut": 97, "among": 97, "sharedquantizationspec": 97, "maxpool": 97, "average_pool": 97, "concat": 97, "edgeornod": 97, "transit": 97, "spec": 97, "conv1": 97, "conv2": 97, "fed": 97, "cat": 97, "conv1_out": 97, "conv2_out": 97, "qspec1": 97, "cat_input0": 97, "cat_input1": 97, "implicitli": 97, "therefor": 97, "ob": 97, "consum": 97, "rewrit": 97, "share_qparams_with_input_act0_qspec": 97, "known": 97, "beforehand": 97, "sigmoid": 97, "fixedqparamsquantizationspec": 97, "act_qspec": 97, "sigmoid_nod": 97, "input_act": 97, "derivedquantizationspec": 97, "derive_qparams_fn": 97, "observerorfakequant": 97, "observerbas": 97, "fakequantizebas": 97, "heurist": 97, "obejct": 97, "obs_or_fq": 97, "fq": 97, "act_obs_or_fq": 97, "weight_obs_or_fq": 97, "act_zp": 97, "weight_zp": 97, "bias_qspec": 97, "derived_from": 97, "backendquant": 97, "get_input_act_qspec": 97, "get_output_act_qspec": 97, "get_weight_qspec": 97, "get_bias_qspec": 97, "intermedi": 97, "call_funct": 97, "relu_": 97, "relu_nod": 97, "maybe_conv_nod": 97, "conv1d": 97, "unexpect": 97, "recognz": 97, "subgraphmatch": 97, "conv_relu_pattern": 97, "name_node_map": 97, "input_nod": 97, "weight_nod": 97, "bias_nod": 97, "caveat": 97, "exhaust": 97, "2d": 97, "4d": 97, "symbol": 97, "outcom": 97}, "objects": {"torchao.dtypes": [[11, 0, 1, "", "AffineQuantizedTensor"], [12, 0, 1, "", "BlockSparseLayout"], [13, 0, 1, "", "CutlassInt4PackedLayout"], [14, 0, 1, "", "CutlassSemiSparseLayout"], [15, 0, 1, "", "Float8Layout"], [16, 0, 1, "", "Int4CPULayout"], [17, 0, 1, "", "Layout"], [18, 0, 1, "", "MarlinQQQLayout"], [19, 0, 1, "", "MarlinQQQTensor"], [20, 0, 1, "", "MarlinSparseLayout"], [21, 0, 1, "", "NF4Tensor"], [22, 0, 1, "", "PlainLayout"], [23, 0, 1, "", "SemiSparseLayout"], [24, 0, 1, "", "TensorCoreTiledLayout"], [25, 0, 1, "", "UintxLayout"], [26, 2, 1, "", "to_affine_quantized_floatx"], [27, 2, 1, "", "to_affine_quantized_floatx_static"], [28, 2, 1, "", "to_affine_quantized_fpx"], [29, 2, 1, "", "to_affine_quantized_intx"], [30, 2, 1, "", "to_affine_quantized_intx_static"], [31, 2, 1, "", "to_marlinqqq_quantized_intx"], [32, 2, 1, "", "to_nf4"]], "torchao.dtypes.AffineQuantizedTensor": [[11, 1, 1, "", "dequantize"], [11, 1, 1, "", "from_hp_to_floatx"], [11, 1, 1, "", "from_hp_to_floatx_static"], [11, 1, 1, "", "from_hp_to_fpx"], [11, 1, 1, "", "from_hp_to_intx"], [11, 1, 1, "", "from_hp_to_intx_static"], [11, 1, 1, "", "to"]], "torchao.dtypes.MarlinQQQTensor": [[19, 1, 1, "", "dequantize"], [19, 1, 1, "", "from_hp_to_intx"]], "torchao.dtypes.MarlinSparseLayout": [[20, 1, 1, "", "pre_process"]], "torchao.dtypes.NF4Tensor": [[21, 1, 1, "", "convert_to_norm_float_weight"], [21, 1, 1, "", "dequantize"], [21, 1, 1, "", "dequantize_scalers"], [21, 1, 1, "", "double_quantize_scalers"], [21, 1, 1, "", "get_original_weight"], [21, 1, 1, "", "quantize_tensor_nearest"]], "torchao.float8": [[33, 0, 1, "", "CastConfig"], [34, 0, 1, "", "Float8LinearConfig"], [35, 0, 1, "", "ScalingGranularity"], [36, 0, 1, "", "ScalingType"], [37, 2, 1, "", "convert_to_float8_training"], [38, 2, 1, "", "precompute_float8_dynamic_scale_for_fsdp"]], "torchao.float8.Float8LinearConfig": [[34, 1, 1, "", "from_recipe_name"]], "torchao.quantization": [[39, 0, 1, "", "FPXWeightOnlyConfig"], [40, 0, 1, "", "Float8DynamicActivationFloat8WeightConfig"], [41, 0, 1, "", "Float8StaticActivationFloat8WeightConfig"], [42, 0, 1, "", "Float8WeightOnlyConfig"], [43, 0, 1, "", "GemliteUIntXWeightOnlyConfig"], [44, 0, 1, "", "Int4WeightOnlyConfig"], [45, 0, 1, "", "Int8DynamicActivationInt4WeightConfig"], [46, 0, 1, "", "Int8DynamicActivationInt8WeightConfig"], [47, 0, 1, "", "Int8WeightOnlyConfig"], [48, 0, 1, "", "MappingType"], [49, 0, 1, "", "TorchAODType"], [50, 0, 1, "", "UIntXWeightOnlyConfig"], [51, 0, 1, "", "ZeroPointDomain"], [52, 2, 1, "", "autoquant"], [53, 2, 1, "", "choose_qparams_affine"], [54, 2, 1, "", "choose_qparams_affine_with_min_max"], [55, 2, 1, "", "dequantize_affine"], [56, 2, 1, "", "int_scaled_matmul"], [65, 2, 1, "", "quantize_"], [66, 2, 1, "", "quantize_affine"], [67, 2, 1, "", "safe_int_mm"], [68, 2, 1, "", "smooth_fq_linear_to_inference"], [69, 2, 1, "", "swap_linear_with_smooth_fq_linear"], [70, 2, 1, "", "to_linear_activation_quantized"]], "torchao.quantization.qat": [[57, 0, 1, "", "ComposableQATQuantizer"], [58, 0, 1, "", "FakeQuantizeConfig"], [59, 0, 1, "", "FromIntXQuantizationAwareTrainingConfig"], [60, 0, 1, "", "Int4WeightOnlyEmbeddingQATQuantizer"], [61, 0, 1, "", "Int4WeightOnlyQATQuantizer"], [62, 0, 1, "", "Int8DynActInt4WeightQATQuantizer"], [63, 0, 1, "", "IntXQuantizationAwareTrainingConfig"], [64, 2, 1, "", "initialize_fake_quantizers"]], "torchao.quantization.qat.FakeQuantizeConfig": [[58, 3, 1, "", "group_size"], [58, 3, 1, "", "is_symmetric"]], "torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer": [[60, 1, 1, "", "convert"], [60, 1, 1, "", "prepare"]], "torchao": [[5, 4, 0, "-", "sparsity"]], "torchao.sparsity": [[71, 0, 1, "", "PerChannelNormObserver"], [72, 0, 1, "", "WandaSparsifier"], [73, 2, 1, "", "apply_fake_sparsity"], [74, 5, 1, "", "semi_sparse_weight"], [75, 2, 1, "", "sparsify_"]], "torchao.sparsity.PerChannelNormObserver": [[71, 1, 1, "", "forward"]], "torchao.sparsity.WandaSparsifier": [[72, 1, 1, "", "prepare"], [72, 1, 1, "", "squash_mask"], [72, 1, 1, "", "update_mask"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:module", "5": "py:attribute"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "module", "Python module"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchao": [0, 1, 2, 3, 4, 5, 8, 10, 76, 78, 79, 88], "dtype": [0, 9, 79], "layout": [0, 8, 17, 79], "tensor": [0, 8, 79, 86, 87, 88, 97], "subclass": [0, 8, 79, 87, 88], "quantiz": [0, 4, 10, 65, 76, 79, 80, 82, 85, 86, 87, 88, 92, 93, 94, 95, 96, 97], "techniqu": 0, "float8": [1, 10, 78], "main": [1, 4], "train": [1, 10, 78, 79, 82, 92, 93, 94, 95, 96], "api": [1, 2, 4, 6, 10, 76, 78, 97], "other": [1, 4, 8, 79], "type": 1, "refer": [2, 76], "python": 2, "kernel": [3, 8, 77, 79, 88], "infer": [4, 82], "quantize_": 4, "qat": [4, 10, 94], "primit": [4, 79], "sparsiti": [5, 84], "benchmark": [6, 7, 8, 82], "guid": [6, 7, 8, 80, 88], "add": [6, 79, 88], "an": [6, 81], "recip": [6, 78], "model": [6, 8, 78, 79, 81, 82, 88, 92, 93, 94], "design": [6, 84], "consider": 6, "hf": 6, "ci": 6, "dashboard": 6, "1": [6, 10, 78, 82, 88, 92, 95, 96, 97], "modifi": 6, "exist": 6, "configur": [6, 84, 88, 93, 94], "2": [6, 10, 80, 82, 88, 92, 93, 94, 95, 96, 97], "run": 6, "3": [6, 10, 82, 88, 92, 95, 96, 97], "output": [6, 87], "format": 6, "4": [6, 92, 97], "integr": [6, 10, 79, 88], "pipelin": 6, "troubleshoot": 6, "test": [6, 8], "common": [6, 97], "issu": 6, "best": 6, "practic": 6, "user": 7, "contributor": 8, "gener": 8, "extend": 8, "ad": [8, 79, 88], "effici": [8, 79], "custom": 8, "triton": 8, "hand": 8, "written": 8, "dispatch": [8, 88], "tensorimpl": [8, 79], "flow": [8, 79, 81, 88, 97], "us": [8, 97], "torch": [8, 92, 93, 94], "compil": [8, 88, 92], "perform": [8, 77, 82, 93], "serial": [8, 81, 88], "featur": 8, "support": [8, 79, 88], "function": [8, 79, 93, 94], "compos": 8, "microbenchmark": 8, "eval": [8, 93], "part": [10, 78, 82], "fine": 10, "tune": 10, "qlora": 10, "awar": [10, 79, 94, 95], "option": [10, 82, 91, 92], "torchtun": 10, "axolotl": 10, "low": [10, 79], "rank": 10, "adapt": 10, "huggingfac": [10, 82, 88], "peft": 10, "affinequantizedtensor": 11, "blocksparselayout": 12, "cutlassint4packedlayout": 13, "cutlasssemisparselayout": 14, "float8layout": 15, "int4cpulayout": 16, "marlinqqqlayout": 18, "marlinqqqtensor": 19, "marlinsparselayout": 20, "nf4tensor": 21, "plainlayout": 22, "semisparselayout": 23, "tensorcoretiledlayout": 24, "uintxlayout": 25, "to_affine_quantized_floatx": 26, "to_affine_quantized_floatx_stat": 27, "to_affine_quantized_fpx": 28, "to_affine_quantized_intx": 29, "to_affine_quantized_intx_stat": 30, "to_marlinqqq_quantized_intx": 31, "to_nf4": 32, "castconfig": 33, "float8linearconfig": 34, "scalinggranular": 35, "scalingtyp": 36, "convert_to_float8_train": 37, "precompute_float8_dynamic_scale_for_fsdp": 38, "fpxweightonlyconfig": 39, "float8dynamicactivationfloat8weightconfig": 40, "float8staticactivationfloat8weightconfig": 41, "float8weightonlyconfig": 42, "gemliteuintxweightonlyconfig": 43, "int4weightonlyconfig": 44, "int8dynamicactivationint4weightconfig": 45, "int8dynamicactivationint8weightconfig": 46, "int8weightonlyconfig": 47, "mappingtyp": 48, "torchaodtyp": 49, "uintxweightonlyconfig": 50, "zeropointdomain": 51, "autoqu": 52, "choose_qparams_affin": 53, "choose_qparams_affine_with_min_max": 54, "dequantize_affin": 55, "int_scaled_matmul": 56, "composableqatquant": 57, "fakequantizeconfig": 58, "fromintxquantizationawaretrainingconfig": 59, "int4weightonlyembeddingqatquant": 60, "int4weightonlyqatquant": 61, "int8dynactint4weightqatquant": 62, "intxquantizationawaretrainingconfig": 63, "initialize_fake_quant": 64, "quantize_affin": 66, "safe_int_mm": 67, "smooth_fq_linear_to_infer": 68, "swap_linear_with_smooth_fq_linear": 69, "to_linear_activation_quant": 70, "perchannelnormobserv": 71, "wandasparsifi": 72, "apply_fake_spars": 73, "semi_sparse_weight": 74, "sparsifi": 75, "welcom": 76, "document": 76, "get": 76, "start": [76, 80], "develop": 76, "note": [76, 78, 97], "eager": 76, "tutori": [76, 91], "pt2e": [76, 97], "pre": 78, "torchtitan": 78, "prerequisit": [78, 92, 95, 96, 97], "rowwis": 78, "scale": 78, "tensorwis": 78, "pick": 78, "import": [78, 93, 94], "directli": [78, 97], "convers": 78, "overview": [79, 84, 91], "basic": 79, "current": 79, "placehold": 79, "pytorch": [79, 80, 92, 93, 94, 95, 96, 97], "implement": [79, 87, 88], "oper": [79, 87, 88, 97], "nativ": 79, "factori": 79, "op": 79, "deriv": [79, 97], "algorithm": 79, "weight": [79, 82], "onli": 79, "dynam": 79, "activ": 79, "static": [79, 85], "insert": 79, "observ": 79, "how": [79, 93, 94, 97], "defin": [79, 93, 94], "modul": [79, 87, 88], "calibr": [79, 85, 93], "bit": 79, "optim": [79, 81, 82], "case": 79, "studi": 79, "int4": 79, "work": 79, "dure": 79, "execut": 79, "save": [79, 93, 94], "load": [79, 93, 94], "quick": 80, "first": 80, "exampl": [80, 88, 97], "export": [80, 82, 92, 93, 94, 95, 96, 97], "next": [80, 87], "step": [80, 82, 87, 88, 91], "deseri": 81, "what": [81, 87], "happen": 81, "when": 81, "serv": [82, 88], "vllm": [82, 88], "sglang": 82, "executorch": 82, "post": [82, 92, 93, 95, 96], "transform": [82, 88], "mobil": 82, "deploy": 82, "unti": 82, "embed": 82, "creat": [82, 88], "characterist": 82, "evalu": [82, 93], "qualiti": 82, "assess": 82, "memori": 82, "latenc": 82, "result": 82, "h100": 82, "machin": 82, "conclus": [82, 91, 92, 93, 94, 95, 96, 97], "comput": [83, 90], "time": [83, 90], "goal": 84, "context": 84, "prune": 84, "criteria": 84, "strategi": 84, "pattern": [84, 97], "phase": 85, "write": [86, 87, 97], "your": [86, 87, 88], "own": [86, 87], "advanc": 86, "ar": 87, "swap": 87, "which": 87, "should": 87, "we": 87, "compar": 87, "architectur": 88, "usag": 88, "system": 88, "class": 88, "level": 88, "new": 88, "method": 88, "minim": 88, "requir": 88, "compat": 88, "why": 88, "regist": 88, "s": 88, "kei": 88, "detail": 88, "hardwar": 88, "specif": [88, 93, 94], "linear": 88, "benefit": 88, "trade": 88, "off": 88, "share": [88, 97], "safetensor": 88, "diagram": 88, "high": 88, "point": 88, "bring": 88, "extern": 88, "templat": 91, "addit": 91, "exercis": 91, "further": 91, "read": 91, "openvino": 92, "backend": [92, 93, 94, 95, 96], "introduct": [92, 95, 96, 97], "nncf": 92, "instal": 92, "captur": [92, 95, 96], "fx": [92, 95, 96], "graph": [92, 95, 96], "appli": [92, 95, 96], "lower": [92, 93, 95, 96], "represent": 92, "improv": 92, "metric": 92, "motiv": [93, 97], "helper": [93, 94], "prepar": [93, 94], "dataset": [93, 94], "set": 93, "mode": 93, "convert": [93, 94], "check": 93, "size": 93, "accuraci": 93, "debug": 93, "loop": 94, "checkpoint": 94, "x86": 95, "through": [95, 96], "inductor": [95, 96], "intel": 96, "gpu": 96, "annot": 97, "param": 97, "fix": 97, "paramet": 97, "5": 97, "A": 97, "toi": 97, "resnet18": 97, "ir": 97, "problem": 97, "match": 97, "aten": 97, "recommend": 97, "subgraphmatcherwithnamenodemap": 97}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})