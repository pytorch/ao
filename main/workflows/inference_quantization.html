

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="Inference quantization" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pytorch.org/workflows/inference_quantization.html" />
<meta property="og:site_name" content="torchao" />
<meta property="og:description" content="Typically quantization algorithms will have different schemes for how the activation and weights are quantized so A16W8 for instance means the activations are quantized to 16 bits wheras the weight..." />
<meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
<meta property="og:image:alt" content="torchao" />
<meta name="description" content="Typically quantization algorithms will have different schemes for how the activation and weights are quantized so A16W8 for instance means the activations are quantized to 16 bits wheras the weight..." />

    <title>Inference quantization &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://pytorch.org/ao/workflows/inference_quantization.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API Reference" href="../api_reference/index.html" />
    <link rel="prev" title="Quantization-Aware Training (QAT)" href="qat.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Workflows
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api_reference/index.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../eager_tutorials/index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing/index.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../pt2e_quantization/index.html">
    PT2E Quantization
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Workflows
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api_reference/index.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../eager_tutorials/index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing/index.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../pt2e_quantization/index.html">
    PT2E Quantization
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="float8_training.html">torchao.float8</a></li>





<li class="toctree-l1"><a class="reference internal" href="qat.html">Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Inference quantization</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Workflows</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Inference...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="index.html">
        <meta itemprop="name" content="Workflows">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Inference quantization">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="inference-quantization">
<h1>Inference quantization<a class="headerlink" href="#inference-quantization" title="Permalink to this heading">#</a></h1>
<p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Jan 31, 2026 | Last Updated On: Jan 31, 2026</p>
<p>Typically quantization algorithms will have different schemes for how the activation and weights are quantized so A16W8 for instance means the activations are quantized to 16 bits wheras the weights are quantized to 8 bits. Trying out different quantization schemes in <code class="docutils literal notranslate"><span class="pre">torchao</span></code> is generally a 1 line change. Note: exact APIs are not stable, we may change them in the future.</p>
<section id="accuracy-benchmarks">
<h2>Accuracy benchmarks<a class="headerlink" href="#accuracy-benchmarks" title="Permalink to this heading">#</a></h2>
<p>All the following benchmarks are for <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.1-8B</span></code> using <code class="docutils literal notranslate"><span class="pre">lm-eval</span></code>.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>weight</p></th>
<th class="head"><p>activation</p></th>
<th class="head"><p>wikitext-perplexity</p></th>
<th class="head"><p>winogrande</p></th>
<th class="head"><p>checkpoint size (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bfloat16</p></td>
<td><p>bfloat16</p></td>
<td><p>7.3315</p></td>
<td><p>0.7380</p></td>
<td><p>16.1</p></td>
</tr>
<tr class="row-odd"><td><p>float8_rowwise</p></td>
<td><p>float8_rowwise</p></td>
<td><p>7.4197</p></td>
<td><p>0.7388</p></td>
<td><p>9.1</p></td>
</tr>
<tr class="row-even"><td><p>int8_rowwise</p></td>
<td><p>bfloat16</p></td>
<td><p>7.3451</p></td>
<td><p>0.7340</p></td>
<td><p>9.1</p></td>
</tr>
<tr class="row-odd"><td><p>int8_rowwise</p></td>
<td><p>int8_rowwise</p></td>
<td><p>7.4535</p></td>
<td><p>0.7285</p></td>
<td><p>9.1</p></td>
</tr>
<tr class="row-even"><td><p>mxfp8</p></td>
<td><p>mxfp8</p></td>
<td><p>7.6034</p></td>
<td><p>0.7316</p></td>
<td><p>9.32</p></td>
</tr>
<tr class="row-odd"><td><p>nvfp4</p></td>
<td><p>nvfp4</p></td>
<td><p>8.4459</p></td>
<td><p>0.7135</p></td>
<td><p>6.05</p></td>
</tr>
</tbody>
</table>
</div>
<p>To reproduce, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>//<span class="w"> </span>on<span class="w"> </span>an<span class="w"> </span>H100
<span class="nv">SKIP_VLLM</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./benchmarks/quantization/measure_accuracy_and_performance.sh<span class="w"> </span>h100
//<span class="w"> </span>on<span class="w"> </span>a<span class="w"> </span>B200
<span class="nv">SKIP_VLLM</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./benchmarks/quantization/measure_accuracy_and_performance.sh<span class="w"> </span>b200
</pre></div>
</div>
</section>
<section id="performance-benchmarks">
<h2>Performance benchmarks<a class="headerlink" href="#performance-benchmarks" title="Permalink to this heading">#</a></h2>
<p>All the following benchmarks are for <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.1-8B</span></code> using <code class="docutils literal notranslate"><span class="pre">torch==2.9.0</span></code> and <code class="docutils literal notranslate"><span class="pre">vllm==0.13.0</span></code>.</p>
<section id="nvidia-b200">
<h3>NVIDIA B200<a class="headerlink" href="#nvidia-b200" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>weight</p></th>
<th class="head"><p>activation</p></th>
<th class="head"><p>prefill toks/s</p></th>
<th class="head"><p>decode toks/s</p></th>
<th class="head"><p>prefill_speedup</p></th>
<th class="head"><p>decode_speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bfloat16</p></td>
<td><p>bfloat16</p></td>
<td><p>59099.9</p></td>
<td><p>14380</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>mxfp8</p></td>
<td><p>mxfp8</p></td>
<td><p>TODO(https://github.com/pytorch/ao/issues/3549)</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>nvfp4</p></td>
<td><p>nvfp4</p></td>
<td><p>102786</p></td>
<td><p>15218.9</p></td>
<td><p>1.739</p></td>
<td><p>1.058</p></td>
</tr>
<tr class="row-odd"><td><p>float8_rowwise</p></td>
<td><p>float8_rowwise</p></td>
<td><p>69313.7</p></td>
<td><p>15984</p></td>
<td><p>1.173</p></td>
<td><p>1.112</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="nvidia-h100">
<h3>NVIDIA H100<a class="headerlink" href="#nvidia-h100" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>weight</p></th>
<th class="head"><p>activation</p></th>
<th class="head"><p>prefill toks/s</p></th>
<th class="head"><p>decode toks/s</p></th>
<th class="head"><p>prefill_speedup</p></th>
<th class="head"><p>decode_speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bfloat16</p></td>
<td><p>bfloat16</p></td>
<td><p>30946.5</p></td>
<td><p>6612</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>float8_rowwise</p></td>
<td><p>float8_rowwise</p></td>
<td><p>45312.5</p></td>
<td><p>8025.95</p></td>
<td><p>1.464</p></td>
<td><p>1.214</p></td>
</tr>
<tr class="row-even"><td><p>int8_rowwwise</p></td>
<td><p>bfloat16</p></td>
<td><p>28231.9</p></td>
<td><p>4309.8</p></td>
<td><p>0.912</p></td>
<td><p>0.652</p></td>
</tr>
<tr class="row-odd"><td><p>int4</p></td>
<td><p>float8_rowwise</p></td>
<td><p>TODO(https://github.com/pytorch/ao/issues/3550)</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
</div>
<p>To reproduce these benchmarks, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>//<span class="w"> </span>on<span class="w"> </span>an<span class="w"> </span>h100
<span class="nv">SKIP_LM_EVAL</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./benchmarks/quantization/measure_accuracy_and_performance.sh<span class="w"> </span>h100
//<span class="w"> </span>on<span class="w"> </span>a<span class="w"> </span>b200
<span class="nv">SKIP_LM_EVAL</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./benchmarks/quantization/measure_accuracy_and_performance.sh<span class="w"> </span>h100

//<span class="w"> </span>under<span class="w"> </span>the<span class="w"> </span>hood,<span class="w"> </span>the<span class="w"> </span>actual<span class="w"> </span>vllm<span class="w"> </span>benchmark<span class="w"> </span>is<span class="w"> </span>doing<span class="w"> </span>the<span class="w"> </span>following:
//<span class="w"> </span><span class="m">1</span>.<span class="w"> </span>prefill
vllm<span class="w"> </span>bench<span class="w"> </span>throughput<span class="w"> </span>--num_prompts<span class="w"> </span><span class="m">32</span><span class="w"> </span>--input_len<span class="w"> </span><span class="m">4096</span><span class="w"> </span>--output_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--max_model_len<span class="w"> </span><span class="m">4128</span>
//<span class="w"> </span><span class="m">2</span>.<span class="w"> </span>decode
vllm<span class="w"> </span>bench<span class="w"> </span>throughput<span class="w"> </span>--num_prompts<span class="w"> </span><span class="m">128</span><span class="w"> </span>--input_len<span class="w"> </span><span class="m">32</span><span class="w"> </span>--output_len<span class="w"> </span><span class="m">2048</span><span class="w"> </span>--max_model_len<span class="w"> </span><span class="m">2080</span>
</pre></div>
</div>
</section>
</section>
<section id="quantization-techniques">
<h2>Quantization Techniques<a class="headerlink" href="#quantization-techniques" title="Permalink to this heading">#</a></h2>
<p>See the <a class="reference external" href="https://docs.pytorch.org/ao/main/api_reference/api_ref_quantization.html">API Reference documentation</a> for code examples and detailed documentation for each quantization config:</p>
<ul class="simple">
<li><p><strong>float8 weight configs</strong>: <code class="docutils literal notranslate"><span class="pre">Float8DynamicActivationFloat8WeightConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">Float8WeightOnlyConfig</span></code></p></li>
<li><p><strong>int8 weight configs</strong>: <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationInt8WeightConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">Int8WeightOnlyConfig</span></code></p></li>
<li><p><strong>int4 weight configs</strong>: <code class="docutils literal notranslate"><span class="pre">Int4WeightOnlyConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">Float8DynamicActivationInt4WeightConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationInt4WeightConfig</span></code></p></li>
<li><p><strong>intx weight configs</strong>: <code class="docutils literal notranslate"><span class="pre">IntxWeightOnlyConfig</span></code>, <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationIntxWeightConfig</span></code></p></li>
</ul>
<p>Notes:</p>
<ul class="simple">
<li><p>The quantization error incurred by applying int4 quantization to your model can be fairly significant, so using external techniques like GPTQ may be necessary to obtain a usable model.</p></li>
<li><p>Float8 quantization requires hardware with CUDA compute capability 8.9 or greater (e.g., H100).</p></li>
<li><p>Third-party backend CI status:</p>
<ul>
<li><p>Ascend NPU(requires torch_npu ≥ 2.7.1)
<a class="reference external" href="https://github.com/Ascend/Ascend-CI/actions/workflows/torchao.yml"><img alt="Ascend NPU" src="https://github.com/Ascend/Ascend-CI/actions/workflows/torchao.yml/badge.svg" /></a></p></li>
</ul>
</li>
</ul>
</section>
<section id="other-available-quantization-techniques">
<h2>Other Available Quantization Techniques<a class="headerlink" href="#other-available-quantization-techniques" title="Permalink to this heading">#</a></h2>
<section id="int8dynamicactivationintxweightconfig-quantization">
<h3>Int8DynamicActivationIntxWeightConfig Quantization<a class="headerlink" href="#int8dynamicactivationintxweightconfig-quantization" title="Permalink to this heading">#</a></h3>
<p>We have kernels that do 8-bit dynamic quantization of activations and uintx groupwise quantization of weights.  These kernels are experimental and can only be run on a device with an ARM CPU (e.g., a Mac computers with Apple silicon).  The benchmarks below were run on an M1 Mac Pro, with 8 perf cores, and 2 efficiency cores, and 32GB of RAM.  In all cases, torch.compile was used.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Technique</p></th>
<th class="head"><p>Tokens/Second</p></th>
<th class="head"><p>Memory Bandwidth (GB/s)</p></th>
<th class="head"><p>Peak Memory (GB)</p></th>
<th class="head"><p>Model Size (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama-3.1-8B</p></td>
<td><p>Base (bfloat16)</p></td>
<td><p>1.24</p></td>
<td><p>18.62</p></td>
<td><p>NA</p></td>
<td><p>15.01</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>int8_dynamic_activation_intx_weight-4-256-false</p></td>
<td><p>16.03</p></td>
<td><p>65.81</p></td>
<td><p>NA</p></td>
<td><p>4.11</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>int8_dynamic_activation_intx_weight-3-256-false</p></td>
<td><p>18.94</p></td>
<td><p>59.97</p></td>
<td><p>NA</p></td>
<td><p>3.17</p></td>
</tr>
</tbody>
</table>
</div>
<p>You can try out these apis with the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> api as above alongside the config <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationIntxWeightConfig</span></code>.  An example can be found in <code class="docutils literal notranslate"><span class="pre">torchao/_models/llama/generate.py</span></code>.</p>
</section>
<section id="codebook-quantization">
<h3>Codebook Quantization<a class="headerlink" href="#codebook-quantization" title="Permalink to this heading">#</a></h3>
<p>The benchmarks below were run on a single NVIDIA-A6000 GPU.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Technique</p></th>
<th class="head"><p>wikitext-perplexity</p></th>
<th class="head"><p>Tokens/Second</p></th>
<th class="head"><p>Memory Bandwidth (GB/s)</p></th>
<th class="head"><p>Peak Memory (GB)</p></th>
<th class="head"><p>Model Size (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama-3-8B</p></td>
<td><p>Base (bfloat16)</p></td>
<td><p>7.590</p></td>
<td><p>32.36</p></td>
<td><p>485.71</p></td>
<td><p>16.19</p></td>
<td><p>15.01</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>codebook-4-64</p></td>
<td><p>9.533</p></td>
<td><p>1.73</p></td>
<td><p>8.62</p></td>
<td><p>23.11</p></td>
<td><p>4.98</p></td>
</tr>
<tr class="row-even"><td><p>Llama-3.1-8B</p></td>
<td><p>Base (bfloat16)</p></td>
<td><p>7.713</p></td>
<td><p>32.16</p></td>
<td><p>482.70</p></td>
<td><p>16.35</p></td>
<td><p>15.01</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>codebook-4-64</p></td>
<td><p>10.095</p></td>
<td><p>1.73</p></td>
<td><p>8.63</p></td>
<td><p>23.11</p></td>
<td><p>4.98</p></td>
</tr>
</tbody>
</table>
</div>
<p>You try can out these apis with the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> api as above alongside the config <code class="docutils literal notranslate"><span class="pre">CodebookWeightOnlyConfig</span></code> an example can be found in  in <code class="docutils literal notranslate"><span class="pre">torchao/_models/llama/generate.py</span></code>.</p>
</section>
<section id="automatic-inductor-configuration">
<h3>Automatic Inductor Configuration<a class="headerlink" href="#automatic-inductor-configuration" title="Permalink to this heading">#</a></h3>
<p>:warning: <em>This functionality is being migrated from the top level <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> API to individual workflows, see https://github.com/pytorch/ao/issues/1715 for more details.</em></p>
<p>The <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> and <code class="docutils literal notranslate"><span class="pre">autoquant</span></code> apis now automatically use our recommended inductor configuration setings. You can mimic the same configuration settings for your own experiments by using the <code class="docutils literal notranslate"><span class="pre">torchao.quantization.utils.recommended_inductor_config_setter</span></code> to replicate our recommended configuration settings. Alternatively if you wish to disable these recommended settings, you can use the key word argument <code class="docutils literal notranslate"><span class="pre">set_inductor_config</span></code> and set it to false in the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> or <code class="docutils literal notranslate"><span class="pre">autoquant</span></code> apis to prevent assignment of those configuration settings. You can also overwrite these configuration settings after they are assigned if you so desire, as long as they are overwritten before passing any inputs to the torch.compiled model. This means that previous flows which referenced a variety of inductor configurations that needed to be set are now outdated, though continuing to manually set those same inductor configurations is unlikely to cause any issues.</p>
<details>
    <summary>Expand to see more!</summary>
</section>
<section id="autoquantization">
<h3>Autoquantization<a class="headerlink" href="#autoquantization" title="Permalink to this heading">#</a></h3>
<p>Autoquantization is a tool to automatically determine the best way to apply quantization to your model by comparing the performance of each quantization technique to each layer for the input types and shapes you care about.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchao</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization</span><span class="w"> </span><span class="kn">import</span>  <span class="n">DEFAULT_INT4_AUTOQUANT_CLASS_LIST</span>

<span class="c1"># Plug in your model and example input</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">use_autoquant_default</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">if</span> <span class="n">use_autoquant_default</span><span class="p">:</span>
    <span class="c1"># perform autoquantization and torch.compile with default settings</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchao</span><span class="o">.</span><span class="n">autoquant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max-autotune&#39;</span><span class="p">))</span>
<span class="k">elif</span> <span class="ow">not</span> <span class="n">use_autoquant_default</span><span class="p">:</span>
    <span class="c1"># perform autoquantization and torch.compile with int4 support</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchao</span><span class="o">.</span><span class="n">autoquant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max-autotune&#39;</span><span class="p">),</span> <span class="n">qtensor_class_list</span><span class="o">=</span><span class="n">DEFAULT_INT4_AUTOQUANT_CLASS_LIST</span><span class="p">)</span>

<span class="c1"># pass in an input which is used in order to pick fastest quantization operations</span>
<span class="c1"># and apply torch compilation.</span>
<span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>When used as in the example above, when the <code class="docutils literal notranslate"><span class="pre">autoquant</span></code> api is called alongside torch.compile, autoquant sets up the model so that when its run on the next input, the autoquantization and torch.compile processes leave you with a heavily optimized model.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">model(input)</span></code> is called, (under the hood) the tool does a preliminary run with the input where each linear layer keeps track of the different shapes and types of activations that it sees. Once the preliminary run is complete, the next step is to check each linear layer and benchmark the tracked shapes for different types of quantization techniques in order to pick the fastest one, attempting to take into account fusions where possible. Finally once the best class is found for each layer, the next step is to apply the necessary quantization technique to each layer, before finally allowing the normal <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> process to occur on the now quantized model. By default the api only uses int8 techniques, i.e. it chooses between no quantization, int8 dynamic quantization and int8 weight only quantization for each layer, though there is also an option add int4 quantization which can be used for maximum performance or to avoid perf regressions from <code class="docutils literal notranslate"><span class="pre">Int4WeightOnlyConfig()</span></code> since for certain (compute bound) regimes, int4 weight only quantization can be very slow.</p>
<p>Sometimes it is desirable to reuse a quantization plan that <code class="docutils literal notranslate"><span class="pre">autoquant</span></code> came up with. <code class="docutils literal notranslate"><span class="pre">torchao.quantization._AUTOQUANT_CACHE</span></code> is a dictionary holding autoquant’s benchmark results. We can save it and restore it later, which will cause <code class="docutils literal notranslate"><span class="pre">autoquant</span></code> to choose the same quantization methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchao.quantization</span>

<span class="c1"># After the first forward pass (when quantization was done)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.autoquant</span><span class="w"> </span><span class="kn">import</span> <span class="n">_AUTOQUANT_CACHE</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;quantization-cache.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">_AUTOQUANT_CACHE</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

<span class="c1"># On load</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.autoquant</span><span class="w"> </span><span class="kn">import</span> <span class="n">_AUTOQUANT_CACHE</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;quantization-cache.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">_AUTOQUANT_CACHE</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="affine-quantization-details">
<h3>Affine Quantization Details<a class="headerlink" href="#affine-quantization-details" title="Permalink to this heading">#</a></h3>
<p>Affine quantization refers to the type of quantization that maps from high precision floating point numbers to quantized numbers (low precision integer or floating point dtypes) with an affine transformation, i.e.: <code class="docutils literal notranslate"><span class="pre">quantized_val</span> <span class="pre">=</span> <span class="pre">high_precision_float_val</span> <span class="pre">/</span> <span class="pre">scale</span> <span class="pre">+</span> <span class="pre">zero_point</span></code> where <code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_point</span></code> are quantization parameters for some granularity and based on some data (also some dtypes may not require a <code class="docutils literal notranslate"><span class="pre">zero_point</span></code>). Each of the techniques in the above section qualify as Affine Quantization.</p>
</section>
<section id="quantization-primitives">
<h3>Quantization Primitives<a class="headerlink" href="#quantization-primitives" title="Permalink to this heading">#</a></h3>
<p>We used to have different quantize and dequantize operators for quantization with different granularities. But in the end these can all be expressed with a <code class="docutils literal notranslate"><span class="pre">block_size</span></code> argument with different settings, so we unified existing quant primitives to <code class="docutils literal notranslate"><span class="pre">choose_qparams_affine</span></code>, <code class="docutils literal notranslate"><span class="pre">quantize_affine</span></code> and <code class="docutils literal notranslate"><span class="pre">dequantize_affine</span></code> that can represent symmetric/asymmetric per tensor/channel/token/channel_group quantization, this can be used to implement the unified quantized tensor subclass.</p>
<p>Note: these primitive ops supports two “types” of quantization, distinguished by whether <code class="docutils literal notranslate"><span class="pre">zero_point</span></code> is in floating point domain or integer domain. See docstrings for <code class="docutils literal notranslate"><span class="pre">choose_qparams</span></code> for more details.</p>
</section>
<section id="quantized-tensor-subclass">
<h3>Quantized Tensor Subclass<a class="headerlink" href="#quantized-tensor-subclass" title="Permalink to this heading">#</a></h3>
<p>We also have a unified quantized tensor subclass that implements how to get a quantized tensor from floating point tensor and what does it mean to call linear ops on an instance of the tensor, e.g. <code class="docutils literal notranslate"><span class="pre">F.linear</span></code> and <code class="docutils literal notranslate"><span class="pre">aten.addmm</span></code>, with this we could dispatch to different operators (e.g. <code class="docutils literal notranslate"><span class="pre">int4mm</span></code> op) based on device (cpu, cuda) and quantization settings (<code class="docutils literal notranslate"><span class="pre">int4</span></code>, <code class="docutils literal notranslate"><span class="pre">int8</span></code>) and also packing formats (e.g. format optimized for cpu int4 mm kernel)</p>
<section id="layouts">
<h4>Layouts<a class="headerlink" href="#layouts" title="Permalink to this heading">#</a></h4>
<p>We extended the <code class="docutils literal notranslate"><span class="pre">layout</span></code> concept to represent different packing formats for a tensor. <code class="docutils literal notranslate"><span class="pre">AffineQuantizedTensor</span></code> supports <code class="docutils literal notranslate"><span class="pre">plain</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor_core_tiled</span></code> layout. <code class="docutils literal notranslate"><span class="pre">plain</span></code> layout is used for workflows backing <code class="docutils literal notranslate"><span class="pre">Int8WeightOnlyConfig</span></code> and <code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationInt8WeightConfig</span></code> and also as a default layout. <code class="docutils literal notranslate"><span class="pre">tensor_core_tiled</span></code> layout is used for workflows backing <code class="docutils literal notranslate"><span class="pre">Int4WeightOnlyConfig</span></code> quantization and is packing the weights in a format that is compatible with tinygemm <a class="reference external" href="https://github.com/pytorch/pytorch/blob/39357ba06f48cda7d293a4995aa5eba2a46598b5/aten/src/ATen/native/native_functions.yaml#L4138">int4mm</a> kernels.</p>
</section>
</section>
<section id="zero-point-domains">
<h3>Zero Point Domains<a class="headerlink" href="#zero-point-domains" title="Permalink to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">ZeroPointDomain</span></code> is used to control the data types of zero points. <code class="docutils literal notranslate"><span class="pre">ZeroPointDomain.None</span></code> means zero_point is None, <code class="docutils literal notranslate"><span class="pre">ZeroPointDomain.FLOAT</span></code> means zero_point is in the floating point domain and <code class="docutils literal notranslate"><span class="pre">ZeroPointDomain.INT</span></code> means integer domain. For detailed implementation of different zero point data types, refer to <a class="reference external" href="https://github.com/pytorch/ao/blob/main/test/quantization/test_quant_primitives.py">the reference implementation</a>.
The following support matrix illustrates the relationship between layouts and zero point domains, which may be updated with backend changes:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Layout</p></th>
<th class="head"><p>None(Symmetric)</p></th>
<th class="head"><p>Float</p></th>
<th class="head"><p>Int</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TensorCoreTiledLayout</p></td>
<td><p>Yes</p></td>
<td><p>Yes(Default)</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p>Int4CPULayout</p></td>
<td><p>Yes</p></td>
<td><p>Yes(Default)</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="full-affine-quantization-flow-example">
<h3>Full Affine Quantization Flow Example<a class="headerlink" href="#full-affine-quantization-flow-example" title="Permalink to this heading">#</a></h3>
<p>Let’s use int4 weight only quantization that’s targeting tinygemm int4 weight only quantized matmul
as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_primitives</span><span class="w"> </span><span class="kn">import</span> <span class="n">MappingType</span><span class="p">,</span> <span class="n">ZeroPointDomain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_intx</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.quant_api</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">quantize_</span><span class="p">,</span>
    <span class="n">Int4WeightOnlyConfig</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ToyLinearModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">example_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">ToyLinearModel</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">m_bf16</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">example_inputs</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">m_bf16</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">m_bf16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max-autotune&#39;</span><span class="p">)</span>
<span class="c1"># apply int4 weight only quant (compatible with tinygemm int4 weight only quant mm kernel in torchao)</span>
<span class="n">group_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="c1"># only works for torch 2.4+</span>
<span class="n">quantize_</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">Int4WeightOnlyConfig</span><span class="p">(</span><span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span> <span class="n">int4_packing_format</span><span class="o">=</span><span class="s2">&quot;tile_packed_to_4d&quot;</span><span class="p">))</span>
<span class="c1"># can also specify different packing format</span>
<span class="c1"># quantize_(m, Int4WeightOnlyConfig(group_size=group_size, int4_packing_format=&quot;plain&quot;))</span>

<span class="c1"># compile the model to improve performance</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max-autotune&#39;</span><span class="p">)</span>

<span class="c1"># benchmark to see the speedup</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">benchmark_model</span>

<span class="n">num_runs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">bf16_time</span> <span class="o">=</span> <span class="n">benchmark_model</span><span class="p">(</span><span class="n">m_bf16</span><span class="p">,</span> <span class="n">num_runs</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;bf16 mean time: </span><span class="si">{</span><span class="n">bf16_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">int4_time</span> <span class="o">=</span> <span class="n">benchmark_model</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">num_runs</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;int4 weight only quantized mean time: </span><span class="si">{</span><span class="n">int4_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;speedup: </span><span class="si">{</span><span class="n">bf16_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">int4_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># output (1xA100 GPU machine)</span>
<span class="n">bf16</span> <span class="n">mean</span> <span class="n">time</span><span class="p">:</span> <span class="mf">71.457685546875</span>
<span class="n">int4</span> <span class="n">weight</span> <span class="n">only</span> <span class="n">quantized</span> <span class="n">mean</span> <span class="n">time</span><span class="p">:</span> <span class="mf">31.4580908203125</span>
<span class="n">speedup</span><span class="p">:</span> <span class="mf">2.2715200981216173</span>
</pre></div>
</div>
<p>What we do underlying the APIs are roughly the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchao.dtypes</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_affine_quantized_intx</span>
<span class="k">def</span><span class="w"> </span><span class="nf">int8wo_quant</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">to_affine_quantized_intx</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">zero_point_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

<span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="c1"># optional filtering for module name, shape etc.</span>
        <span class="n">m</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">int8wo_quant</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>

        <span class="c1"># note: quantization for activation need to be applied after the weight quantization</span>
        <span class="c1"># quantization activation (needed by dynamic quantization)</span>
        <span class="n">input_quant_func</span> <span class="o">=</span> <span class="n">int8wo_quant</span>  <span class="c1"># specify how input activation is quantized</span>
        <span class="n">module</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">to_linear_activation_quantized</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">input_quant_func</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="kv-cache-quantization">
<h3>KV Cache Quantization<a class="headerlink" href="#kv-cache-quantization" title="Permalink to this heading">#</a></h3>
<p>We’ve added kv cache quantization and other features in order to enable long context length (and necessarily memory efficient) inference.</p>
<p>In practice these features alongside int4 weight only quantization allow us to <strong>reduce peak memory by ~55%</strong>, meaning we can Llama3.1-8B inference with a <strong>130k context length with only 18.9 GB of peak memory.</strong> More details can be found <a class="reference external" href="https://github.com/pytorch/ao/blob/main/torchao/_models/llama/README.md#KV-Cache-Quantization-Memory-Efficient-Inference">here</a></p>
</section>
<section id="gemlite-triton">
<h3>Gemlite Triton<a class="headerlink" href="#gemlite-triton" title="Permalink to this heading">#</a></h3>
<p>Int4 and Int8 quantization using the <a class="reference external" href="https://github.com/mobiusml/gemlite">Gemlite Triton</a> kernels. You can try it out with the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> api as above alongside the constructor <code class="docutils literal notranslate"><span class="pre">GemliteUIntXWeightOnlyConfig</span></code>.  An example can be found in <code class="docutils literal notranslate"><span class="pre">torchao/_models/llama/generate.py</span></code>.</p>
<p>Note: we test on gemlite 0.4.1, but should be able to use any version after that, we’d recommend to use the latest release to get the most recent performance improvements.</p>
</section>
<section id="uintx-quantization">
<h3>UINTx Quantization<a class="headerlink" href="#uintx-quantization" title="Permalink to this heading">#</a></h3>
<p>We’re trying to develop kernels for low bit quantization for intx quantization formats. While the current performance is not ideal, we’re hoping to continue to iterate on these kernels to improve their performance.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Technique</p></th>
<th class="head"><p>wikitext-perplexity</p></th>
<th class="head"><p>Tokens/Second</p></th>
<th class="head"><p>Memory Bandwidth (GB/s)</p></th>
<th class="head"><p>Peak Memory (GB)</p></th>
<th class="head"><p>Model Size (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama-2-7B</p></td>
<td><p>Base (bfloat16)</p></td>
<td><p>12.212</p></td>
<td><p>107.38</p></td>
<td><p>1418.93</p></td>
<td><p>13.88</p></td>
<td><p>13.21</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>uintx-4-64-hqq</p></td>
<td><p>12.775</p></td>
<td><p>50.99</p></td>
<td><p>200.08</p></td>
<td><p>6.29</p></td>
<td><p>3.92</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>uintx-2-8-hqq</p></td>
<td><p>24.500</p></td>
<td><p>40.25</p></td>
<td><p>265.95</p></td>
<td><p>9.24</p></td>
<td><p>6.61</p></td>
</tr>
<tr class="row-odd"><td><p>Llama-3-8B</p></td>
<td><p>Base (bfloat16)</p></td>
<td><p>7.441</p></td>
<td><p>95.64</p></td>
<td><p>1435.54</p></td>
<td><p>16.43</p></td>
<td><p>15.01</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>uintx-4-64-hqq</p></td>
<td><p>8.124</p></td>
<td><p>47.85</p></td>
<td><p>213.24</p></td>
<td><p>11.85</p></td>
<td><p>4.46</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>uintx-2-8-hqq</p></td>
<td><p>39.605</p></td>
<td><p>34.83</p></td>
<td><p>261.42</p></td>
<td><p>14.99</p></td>
<td><p>7.51</p></td>
</tr>
</tbody>
</table>
</div>
<p>You try can out these apis with the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> api as above alongside the config <code class="docutils literal notranslate"><span class="pre">UIntXWeightOnlyConfig</span></code>. An example can be found in  in <code class="docutils literal notranslate"><span class="pre">torchao/_models/llama/generate.py</span></code>.</p>
</details>
</section>
</section>
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>APIs have been hardware tested on A100 and T4(colab)</p></li>
<li><p>While these techniques are designed to improve model performance, in some cases the opposite can occur. This is because quantization adds additional overhead to the model that is hopefully made up for by faster matmuls (dynamic quantization) or loading weights faster (weight-only quantization). If your matmuls are small enough or your non-quantized perf isn’t bottlenecked by weight load time, these techniques may reduce performance.</p></li>
<li><p>Use the PyTorch nightlies so you can leverage <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor">tensor subclasses</a> which is preferred over older module swap based methods because it doesn’t modify the graph and is generally more composable and flexible.</p></li>
</ol>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="qat.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quantization-Aware Training (QAT)</p>
      </div>
    </a>
    <a class="right-next"
       href="../api_reference/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="qat.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quantization-Aware Training (QAT)</p>
      </div>
    </a>
    <a class="right-next"
       href="../api_reference/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-benchmarks">Accuracy benchmarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-benchmarks">Performance benchmarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nvidia-b200">NVIDIA B200</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nvidia-h100">NVIDIA H100</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-techniques">Quantization Techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-available-quantization-techniques">Other Available Quantization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#int8dynamicactivationintxweightconfig-quantization">Int8DynamicActivationIntxWeightConfig Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codebook-quantization">Codebook Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-inductor-configuration">Automatic Inductor Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoquantization">Autoquantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#affine-quantization-details">Affine Quantization Details</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-primitives">Quantization Primitives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantized-tensor-subclass">Quantized Tensor Subclass</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layouts">Layouts</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-point-domains">Zero Point Domains</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-affine-quantization-flow-example">Full Affine Quantization Flow Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kv-cache-quantization">KV Cache Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gemlite-triton">Gemlite Triton</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uintx-quantization">UINTx Quantization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/ao/edit/main/docs/source/workflows/inference_quantization.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/workflows/inference_quantization.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Inference quantization",
       "headline": "Inference quantization",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/workflows/inference_quantization.html",
       "articleBody": "Inference quantization# Created On: Jan 31, 2026 | Last Updated On: Jan 31, 2026 Typically quantization algorithms will have different schemes for how the activation and weights are quantized so A16W8 for instance means the activations are quantized to 16 bits wheras the weights are quantized to 8 bits. Trying out different quantization schemes in torchao is generally a 1 line change. Note: exact APIs are not stable, we may change them in the future. Accuracy benchmarks# All the following benchmarks are for meta-llama/Llama-3.1-8B using lm-eval. weight activation wikitext-perplexity winogrande checkpoint size (GB) bfloat16 bfloat16 7.3315 0.7380 16.1 float8_rowwise float8_rowwise 7.4197 0.7388 9.1 int8_rowwise bfloat16 7.3451 0.7340 9.1 int8_rowwise int8_rowwise 7.4535 0.7285 9.1 mxfp8 mxfp8 7.6034 0.7316 9.32 nvfp4 nvfp4 8.4459 0.7135 6.05 To reproduce, run the following command: // on an H100 SKIP_VLLM=1 ./benchmarks/quantization/measure_accuracy_and_performance.sh h100 // on a B200 SKIP_VLLM=1 ./benchmarks/quantization/measure_accuracy_and_performance.sh b200 Performance benchmarks# All the following benchmarks are for meta-llama/Llama-3.1-8B using torch==2.9.0 and vllm==0.13.0. NVIDIA B200# weight activation prefill toks/s decode toks/s prefill_speedup decode_speedup bfloat16 bfloat16 59099.9 14380 1 1 mxfp8 mxfp8 TODO(https://github.com/pytorch/ao/issues/3549) - - - nvfp4 nvfp4 102786 15218.9 1.739 1.058 float8_rowwise float8_rowwise 69313.7 15984 1.173 1.112 NVIDIA H100# weight activation prefill toks/s decode toks/s prefill_speedup decode_speedup bfloat16 bfloat16 30946.5 6612 1 1 float8_rowwise float8_rowwise 45312.5 8025.95 1.464 1.214 int8_rowwwise bfloat16 28231.9 4309.8 0.912 0.652 int4 float8_rowwise TODO(https://github.com/pytorch/ao/issues/3550) - - - To reproduce these benchmarks, run // on an h100 SKIP_LM_EVAL=1 ./benchmarks/quantization/measure_accuracy_and_performance.sh h100 // on a b200 SKIP_LM_EVAL=1 ./benchmarks/quantization/measure_accuracy_and_performance.sh h100 // under the hood, the actual vllm benchmark is doing the following: // 1. prefill vllm bench throughput --num_prompts 32 --input_len 4096 --output_len 32 --max_model_len 4128 // 2. decode vllm bench throughput --num_prompts 128 --input_len 32 --output_len 2048 --max_model_len 2080 Quantization Techniques# See the API Reference documentation for code examples and detailed documentation for each quantization config: float8 weight configs: Float8DynamicActivationFloat8WeightConfig, Float8WeightOnlyConfig int8 weight configs: Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig int4 weight configs: Int4WeightOnlyConfig, Float8DynamicActivationInt4WeightConfig, Int8DynamicActivationInt4WeightConfig intx weight configs: IntxWeightOnlyConfig, Int8DynamicActivationIntxWeightConfig Notes: The quantization error incurred by applying int4 quantization to your model can be fairly significant, so using external techniques like GPTQ may be necessary to obtain a usable model. Float8 quantization requires hardware with CUDA compute capability 8.9 or greater (e.g., H100). Third-party backend CI status: Ascend NPU(requires torch_npu \u2265 2.7.1) Other Available Quantization Techniques# Int8DynamicActivationIntxWeightConfig Quantization# We have kernels that do 8-bit dynamic quantization of activations and uintx groupwise quantization of weights. These kernels are experimental and can only be run on a device with an ARM CPU (e.g., a Mac computers with Apple silicon). The benchmarks below were run on an M1 Mac Pro, with 8 perf cores, and 2 efficiency cores, and 32GB of RAM. In all cases, torch.compile was used. Model Technique Tokens/Second Memory Bandwidth (GB/s) Peak Memory (GB) Model Size (GB) Llama-3.1-8B Base (bfloat16) 1.24 18.62 NA 15.01 int8_dynamic_activation_intx_weight-4-256-false 16.03 65.81 NA 4.11 int8_dynamic_activation_intx_weight-3-256-false 18.94 59.97 NA 3.17 You can try out these apis with the quantize_ api as above alongside the config Int8DynamicActivationIntxWeightConfig. An example can be found in torchao/_models/llama/generate.py. Codebook Quantization# The benchmarks below were run on a single NVIDIA-A6000 GPU. Model Technique wikitext-perplexity Tokens/Second Memory Bandwidth (GB/s) Peak Memory (GB) Model Size (GB) Llama-3-8B Base (bfloat16) 7.590 32.36 485.71 16.19 15.01 codebook-4-64 9.533 1.73 8.62 23.11 4.98 Llama-3.1-8B Base (bfloat16) 7.713 32.16 482.70 16.35 15.01 codebook-4-64 10.095 1.73 8.63 23.11 4.98 You try can out these apis with the quantize_ api as above alongside the config CodebookWeightOnlyConfig an example can be found in in torchao/_models/llama/generate.py. Automatic Inductor Configuration# :warning: This functionality is being migrated from the top level quantize_ API to individual workflows, see https://github.com/pytorch/ao/issues/1715 for more details. The quantize_ and autoquant apis now automatically use our recommended inductor configuration setings. You can mimic the same configuration settings for your own experiments by using the torchao.quantization.utils.recommended_inductor_config_setter to replicate our recommended configuration settings. Alternatively if you wish to disable these recommended settings, you can use the key word argument set_inductor_config and set it to false in the quantize_ or autoquant apis to prevent assignment of those configuration settings. You can also overwrite these configuration settings after they are assigned if you so desire, as long as they are overwritten before passing any inputs to the torch.compiled model. This means that previous flows which referenced a variety of inductor configurations that needed to be set are now outdated, though continuing to manually set those same inductor configurations is unlikely to cause any issues. Expand to see more! Autoquantization# Autoquantization is a tool to automatically determine the best way to apply quantization to your model by comparing the performance of each quantization technique to each layer for the input types and shapes you care about. import torch import torchao from torchao.quantization import DEFAULT_INT4_AUTOQUANT_CLASS_LIST # Plug in your model and example input model = torch.nn.Sequential(torch.nn.Linear(32, 64)).cuda().to(torch.bfloat16) input = torch.randn(32,32, dtype=torch.bfloat16, device=\u0027cuda\u0027) use_autoquant_default = True if use_autoquant_default: # perform autoquantization and torch.compile with default settings model = torchao.autoquant(torch.compile(model, mode=\u0027max-autotune\u0027)) elif not use_autoquant_default: # perform autoquantization and torch.compile with int4 support model = torchao.autoquant(torch.compile(model, mode=\u0027max-autotune\u0027), qtensor_class_list=DEFAULT_INT4_AUTOQUANT_CLASS_LIST) # pass in an input which is used in order to pick fastest quantization operations # and apply torch compilation. model(input) When used as in the example above, when the autoquant api is called alongside torch.compile, autoquant sets up the model so that when its run on the next input, the autoquantization and torch.compile processes leave you with a heavily optimized model. When model(input) is called, (under the hood) the tool does a preliminary run with the input where each linear layer keeps track of the different shapes and types of activations that it sees. Once the preliminary run is complete, the next step is to check each linear layer and benchmark the tracked shapes for different types of quantization techniques in order to pick the fastest one, attempting to take into account fusions where possible. Finally once the best class is found for each layer, the next step is to apply the necessary quantization technique to each layer, before finally allowing the normal torch.compile process to occur on the now quantized model. By default the api only uses int8 techniques, i.e. it chooses between no quantization, int8 dynamic quantization and int8 weight only quantization for each layer, though there is also an option add int4 quantization which can be used for maximum performance or to avoid perf regressions from Int4WeightOnlyConfig() since for certain (compute bound) regimes, int4 weight only quantization can be very slow. Sometimes it is desirable to reuse a quantization plan that autoquant came up with. torchao.quantization._AUTOQUANT_CACHE is a dictionary holding autoquant\u2019s benchmark results. We can save it and restore it later, which will cause autoquant to choose the same quantization methods. import pickle import torchao.quantization # After the first forward pass (when quantization was done) from torchao.quantization.autoquant import _AUTOQUANT_CACHE with open(\"quantization-cache.pkl\", \"wb\") as f: pickle.dump(_AUTOQUANT_CACHE, f) # On load from torchao.quantization.autoquant import _AUTOQUANT_CACHE with open(\"quantization-cache.pkl\", \"rb\") as f: _AUTOQUANT_CACHE.update(pickle.load(f)) Affine Quantization Details# Affine quantization refers to the type of quantization that maps from high precision floating point numbers to quantized numbers (low precision integer or floating point dtypes) with an affine transformation, i.e.: quantized_val = high_precision_float_val / scale + zero_point where scale and zero_point are quantization parameters for some granularity and based on some data (also some dtypes may not require a zero_point). Each of the techniques in the above section qualify as Affine Quantization. Quantization Primitives# We used to have different quantize and dequantize operators for quantization with different granularities. But in the end these can all be expressed with a block_size argument with different settings, so we unified existing quant primitives to choose_qparams_affine, quantize_affine and dequantize_affine that can represent symmetric/asymmetric per tensor/channel/token/channel_group quantization, this can be used to implement the unified quantized tensor subclass. Note: these primitive ops supports two \u201ctypes\u201d of quantization, distinguished by whether zero_point is in floating point domain or integer domain. See docstrings for choose_qparams for more details. Quantized Tensor Subclass# We also have a unified quantized tensor subclass that implements how to get a quantized tensor from floating point tensor and what does it mean to call linear ops on an instance of the tensor, e.g. F.linear and aten.addmm, with this we could dispatch to different operators (e.g. int4mm op) based on device (cpu, cuda) and quantization settings (int4, int8) and also packing formats (e.g. format optimized for cpu int4 mm kernel) Layouts# We extended the layout concept to represent different packing formats for a tensor. AffineQuantizedTensor supports plain and tensor_core_tiled layout. plain layout is used for workflows backing Int8WeightOnlyConfig and Int8DynamicActivationInt8WeightConfig and also as a default layout. tensor_core_tiled layout is used for workflows backing Int4WeightOnlyConfig quantization and is packing the weights in a format that is compatible with tinygemm int4mm kernels. Zero Point Domains# ZeroPointDomain is used to control the data types of zero points. ZeroPointDomain.None means zero_point is None, ZeroPointDomain.FLOAT means zero_point is in the floating point domain and ZeroPointDomain.INT means integer domain. For detailed implementation of different zero point data types, refer to the reference implementation. The following support matrix illustrates the relationship between layouts and zero point domains, which may be updated with backend changes: Layout None(Symmetric) Float Int TensorCoreTiledLayout Yes Yes(Default) No Int4CPULayout Yes Yes(Default) No Full Affine Quantization Flow Example# Let\u2019s use int4 weight only quantization that\u2019s targeting tinygemm int4 weight only quantized matmul as an example: import torch from torchao.quantization.quant_primitives import MappingType, ZeroPointDomain from torchao.dtypes import to_affine_quantized_intx import copy from torchao.quantization.quant_api import ( quantize_, Int4WeightOnlyConfig, ) class ToyLinearModel(torch.nn.Module): def __init__(self, m=64, n=32, k=64): super().__init__() self.linear1 = torch.nn.Linear(m, n, bias=False) self.linear2 = torch.nn.Linear(n, k, bias=False) def example_inputs(self, batch_size=1, dtype=torch.float32, device=\"cpu\"): return (torch.randn(batch_size, self.linear1.in_features, dtype=dtype, device=device),) def forward(self, x): x = self.linear1(x) x = self.linear2(x) return x dtype = torch.bfloat16 m = ToyLinearModel(1024, 1024, 1024).eval().to(dtype).to(\"cuda\") m_bf16 = copy.deepcopy(m) example_inputs = m.example_inputs(dtype=dtype, device=\"cuda\") m_bf16 = torch.compile(m_bf16, mode=\u0027max-autotune\u0027) # apply int4 weight only quant (compatible with tinygemm int4 weight only quant mm kernel in torchao) group_size = 32 # only works for torch 2.4+ quantize_(m, Int4WeightOnlyConfig(group_size=group_size, int4_packing_format=\"tile_packed_to_4d\")) # can also specify different packing format # quantize_(m, Int4WeightOnlyConfig(group_size=group_size, int4_packing_format=\"plain\")) # compile the model to improve performance m = torch.compile(m, mode=\u0027max-autotune\u0027) # benchmark to see the speedup from torchao.utils import benchmark_model num_runs = 100 torch._dynamo.reset() bf16_time = benchmark_model(m_bf16, num_runs, example_inputs) print(f\"bf16 mean time: {bf16_time}\") int4_time = benchmark_model(m, num_runs, example_inputs) print(f\"int4 weight only quantized mean time: {int4_time}\") print(f\"speedup: {bf16_time / int4_time}\") # output (1xA100 GPU machine) bf16 mean time: 71.457685546875 int4 weight only quantized mean time: 31.4580908203125 speedup: 2.2715200981216173 What we do underlying the APIs are roughly the following: from torchao.dtypes import to_affine_quantized_intx def int8wo_quant(weight): return to_affine_quantized_intx(weight, MappingType.SYMMETRIC, (1, weight.shape[1]), torch.int8, eps=torch.finfo(torch.float32).eps, zero_point_dtype=torch.int64) for module, name in model.named_modules(): if isinstance(module, torch.nn.Linear): # optional filtering for module name, shape etc. m.weight = nn.Parameter(int8wo_quant(module.weight)) # note: quantization for activation need to be applied after the weight quantization # quantization activation (needed by dynamic quantization) input_quant_func = int8wo_quant # specify how input activation is quantized module.weight = nn.Parameter(to_linear_activation_quantized(module.weight, input_quant_func)) KV Cache Quantization# We\u2019ve added kv cache quantization and other features in order to enable long context length (and necessarily memory efficient) inference. In practice these features alongside int4 weight only quantization allow us to reduce peak memory by ~55%, meaning we can Llama3.1-8B inference with a 130k context length with only 18.9 GB of peak memory. More details can be found here Gemlite Triton# Int4 and Int8 quantization using the Gemlite Triton kernels. You can try it out with the quantize_ api as above alongside the constructor GemliteUIntXWeightOnlyConfig. An example can be found in torchao/_models/llama/generate.py. Note: we test on gemlite 0.4.1, but should be able to use any version after that, we\u2019d recommend to use the latest release to get the most recent performance improvements. UINTx Quantization# We\u2019re trying to develop kernels for low bit quantization for intx quantization formats. While the current performance is not ideal, we\u2019re hoping to continue to iterate on these kernels to improve their performance. Model Technique wikitext-perplexity Tokens/Second Memory Bandwidth (GB/s) Peak Memory (GB) Model Size (GB) Llama-2-7B Base (bfloat16) 12.212 107.38 1418.93 13.88 13.21 uintx-4-64-hqq 12.775 50.99 200.08 6.29 3.92 uintx-2-8-hqq 24.500 40.25 265.95 9.24 6.61 Llama-3-8B Base (bfloat16) 7.441 95.64 1435.54 16.43 15.01 uintx-4-64-hqq 8.124 47.85 213.24 11.85 4.46 uintx-2-8-hqq 39.605 34.83 261.42 14.99 7.51 You try can out these apis with the quantize_ api as above alongside the config UIntXWeightOnlyConfig. An example can be found in in torchao/_models/llama/generate.py. Notes# APIs have been hardware tested on A100 and T4(colab) While these techniques are designed to improve model performance, in some cases the opposite can occur. This is because quantization adds additional overhead to the model that is hopefully made up for by faster matmuls (dynamic quantization) or loading weights faster (weight-only quantization). If your matmuls are small enough or your non-quantized perf isn\u2019t bottlenecked by weight load time, these techniques may reduce performance. Use the PyTorch nightlies so you can leverage tensor subclasses which is preferred over older module swap based methods because it doesn\u2019t modify the graph and is generally more composable and flexible.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/workflows/inference_quantization.html"
       },
       "datePublished": "Jan 31, 2026T00:00:00Z",
       "dateModified": "Jan 31, 2026T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>