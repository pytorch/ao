
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Quantization-Aware Training (QAT)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pytorch.org/workflows/qat.html" />
<meta property="og:site_name" content="torchao" />
<meta property="og:description" content="Quantization-Aware Training (QAT) refers to applying fake quantization during the training or fine-tuning process, such that the final quantized model will exhibit higher accuracies and lower perpl..." />
<meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
<meta property="og:image:alt" content="torchao" />
<meta name="description" content="Quantization-Aware Training (QAT) refers to applying fake quantization during the training or fine-tuning process, such that the final quantized model will exhibit higher accuracies and lower perpl..." />

    <title>Quantization-Aware Training (QAT) &#8212; torchao main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=b417fedc" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=ca3c1c84" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=e5fbc548" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=f533b996" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'workflows/qat';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/pytorch/ao/gh-pages/torchao-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://pytorch.org/ao/workflows/qat.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quantized Inference" href="inference.html" />
    <link rel="prev" title="Quantized Training" href="training.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
  /* Ensure proper mobile layout when LF header is hidden */
  @media (max-width: 960px) {
    .bd-header {
      top: 0 !important;
      position: sticky !important;
      z-index: 1020 !important;
    }
    .bd-main {
      padding-top: 0 !important;
      margin-top: 0 !important;
    }
    .bd-article-container {
      padding-top: 0 !important;
    }
    .header-article__inner {
      padding-top: 1rem !important;
    }

  }
</style>


<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main (0.17.0+gitb5ca5dc )');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->


<!-- Script to Fix scrolling with fast fixed-duration animation -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const SCROLL_DURATION = 150; // Fixed duration in ms regardless of distance
    let lockedTargetId = null; // Lock the TOC to this target until user scrolls manually
    let isUpdatingToc = false; // Guard against infinite loops

    function smoothScrollTo(targetY, duration, onComplete) {
      const startY = window.pageYOffset;
      const difference = targetY - startY;
      const startTime = performance.now();

      function step(currentTime) {
        const elapsed = currentTime - startTime;
        const progress = Math.min(elapsed / duration, 1);
        // Ease-out cubic for smooth deceleration
        const easeOut = 1 - Math.pow(1 - progress, 3);
        window.scrollTo(0, startY + difference * easeOut);
        if (progress < 1) {
          requestAnimationFrame(step);
        } else if (onComplete) {
          onComplete();
        }
      }
      requestAnimationFrame(step);
    }

    function updateTocHighlight(targetId) {
      if (isUpdatingToc) return; // Prevent infinite loop
      isUpdatingToc = true;

      // Find the TOC link that points to this target
      const tocNav = document.querySelector('.bd-toc-nav');
      if (!tocNav) {
        isUpdatingToc = false;
        return;
      }

      // Remove active class from all TOC items
      tocNav.querySelectorAll('.nav-link').forEach(link => {
        link.classList.remove('active');
        link.parentElement.classList.remove('active');
      });

      // Add active class to the matching link
      const matchingLink = tocNav.querySelector(`a[href="#${CSS.escape(targetId)}"]`);
      if (matchingLink) {
        matchingLink.classList.add('active');
        matchingLink.parentElement.classList.add('active');
      }

      // Use setTimeout to reset the guard after the current call stack
      setTimeout(function() {
        isUpdatingToc = false;
      }, 0);
    }

    // Watch for ScrollSpy trying to change the active state and override it
    const tocNav = document.querySelector('.bd-toc-nav');
    if (tocNav) {
      const observer = new MutationObserver(function(mutations) {
        if (lockedTargetId && !isUpdatingToc) {
          // Force our target to stay highlighted
          updateTocHighlight(lockedTargetId);
        }
      });
      observer.observe(tocNav, {
        attributes: true,
        attributeFilter: ['class'],
        subtree: true
      });
    }

    // Release the lock when user scrolls manually (not programmatically)
    window.addEventListener('wheel', function() {
      lockedTargetId = null;
    });
    window.addEventListener('touchmove', function() {
      lockedTargetId = null;
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const targetId = this.getAttribute('href').substring(1);
        if (!targetId) return; // Skip empty hash links
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          e.preventDefault();

          // Lock the TOC to this target
          lockedTargetId = targetId;

          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;

          // Update TOC highlight immediately
          updateTocHighlight(targetId);

          smoothScrollTo(targetPosition, SCROLL_DURATION, function() {
            // Keep it highlighted after scroll
            updateTocHighlight(targetId);
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
<!-- RunLLM Widget Configuration -->


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">

<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>

  
  <div class="navbar-header-items__mobile-logo">
    







  
  
  
  


<a class="navbar-brand logo" href="../index.html">
  
    
    <img src="../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
  </div>

  
  
  <div class=" navbar-header-items__start">
    
      
      
        <div class="navbar-item">
          







  
  
  
  


<a class="navbar-brand logo" href="../index.html">
  
    
    <img src="../_static/img/logo-dark.svg" class="logo__image only-light" alt="torchao - Home"/>
    
    
    <script>document.write(`<img src="../_static/img/logo-white.svg" class="logo__image only-dark" alt="torchao - Home"/>`);</script>
    
  
</a>
        </div>
      
    
      
      
        
        <div class="navbar-item desktop-only-version">
          
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
        </div>
      
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown current active">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal active" href="index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="#">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
      
        <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        




  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    <div class="sidebar-header-items__start">
      <div class="navbar-item">
        
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script>
      </div>
    </div>
    

    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">














<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
    
      <li class="nav-item dropdown current active">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal active" href="index.html">
              Workflows
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-1">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="training.html">
                  Quantized Training
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="#">
                  Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="inference.html">
                  Quantized Inference
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../api_reference/index.html">
              API Reference
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-2">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_quantization.html">
                  torchao.quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_qat.html">
                  torchao.quantization.qat
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_sparsity.html">
                  torchao.sparsity
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_float8.html">
                  torchao.float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../api_reference/api_ref_utils.html">
                  torchao.core
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../eager_tutorials/index.html">
              Tutorials
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-3">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/first_quantization_example.html">
                  First Quantization Example
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/pretraining.html">
                  (Part 1) Pre-training with float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/finetuning.html">
                  (Part 2) Fine-tuning with QAT, QLoRA, and float8
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/serving.html">
                  (Part 3) Serving on vLLM, SGLang, ExecuTorch
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/torchao_vllm_integration.html">
                  Integration with VLLM: Architecture and Usage Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/torchao_hf_integration.html">
                  Hugging Face Integration
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/serialization.html">
                  Serialization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/static_quantization.html">
                  Static Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/subclass_basic.html">
                  Writing Your Own Quantized Tensor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/subclass_advanced.html">
                  Writing Your Own Quantized Tensor (advanced)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../eager_tutorials/mxfp8_expert_parallel_training.html">
                  MXFP8 Expert Parallel Training
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../contributing/index.html">
              Contributing
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-4">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/quantization_overview.html">
                  Quantization Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/contributor_guide.html">
                  Contributor Guide
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/sparsity.html">
                  Sparsity Overview
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../contributing/benchmarking_api_guide.html">
                  Benchmarking API Guide
                </a>
              </li>
            
          </ul>
        
      </li>
    
      <li class="nav-item dropdown">
        
          
          <div class="nav-item-with-toggle">
            <a class="nav-link nav-internal" href="../pt2e_quantization/index.html">
              PT2E Quantization
            </a>
          </div>
          <ul class="dropdown-menu" id="dropdown-5">
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_ptq.html">
                  PyTorch 2 Export Post Training Quantization
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_qat.html">
                  PyTorch 2 Export Quantization-Aware Training (QAT)
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_x86_inductor.html">
                  PyTorch 2 Export Quantization with X86 Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_xpu_inductor.html">
                  PyTorch 2 Export Quantization with Intel GPU Backend through Inductor
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quant_openvino_inductor.html">
                  PyTorch 2 Export Quantization for OpenVINO torch.compile Backend
                </a>
              </li>
            
              <li class=" ">
                <a class="nav-link dropdown-item nav-internal" href="../pt2e_quantization/pt2e_quantizer.html">
                  How to Write a Quantizer for PyTorch 2 Export Quantization
                </a>
              </li>
            
          </ul>
        
      </li>
    

    
    
  </ul>
</nav>
</div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
        
          <div class="navbar-item"><!-- PyTorch.org site link - desktop only, two-line layout -->
<!-- Note: The show_pytorch_org_link check is handled in layout.html's navbar_end block -->
<a href="https://pytorch.org" class="pytorch-site-link nav-link nav-external" data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-title="Go to PyTorch.org">
  <span class="pytorch-site-link-text">
    <span>Go to</span>
    <span>pytorch.org <i class="fa-solid fa-arrow-up-right-from-square external-icon"></i></span>
  </span>
</a></div>
        
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/ao" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchao/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="training.html">Quantized Training</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Quantization-Aware Training (QAT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Quantized Inference</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



<div id="rtd-footer-container"></div>
      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Workflows</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Quantization...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              

<div id="searchbox"></div>
<div id="pytorch-article">
  <!-- Hidden breadcrumb schema for SEO only -->
  <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <link itemprop="item" href="index.html">
      <meta itemprop="name" content="Workflows">
      <meta itemprop="position" content="1">
    </div>
    
    <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <meta itemprop="name" content="Quantization-Aware Training (QAT)">
      <meta itemprop="position" content="2">
    </div>
  </div>

  
  

  
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="quantization-aware-training-qat">
<h1>Quantization-Aware Training (QAT)<a class="headerlink" href="#quantization-aware-training-qat" title="Link to this heading">#</a></h1>
<p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Feb 18, 2026 | Last Updated On: Feb 18, 2026</p>
<p>Quantization-Aware Training (QAT) refers to applying fake quantization during the
training or fine-tuning process, such that the final quantized model will exhibit
higher accuracies and lower perplexities. Fake quantization refers to rounding the float
values to quantized values without actually casting them to dtypes with lower
bit-widths, in contrast to post-training quantization (PTQ), which does cast the
quantized values to lower bit-width dtypes, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PTQ: x_q is quantized and cast to int8</span>
<span class="c1"># scale and zero point (zp) refer to parameters used to quantize x_float</span>
<span class="c1"># qmin and qmax refer to the range of quantized values</span>
<span class="n">x_q</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_float</span> <span class="o">/</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zp</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">int8</span><span class="p">)</span>

<span class="c1"># QAT: x_fq is still in float</span>
<span class="c1"># Fake quantize simulates the numerics of quantize + dequantize</span>
<span class="n">x_fq</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_float</span> <span class="o">/</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zp</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>
<span class="n">x_fq</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_fq</span> <span class="o">-</span> <span class="n">zp</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
</pre></div>
</div>
<p>QAT typically involves applying a transformation to your model before and after training.
In torchao, these are represented as the prepare and convert steps: (1) prepare inserts
fake quantize operations into linear layers, and (2) convert transforms the fake quantize
operations to actual quantize and dequantize operations after training, thereby producing
a quantized model (dequantize operations are typically fused with linear after lowering).
Between these two steps, training can proceed exactly as before.</p>
<p><img alt="qat" src="https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/qat/images/qat_diagram.png" /></p>
<section id="torchao-apis">
<h2>torchao APIs<a class="headerlink" href="#torchao-apis" title="Link to this heading">#</a></h2>
<p>torchao currently supports two QAT APIs, one through the <a class="reference external" href="https://pytorch.org/ao/stable/generated/torchao.quantization.quantize_.html#torchao.quantization.quantize_"><code class="docutils literal notranslate"><span class="pre">quantize_</span></code></a>
API (recommended) and one through the Quantizer classes (legacy). The <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> API
allows flexible configuration of quantization settings for both activations and weights,
while the Quantizer classes each hardcode a specific quantization setting.</p>
<p>For example, running QAT on a single GPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtune.models.llama3</span><span class="w"> </span><span class="kn">import</span> <span class="n">llama3</span>

<span class="c1"># Set up smaller version of llama3 to fit in a single GPU</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">llama3</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">num_kv_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># Example training loop</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_loop</span><span class="p">(</span><span class="n">m</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4096</span><span class="p">))</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<section id="quantize-api-recommended">
<h3>quantize_ API (recommended)<a class="headerlink" href="#quantize-api-recommended" title="Link to this heading">#</a></h3>
<p>The recommended way to run QAT in torchao is through the <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> API.</p>
<ol class="arabic simple">
<li><p><strong>Prepare:</strong> The main <a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.qat.QATConfig.html"><code class="docutils literal notranslate"><span class="pre">QATConfig</span></code></a>
accepts a post-training quantization (PTQ) config and automatically infers
the corresponding fake quantization configs to use.</p></li>
<li><p><strong>Convert:</strong> quantize the model using the base config provided</p></li>
</ol>
<p>Currently only the following PTQ base configs are supported:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.Int8DynamicActivationInt4WeightConfig.html"><code class="docutils literal notranslate"><span class="pre">Int8DynamicActivationInt4WeightConfig</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.Int4WeightOnlyConfig.html"><code class="docutils literal notranslate"><span class="pre">Int4WeightOnlyConfig</span></code></a></p></li>
</ul>
<p>For example (most use cases):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantize_</span><span class="p">,</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat</span><span class="w"> </span><span class="kn">import</span> <span class="n">QATConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="c1"># prepare: swap `torch.nn.Linear` -&gt; `FakeQuantizedLinear`</span>
<span class="n">base_config</span> <span class="o">=</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span><span class="p">(</span><span class="n">group_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">quantize_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">QATConfig</span><span class="p">(</span><span class="n">base_config</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="s2">&quot;prepare&quot;</span><span class="p">))</span>

<span class="c1"># train</span>
<span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># convert: swap `FakeQuantizedLinear` -&gt; `torch.nn.Linear`, then quantize using `base_config`</span>
<span class="n">quantize_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">QATConfig</span><span class="p">(</span><span class="n">base_config</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="s2">&quot;convert&quot;</span><span class="p">))</span>

<span class="c1"># inference or generate</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">quantize_</span></code> API also allows more general quantization settings that
may not have a corresponding PTQ base config, e.g. for experimentation
purposes. Users can specify custom fake quantization configs for activations
and/or weights. For example, the following usage is numerically equivalent
to the above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantize_</span><span class="p">,</span> <span class="n">Int8DynamicActivationInt4WeightConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat</span><span class="w"> </span><span class="kn">import</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">,</span> <span class="n">QATConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="c1"># prepare: swap `torch.nn.Linear` -&gt; `FakeQuantizedLinear`</span>
<span class="n">activation_config</span> <span class="o">=</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="s2">&quot;per_token&quot;</span><span class="p">,</span> <span class="n">is_symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">weight_config</span> <span class="o">=</span> <span class="n">IntxFakeQuantizeConfig</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int4</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">qat_config</span> <span class="o">=</span> <span class="n">QATConfig</span><span class="p">(</span>
    <span class="n">activation_config</span><span class="o">=</span><span class="n">activation_config</span><span class="p">,</span>
    <span class="n">weight_config</span><span class="o">=</span><span class="n">weight_config</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="s2">&quot;prepare&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">quantize_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qat_config</span><span class="p">)</span>

<span class="c1"># train</span>
<span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># convert: (not shown, same as before)</span>
</pre></div>
</div>
<p>To fake quantize embedding in addition to linear, you can additionally call
the following with a filter function during the prepare step:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># First apply linear transformation to the model as above</span>
<span class="c1"># Then apply weight-only transformation to embedding layers</span>
<span class="c1"># (activation fake quantization is not supported for embedding layers)</span>
<span class="n">qat_config</span> <span class="o">=</span> <span class="n">QATConfig</span><span class="p">(</span><span class="n">weight_config</span><span class="o">=</span><span class="n">weight_config</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="s2">&quot;prepare&quot;</span><span class="p">)</span>
<span class="n">quantize_</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">qat_config</span><span class="p">,</span> <span class="n">filter_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">))</span>
</pre></div>
</div>
<details>
  <summary><h3>Quantizer API (legacy)</h3></summary>
<p>Alternatively, torchao provides a few hardcoded quantization settings through
the following Quantizers, but these may be removed soon:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer.html#torchao.quantization.qat.Int8DynActInt4WeightQATQuantizer">Int8DynActInt4QATQuantizer</a> (linear), targeting int8 per-token dynamic asymmetric activation + int4 per-group symmetric weight</p></li>
<li><p><a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.qat.Int4WeightOnlyQATQuantizer.html#torchao.quantization.qat.Int4WeightOnlyQATQuantizer">Int4WeightOnlyQATQuantizer</a> (linear), targeting int4 per-group asymmetric weight using the efficient <a class="reference external" href="https://github.com/pytorch/pytorch/blob/a672f6c84e318bbf455f13dfdd3fd7c68a388bf5/aten/src/ATen/native/cuda/int4mm.cu#L1097">int4 tinygemm kernel</a> after training)</p></li>
<li><p><a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer.html#torchao.quantization.qat.Int4WeightOnlyEmbeddingQATQuantizer">Int4WeightOnlyEmbeddingQATQuantizer</a> (embedding), targeting int4 per-group symmetric weight</p></li>
</ul>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat</span><span class="w"> </span><span class="kn">import</span> <span class="n">Int8DynActInt4WeightQATQuantizer</span>
<span class="n">qat_quantizer</span> <span class="o">=</span> <span class="n">Int8DynActInt4WeightQATQuantizer</span><span class="p">(</span><span class="n">group_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="c1"># prepare: insert fake quantization ops</span>
<span class="c1"># swaps `torch.nn.Linear` with `Int8DynActInt4WeightQATLinear`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">qat_quantizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># train</span>
<span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># convert: transform fake quantization ops into actual quantized ops</span>
<span class="c1"># swaps `Int8DynActInt4WeightQATLinear` with `Int8DynActInt4WeightLinear`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">qat_quantizer</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># inference or generate</span>
</pre></div>
</div>
<p>To use multiple Quantizers in the same model for different layer types,
users can also leverage the <a class="reference external" href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.qat.ComposableQATQuantizer.html#torchao.quantization.qat.ComposableQATQuantizer">ComposableQATQuantizer</a>
as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.qat</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ComposableQATQuantizer</span><span class="p">,</span>
    <span class="n">Int4WeightOnlyEmbeddingQATQuantizer</span><span class="p">,</span>
    <span class="n">Int8DynActInt4WeightQATQuantizer</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ComposableQATQuantizer</span><span class="p">([</span>
    <span class="n">Int8DynActInt4WeightQATQuantizer</span><span class="p">(</span><span class="n">groupsize</span><span class="o">=</span><span class="n">group_size</span><span class="p">),</span>
    <span class="n">Int4WeightOnlyEmbeddingQATQuantizer</span><span class="p">(</span><span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">),</span>
<span class="p">])</span>

<span class="c1"># prepare + train + convert as before</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">qat_quantizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">qat_quantizer</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
</section>
<section id="axolotl-integration">
<h2>Axolotl integration<a class="headerlink" href="#axolotl-integration" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/axolotl-ai-cloud">Axolotl</a> uses TorchAO to support quantized-aware fine-tuning. You can use the following commands to fine-tune, and then quantize a Llama-3.2-3B model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>axolotl<span class="w"> </span>train<span class="w"> </span>examples/llama-3/3b-qat-fsdp2.yaml
<span class="c1"># once training is complete, perform the quantization step</span>
axolotl<span class="w"> </span>quantize<span class="w"> </span>examples/llama-3/3b-qat-fsdp2.yaml
<span class="c1"># you should now have a quantized model saved in ./outputs/qat_out/quatized</span>
</pre></div>
</div>
<p>Please see the <a class="reference external" href="https://docs.axolotl.ai/docs/qat.html">QAT documentation</a> in axolotl for more details.</p>
</section>
<section id="unsloth-integration">
<h2>Unsloth integration<a class="headerlink" href="#unsloth-integration" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/unslothai/unsloth">Unsloth</a> also leverages TorchAO for quantized-aware fine-tuning. Unsloth’s QAT support can be used with both full and LoRA fine-tuning. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">unsloth</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastLanguageModel</span>

<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;unsloth/Qwen3-4B-Instruct-2507&quot;</span><span class="p">,</span>
    <span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">load_in_4bit</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">full_finetuning</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">get_peft_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">r</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;down_proj&quot;</span><span class="p">,],</span>
    <span class="n">lora_alpha</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">qat_scheme</span> <span class="o">=</span> <span class="s2">&quot;int4&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For a full notebook example, see: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb. A QAT-specific notebook is coming soon.</p>
<details>
    <summary><h2>torchtune integration (legacy)</h2></summary>
<p>torchao QAT is integrated with <a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a>
to allow users to run quantized-aware fine-tuning as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tune</span> <span class="n">run</span> <span class="o">--</span><span class="n">nproc_per_node</span> <span class="mi">8</span> <span class="n">qat_distributed</span> <span class="o">--</span><span class="n">config</span> <span class="n">llama3</span><span class="o">/</span><span class="mi">8</span><span class="n">B_qat_full</span>
</pre></div>
</div>
<p>torchtune also supports a <a class="reference external" href="https://github.com/pytorch/torchtune/blob/main/recipes/qat_lora_finetune_distributed.py">QAT + LoRA distributed training recipe</a>
that is 1.89x faster and uses 36.1% memory compared to vanilla QAT in our early experiments.
You can read more about it <a class="reference external" href="https://dev-discuss.pytorch.org/t/speeding-up-qat-by-1-89x-with-lora/2700">here</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tune</span> <span class="n">run</span> <span class="o">--</span><span class="n">nnodes</span> <span class="mi">1</span> <span class="o">--</span><span class="n">nproc_per_node</span> <span class="mi">4</span> <span class="n">qat_lora_finetune_distributed</span> <span class="o">--</span><span class="n">config</span> <span class="n">llama3</span><span class="o">/</span><span class="mi">8</span><span class="n">B_qat_lora</span>
</pre></div>
</div>
<p>For more detail, please refer to <a class="reference external" href="https://pytorch.org/torchtune/main/tutorials/qat_finetune.html">this QAT tutorial</a>.</p>
</details>
</section>
<section id="evaluation-results">
<h2>Evaluation Results<a class="headerlink" href="#evaluation-results" title="Link to this heading">#</a></h2>
<p>Int4 weight-only QAT + LoRA using a group size of 128, fine-tuned using Unsloth.
Both fine-tuning and evaluation was done on a single H100 GPU using the
<a class="reference external" href="https://huggingface.co/datasets/mlabonne/FineTome-100k">mlabonne/FineTome-100k</a>
dataset. Learning rate was 2e-5 and batch size was 64 with no gradient accumulation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># gemma3-12b-it</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">Eval</span> <span class="n">task</span>   <span class="o">|</span>   <span class="n">bf16</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">QAT</span> <span class="o">|</span> <span class="n">recovered</span>   <span class="o">|</span>
<span class="o">+=============+=================+=================+============+=============+</span>
<span class="o">|</span> <span class="n">wikitext</span>    <span class="o">|</span>          <span class="mf">9.1477</span> <span class="o">|</span>          <span class="mf">9.7745</span> <span class="o">|</span>     <span class="mf">9.5631</span> <span class="o">|</span> <span class="mf">33.727</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">bbh</span>         <span class="o">|</span>          <span class="mf">0.8079</span> <span class="o">|</span>          <span class="mf">0.7624</span> <span class="o">|</span>     <span class="mf">0.7831</span> <span class="o">|</span> <span class="mf">45.495</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>

<span class="c1"># gemma3-4b-it</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">Eval</span> <span class="n">task</span>   <span class="o">|</span>   <span class="n">bf16</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">QAT</span> <span class="o">|</span> <span class="n">recovered</span>   <span class="o">|</span>
<span class="o">+=============+=================+=================+============+=============+</span>
<span class="o">|</span> <span class="n">wikitext</span>    <span class="o">|</span>         <span class="mf">12.1155</span> <span class="o">|</span>         <span class="mf">13.247</span>  <span class="o">|</span>    <span class="mf">12.797</span>  <span class="o">|</span> <span class="mf">39.770</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">bbh</span>         <span class="o">|</span>          <span class="mf">0.7074</span> <span class="o">|</span>          <span class="mf">0.6415</span> <span class="o">|</span>     <span class="mf">0.6666</span> <span class="o">|</span> <span class="mf">38.088</span>      <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">gpqa</span>        <span class="o">|</span>          <span class="mf">0.3232</span> <span class="o">|</span>          <span class="mf">0.3081</span> <span class="o">|</span>     <span class="mf">0.3182</span> <span class="o">|</span> <span class="mf">66.887</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>

<span class="c1"># Qwen3-4B-Instruct</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">Eval</span> <span class="n">task</span>   <span class="o">|</span>   <span class="n">bf16</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">QAT</span> <span class="o">|</span> <span class="n">recovered</span>   <span class="o">|</span>
<span class="o">+=============+=================+=================+============+=============+</span>
<span class="o">|</span> <span class="n">mmlu</span><span class="o">-</span><span class="n">pro</span>    <span class="o">|</span>          <span class="mf">0.4909</span> <span class="o">|</span>          <span class="mf">0.4328</span> <span class="o">|</span>     <span class="mf">0.4524</span> <span class="o">|</span> <span class="mf">33.735</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>

<span class="c1"># Llama3.2-3B</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">Eval</span> <span class="n">task</span>   <span class="o">|</span>   <span class="n">bf16</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">int4</span> <span class="n">QAT</span> <span class="o">|</span> <span class="n">recovered</span>   <span class="o">|</span>
<span class="o">+=============+=================+=================+============+=============+</span>
<span class="o">|</span> <span class="n">wikitext</span>    <span class="o">|</span>         <span class="mf">12.1322</span> <span class="o">|</span>         <span class="mf">13.3459</span> <span class="o">|</span>    <span class="mf">12.8796</span> <span class="o">|</span> <span class="mf">38.420</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">bbh</span>         <span class="o">|</span>          <span class="mf">0.5483</span> <span class="o">|</span>          <span class="mf">0.4967</span> <span class="o">|</span>     <span class="mf">0.5174</span> <span class="o">|</span> <span class="mf">40.116</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">gpqa</span>        <span class="o">|</span>          <span class="mf">0.3333</span> <span class="o">|</span>          <span class="mf">0.2879</span> <span class="o">|</span>     <span class="mf">0.303</span>  <span class="o">|</span> <span class="mf">33.260</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
<span class="o">|</span> <span class="n">mmlu</span><span class="o">-</span><span class="n">pro</span>    <span class="o">|</span>          <span class="mf">0.2771</span> <span class="o">|</span>          <span class="mf">0.2562</span> <span class="o">|</span>     <span class="mf">0.2629</span> <span class="o">|</span> <span class="mf">32.057</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+-----------------+------------+-------------+</span>
</pre></div>
</div>
<p>NVFP4 QAT full fine-tuning, fine-tuned using Axolotl on 8x B200 GPUs on the
<a class="reference external" href="https://huggingface.co/datasets/yahma/alpaca-cleaned">yahma/alpaca-cleaned</a>
dataset. Learning rate was 2e-5 and batch size was 128 for <code class="docutils literal notranslate"><span class="pre">gemma3-12b-it</span></code>
and 32 for <code class="docutils literal notranslate"><span class="pre">Qwen3-8B</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># gemma3-12b-it</span>
<span class="o">+-------------+-----------------+------------------+-------------+-------------+</span>
<span class="o">|</span> <span class="n">Eval</span> <span class="n">task</span>   <span class="o">|</span>   <span class="n">bf16</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">nvfp4</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">nvfp4</span> <span class="n">QAT</span> <span class="o">|</span> <span class="n">recovered</span>   <span class="o">|</span>
<span class="o">+=============+=================+==================+=============+=============+</span>
<span class="o">|</span> <span class="n">bbh</span>         <span class="o">|</span>          <span class="mf">0.7527</span> <span class="o">|</span>           <span class="mf">0.7068</span> <span class="o">|</span>      <span class="mf">0.7222</span> <span class="o">|</span> <span class="mf">33.551</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+------------------+-------------+-------------+</span>
<span class="o">|</span> <span class="n">mmlu</span><span class="o">-</span><span class="n">pro</span>    <span class="o">|</span>          <span class="mf">0.4074</span> <span class="o">|</span>           <span class="mf">0.3621</span> <span class="o">|</span>      <span class="mf">0.3702</span> <span class="o">|</span> <span class="mf">17.881</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+------------------+-------------+-------------+</span>

<span class="c1"># Qwen3-8B</span>
<span class="o">+-------------+-----------------+------------------+-------------+-------------+</span>
<span class="o">|</span> <span class="n">Eval</span> <span class="n">task</span>   <span class="o">|</span>   <span class="n">bf16</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">nvfp4</span> <span class="n">baseline</span> <span class="o">|</span>   <span class="n">nvfp4</span> <span class="n">QAT</span> <span class="o">|</span> <span class="n">recovered</span>   <span class="o">|</span>
<span class="o">+=============+=================+==================+=============+=============+</span>
<span class="o">|</span> <span class="n">bbh</span>         <span class="o">|</span>          <span class="mf">0.7771</span> <span class="o">|</span>           <span class="mf">0.7262</span> <span class="o">|</span>      <span class="mf">0.7397</span> <span class="o">|</span> <span class="mf">26.523</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+------------------+-------------+-------------+</span>
<span class="o">|</span> <span class="n">mmlu</span><span class="o">-</span><span class="n">pro</span>    <span class="o">|</span>          <span class="mf">0.4929</span> <span class="o">|</span>           <span class="mf">0.4519</span> <span class="o">|</span>      <span class="mf">0.4686</span> <span class="o">|</span> <span class="mf">40.732</span><span class="o">%</span>     <span class="o">|</span>
<span class="o">+-------------+-----------------+------------------+-------------+-------------+</span>
</pre></div>
</div>
<p>For more details, please refer to <a class="reference external" href="https://pytorch.org/blog/quantization-aware-training">this blog post</a>.</p>
</section>
</section>


                </article>
              
</div>

              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="training.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quantized Training</p>
      </div>
    </a>
    <a class="right-next"
       href="inference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Quantized Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="training.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quantized Training</p>
      </div>
    </a>
    <a class="right-next"
       href="inference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Quantized Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchao-apis">torchao APIs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantize-api-recommended">quantize_ API (recommended)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#axolotl-integration">Axolotl integration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsloth-integration">Unsloth integration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-results">Evaluation Results</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/ao/edit/main/docs/source/workflows/qat.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/workflows/qat.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-present, torchao Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Quantization-Aware Training (QAT)",
       "headline": "Quantization-Aware Training (QAT)",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment.",
       "url": "/workflows/qat.html",
       "articleBody": "Quantization-Aware Training (QAT)# Created On: Feb 18, 2026 | Last Updated On: Feb 18, 2026 Quantization-Aware Training (QAT) refers to applying fake quantization during the training or fine-tuning process, such that the final quantized model will exhibit higher accuracies and lower perplexities. Fake quantization refers to rounding the float values to quantized values without actually casting them to dtypes with lower bit-widths, in contrast to post-training quantization (PTQ), which does cast the quantized values to lower bit-width dtypes, e.g.: # PTQ: x_q is quantized and cast to int8 # scale and zero point (zp) refer to parameters used to quantize x_float # qmin and qmax refer to the range of quantized values x_q = (x_float / scale + zp).round().clamp(qmin, qmax).cast(int8) # QAT: x_fq is still in float # Fake quantize simulates the numerics of quantize + dequantize x_fq = (x_float / scale + zp).round().clamp(qmin, qmax) x_fq = (x_fq - zp) * scale QAT typically involves applying a transformation to your model before and after training. In torchao, these are represented as the prepare and convert steps: (1) prepare inserts fake quantize operations into linear layers, and (2) convert transforms the fake quantize operations to actual quantize and dequantize operations after training, thereby producing a quantized model (dequantize operations are typically fused with linear after lowering). Between these two steps, training can proceed exactly as before. torchao APIs# torchao currently supports two QAT APIs, one through the quantize_ API (recommended) and one through the Quantizer classes (legacy). The quantize_ API allows flexible configuration of quantization settings for both activations and weights, while the Quantizer classes each hardcode a specific quantization setting. For example, running QAT on a single GPU: import torch from torchtune.models.llama3 import llama3 # Set up smaller version of llama3 to fit in a single GPU def get_model(): return llama3( vocab_size=4096, num_layers=16, num_heads=16, num_kv_heads=4, embed_dim=2048, max_seq_len=2048, ).cuda() # Example training loop def train_loop(m: torch.nn.Module): optimizer = torch.optim.SGD(m.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5) loss_fn = torch.nn.CrossEntropyLoss() for i in range(10): example = torch.randint(0, 4096, (2, 16)).cuda() target = torch.randn((2, 16, 4096)).cuda() output = m(example) loss = loss_fn(output, target) loss.backward() optimizer.step() optimizer.zero_grad() quantize_ API (recommended)# The recommended way to run QAT in torchao is through the quantize_ API. Prepare: The main QATConfig accepts a post-training quantization (PTQ) config and automatically infers the corresponding fake quantization configs to use. Convert: quantize the model using the base config provided Currently only the following PTQ base configs are supported: Int8DynamicActivationInt4WeightConfig Int4WeightOnlyConfig For example (most use cases): from torchao.quantization import quantize_, Int8DynamicActivationInt4WeightConfig from torchao.quantization.qat import QATConfig model = get_model() # prepare: swap `torch.nn.Linear` -\u003e `FakeQuantizedLinear` base_config = Int8DynamicActivationInt4WeightConfig(group_size=32) quantize_(model, QATConfig(base_config, step=\"prepare\")) # train train_loop(model) # convert: swap `FakeQuantizedLinear` -\u003e `torch.nn.Linear`, then quantize using `base_config` quantize_(model, QATConfig(base_config, step=\"convert\")) # inference or generate The quantize_ API also allows more general quantization settings that may not have a corresponding PTQ base config, e.g. for experimentation purposes. Users can specify custom fake quantization configs for activations and/or weights. For example, the following usage is numerically equivalent to the above: from torchao.quantization import quantize_, Int8DynamicActivationInt4WeightConfig from torchao.quantization.qat import IntxFakeQuantizeConfig, QATConfig model = get_model() # prepare: swap `torch.nn.Linear` -\u003e `FakeQuantizedLinear` activation_config = IntxFakeQuantizeConfig(torch.int8, \"per_token\", is_symmetric=False) weight_config = IntxFakeQuantizeConfig(torch.int4, group_size=32) qat_config = QATConfig( activation_config=activation_config, weight_config=weight_config, step=\"prepare\", ) quantize_(model, qat_config) # train train_loop(model) # convert: (not shown, same as before) To fake quantize embedding in addition to linear, you can additionally call the following with a filter function during the prepare step: # First apply linear transformation to the model as above # Then apply weight-only transformation to embedding layers # (activation fake quantization is not supported for embedding layers) qat_config = QATConfig(weight_config=weight_config, step=\"prepare\") quantize_(m, qat_config, filter_fn=lambda m, _: isinstance(m, torch.nn.Embedding)) Quantizer API (legacy) Alternatively, torchao provides a few hardcoded quantization settings through the following Quantizers, but these may be removed soon: Int8DynActInt4QATQuantizer (linear), targeting int8 per-token dynamic asymmetric activation + int4 per-group symmetric weight Int4WeightOnlyQATQuantizer (linear), targeting int4 per-group asymmetric weight using the efficient int4 tinygemm kernel after training) Int4WeightOnlyEmbeddingQATQuantizer (embedding), targeting int4 per-group symmetric weight For example: from torchao.quantization.qat import Int8DynActInt4WeightQATQuantizer qat_quantizer = Int8DynActInt4WeightQATQuantizer(group_size=32) model = get_model() # prepare: insert fake quantization ops # swaps `torch.nn.Linear` with `Int8DynActInt4WeightQATLinear` model = qat_quantizer.prepare(model) # train train_loop(model) # convert: transform fake quantization ops into actual quantized ops # swaps `Int8DynActInt4WeightQATLinear` with `Int8DynActInt4WeightLinear` model = qat_quantizer.convert(model) # inference or generate To use multiple Quantizers in the same model for different layer types, users can also leverage the ComposableQATQuantizer as follows: from torchao.quantization.qat import ( ComposableQATQuantizer, Int4WeightOnlyEmbeddingQATQuantizer, Int8DynActInt4WeightQATQuantizer, ) quantizer = ComposableQATQuantizer([ Int8DynActInt4WeightQATQuantizer(groupsize=group_size), Int4WeightOnlyEmbeddingQATQuantizer(group_size=group_size), ]) # prepare + train + convert as before model = qat_quantizer.prepare(model) train_loop(model) model = qat_quantizer.convert(model) Axolotl integration# Axolotl uses TorchAO to support quantized-aware fine-tuning. You can use the following commands to fine-tune, and then quantize a Llama-3.2-3B model: axolotl train examples/llama-3/3b-qat-fsdp2.yaml # once training is complete, perform the quantization step axolotl quantize examples/llama-3/3b-qat-fsdp2.yaml # you should now have a quantized model saved in ./outputs/qat_out/quatized Please see the QAT documentation in axolotl for more details. Unsloth integration# Unsloth also leverages TorchAO for quantized-aware fine-tuning. Unsloth\u2019s QAT support can be used with both full and LoRA fine-tuning. For example: from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( \"unsloth/Qwen3-4B-Instruct-2507\", max_seq_len = 2048, dtype = torch.bfloat16, load_in_4bit = False, full_finetuning = False, ) model = FastLanguageModel.get_peft_model( model, r = 16, target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha = 16, qat_scheme = \"int4\", ) For a full notebook example, see: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb. A QAT-specific notebook is coming soon. torchtune integration (legacy) torchao QAT is integrated with torchtune to allow users to run quantized-aware fine-tuning as follows: tune run --nproc_per_node 8 qat_distributed --config llama3/8B_qat_full torchtune also supports a QAT + LoRA distributed training recipe that is 1.89x faster and uses 36.1% memory compared to vanilla QAT in our early experiments. You can read more about it here: tune run --nnodes 1 --nproc_per_node 4 qat_lora_finetune_distributed --config llama3/8B_qat_lora For more detail, please refer to this QAT tutorial. Evaluation Results# Int4 weight-only QAT + LoRA using a group size of 128, fine-tuned using Unsloth. Both fine-tuning and evaluation was done on a single H100 GPU using the mlabonne/FineTome-100k dataset. Learning rate was 2e-5 and batch size was 64 with no gradient accumulation. # gemma3-12b-it +-------------+-----------------+-----------------+------------+-------------+ | Eval task | bf16 baseline | int4 baseline | int4 QAT | recovered | +=============+=================+=================+============+=============+ | wikitext | 9.1477 | 9.7745 | 9.5631 | 33.727% | +-------------+-----------------+-----------------+------------+-------------+ | bbh | 0.8079 | 0.7624 | 0.7831 | 45.495% | +-------------+-----------------+-----------------+------------+-------------+ # gemma3-4b-it +-------------+-----------------+-----------------+------------+-------------+ | Eval task | bf16 baseline | int4 baseline | int4 QAT | recovered | +=============+=================+=================+============+=============+ | wikitext | 12.1155 | 13.247 | 12.797 | 39.770% | +-------------+-----------------+-----------------+------------+-------------+ | bbh | 0.7074 | 0.6415 | 0.6666 | 38.088 | +-------------+-----------------+-----------------+------------+-------------+ | gpqa | 0.3232 | 0.3081 | 0.3182 | 66.887% | +-------------+-----------------+-----------------+------------+-------------+ # Qwen3-4B-Instruct +-------------+-----------------+-----------------+------------+-------------+ | Eval task | bf16 baseline | int4 baseline | int4 QAT | recovered | +=============+=================+=================+============+=============+ | mmlu-pro | 0.4909 | 0.4328 | 0.4524 | 33.735% | +-------------+-----------------+-----------------+------------+-------------+ # Llama3.2-3B +-------------+-----------------+-----------------+------------+-------------+ | Eval task | bf16 baseline | int4 baseline | int4 QAT | recovered | +=============+=================+=================+============+=============+ | wikitext | 12.1322 | 13.3459 | 12.8796 | 38.420% | +-------------+-----------------+-----------------+------------+-------------+ | bbh | 0.5483 | 0.4967 | 0.5174 | 40.116% | +-------------+-----------------+-----------------+------------+-------------+ | gpqa | 0.3333 | 0.2879 | 0.303 | 33.260% | +-------------+-----------------+-----------------+------------+-------------+ | mmlu-pro | 0.2771 | 0.2562 | 0.2629 | 32.057% | +-------------+-----------------+-----------------+------------+-------------+ NVFP4 QAT full fine-tuning, fine-tuned using Axolotl on 8x B200 GPUs on the yahma/alpaca-cleaned dataset. Learning rate was 2e-5 and batch size was 128 for gemma3-12b-it and 32 for Qwen3-8B. # gemma3-12b-it +-------------+-----------------+------------------+-------------+-------------+ | Eval task | bf16 baseline | nvfp4 baseline | nvfp4 QAT | recovered | +=============+=================+==================+=============+=============+ | bbh | 0.7527 | 0.7068 | 0.7222 | 33.551% | +-------------+-----------------+------------------+-------------+-------------+ | mmlu-pro | 0.4074 | 0.3621 | 0.3702 | 17.881% | +-------------+-----------------+------------------+-------------+-------------+ # Qwen3-8B +-------------+-----------------+------------------+-------------+-------------+ | Eval task | bf16 baseline | nvfp4 baseline | nvfp4 QAT | recovered | +=============+=================+==================+=============+=============+ | bbh | 0.7771 | 0.7262 | 0.7397 | 26.523% | +-------------+-----------------+------------------+-------------+-------------+ | mmlu-pro | 0.4929 | 0.4519 | 0.4686 | 40.732% | +-------------+-----------------+------------------+-------------+-------------+ For more details, please refer to this blog post.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/workflows/qat.html"
       },
       "datePublished": "Feb 18, 2026T00:00:00Z",
       "dateModified": "Feb 18, 2026T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>